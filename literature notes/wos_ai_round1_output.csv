paper_id,title,abstract,keywords,relevance,relevance_reason,tags,research_question,method,key_claim_about_representation,notes
P0001,INTONATION AND AUDITORY GROUPING IN IMMEDIATE SERIAL-RECALL,"The concept of auditory buffer storage is supported by evidence from studies of dichotic memory and serial recall tasks. Previously reported studies of modality-specific grouping effects can also be accommodated within this theoretical framework, with the effects of stimulus grouping attributed to more effective utilization of the buffer. In this study, the representation of structured sequences in the auditory buffer was further explored in five experiments which investigated the extent to which this system can make use of prosodic cues. Experiment 1 demonstrated that the variations in pitch which constitute the intonation pattern of a natural utterance are extremely effective in enhancing memory performance. Experiment 2 demonstrated that the presence of a strong melodic contour is not a sufficient condition for improvement in recall, even when the melodic structure is both familiar and well-formed. The results of Experiments 3 and 4 suggested that improvements in recall are most likely to occur when abrupt changes in pitch at group boundaries are the most prominent features of the pitch contour. In Experiment 5 the natural pitch contour was simplified to produce a pattern in which the final item in each group was spoken with accented pitch. This structure was found to be as effective as grouping by pauses. These findings are related to perceptual studies of intonation, and implications for theoretical accounts of auditory memory are discussed.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0002,Disorders of affective and linguistic prosody in children after early unilateral brain damage,"Prosody is that quality of speech that imparts meaning by changes in intonation, pitch, and stress. The right hemisphere (RH) appears to be dominant for affective prosody in adults, while the left hemisphere (LH) mediates the more linguistic aspects of nonverbal communication. Few similar studies have been reported of individuals who suffered early unilateral brain damage, when brain reorganization or plasticity might be expected to play a role in ameliorating the adverse effects of focal brain damage. In this study, comprehension and expression of affective and linguistic prosody were tested in subjects with documented unilateral brain damage of pre- or perinatal onset and in matched controls. Both RH- and LH-lesion groups demonstrated difficulty on tasks involving expression of affective prosody, and on tests of linguistic prosody, compared with controls. Only the RH-lesion group was impaired on an affective comprehension task. The results indicate that even after very early unilateral brain damage, prosodic deficits may be present. However, only for affective comprehension does the side of the lesion appear to determine such deficits. The findings suggest that during brain development there is not clear brain lateralization for prosody and there may be bilateral representation for these skills during early development. There may be limitations to the ability of the developing brain to reorganize after early injury.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0003,CSLML: a markup language for expressive Chinese sign language synthesis,"This paper presents a Chinese Sign Language Markup Language (CSLML), which is developed for expressive Chinese sign language synthesis by introducing features and structure of sign language prosody. The tags of CSLML are divided into two levels: function level and phonetic level. Function level provides abstract information about signed content and prosody, so it facilitates text annotating for text-driven automatic synthetic system and adapts to diversified synthetic methods, such as motion capture animation or image-based synthesis, Which may not be good at processing lower-level information. Phonetic level provides detailed behavioral manners based on phonetics and phonology to interpret the meaning in function level. It facilitates creation and edit of any motions. The two levels co-exist in CSLML documents and high-level description can be mapped into corresponding low-level behavior to provide one-to-many variability and expression of synthesis. Therefore, We also introduce a framework for this mapping processing mid exhibit results of our animated prototype system based on this framework. Copyright (C) 2009 John Wiley & Sons, Ltd.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0004,Natural head motion synthesis driven by acoustic prosodic features,"Natural head motion is important to realistic facial animation and engaging human-computer interactions. In this paper, we present a novel data-driven approach to synthesize appropriate head motion by sampling from trained hidden markov models (HMMs). First, while an actress recited a corpus specifically designed to elicit various emotions, her 3D head motion was captured and further processed to construct a head motion database that included synchronized speech information. Then, an HMM for each discrete head motion representation (derived directly from data using vector quantization) was created by using acoustic prosodic features derived from speech. Finally, first-order Markov models and interpolation techniques were used to smooth the synthesized sequence. Our comparison experiments and novel synthesis results show that synthesized head motions follow the temporal dynamic behavior of real human subjects. Copyright (c) 2005 John Wiley & Sons, Ltd.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0005,Prosodic Similarity Effects in Short-Term Memory in Developmental Dyslexia,"Children with developmental dyslexia are characterized by phonological difficulties across languages. Classically, this phonological deficit' in dyslexia has been investigated with tasks using single-syllable words. Recently, however, several studies have demonstrated difficulties in prosodic awareness in dyslexia. Potential prosodic effects in short-term memory have not yet been investigated. Here we create a new instrument based on three-syllable words that vary in stress patterns, to investigate whether prosodic similarity (the same prosodic pattern of stressed and unstressed syllables) exerts systematic effects on short-term memory. We study participants with dyslexia and age-matched and younger reading-level-matched typically developing controls. We find that all participants, including dyslexic participants, show prosodic similarity effects in short-term memory. All participants exhibited better retention of words that differed in prosodic structure, although participants with dyslexia recalled fewer words accurately overall compared to age-matched controls. Individual differences in prosodic memory were predicted by earlier vocabulary abilities, by earlier sensitivity to syllable stress and by earlier phonological awareness. To our knowledge, this is the first demonstration of prosodic similarity effects in short-term memory. The implications of a prosodic similarity effect for theories of lexical representation and of dyslexia are discussed. (c) 2016 The Authors. Dyslexia published by John Wiley & Sons Ltd.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0006,Lower prosodic sensitivity in Chinese children with dyslexia and its impact on Chinese reading,"The current study aims to examine prosodic sensitivity in Chinese children with dyslexia and its relation to Chinese reading in children with and without dyslexia. A total of 172 Chinese children from third grade to sixth grade in Taiwanese primary schools were recruited. Thirty (14 male) children were identified as having dyslexia, and the remaining children (N = 142; 67 male) were typically developing children matched with those with dyslexia as carefully as possible with respect to school, grade, and gender. Our results indicated that group differences were found for all three types of prosodic sensitivity. Moderation analyses showed that group had no significant interaction with prosodic sensitivity in predicting Chinese reading, so the participants in the two groups were combined in the following analyses. The results of the stepwise regression analyses showed that only lexical tone awareness could significantly predict Chinese character reading after controlling for phonological awareness, while only intonation awareness could significantly predict reading comprehension after controlling for Chinese character reading. The results provide preliminary evidence on the issue of prosodic sensitivity in Chinese children with dyslexia and its role in Chinese reading, which might provide a novel approach to the teaching of Chinese languages.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0007,Lexical prosodic representation and access in Japanese children with developmental dyslexia,"Recent research indicates that awareness of the prosodic information present in spoken language could be an important factor for literacy development, and that adults with developmental dyslexia show impaired awareness of lexical prosodic information, while the phonological representations remain intact. We investigated lexical prosodic representation and awareness in Japanese children with and without developmental dyslexia. Lexical prosodic representation was investigated using a cross-modal fragment priming task, and awareness was examined using a fragment identification task. The task was modified for children by selecting words with higher familiarity and fewer trials. As a result, the same pattern of prosodic priming effects was observed between groups; lexical decision time was faster in the prosodic congruent condition than in the incongruent condition. In addition, accuracy and reaction time did not show group differences in the fragment identification task. Relationship between prosody and literacy development may differ between languages but the sample size were small in both groups. Further investigation with larger sample size is required.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0008,FMRI reveals brain regions mediating slow prosodic modulations in spoken sentences,"By means of fMRI measurements, the present study identifies brain regions in left and right peri-sylvian areas that subserve grammatical or prosodic processing. Normal volunteers heard 1) normal sentences; 2) so-called syntactic sentences comprising syntactic, but no lexical-semantic information; and 3) manipulated speech signals comprising only prosodic information, i.e., speech melody. For all conditions, significant blood oxygenation signals were recorded from the supratemporal plane bilaterally. Left hemisphere areas that surround Heschl gyrus responded more strongly during the two sentence conditions than to speech melody. This finding suggests that the anterior and posterior portions of the superior temporal region (STR) support lexical-semantic and syntactic aspects of sentence processing. In contrast, the right superior temporal region, in especially the planum temporale, responded more strongly to speech melody. Significant brain activation in the fronto-opercular cortices was observed when participants heard pseudo sentences and was strongest during the speech melody condition. In contrast, the fronto-opercular area is not prominently involved in listening to normal sentences. Thus, the functional activation in fronto-opercular regions increases as the grammatical information available in the sentence decreases. Generally, brain responses to speech melody were stronger in right than left hemisphere sites, suggesting a particular role of right cortical areas in the processing of slow prosodic modulations.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0009,A cross-linguistic fMRI study of perception of intonation and emotion in Chinese,"Conflicting data from neurobehavioral studies of the perception of intonation (linguistic) and emotion (affective) in spoken language highlight the need to further examine how functional attributes of prosodic stimuli are related to hemispheric differences in processing capacity. Because of similarities in their acoustic profiles, intonation and emotion permit us to assess to what extent hemispheric lateralization of speech prosody depends on functional instead of acoustical properties. To examine how the brain processes linguistic and affective prosody, an fMRI study was conducted using Chinese, a tone language in which both intonation and emotion may be signaled prosodically, in addition to lexical tones. Ten Chinese and 10 English subjects were asked to perform discrimination judgments of intonation (1: statement, question) and emotion (E: happy, angry, sad) presented in semantically neutral Chinese sentences. A baseline task required passive listening to the same speech stimuli (S). In direct between-group comparisons, the Chinese group showed left-sided frontoparietal activation for both intonation (I vs. S) and emotion (E vs. S) relative to baseline. When comparing intonation relative to emotion (I vs. E), the Chinese group demonstrated prefrontal activation bilaterally; parietal activation in the left hemisphere only. The reverse comparison (E vs. I), on the other hand, revealed that activation occurred in anterior and posterior prefrontal regions of the right hemisphere only. These findings show that some aspects of perceptual processing of emotion are dissociable from intonation, and, moreover, that they are mediated by the right hemisphere. Hum. Brain Mapping 18:149-157, 2003. (C) 2003 Wiley-Liss, Inc.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0010,Cerebral mechanisms of prosodic sensory integration using low-frequency bands of connected speech,"Even if speech perception has been reported to involve both left and right hemispheres, converging data have posited the existence of a functional asymmetry at the level of secondary auditory cortices. Using fMRI in 12 right-handed French men listening passively to long connected speech stimuli, we addressed the question of neuronal networks involved in the integration of low frequency bands of speech by comparing 1) differences in brain activity in two listening conditions (IN, NF) differing in the integration of pitch modulations (in FN, low frequencies, obtained by a low-pass filter, are addressed to the left ear while the whole acoustic message is simultaneously addressed to the right ear, NF being the reverse position); 2) differences in brain activity induced by high and low degrees of prosodic expression (expressive vs. flat); and 3) effects of the same connected speech stimulus in the two listening conditions. Each stimulus induced a specific cerebral network, the flat one weakening activations which were mainly reduced to the bilateral STG for both listening conditions. In the expressive condition, the specific sensory integration FN results in an increase of the articulatory loop and new recruitments such as right BA6-44, left BA39-40, the left posterior insula and the bilateral BA30. This finding may be accounted for by the existence of temporal windows differing both in length and in acoustic cues decoding, strengthening the ""asymmetric sampling in time"" hypothesis posited by Poeppel (Speech Commun 2003; 41:245-255). Such an improvement of prosodic integration could find applications in the rehabilitation of some speech disturbances.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0011,Activation of the left planum temporale in pitch processing is shaped by language experience,"Implicit, abstract knowledge acquired through language experience can alter cortical processing of complex auditory signals. To isolate prelexical processing of linguistic tones (i.e., pitch variations that convey part of word meaning), a novel design was used in which hybrid stimuli were created by superimposing Thai tones onto Chinese syllables (tonal chimeras) and Chinese tones onto the same syllables (Chinese words). Native speakers of tone languages (Chinese, Thai) underwent fMRI scans as they judged tones from both stimulus sets. In a comparison of native vs. non-native tones, overlapping activity was identified in the left planum temporale (PT). In this area a double dissociation between language experience and neural representation of pitch occurred such that stronger activity was elicited in response to native as compared to non-native tones. This finding suggests that cortical processing of pitch information can be shaped by language experience and, moreover, that lateratized PT activation can be driven by top-down cognitive processing.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0012,Neural Signatures of Lexical Tone Reading,"Research on how lexical tone is neuroanatomically represented in the human brain is central to our understanding of cortical regions subserving language. Past studies have exclusively focused on tone perception of the spoken language, and little is known as to the lexical tone processing in reading visual words and its associated brain mechanisms. In this study, we performed two experiments to identify neural substrates in Chinese tone reading. First, we used a tone judgment paradigm to investigate tone processing of visually presented Chinese characters. We found that, relative to baseline, tone perception of printed Chinese characters were mediated by strong brain activation in bilateral frontal regions, left inferior parietal lobule, left posterior middle/medial temporal gyrus, left inferior temporal region, bilateral visual systems, and cerebellum. Surprisingly, no activation was found in superior temporal regions, brain sites well known for speech tone processing. In activation likelihood estimation (ALE) meta-analysis to combine results of relevant published studies, we attempted to elucidate whether the left temporal cortex activities identified in Experiment one is consistent with those found in previous studies of auditory lexical tone perception. ALE results showed that only the left superior temporal gyrus and putamen were critical in auditory lexical tone processing. These findings suggest that activation in the superior temporal cortex associated with lexical tone perception is modality-dependent. Hum Brain Mapp, 36:304-312, 2015. (c) 2014 Wiley Periodicals, Inc.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0013,Primary auditory cortex representation of fear-conditioned musical sounds,"Auditory cortex is required for discriminative fear conditioning beyond the classical amygdala microcircuit, but its precise role is unknown. It has previously been suggested that Heschl's gyrus, which includes primary auditory cortex (A1), but also other auditory areas, encodes threat predictions during presentation of conditioned stimuli (CS) consisting of monophones, or frequency sweeps. The latter resemble natural prosody and contain discriminative spectro-temporal information. Here, we use functional magnetic resonance imaging (fMRI) in humans to address CS encoding in A1 for stimuli that contain only spectral but no temporal discriminative information. Two musical chords (complex) or two monophone tones (simple) were presented in a signaled reinforcement context (reinforced CS+ and nonreinforced CS-), or in a different context without reinforcement (neutral sounds, NS1 and NS2), with an incidental sound detection task. CS/US association encoding was quantified by the increased discriminability of BOLD patterns evoked by CS+/CS-, compared to NS pairs with similar physical stimulus differences and task demands. A1 was defined on a single-participant level and based on individual anatomy. We find that in A1, discriminability of CS+/CS- was higher than for NS1/NS2. This representation of unconditioned stimulus (US) prediction was of comparable magnitude for both types of sounds. We did not observe such encoding outside A1. Different from frequency sweeps investigated previously, musical chords did not share representations of US prediction with monophone sounds. To summarize, our findings suggest decodable representation of US predictions in A1, for various types of CS, including musical chords that contain no temporal discriminative information.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0014,Neural correlates of intonation and lexical tone in tonal and non-tonal language speakers,"Intonation, the modulation of pitch in speech, is a crucial aspect of language that is processed in right-hemispheric regions, beyond the classical left-hemispheric language system. Whether or not this notion generalises across languages remains, however, unclear. Particularly, tonal languages are an interesting test case because of the dual linguistic function of pitch that conveys lexical meaning in form of tone, in addition to intonation. To date, only few studies have explored how intonation is processed in tonal languages, how this compares to tone and between tonal and non-tonal language speakers. The present fMRI study addressed these questions by testing Mandarin and German speakers with Mandarin material. Both groups categorised mono-syllabic Mandarin words in terms of intonation, tone, and voice gender. Systematic comparisons of brain activity of the two groups between the three tasks showed large cross-linguistic commonalities in the neural processing of intonation in left fronto-parietal, right frontal, and bilateral cingulo-opercular regions. These areas are associated with general phonological, specific prosodic, and controlled categorical decision-making processes, respectively. Tone processing overlapped with intonation processing in left fronto-parietal areas, in both groups, but evoked additional activity in bilateral temporo-parietal semantic regions and subcortical areas in Mandarin speakers only. Together, these findings confirm cross-linguistic commonalities in the neural implementation of intonation processing but dissociations for semantic processing of tone only in tonal language speakers.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0015,Intonation processing increases task-specific fronto-temporal connectivity in tonal language speakers,"Language comprehension depends on tight functional interactions between distributed brain regions. While these interactions are established for semantic and syntactic processes, the functional network of speech intonation - the linguistic variation of pitch - has been scarcely defined. Particularly little is known about intonation in tonal languages, in which pitch not only serves intonation but also expresses meaning via lexical tones. The present study used psychophysiological interaction analyses of functional magnetic resonance imaging data to characterise the neural networks underlying intonation and tone processing in native Mandarin Chinese speakers. Participants categorised either intonation or tone of monosyllabic Mandarin words that gradually varied between statement and question and between Tone 2 and Tone 4. Intonation processing induced bilateral fronto-temporal activity and increased functional connectivity between left inferior frontal gyrus and bilateral temporal regions, likely linking auditory perception and labelling of intonation categories in a phonological network. Tone processing induced bilateral temporal activity, associated with the auditory representation of tonal (phonemic) categories. Together, the present data demonstrate the breadth of the functional intonation network in a tonal language including higher-level phonological processes in addition to auditory representations common to both intonation and tone.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0016,Social cognition in the blind brain: A coordinate-based meta-analysis,"Social cognition skills are typically acquired on the basis of visual information (e.g., the observation of gaze, facial expressions, gestures). In light of this, a critical issue is whether and how the lack of visual experience affects neurocognitive mechanisms underlying social skills. This issue has been largely neglected in the literature on blindness, despite difficulties in social interactions may be particular salient in the life of blind individuals (especially children). Here we provide a meta-analysis of neuroimaging studies reporting brain activations associated to the representation of self and others' in early blind individuals and in sighted controls. Our results indicate that early blindness does not critically impact on the development of the ""social brain,"" with social tasks performed on the basis of auditory or tactile information driving consistent activations in nodes of the action observation network, typically active during actual observation of others in sighted individuals. Interestingly though, activations along this network appeared more left-lateralized in the blind than in sighted participants. These results may have important implications for the development of specific training programs to improve social skills in blind children and young adults.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0017,Cortical haemodynamic responses predict individual ability to recognise vocal emotions with uninformative pitch cues but do not distinguish different emotions,"We investigated the cortical representation of emotional prosody in normal-hearing listeners using functional near-infrared spectroscopy (fNIRS) and behavioural assessments. Consistent with previous reports, listeners relied most heavily on F0 cues when recognizing emotion cues; performance was relatively poor-and highly variable between listeners-when only intensity and speech-rate cues were available. Using fNIRS to image cortical activity to speech utterances containing natural and reduced prosodic cues, we found right superior temporal gyrus (STG) to be most sensitive to emotional prosody, but no emotion-specific cortical activations, suggesting that while fNIRS might be suited to investigating cortical mechanisms supporting speech processing it is less suited to investigating cortical haemodynamic responses to individual vocal emotions. Manipulating emotional speech to render F0 cues less informative, we found the amplitude of the haemodynamic response in right STG to be significantly correlated with listeners' abilities to recognise vocal emotions with uninformative F0 cues. Specifically, listeners more able to assign emotions to speech with degraded F0 cues showed lower haemodynamic responses to these degraded signals. This suggests a potential objective measure of behavioural sensitivity to vocal emotions that might benefit neurodiverse populations less sensitive to emotional prosody or hearing-impaired listeners, many of whom rely on listening technologies such as hearing aids and cochlear implants-neither of which restore, and often further degrade, the F0 cues essential to parsing emotional prosody conveyed in speech.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0018,Overt speech feasibility using continuous functional magnetic resonance imaging: Isolation of areas involved in phonology and prosody,"To avoid motion artifacts, almost all speech-related functional magnetic resonance imagings (fMRIs) are performed covertly to detect language activations. This method may be difficult to execute, especially by patients with brain tumors, and does not allow the identification of phonological areas. Here, we aimed to evaluate overt task feasibility. Thirty-three volunteers participated in this study. They performed two functional sessions of covert and overt generation of a short sentence semantically linked with a word. Three main contrasts were performed: Covert and Overt for the isolation of language-activated areas, and Overt > Covert for the isolation of the motor cortical activation of speech. fMRI data preprocessing was performed with and without unwarping, and with and without regression of movement parameters as confounding variables. All types of results were compared to each other. For the Overt contrast, Dice coefficients showed strong overlap between each pair of types of results: 0.98 for the pair with and without unwarping, and 0.9 for the pair with and without movement parameter regression. The Overt > Covert contrast allowed isolation of motor laryngeal activations with high statistical reliability and revealed the right-lateralized temporal activity related to acoustic feedback. Overt speaking during magnetic resonance imaging induced few artifacts and did not significantly affect the results, allowing the identification of areas involved in primary motor control and prosodic regulation of speech. Unwarping and motion artifact regression in the postprocessing step, seem to not be necessary. Changes in lateralization of cortical activity by overt speech shall be explored before using these tasks for presurgical mapping.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0019,Music and lexical tone perception in chinese adult cochlear implant users,"Objectives/Hypothesis: The present study's aim was to assess the music perception ability for Chinese adult cochlear implant users and to investigate the correlation between music and Mandarin-Chinese lexical tone perception. Study Design: Case-control study. Methods: Twenty normal-hearing and 21 adult cochlear implant users participated in the Musical Sounds in Cochlear Implants (MuSIC) perception test, including six objective and two subjective musical subtests. The comparison of music perception performance was made between normal-hearing and cochlear implant subjects. Sixteen of the 21 cochlear implant users also performed a tone identification test to investigate the correlation between music and tone perception. Results: Cochlear implant users performed significantly worse than normal-hearing subjects on pitch discrimination, instrument identification, and instrument detection tests, whereas close to normal-hearing subjects on melody discrimination, chords discrimination, rhythm discrimination, and emotion and dissonance rating subtests. Lexical tone perception was significantly correlated with pitch discrimination, melody discrimination, and instrument identification tests. Duration of hearing aid use was found to be correlated with pitch discrimination ability of cochlear implant users. Conclusions: Chinese postlingually deafened cochlear implant users performed significantly poorer in pitch discrimination and timbre perception tasks than normal-hearing listeners. Lexical tone perception was found to be significantly correlated with music pitch perception, supporting the notion that tone and music perception may share a similar pitch perception mechanism.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0020,Effects of the electrode location on tonal discrimination and speech perception of mandarin-speaking patients with a cochlear implant,"Objectives/Hypothesis: This study assessed the effects of varying the electrode location on tonal discrimination and speech perception in Mandarin Chinese-speaking patients. Study Design: A controlled study with six experimental conditions. Methods: Seven Mandarin-speaking listeners who received a MED-EL cochlear implant (CI), ranging in age from 12.88 to 36.43 years (mean, 25.51 years), with an average of 5.28 years of device experience, participated this study. To evaluate the effects of electrode location, six experimental conditions each with the switch off at six different electrodes were designed. Identification tests of Mandarin lexical tones and words were performed. Results: Among experimental conditions with electrode lengths of 31, 23.8, and 16.6 mm, the CI subjects exhibited improved vowel and consonant identification in the condition of 31 mm, reflecting the apical location of electrodes. Specifically, the improvement was observed in the identification score for the vowel backness and height, as well as for the consonant place of articulation. Comparison among three settings with a same electrode length of 12.6 mm and the setting with stimulation to the midregion of the cochlea produces better words as well as the vowel and consonant identification compared with stimulation to basal and apical regions. However, no significant difference was observed for the lexical tone identification among conditions with different electrode location and stimulating region. Conclusions: Less mismatch of the frequency-to-place alignment may account for the improvement of word identification in conditions with electrodes coverage to more apical location; and in conditions where the mid-region of the cochlea were stimulated. Laryngoscope, 2012",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0021,Perceptual Separation of Sensorineural Hearing Loss and Auditory Neuropathy Spectrum Disorder,"Objectives/Hypothesis: The present study aimed to examine whether the response patterns to the chimeric lexical tone tokens, combined with their pure tone audiometry (PTA) results, could separate listeners with sensorineural hearing loss (SNHL) from listeners with auditory neuropathy spectrum disorder (ANSD). Study Design: Case-control study. Methods: Forty-three SNHL subjects and 46 ANSD subjects participated in a Mandarin lexical tone perception test using original and chimeric tone tokens. Ten sets of monosyllables, with four tone patterns for each, were processed through a 16-channel chimeric synthesizer in which a temporal envelope (E) from a monosyllabic word of one tone was paired with a fine structure (FS) from the same monosyllable of other tones. Results: Significantly negative correlations were present between tone perception scores and PTA0.5-4 kHz for both SNHL (P < 0.001) and ANSD (P < 0.001) subjects. Overall, 72.4%, 66.4%, and 46.3% of the tone responses were consistent with FS for the SNHL subjects with mild, moderate, and severe degree of hearing loss, respectively; and 28.4%, 23.1%, and 22.7% were consistent with FS for the ANSD subjects, with the equivalent degree of hearing loss. Similarly, 17.6%, 24.2%, and 37.7% of the tone responses were consistent with E for the SNHL subjects with mild, moderate, and severe degree of hearing loss, respectively; and 45.5%, 44.3%, and 36.5% were consistent with E for the ANSD subjects. Conclusion: Subjects with SNHL and ANSD may be separated by representing their FS- and E-consistent tone responses as a function of their pure-tone hearing thresholds.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0022,Voice emotion recognition by Mandarin-speaking pediatric cochlear implant users in Taiwan,"Objectives To explore the effects of obligatory lexical tone learning on speech emotion recognition and the cross-culture differences between United States and Taiwan for speech emotion understanding in children with cochlear implant. Methods This cohort study enrolled 60 cochlear-implanted (cCI) Mandarin-speaking, school-aged children who underwent cochlear implantation before 5 years of age and 53 normal-hearing children (cNH) in Taiwan. The emotion recognition and the sensitivity of fundamental frequency (F0) changes for those school-aged cNH and cCI (6-17 years old) were examined in a tertiary referred center. Results The mean emotion recognition score of the cNH group was significantly better than the cCI. Female speakers' vocal emotions are more easily to be recognized than male speakers' emotion. There was a significant effect of age at test on voice recognition performance. The average score of cCI with full-spectrum speech was close to the average score of cNH with eight-channel narrowband vocoder speech. The average performance of voice emotion recognition across speakers for cCI could be predicted by their sensitivity to changes in F0. Conclusions Better pitch discrimination ability comes with better voice emotion recognition for Mandarin-speaking cCI. Besides the F0 cues, cCI are likely to adapt their voice emotion recognition by relying more on secondary cues such as intensity and duration. Although cross-culture differences exist for the acoustic features of voice emotion, Mandarin-speaking cCI and their English-speaking cCI peer expressed a positive effect for age at test on emotion recognition, suggesting the learning effect and brain plasticity. Therefore, further device/processor development to improve presentation of pitch information and more rehabilitative efforts are needed to improve the transmission and perception of voice emotion in Mandarin. Level of evidence 3.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0023,The Linguistic Pathways Model: Capturing the Multiple Dimensions of Reading Development,"The importance of oral language skills in reading comprehension is widely recognized in contemporary models. Building on this foundation, we propose the Linguistic Pathways Model. In this model, we illuminate mechanistic and developmental detail by which individual components of oral language support reading comprehension and embrace the multiple dimensions across which reading development plays out. This is the level of theoretical detail needed to inform instruction in the classroom that is most likely to propel children on strong trajectories of reading development. We illustrate the value of this model by focusing on syntactic skills-the ability to understand and manipulate sentence structure. We hypothesize two core pathways by which syntactic skills impact reading comprehension. In the syntax-to-lexicon pathway, syntactic skills influence how readers construct lexical representations, ultimately impacting reading comprehension. In the syntax-to-sentence pathway, syntactic skills affect reading comprehension by shaping how readers parse sentences and generate predictions about upcoming information. In each, we elaborate on mechanisms of these influences. We also detail the nature of developmental effects, including changes in relative reliance on skills over time and the temporal order of effects, and the interactions between the two. This work provides a new theoretical model for understanding the precise pathways through which individual oral language skills contribute to reading comprehension development, making predictions that are testable in classrooms.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0024,"Aphasias and theories of linguistic representation: representing frequency, hierarchy, constructions, and sequential structure","Error and preservation patterns in aphasic speech show that the brain makes use of the frequencies of words, constructions, and collocations, as well as category membership and hierarchical structure, during language processing. Frequency effects are evident along two quasi-independent axes: syntagmatic (the sequential context, e.g., deploying correct functors, categories, and utterance-level intonation) and paradigmatic (the choice at any given linguistic level, e.g., selecting content words and modifying structures). Frequency along the syntagmatic axis is shown to play a role in errors involving idioms, constructions, and collocations that cross major phrasal boundaries. Along the paradigmatic axis, frequency affects errors involving lexical selection, competing functors and inflected forms (e.g., using plural where singular is required). An account of language representation and processing that encompasses frequency as well as categorization and structure is compatible with what we know about how the brain works: increased experience with a linguistic structure results in increased activationand strengtheningof the neural networks involved in processing that structure. These claims are supported by the literature on experimental work in normal speakers. Parsimony, plus the unexamined assumption that mental representation is like a written record (entries either present or absent, structure displayable in two dimensions), has been a misleading guide to modeling language representation. The substantial redundancy in representations and processing that is introduced by incorporating both frequency-based and hierarchy-based information is in fact appropriate for the brain as a fast, reliable, massively parallel error-correcting network with very large storage capacity and gradient representation strength. (C) 2013 John Wiley & Sons, Ltd. WIREs Cogn Sci 2013, 4:651-663. doi: 10.1002/wcs.1257 Conflict of interest: The authors have declared no conflicts of interest for this article. For further resources related to this article, please visit the WIREs website.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0025,The role of prosody in the mental lexicon,"Current models of spoken word recognition take into account factors such as word frequency, word onset cohort size, and phonological neighborhood density. Using the word onset gating technique we tested word recognition when bandpass filtering was used to allow subjects to hear the full prosodic pattern of a word (number of syllables and syllabic stress), deprived of segmental information beyond that contained in the onset gate. Subjects also heard either word onsets plus duration information or only word onsets. Results suggest that word prosody is represented in the mental lexicon and is effectively used by listeners in spoken word recognition. (C) 1999 Academic Press.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0026,Pitch and timing abilities in inherited speech and language impairment,"Members of the KE family who suffer from an inherited developmental speech-and-language disorder and normal, age-matched, controls were tested on musical abilities, including perception and production of pitch and rhythm. Affected family members were not deficient in either the perception or production of pitch, whether this involved either single notes or familiar melodies. However, they were deficient in both the perception and production of rhythm in both vocal and manual modalities. It is concluded that intonation abilities are not impaired in the affected family members, whereas their timing abilities are impaired. Neither their linguistic nor oral praxic deficits can be at the root of their impairment in timing; rather, the reverse may be true, (C) 2000 Academic Press.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0027,Functional heterogeneity of inferior frontal gyrus is shaped by linguistic experience,"A crosslinguistic, positron emission tomography (PET) study was conducted to determine the influence of linguistic experience on the perception of segmental (consonants and vowels) and suprasegmental (tones) information. Chinese and English subjects (10 per group) were presented binaurally with lists consisting of five Chinese monosyllabic morphemes (speech) or low-pass-filtered versions of the same stimuli (nonspeech). The first and last items were targeted for comparison: the time interval between target tones was filled with irrelevant dis tractor tones. A speeded-response, selective attention paradigm required subjects to make discrimination judgments of the target items while ignoring intervening distractor tones. PET scans were acquired for five tasks presented twice: one passive listening to pitch (nonspeech) and four active (speech = consonant. von el, and tone; nonspeech = pitch). Significant regional changes in blood Row were identified from comparisons of group-averaged images of active tasks relative to passive listening. Chinese subjects show increased activity in left premotor cortex, pars opercularis, and pars triangularis across the four tasks. English subjects, on the other hand, show increased activity in left inferior frontal gyrus regions only in the vowel task and in right inferior frontal gyrus regions in the pitch task. Findings suggest that functional circuits engaged in speech perception depend on Linguistic experience. All linguistic information signaled by prosodic cues engages left-hemisphere mechanisms. Storage and executive processes of workings memory that are implicated in phonological processing are mediated in discrete regions of the left frontal lobe. (C) 2001 Academic Press.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0028,Dichotic perception of mandarin tones by Chinese and American listeners,"The dichotic perception of Mandarin tones by native and normative listeners was examined in order to investigate the lateralization of lexical tone. Twenty American listeners with no tone language background and 20 Chinese listeners were asked to identify dichotically presented tone pairs by identifying which tone they heard in each ear. For the Chinese listeners, 57% of the total errors occurred via the left ear. indicating a significant right ear advantage. However, the American listeners revealed no significant ear preference. with 48% of the errors attributable to the left ear. These results indicated that Mandarin tones are predominantly processed in the left hemisphere by native Mandarin speakers, whereas they are bilaterally processed by American English speakers with no prior tone experience. The results also suggest that the left hemisphere superiority for native Mandarin tone processing is similar to native processing of other tone languages. (C) 2001 Academic Press.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0029,AUTOMATIC PITCH CONTOUR STYLIZATION USING A MODEL OF TONAL PERCEPTION,"A new quantitative model of tonal perception for continuous speech is described. The paper illustrates its ability for automatic stylization of pitch contours, with applications to prosodic analysis and speech synthesis in mind, and evaluates it in a perception experiment. After a discussion of the psyche-acoustics of tonal perception, and an overview of existing tonal perception models and systems for automatic analysis of intonation, the model and its computer implementation are described in detail. It includes parameter extraction, segmentation into syllables, perceptual integration of short term pitch change, tonal segment computation, and pitch contour stylization. This is followed by a perception experiment in which subjects are asked to distinguish original signals from resynthesized signals with automatically stylized pitch contours. The aim of this experiment is to show the usefulness of the model as a basis for intonation representation, and to study the influence of the model parameters. It is shown that the stylization obtained with the model is an economic representation of intonation which can be useful for speech synthesis and prosodic analysis. (C) 1995 Academic Press Limited",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0030,PURR - A method for prosody evaluation and investigation,"Since intelligibility of synthetic speech is no longer the main criterion on which to base quality judgements, reliable methods for prosody evaluation become more important. We propose a method called PURR (Prosody Unveiling through Restricted Representation) to evaluate the prosodic component of a synthesis system without the interference of other system components. In PURR, the stimuli are reduced to their prosodic content. The method has proven to be suitable for test designs with naive listeners. It can be used for comparative studies as well as for diagnostic analyses and is, therefore, a useful tool for basic research on the perception of prosodic phenomena. In this paper we first describe how the best signal manipulation method was determined using perception tests. The appropriateness of the resulting signal is further assessed in a recognition test of syntactic structure. We then report on further validations of the proposed method: different ways of synthetic prosody modelling are evaluated, both in comparison with human prosody and amongst different synthesis systems. (C) 1998 Academic Press.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0031,Prosodic facilitation and interference in the resolution of temporary syntactic closure ambiguity,"Subjects listened to sentences with early closure (e.g., When Roger leaves the house is dark) or late closure syntax (e.g., When Roger leaves the house it's dark) and one of three prosodies: cooperating (coinciding prosodic and syntactic boundary), baseline (phonetically neutralized prosodic boundary), and conflicting (prosodic boundary at a misleading syntactic location). Prosodic manipulations were verified by phonetic measurements and listener judgments. Four experiments demonstrated facilitation in speeded phonosyntactic grammaticality judgment, end-of-sentence comprehension, and crossmodal naming tasks: Sentences with cooperating prosody were processed more quickly than those with baseline prosody. Three experiments showed interference: Sentences with conflicting prosody were processed more slowly than those with baseline prosody. All experiments demonstrated a processing advantage for late closure structures in the conflicting and baseline conditions, but no differences between syntactic types in the cooperating condition. Cross-modal naming results showed early syntactic effects due to both high-level and intermediate-level prosodic boundaries. We argue that the initial syntactic structure assigned to an utterance can be determined by its prosodic phonological representation. (C) 1999 Academic Press.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0032,Implications of stress-pattern differences in spoken-word recognition,"Existing models of spoken-word recognition positing that stressed syllables tend to be perceived as word onsets have not provided an account of the processings of non-initial-stress words. The present study suggests that such words require additional, time-consuming processing. Two experiments showed that phoneme monitoring is slower in non-initial-stress than initial-stress words, even when the target-carrying syllable is made identical through splicing. In a third experiment, the processing of non-initial-stress words was also found to be more memory-taxing than that of initial-stress words, a result consistent with the need for additional memory storage generated by incorrect lexical activation in non-initial-stress words. Taken together, the results support the view that words bearing different stress patterns are processed differently, with extra processing required for non-initial-stress words. The implementation of such a distinction is discussed in the framework of current models of word recognition, with an emphasis on processing time-course differences. (C) 2000 Academic Press.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0033,Effects of tone and focus on the formation and alignment of f0 contours,"The present study examines how lexical tone and focus contribute to the formation and alignment off, contours in speech. This was done through an investigation of f(0) contour formation in short Mandarin sentences. These sentences all consisted of five syllables with varying tones on the middle three syllables. The sentences were produced by eight Mandarin speakers with four different focus patterns: focus on the first, second, or last word, or with no narrow focus. The f(0) patterns of these sentences were examined through point-by-point f(0) tracing, graphical comparison of averaged f(0) contours, f(0)-contour-syllable alignment analysis, and analysis of maximum, minimum f(0), and slope of f(0) contours. The results indicate that (a) while the lexical tone of a syllable is the most important determining factor for the local f(0) contour of the syllable, focus extensively modulates the global shape of the f(0) curve, which in turn affects the height and even the shape of local contours; (b) the tones of adjacent syllables also extensively influence both the shape and height of the f(0) contour of a syllable, with the preceding tone exerting more influence than the following tone; (c) despite extensive variations in shape and height, the f(0) contour of a tone remains closely aligned with the associated syllable; and (d) both focus and tonal interaction may generate substantial f(0) decline over the course of an utterance. These findings seem to be able to reduce the unpredictability in the formation and alignment of f(0) contours in speech. (C) 1999 Academic Press.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0034,A cross-linguistic PET study of tone perception in mandarin Chinese and English speakers,"PET was used in a cross-linguistic study to determine whether neural mechanisms subserving pitch perception differ as a function of linguistic relevance. We compared tone perception in 12 native Mandarin speakers, who use tonal patterns to distinguish lexical meaning, with that of 12 native speakers of a nontone language, English. Subjects were scanned under two conditions: a silent resting baseline and a tonal task involving discrimination of pitch patterns in Mandarin words. Both groups showed common regions of CBF increase, but only Mandarin speakers showed additional activation in frontal, parietal, and parietooccipital regions of the left hemisphere; this latter finding indicates that language experience may influence brain circuitry in the processing of auditory cues. In contrast, only the English group showed activity in the right inferior frontal cortex, consistent with a right-hemispheric role in pitch perception. (C) 2001 Academic Press.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0035,"Dynamic brain activation during processing of emotional intonation: Influence of acoustic parameters, emotional valence, and sex","Appreciation of the emotional tone of verbal utterances represents an important aspect of social life. It is still unsettled, however, which brain areas mediate processing of intonational information and whether the presumed right-sided superiority depends upon acoustic properties of the speech signal. Functional magnetic resonance imaging was used to disentangle brain activation associated with W extraction of specific acoustic cues and (ii) detection of specific emotional states. Stimulus material comprised pairs of emotionally intonated utterances, exclusively differing either in pitch range or in the length of stressed vowels. Hemodynamic responses showed a dynamic pattern of cerebral activation including sequenced bilateral responses of various cortical and subcortical structures. Activation associated with discrimination of emotional expressiveness predominantly emerged within the right inferior parietal lobule, within the bilateral mesiofrontal cortex and-with an asymmetry toward the right hemisphere at the level of bilateral dorsolateral frontal cortex. Lateralization did not depend upon acoustic structure or emotional valence of stimuli. These findings might prove helpful in reconciling the controversial previous clinical and experimental data. (C) 2002 Elsevier Science (USA).",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0036,Dissociating sentential prosody from sentence processing: Activation interacts with task demands,"Sentence processing was contrasted with processing of syntactic prosody under two task conditions in order to examine the representation of these components of language and their interaction with working memory load. Twelve adults received fMRI scans while they listened to low-pass filtered and unfiltered sentences either passively, or during tasks that required subjects to remember and recognize information contained in the stimuli. Results indicated that temporal activation for prosodic stimuli differed compared to activation for sentence stimuli only during passive listening tasks. The inclusion of memory demands was associated with frontal activation, which was differentially lateralized for sentence and prosodic stimuli. The results demonstrate differential brain activation for prosodic vs sentential stimuli which interacts with the memory demands placed on the subjects. (C) 2002 Elsevier Science (USA).",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0037,The Relative Contributions of Temporal Envelope and Fine Structure to Mandarin Lexical Tone Perception in Auditory Neuropathy Spectrum Disorder,"Previous studies have demonstrated that temporal envelope (E) is sufficient for speech perception, while fine structure (FS) is important for pitch perception for normal-hearing (NH) listeners. Listeners with sensorineural hearing loss (SNHL) have an impaired ability to use FS in lexical tone perception due to the reduced frequency resolution. Listeners with auditory neuropathy spectrum disorder (ANSD) may have deficits in temporal resolution. Little is known about how such deficits may impact their ability to use E and FS to perceive lexical tone, and whether it is the deficit in temporal resolution or frequency resolution that may lead to more detrimental effects on FS processing in pitch perception. Three experiments were conducted in the present study. Experiment I used the ""auditory chimera"" technique to investigate how SNHL and ANSD listeners would achieve lexical tone recognition using either the E or the FS cues. Experiment II tested their frequency resolution as measured with their psychophysical tuning curves (PTCs). Experiment III tested their temporal resolution as measured with the temporal gap detection (TGD) threshold. The results showed that the SNHL listeners had reduced frequency selectivity, but intact temporal resolution ability, while the ANSD listeners had degraded temporal resolution ability, but intact frequency selectivity. In comparison with the SNHL listeners, the ANSD listeners had severely degraded ability to process the FS cues and thus their ability to perceive lexical tone mainly depended on the ability to use the E cues. These results suggested that, in comparison with the detrimental impact of the reduced frequency selectivity, the impaired temporal resolution may lead to more degraded FS processing in pitch perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0038,Neural Segregation of Concurrent Speech: Effects of Background Noise and Reverberation on Auditory Scene Analysis in the Ventral Cochlear Nucleus,"Concurrent complex sounds (e.g., two voices speaking at once) are perceptually disentangled into separate ""auditory objects"". This neural processing often occurs in the presence of acoustic-signal distortions from noise and reverberation (e.g., in a busy restaurant). A difference in periodicity between sounds is a strong segregation cue under quiet, anechoic conditions. However, noise and reverberation exert differential effects on speech intelligibility under ""cocktail-party"" listening conditions. Previous neurophysiological studies have concentrated on understanding auditory scene analysis under ideal listening conditions. Here, we examine the effects of noise and reverberation on periodicity-based neural segregation of concurrent vowels /a/ and /i/, in the responses of single units in the guinea-pig ventral cochlear nucleus (VCN): the first processing station of the auditory brain stem. In line with human psychoacoustic data, we find reverberation significantly impairs segregation when vowels have an intonated pitch contour, but not when they are spoken on a monotone. In contrast, noise impairs segregation independent of intonation pattern. These results are informative for models of speech processing under ecologically valid listening conditions, where noise and reverberation abound.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0039,Child-directed speech,"Infant- or child-directed speech (CDS) defines the code used to communicate with infants or children, which differs from standard adult speech in prosody, expressions, diction and word repetition etc. A selective literature search in PubMed was carried out for the purposes of this systematic review. Due to its specific advantages, child-directed speech facilitates the extraction and representation of relevant, meaningful sections from the continuous speech signal. Different speech communities use different variants of CDS. CDS is not only seen to be used by adults, but also by children communicating with younger children. However, there are speech communities that do not use CDS. Taking into consideration findings previously described in the literature, CDS appears to positively support language acquisition in children, but does not represent a necessary prerequisite. However, there are no findings in the literature to indicate that the linguistically reduced CDS hinders early language acquisition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0040,"Prosody, speech input and language acquisition","Background. In order to acquire language, children require speech input. The prosody of the speech input plays an important role. In most cultures adults modify their code when communicating with children. Compared to normal speech this code differs especially with regard to prosody. Method. For this review a selective literature search in PubMed and Scopus was performed. Results. Prosodic characteristics are a key feature of spoken language. By analysing prosodic features, children gain knowledge about underlying grammatical structures. Child-directed speech (CDS) is modified in a way that meaningful sequences are highlighted acoustically so that important information can be extracted from the continuous speech flow more easily. CDS is said to enhance the representation of linguistic signs. Discussion. Taking into consideration what has previously been described in the literature regarding the perception of suprasegmentals, CDS seems to be able to support language acquisition due to the correspondence of prosodic and syntactic units. However, no findings have been reported, stating that the linguistically reduced CDS could hinder first language acquisition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0041,Recalibration of vocal affect by a dynamic face,"Perception of vocal affect is influenced by the concurrent sight of an emotional face. We demonstrate that the sight of an emotional face also can induce recalibration of vocal affect. Participants were exposed to videos of a 'happy' or 'fearful' face in combination with a slightly incongruous sentence with ambiguous prosody. After this exposure, ambiguous test sentences were rated as more 'happy' when the exposure phase contained 'happy' instead of 'fearful' faces. This auditory shift likely reflects recalibration that is induced by error minimization of the inter-sensory discrepancy. In line with this view, when the prosody of the exposure sentence was non-ambiguous and congruent with the face (without audiovisual discrepancy), aftereffects went in the opposite direction, likely reflecting adaptation. Our results demonstrate, for the first time, that perception of vocal affect is flexible and can be recalibrated by slightly discrepant visual information.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0042,Influence of different acoustic cues in L1 lexical tone on the perception of L2 lexical stress using principal component analysis: an ERP study,"Previous studies have widely explored the prosodic transfer from L1 to L2 during speech perception across stress languages. However, few if any studies have investigated the transfer from L1 tonal language to L2 stress language and the relative roles of different acoustic cues underlying the transfer. Therefore, the current study was conducted to compare the perception of English lexical stress between Mandarin and Cantonese speakers who learn English as a foreign language. The event-related potential measurements and the principal component analysis were conducted for the two groups to explore the roles of different acoustic cues in the perception of English speech. The results demonstrated that compared with the Mandarin group, the Cantonese speakers relied more on pitch information and the reliance holds even when all the three cues varied simultaneously. Therefore, it was concluded that prosodic transfer from L1 lexical tone to L2 lexical stress occurred at the acoustic level, and the native linguistic background shaped the manner how speakers perceived the L2 speech.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0043,Primary face motor area as the motor representation of articulation,"No clinical data have yet been presented to show that a lesion localized to the primary motor area (M1) can cause severe transient impairment of articulation, although a motor representation for articulation has been suggested to exist within M1. Here We describe three cases of patients who developed severe dysarthria, temporarily mimicking speech arrest or aphemia, due to a localized brain lesion near the left face representation of the human primary motor cortex (face-M1). Speech was slow, effortful, lacking normal prosody, and more affected than expected from the degree of facial or tongue palsy. There was a mild deficit in tongue movements in the sagittal plane that impaired palatolingual contact and rapid tongue movements. The speech disturbance was limited to verbal output, without aphasia or orofacial apraxia. Overlay of magnetic resonance images revealed a localized cortical region near face-M1, which displayed high intensity on diffusion weighted images, while the main portion of the corticobulbar fibers arising from the lower third of the motor cortex was preserved. The cases suggest the existence of a localized brain region specialized for articulation near face-M1. Cortico-cortical fibers connecting face-M1 with the lower premotor areas including Broca's area may also be important for articulatory control.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0044,The lateralized arcuate fasciculus in developmental pitch disorders among mandarin amusics: left for speech and right for music,"The arcuate fasciculus (AF) is a neural fiber tract that is critical to speech and music development. Although the predominant role of the left AF in speech development is relatively clear, how the AF engages in music development is not understood. Congenital amusia is a special neurodevelopmental condition, which not only affects musical pitch but also speech tone processing. Using diffusion tensor tractography, we aimed at understanding the role of AF in music and speech processing by examining the neural connectivity characteristics of the bilateral AF among thirty Mandarin amusics. Compared to age- and intelligence quotient (IQ)-matched controls, amusics demonstrated increased connectivity as reflected by the increased fractional anisotropy in the right posterior AF but decreased connectivity as reflected by the decreased volume in the right anterior AF. Moreover, greater fractional anisotropy in the left direct AF was correlated with worse performance in speech tone perception among amusics. This study is the first to examine the neural connectivity of AF in the neurodevelopmental condition of amusia as a result of disrupted music pitch and speech tone processing. We found abnormal white matter structural connectivity in the right AF for the amusic individuals. Moreover, we demonstrated that the white matter microstructural properties of the left direct AF is modulated by lexical tone deficits among the amusic individuals. These data support the notion of distinctive pitch processing systems between music and speech.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0045,Audiovisual emotion recognition using ANOVA feature selection method and multi-classifier neural networks,"To make human-computer interaction more naturally and friendly, computers must enjoy the ability to understand human's affective states the same way as human does. There are many modals such as face, body gesture and speech that people use to express their feelings. In this study, we simulate human perception of emotion through combining emotion-related information using facial expression and speech. Speech emotion recognition system is based on prosody features, mel-frequency cepstral coefficients (a representation of the short-term power spectrum of a sound) and facial expression recognition based on integrated time motion image and quantized image matrix, which can be seen as an extension to temporal templates. Experimental results showed that using the hybrid features and decision-level fusion improves the outcome of unimodal systems. This method can improve the recognition rate by about 15 % with respect to the speech unimodal system and by about 30 % with respect to the facial expression system. By using the proposed multi-classifier system that is an improved hybrid system, recognition rate would increase up to 7.5 % over the hybrid features and decision-level fusion with RBF, up to 22.7 % over the speech-based system and up to 38 % over the facial expression-based system.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0046,Robust emotion recognition in noisy speech via sparse representation,"Emotion recognition in speech signals is currently a very active research topic and has attracted much attention within the engineering application area. This paper presents a new approach of robust emotion recognition in speech signals in noisy environment. By using a weighted sparse representation model based on the maximum likelihood estimation, an enhanced sparse representation classifier is proposed for robust emotion recognition in noisy speech. The effectiveness and robustness of the proposed method is investigated on clean and noisy emotional speech. The proposed method is compared with six typical classifiers, including linear discriminant classifier, K-nearest neighbor, C4.5 decision tree, radial basis function neural networks, support vector machines as well as sparse representation classifier. Experimental results on two publicly available emotional speech databases, that is, the Berlin database and the Polish database, demonstrate the promising performance of the proposed method on the task of robust emotion recognition in noisy speech, outperforming the other used methods.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0047,Spoken emotion recognition via locality-constrained kernel sparse representation,"Spoken emotion recognition is currently a very active research topic and has attracted extensive attention in signal processing, pattern recognition, artificial intelligence, etc. In this paper, a new emotion classification method based on kernel sparse representation, named locality-constrained kernel sparse representation-based classification (LC-KSRC), is proposed for spoken emotion recognition. LC-KSRC is able to learn more discriminating sparse representation coefficients for spoken emotion recognition, since it integrates both sparsity and data locality in the kernel feature space. The proposed method is compared with six representative emotion classification methods, including linear discriminant classifier, K-nearest-neighbor, radial basis function neural networks, support vector machines, sparse representation-based classification and kernel sparse representation-based classification. Experimental results on two publicly available emotional speech databases, i.e., the Berlin database and the Polish database, demonstrate the promising performance of the proposed method on spoken emotion recognition tasks, outperforming the other used methods.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0048,Relative Contributions of Temporal Envelope and Fine Structure Cues to Lexical Tone Recognition in Hearing-Impaired Listeners,"It has been reported that normal-hearing Chinese speakers base their lexical tone recognition on fine structure regardless of temporal envelope cues. However, a few psychoacoustic and perceptual studies have demonstrated that listeners with sensorineural hearing impairment may have an impaired ability to use fine structure information, whereas their ability to use temporal envelope information is close to normal. The purpose of this study is to investigate the relative contributions of temporal envelope and fine structure cues to lexical tone recognition in normal-hearing and hearing-impaired native Mandarin Chinese speakers. Twenty-two normal-hearing subjects and 31 subjects with various degrees of sensorineural hearing loss participated in the study. Sixteen sets of Mandarin monosyllables with four tone patterns for each were processed through a ""chimeric synthesizer"" in which temporal envelope from a monosyllabic word of one tone was paired with fine structure from the same monosyllable of other tones. The chimeric tokens were generated in the three channel conditions (4, 8, and 16 channels). Results showed that differences in tone responses among the three channel conditions were minor. On average, 90.9%, 70.9%, 57.5%, and 38.2% of tone responses were consistent with fine structure for normal-hearing, moderate, moderate to severe, and severely hearing-impaired groups respectively, whereas 6.8%, 21.1%, 31.4%, and 44.7% of tone responses were consistent with temporal envelope cues for the above-mentioned groups. Tone responses that were consistent neither with temporal envelope nor fine structure had averages of 2.3%, 8.0%, 11.1%, and 17.1% for the above-mentioned groups of subjects. Pure-tone average thresholds were negatively correlated with tone responses that were consistent with fine structure, but were positively correlated with tone responses that were based on the temporal envelope cues. Consistent with the idea that the spectral resolvability is responsible for fine structure coding, these results demonstrated that, as hearing loss becomes more severe, lexical tone recognition relies increasingly on temporal envelope rather than fine structure cues due to the widened auditory filters.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0049,Voice Discrimination by Adults with Cochlear Implants: the Benefits of Early Implantation for Vocal-Tract Length Perception,"Cochlear implant (CI) users find it extremely difficult to discriminate between talkers, which may partially explain why they struggle to understand speech in a multi-talker environment. Recent studies, based on findings with postlingually deafened CI users, suggest that these difficulties may stem from their limited use of vocal-tract length (VTL) cues due to the degraded spectral resolution transmitted by the CI device. The aim of the present study was to assess the ability of adult CI users who had no prior acoustic experience, i.e., prelingually deafened adults, to discriminate between resynthesized ""talkers"" based on either fundamental frequency (F0) cues, VTL cues, or both. Performance was compared to individuals with normal hearing (NH), listening either to degraded stimuli, using a noise-excited channel vocoder, or non-degraded stimuli. Results show that (a) age of implantation was associated with VTL but not F0 cues in discriminating between talkers, with improved discrimination for those subjects who were implanted at earlier age; (b) there was a positive relationship for the CI users between VTL discrimination and speech recognition score in quiet and in noise, but not with frequency discrimination or cognitive abilities; (c) early-implanted CI users showed similar voice discrimination ability as the NH adults who listened to vocoded stimuli. These data support the notion that voice discrimination is limited by the speech processing of the CI device. However, they also suggest that early implantation may facilitate sensory-driven tonotopicity and/or improve higher-order auditory functions, enabling better perception of VTL spectral cues for voice discrimination.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0050,Enhanced music sensitivity in 9-month-old bilingual infants,"This study explores the influence of bilingualism on the cognitive processing of language and music. Specifically, we investigate how infants learning a non-tone language perceive linguistic and musical pitch and how bilingualism affects cross-domain pitch perception. Dutch monolingual and bilingual infants of 8-9 months participated in the study. All infants had Dutch as one of the first languages. The other first languages, varying among bilingual families, were not tone or pitch accent languages. In two experiments, infants were tested on the discrimination of a lexical (N = 42) or a violin (N = 48) pitch contrast via a visual habituation paradigm. The two contrasts shared identical pitch contours but differed in timbre. Non-tone language learning infants did not discriminate the lexical contrast regardless of their ambient language environment. When perceiving the violin contrast, bilingual but not monolingual infants demonstrated robust discrimination. We attribute bilingual infants' heightened sensitivity in the musical domain to the enhanced acoustic sensitivity stemming from a bilingual environment. The distinct perceptual patterns between language and music and the influence of acoustic salience on perception suggest processing diversion and association in the first year of life. Results indicate that the perception of music may entail both shared neural network with language processing, and unique neural network that is distinct from other cognitive functions.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0051,A probabilistic multimodal approach for predicting listener backchannels,"During face-to-face interactions, listeners use backchannel feedback such as head nods as a signal to the speaker that the communication is working and that they should continue speaking. Predicting these backchannel opportunities is an important milestone for building engaging and natural virtual humans. In this paper we show how sequential probabilistic models ( e. g., Hidden Markov Model or Conditional Random Fields) can automatically learn from a database of human-to-human interactions to predict listener backchannels using the speaker multimodal output features ( e. g., prosody, spoken words and eye gaze). The main challenges addressed in this paper are automatic selection of the relevant features and optimal feature representation for probabilistic models. For prediction of visual backchannel cues (i.e., head nods), our prediction model shows a statistically significant improvement over a previously published approach based on hand-crafted rules.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0052,Understanding Sarcasm's Neural Correlates Through a Novel fMRI Spanish Paradigm,"There is growing interest in the neural network of pragmatic language and its potential overlap with the Theory of Mind (ToM) network. However, no Spanish-adapted fMRI tasks were used for studying sarcasm, the subtype of pragmatic language most related to ToM. Furthermore, stimuli used in prior studies often impose high cognitive demands, confounding its sarcasm brain representation with the executive network. We investigate the neural correlates of sarcasm in Spanish using a novel experimental paradigm designed to minimize cognitive load and enhance ecological validity. Eighteen healthy, right-handed participants underwent a 3T fMRI session with a sarcasm comprehension task. Brain activations analysed with SPM12 were calculated for sarcasm vs. literal contrast. Sarcasm activated the left temporo-parietal junction, Medial Prefrontal Cortex (BA 10), Left Inferior Frontal Gyrus (BA 45), Left Medial and Superior Temporal Gyrus (BA 21 & 22), and Left Temporal Pole (BA 38). Sarcasm comprehension involves an extensive fronto-temporal-parietal network, with prominent activation of ToM-related areas. These findings suggest an overlap between sarcasm and ToM networks, emphasizing the role of the medial prefrontal cortex in pragmatic language, the left inferior frontal gyrus in semantic integration, and the role of a left-lateralized frontotemporal network for sarcasm processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0053,An investigation on the degradation of different features extracted from the compressed American English speech using narrowband and wideband codecs,"Speech coding facilitates speech compression without perceptual loss that results in the elimination or deterioration of both speech and speaker specific features used for a wide range of applications like automatic speaker and speech recognition, biometric authentication, prosody evaluations etc. The present work investigates the effect of speech coding in the quality of features which include Mel Frequency Cepstral Coefficients, Gammatone Frequency Cepstral Coefficients, Power-Normalized Cepstral Coefficients, Perceptual Linear Prediction Cepstral Coefficients, Rasta-Perceptual Linear Prediction Cepstral Coefficients, Residue Cepstrum Coefficients and Linear Predictive Coding-derived cepstral coefficients extracted from codec compressed speech. The codecs selected for this study are G.711, G.729, G.722.2, Enhanced Voice Services, Mixed Excitation Linear Prediction and also three codecs based on compressive sensing frame work. The analysis also includes the variation in the quality of extracted features with various bit-rates supported by Enhanced Voice Services, G.722.2 and compressive sensing codecs. The quality analysis of extracted epochs, fundamental frequency and formants estimated from codec compressed speech was also performed here. In the case of various features extracted from the output of selected codecs, the variation introduced by Mixed Excitation Linear Prediction codec is the least due to its unique method for the representation of excitation. In the case of compressive sensing based codecs, there is a drastic improvement in the quality of extracted features with the augmentation of bit rate due to the waveform type coding used in compressive sensing based codecs. For the most popular Code Excited Linear Prediction codec based on Analysis-by-Synthesis coding paradigm, the impact of Linear Predictive Coding order in feature extraction is investigated. There is an improvement in the quality of extracted features with the order of linear prediction and the optimum performance is obtained for Linear Predictive Coding order between 20 and 30, and this varies with gender and statistical characteristics of speech. Even though the basic motive of a codec is to compress single voice source, the performance of codecs in multi speaker environment is also studied, which is the most common environment in majority of the speech processing applications. Here, the multi speaker environment with two speakers is considered and there is an augmentation in the quality of individual speeches with increase in diversity of mixtures that are passed through codecs. The perceptual quality of individual speeches extracted from the codec compressed speech is almost same for both Mixed Excitation Linear Prediction and Enhanced Voice Services codecs but regarding the preservation of features, the Mixed Excitation Linear Prediction codec has shown a superior performance over Enhanced Voice Services codec.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0054,RETRACTED: Preserving learnability and intelligibility at the point of care with assimilation of different speech recognition techniques (Retracted Article),"The neoteric introduction of 5G technology in mobile internet is transforming Internet of mobile Things (IoMT) massively by addressing low latency, support for a large number of IoMT devices, and less power consumption, thereby delivering cost-effective solutions to low-end devices. This transformational technology enables a ubiquitous connected critical communication network between the healthcare system and IoT, as they largely depend on low-end devices for gathering data at the point of care. Gathering and interpolation of data from things are moved over to the cloud, making the extraction of knowledge and decision-making capabilities more robust. Vocal signals form the basis of communication between human beings with the transfer of complex data with variations in thrust, pitch, and tones. The representation and recognition of these analog signals by digital systems prove to be quite exciting and challenging. The spoken language models are converted to digital signals to be identifiable based on different cues like phonetic, prosodic, phonotactic, and lexical features. Voice patterns tend to be specific for every individual with a slight orientation towards the language spoken by the individual of a particular region. While speech patterns tend to alter the meaning of words with tones, high and low pitches in the utterance of the words, NLP tends to learn specific associations of words through vectors. The focus on learned networks in solving the problem of speech synthesis to text with minimal loss and high predictability of syllable of word, sentence, and paraphrase is needed. The creation of a knowledge base corpus of learned variable prosody of features helps in the learnability of interestingness directly without any perturbations. The learning algorithms to realize the degree of understandability of speech with the word, sentence identified, and transcription with substantial noise interference. The transfer of the acoustic features learned by algorithms proves to be quiet challenging as they are distorted by sudden environmental changes. Syllable extracted from the speech translation may or may not represent the Sentiment of the word, with different phonetical modulation. Utilization of the MobileNets and DistillBERT to transfer the language extraction and the edge reducing the time of processing and reducing the corpus of the size, reducing the adversial learning of the voice features and the patterns, reducing the Transfer of learned corpus and patterns.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0055,Affective Prosody Perception and the Relation to Social Competence in Autistic and Typically Developing Children,"Individuals diagnosed with autism spectrum disorder (ASD) have difficulty perceiving and expressing emotions. Since prosodic changes in speech (i.e. changes in intonation, stress, rhythm, etc.) are crucial for extracting information about the emotional state of a speaker, an inability to perceive and interpret these prosodic changes may be related to impairments in social communication. This study used non-verbal emotional voice-clips to examine the ability of autistic and typically-developing children (7-13 years old) to extract affect from changes in prosody. This research also explored whether difficulty extracting affective intent from changes in prosody may be related to social competence. Autistic (n = 26) and typically-developing (n = 26) children accurately matched emotional voice-clips to emotion words, suggesting autistic children can accurately extract the affective meaning conveyed by changes in prosody. Autistic children were less accurate at matching the voice-clips to emotional faces, suggesting that autistic children may struggle to make use of prosodic information in a social context. Across both autistic and typically-developing children, prosody-face matching accuracy was found to predict overall social competence, as well as social inferencing abilities, suggesting that the inability to utilize affective information derived from a speaker's voice may interfere with effective social communication.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0056,Perception of Melodic Contour and Intonation in Autism Spectrum Disorder: Evidence From Mandarin Speakers,"Tone language experience benefits pitch processing in music and speech for typically developing individuals. No known studies have examined pitch processing in individuals with autism who speak a tone language. This study investigated discrimination and identification of melodic contour and speech intonation in a group of Mandarin-speaking individuals with high-functioning autism. Individuals with autism showed superior melodic contour identification but comparable contour discrimination relative to controls. In contrast, these individuals performed worse than controls on both discrimination and identification of speech intonation. These findings provide the first evidence for differential pitch processing in music and speech in tone language speakers with autism, suggesting that tone language experience may not compensate for speech intonation perception deficits in individuals with autism.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0057,Pitch Processing in Tonal-Language-Speaking Children with Autism: An Event-Related Potential Study,"The present study investigated pitch processing in Mandarin-speaking children with autism using event-related potential measures. Two experiments were designed to test how acoustic, phonetic and semantic properties of the stimuli contributed to the neural responses for pitch change detection and involuntary attentional orienting. In comparison with age-matched (6-12 years) typically developing controls (16 participants in Experiment 1, 18 in Experiment 2), children with autism (18 participants in Experiment 1, 16 in Experiment 2) showed enhanced neural discriminatory sensitivity in the nonspeech conditions but not for speech stimuli. The results indicate domain specificity of enhanced pitch processing in autism, which may interfere with lexical tone acquisition and language development for children who speak a tonal language.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0058,Linguistic Tone and Non-Linguistic Pitch Imitation in Children with Autism Spectrum Disorders: A Cross-Linguistic Investigation,"The conclusions on prosodic pitch features in autism spectrum disorders (ASD) have primarily been derived from studies in non-tonal language speakers. This cross-linguistic study evaluated the performance of imitating Cantonese lexical tones and their non-linguistic (nonspeech) counterparts by Cantonese- and Mandarin-speaking children with and without ASD. Acoustic analyses showed that, compared with typically developing peers, children with ASD exhibited increased pitch variations when imitating lexical tones, while performed similarly when imitating the nonspeech counterparts. Furthermore, Mandarin-speaking children with ASD failed to exploit the phonological knowledge of segments to improve the imitation accuracy of non-native lexical tones. These findings help clarify the speech-specific pitch processing atypicality and phonological processing deficit in tone-language-speaking children with ASD.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0059,Neural Processing of Speech Sounds in ASD and First-Degree Relatives,"Efficient neural encoding of sound plays a critical role in speech and language, and when impaired, may have reverberating effects on communication skills. This study investigated disruptions to neural processing of temporal and spectral properties of speech in individuals with ASD and their parents and found evidence of inefficient temporal encoding of speech sounds in both groups. The ASD group further demonstrated less robust neural representation of spectral properties of speech sounds. Associations between neural processing of speech sounds and language-related abilities were evident in both groups. Parent-child associations were also detected in neural pitch processing. Together, results suggest that atypical neural processing of speech sounds is a heritable ingredient contributing to the ASD language phenotype.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0060,Reduced Context Effect on Lexical Tone Normalization in Children with Autism Spectrum Disorder: A Speech-Specific Mechanism,"Existing literature has demonstrated that individuals with autism spectrum disorder (ASD) exhibit atypical use of contextual information in their surroundings. However, there is limited understanding regarding their integration of contextual cues in speech processing. This study aims to explore how Mandarin-speaking children with and without ASD identify lexical tones in speech and nonspeech contexts, and to determine whether the size of context effect would be modulated by children's cognitive abilities. Twenty-five children with ASD and 25 typically developing (TD) children were asked to identify Mandarin lexical tones preceded by three types of contexts (speech, nonspeech, and nonspeech-flattened contexts). We also tested child participants' verbal intelligence, nonverbal intelligence, and working memory capacity. Results revealed that the context effect was only observed in the speech contexts, where Mandarin-speaking children with ASD exhibited a reduced context effect compared to TD children. Moreover, TD children with higher verbal intelligence demonstrated a diminished context effect. However, nonverbal intelligence and working memory capacity were not significantly associated with the size of context effect in either group. These findings revealed a subtle yet important difference between ASD and TD children's utilization of speech contexts in lexical tone identification, and validated a speech-specific mechanism underpinning children's lexical tone normalization.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0061,A cross-linguistic study of Taiwanese tone perception by Taiwanese and English listeners,"The present study investigated tone perception by speakers of Taiwanese Southern Min and those of American English with an AX discrimination task. Two Taiwanese Southern Min tone continua were constructed from natural speech stimuli. One continuum ranged from a high level tone (T55) to a mid level tone (T33), and the other from a high level (T55) to a high falling tone (T51). The results showed that perception by Taiwanese listeners was quasi-categorical for the contour-level tone continuum but mostly continuous for the level tone-level tone one. This suggests that the findings by Abramson (J Acoust Soc Am 61:S66, 1979a; In: Lindblom B, A-hman S (eds) Frontiers of speech communication, 1979b) and Wang (Ann N Y Acad Sci 280:61-72, 1976) and Chan et al. (J Acoust Soc Am 58:S119, 1975) should be seen as complementary to each other rather than contradictory. Differences on tone perception between Taiwanese and English listeners were also found. Taiwanese listeners exhibited a region of higher discriminability on the T55-T51 continuum, while no discrimination peak was observed in English listeners' data. In addition, Taiwanese listeners were more accurate than English listeners in tone discrimination. These results indicate a qualitative difference in lexical tone perception between tone and nontone language listeners: tone language listeners appear to perceive tones as phonemic categories, utilizing cues such as pitch contour, while nontone language listeners rely more on psychoacoustic factors such as pitch height (cf. Hume and Johnson, In: Hume E, Johnson K (eds) The role of perception in phonology, 2001; Hall, et al., J Phonet, 32:395-421, 2004; Huang, In: Trouvain J, Barry W (eds) Proceedings of the 16th international congress of phonetic science, 2007; Huang and Johnson, Phonetica 67:243-267, 2010).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0062,Lexical and metrical stress in word recognition: Lexical or pre-lexical influences?,"The influence of lexical stress and/or metrical stress on spoken word recognition was examined. Two experiments were designed to determine whether response times in lexical decision or shadowing tasks are influenced when primes and targets share lexical stress patterns (JUVenile-BIBlical [Syllables printed in capital letters indicate those syllables receiving primary lexical stress.]). The results did not support an effect of lexical stress on the organization of lexical memory. In Experiment 3 primes and targets whose first syllables shared lexical stress only (MUDdy-PASta), metrical stress only (alTHOUGH-PASta), both cues (LECtern-PASta), or neither cue (conTROL-PASta) revealed no priming effect. However, targets whose first syllables were strong were responded to faster than targets whose first syllables were weak. Experiment 4 manipulated the metrical stress patterns of bi-syllabic primes and targets. Targets with strong-weak metrical stress patterns were responded to more quickly than those with strong-strong or weak-strong patterns. Although the priming paradigm did not reveal an influence of lexical and metrical stress on the organization of lexical memory, the data do support an influence of strong syllables on the processing of auditorily presented words.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0063,Perceptual Recovery from Consonant-Cluster Simplification in Korean Using Language-Specific Phonological Knowledge,"Two experiments examined whether perceptual recovery from Korean consonant-cluster simplification is based on language-specific phonological knowledge. In tri-consonantal C1C2C3 sequences such as /lkt/ and /lpt/ in Seoul Korean, either C1 or C2 can be completely deleted. Seoul Koreans monitored for C2 targets (/p/ or /k/, deleted or preserved) in the second word of a two-word phrase with an underlying /l/-C2-/t/ sequence. In Experiment 1 the target-bearing words had contextual lexical-semantic support. Listeners recovered deleted targets as fast and as accurately as preserved targets with both Word and Intonational Phrase (IP) boundaries between the two words. In Experiment 2, contexts were low-pass filtered. Listeners were still able to recover deleted targets as well as preserved targets in IP-boundary contexts, but better with physically-present targets than with deleted targets in Word-boundary contexts. This suggests that the benefit of having target acoustic-phonetic information emerges only when higher-order (contextual and phrase-boundary) information is not available. The strikingly efficient recovery of deleted phonemes with neither acoustic-phonetic cues nor contextual support demonstrates that language-specific phonological knowledge, rather than language-universal perceptual processes which rely on fine-grained phonetic details, is employed when the listener perceives the results of a continuous-speech process in which reduction is phonetically complete.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0064,The Effect of Non-sentential Context Prosody on Homographs' Lexical Activation in Persian,"This study examines the effect of non-sentential context prosody pattern on lexical activation in Persian. For this purpose a questionnaire including target and non-target words is used. The target words are homographs with two possible stress patterns belonging to different syntactic categories. Participants are asked to read out the words aloud and note the first word that comes to their mind. The results show that by reading the target words, both meanings of the target words are activated in mind and the prosodic pattern of the non-sentential preceding context does not affect the activation of the other stress pattern meaning. This result suggests that the metrical prosodic pattern of non-sentential context is not a strong constraint to determine which meaning of the target word must be activated. The experiment also illustrates that the stress pattern used to read the target words does not necessarily matches the stress pattern of the target word which relates to the written word. These findings confirm Swinney (Verb Learn Verb Behav 18:645-665, 1979) and Elston-Guttler and Friederici's (J Mem Lang 52(2):256-283, 2005) finding that both meanings of an ambiguous word are accessed at the first stage. This study shows that in lack of semantic context, Persian natives behave homographs as ambiguous words and there is no bias towards preferring one meaning over another.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0065,"Using Sarcasm to Compliment: Context, Intonation, and the Perception of Statements with a Negative Literal Meaning","The present study extended findings of contrast effects in an auditory sarcasm perception task manipulating context and tone of voice. In contrast to previous research that had used sarcastic and sincere statements with a positive literal meaning, the present experiment examined how statements with a negative literal meaning would affect the results. Eighty-four undergraduate students completed a task in which an ambiguous, positive, or negative computer-generated context spoken in a flat emotional tone was followed by a statement with a negative literal meaning spoken in a sincere or sarcastic tone of voice. Results for both the proportion of sarcastic responses and response time showed a significant context by tone interaction, reflecting relatively fast sarcastic responses for the situation in which sarcasm would turn the statement into a compliment (positive context, sarcastic intonation) and fast sincere responses when the literal insult was emphasized (negative context, sincere intonation). However, the ambiguous context produced a pattern of results modulated by the tone of voice that was similar to that observed when the context/intonation pairing could not be interpreted as a compliment or an insult (negative context/sarcastic intonation or positive context/sincere intonation). These findings add to the body of literature suggesting that situational contrast, context, and intonation influence how sarcasm is perceived while demonstrating the importance of the literal meaning in sarcasm perception. They can be interpreted in the context of models of sarcasm comprehension that postulate two stages of processing.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0066,Prosodic Disambiguation of Morphological Ambiguities in Turkish,"This study investigates the production and processing of lexical prosody in morphological ambiguities in Turkish. Native speakers of Turkish took part in two read-aloud and two lexical decision experiments. The results showed that in speaking, for both genuine and pseudo words that contrasted in stress, participants changed the fundamental frequency (F0) and intensity to disambiguate; and they changed duration (but not F0 or intensity) to disambiguate words and pseudo-words that did not contrast in stress. In listening, the participants were sensitive to the prosodic (mis)match in stress-contrasting pairs, but not to durational (mis)match presumably because the durational differences between the comparison pairs were shorter than perceivable. The findings show that Turkish speakers use prosody to disambiguate morphologically ambiguous word pairs and that they are sensitive to prosodic cues (at least to those used in stress contrast) when they hear them. Their behavior for pseudo-words suggests that they do so not on the basis of individual word knowledge but productively. The comparison pairs in the current study were segmentally identical, allowing us to attribute the observed prosodic variation only to the morpho-syntactic structure of the ambiguous pairs.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0067,Contributions of Language-Specific and Domain-General Cognitive Abilities to the Comprehension of Focus Prosody in Jianghuai Mandarin: Effects of Age,"A growing body of research has explored the cognitive factors influencing aging adults' recognition of spoken words and phrases. In contrast, research on the cognitive contributions to speech prosody comprehension in tonal languages across adulthood remains relatively limited. This study aimed to bridge this gap by investigating the contributions of language-specific and domain-general cognitive factors to focus prosody comprehension performance among aging speakers of Jianghuai Mandarin. Young, middle-aged, and older healthy native speakers of Jianghuai Mandarin (N = 30 per group) performed a focus comprehension task, where they inferred the underlying intentions conveyed by different conditions (initial focus, medial focus, and final focus) of focus prosody. They also completed a series of language-specific (acoustic representation, meaning categorization, and focus knowledge) and domain-general (inhibitory control, attention switching, and working memory) cognitive assessments pertinent to understanding focus prosody. Findings showed an age-related decline in the comprehension of focus prosody, along with different rates of reduction in language-specific and domain-general cognitive abilities. These cognitive abilities did not modulate the focus comprehension performance among the young and middle-aged groups. In the older group, however, positive associations were observed between focus comprehension performance and certain domain-general abilities, as evidenced by the strong predictive power of attention switching and working memory. The findings provide insights into the mechanisms underpinning linguistic prosody processing among aging adults.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0068,Aspectual and focus adverbs in English and Korean,"This article presents a comparative semantic analysis of the aspectual and focus adverbs already, still and STILL in English and imi/pelsse 'already' and acik/yothay 'still' in Korean based on their presuppositions and their focus interpretation. I argue that the two contrasting views of aspectual adverbs as logical duals (Lobner 1989, 1999) and as scalar (focus) particles (Michaelis 1993, 1996; Israel 1995) are both necessary in order to explain the English and Korean data. Aspect concerns the internal structure of events, relating a current state with the onset or the end of the state. These transitions are available for focusing, which triggers an explicit contrast between the asserted state and an alternative state with an opposite polarity. Korean is shown to lexicalize aspectual and focus adverbs differently from what is expressed in English by a single adverb with focus marked prosody. The meaning of aspectual and focus adverbs in both English and Korean is representated in Discourse Representation Theory (Kamp and Reyle 1993; van Eijck and Kamp 1997).",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0069,On the prosody and syntax of DPs: evidence from Italian noun adjective sequences,"This study tests a syntactic property-namely the availability of N- vs. NP-raising in DPs-through prosodic means. The opposition between N- and NP-raising is central to the ongoing debate about the internal representation of DPs, yet it often eludes testing by syntactic means alone. As we show in this study, the two syntactic hypotheses are instead neatly distinguished by the distinct prosodic phrasing predicted by each operation. In this paper, we present the results of an empirical experiment designed to test the prosodic phrasing of N-A and A-N sequences in Italian and the corresponding syntactic implications. As prosodic cues, we use syllabic and word lengthening effects induced by phonological phrase boundaries. According to our results, A and N share the same phonological phrase in both orders. Regarding the syntactic implications of this finding, we show that under all current models of syntax-prosody mapping the underlying syntactic structure responsible for the attested prosodic phrasing must necessarily rely on N-raising. Finally, we propose an analysis of Italian DPs where the N-raising operation found necessary in light of the attested prosodic phrasing is reconciled with the evidence for DP-internal phrasal movement discussed in Cinque (2005 2006).",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0070,Prosody and recursion in coordinate structures and beyond,"Generalizations about relative prosodic boundary strength are recursive. Initial evidence comes from the fragment of English consisting only of proper names and and and or. A systematic relation between the semantics, the syntactic combinatorics, and the prosodic phrasing of coordinate structures can be captured by recursively building up their prosody, in tandem with assembling their compositional meaning. Alternative edge-based approaches to prosodic phrasing fail to capture the recursive nature of the generalization, a result independent of whether or not prosodic representation itself is assumed to be recursive. The pattern generalizes beyond the grammar of coordination, despite two types of apparent counterexamples: Structures that are prosodically flat but syntactically articulated, and structures with an apparent outright mismatch between prosody and syntax. Closer inspection suggests that the syntax might actually be quite in tune with prosody. In both cases, natural language employs strategies to construe complex meaning with list-like structures rather than nested ones. The privileged status of lists may be due to processing factors.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0071,Focus inwh-questions Evidence from Italian,"This paper addresses two long-standing issues concerning focus: first, the question of whether the focal interpretation is directly read off the prosodic structure of a sentence, or it is rather mediated by a [focus] feature encoded in the syntactic representation; second, whether interrogativewh-phrases are inherently endowed with a [focus] feature. We provide evidence from two prosodic experiments on directwh-questions in Italian, showing that the Nuclear Pitch Accent (NPA) and main stress fall on the lexical verb, without a concomitant focal interpretation of the latter. Furthermore, we show that NPA assignment is sensitive to the derivational history of thewh-phrase under short-distance vs. long-distance extraction. We account for the observed NPA distribution in terms of a [focus] feature which is bundled with the [wh] in direct questions, and is specified on each phase head that hosts in its edge one link of thewh-chain. Thus, v degrees is specified for the feature bundle {wh, focus} and attracts the assignment of the NPA, which is then realized on the lexical verb. Our findings, thus, cast doubt on the direct association between prosodic prominence and a focal interpretation.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0072,"Auditory processing, linguistic prosody awareness, and word reading in Mandarin-speaking children learning English","This study examined language-specific links among auditory processing, linguistic prosody awareness, and Mandarin (L1) and English (L2) word reading in 61 Mandarin-speaking, English-learning children. Three auditory discrimination abilities were measured: pitch contour, pitch interval, and rise time (rate of intensity change at tone onset). Linguistic prosody awareness was measured three ways: Mandarin tone perception, English stress perception, and English stress production. A Chinese character recognition task was the Mandarin L1 reading metric. English L2 word reading was assessed by English real word reading and nonword decoding tasks. The importance of the auditory processing measures to reading was different in the two languages. Pitch contour discrimination predicted Mandarin L1 word reading and rise time discrimination predicted English L2 word reading, after controlling for age and nonverbal IQ. For the prosodic and phonological measures, Mandarin tone perception, but not rhyme awareness, predicted Chinese character recognition after controlling for age and nonverbal IQ. In contrast, English rhyme awareness predicted more unique variance in English real word reading and nonword decoding than did English stress perception and production. Linguistic prosody awareness appears to play a more important role in L1 word reading than phonological awareness; while the reverse seems true for English L2 word reading in Mandarin-speaking children. Taken together, auditory processing, language-specific linguistic prosody awareness, and phonological awareness play different roles in L1/L2 reading, reflecting different prosodic and segmental structures between Mandarin and English.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0073,"Mandarin-speaking preschoolers' pitch discrimination, prosodic and phonological awareness, and their relation to receptive vocabulary and reading abilities","Cross-linguistic studies have reported that prosodic pattern awareness (e.g., lexical stress and lexical tone) is more important to reading acquisition than phonological awareness. However, few longitudinal studies have been conducted to explore the relations between these variables. This study examined preschoolers' pitch discrimination, prosodic and phonological awareness, and their connection to receptive vocabulary in preschool and reading abilities in first grade. Findings reveal (1) children improve their pitch discrimination and prosodic awareness from preschool to fourth grade; (2) pitch interval discrimination (frequency separation between tones) contributes to receptive vocabulary whereas pitch contour discrimination (patterns of rising and falling pitch) predicts word reading; (3) phonological awareness accounts for more variability in receptive vocabulary than prosodic awareness; whereas the reverse was found for word reading and reading comprehension. Together, prosody and its acoustic cue (i.e., pitch) play a vital role in learning to read Mandarin.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0074,Script relativity hypothesis: evidence from reading with different spatial layouts and varied lexical tone,"A contemporary question is whether the script we read in affects our cognition, termed the script relativity hypothesis (Pae in: Script effects as the hidden drive of the mind, cognition, and culture, Springer, Berlin, 2020). The aim of this review is to examine variation in spatial layout (interword spaces and linear-nonlinear configuration) and representation of lexical tone across scripts and whether disparities in those features affect cognition. Both script features are strong candidates for potentially producing script relativity effects. Readers of densely crowded nonlinear scripts (e.g., Thai, Sinhala) may have heightened visuo-perceptual abilities in comparison to readers of linear scripts (e.g., Roman script). Tonal languages vary in terms of both their relative complexity and whether they orthographically encode this feature in their script. This variation may produce differences in sensitivity to tone perception and auditory perceptual skills in readers of tonal languages that do and do not orthographically represent tone in the script and in contrast to readers of non-tonal languages. The empirical research reviewed tends to support a weaker version of the script relativity hypothesis, where there is a channeling effect on attention due to script-specific features while actually reading. The question is still open to debate as to whether this attention allocation translates into more profound, nonlinguistic cognitive consequences. Notably, the research reviewed was not specifically designed to investigate the script relativity hypothesis. In order to investigate longer-term cognitive consequences of this script variation, carefully designed studies need to be conducted with this overriding goal in mind. Future research needs to include other lesser studied languages and their scripts so that we can ascertain what are common cognitive patterns or processes and what are shaped by variation in script-specific features.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0075,Context Effect in the Categorical Perception of Mandarin Tones,"The categorical perception of tones is based not only on word-internal F0 cues but also on external F0 cues in the contexts. The present study focuses on the effects of different types of preceding contexts on Mandarin tone perception. In the experiment, subjects were required to identify a target tone with the preceding context. The target tone was from a tone continuum ranging from Mandarin Tone 1 (high-level tone) to Tone 2 (mid-rising tone). It was preceded by four types of contexts (normal speech, reversal speech, fine-structure sound, and non-speech) with different mean F0 values. Results indicate that the categorical perception of Mandarin tones is influenced only by the normal speech context, and the effect is contrastive. For instance, in a normal speech context with a higher mean F0, the following tone is more likely to be perceived as a lower frequency tone (Tone 2), whereas with a lower mean F0, the following tone is more likely to be perceived as a higher frequency tone (Tone 1). These findings suggest that Mandarin tone normalization is mediated by speech-specific processes and that the speech context needs to be intelligible.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0076,Surface Electromyographic Activity of Extrinsic Laryngeal Muscles in Cantonese Tone Production,"Patients after total laryngectomy lose their ability to speak. Electrolarynx is a commonly used electronic device that helps these patients to verbally communicate. However, existing electrolarynx systems do not provide pitch control function, which is critical in speech communication especially for tonal languages. This study investigated the surface electromyographic (sEMG) activity of extrinsic laryngeal muscles in producing speech sounds of different pitches by normal speakers. In particular, the sEMG signals for producing different lexical tones of Cantonese were extracted and analyzed. The experimental results on Cantonese tone production confirmed that the sEMGsignal from sternocleidomastoid muscle can be used to differentiate high-pitch tones from low-pitch tones. This reveals the potential of developing pitch-controlled EL systems for laryngectomees who speak Cantonese and other tonal languages.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0077,Multimodal user's affective state analysis in naturalistic interaction,"Affective and human-centered computing have attracted an abundance of attention during the past years, mainly due to the abundance of environments and applications able to exploit and adapt to multimodal input from the users. The combination of facial expressions with prosody information allows us to capture the users' emotional state in an unintrusive manner, relying on the best performing modality in cases where one modality suffers from noise or bad sensing conditions. In this paper, we describe a multi-cue, dynamic approach to detect emotion in naturalistic video sequences, where input is taken from nearly real world situations, contrary to controlled recording conditions of audiovisual material. Recognition is performed via a recurrent neural network, whose short term memory and approximation capabilities cater for modeling dynamic events in facial and prosodic expressivity. This approach also differs from existing work in that it models user expressivity using a dimensional representation, instead of detecting discrete 'universal emotions', which are scarce in everyday human-machine interaction. The algorithm is deployed on an audiovisual database which was recorded simulating human-human discourse and, therefore, contains less extreme expressivity and subtle variations of a number of emotion labels. Results show that in turns lasting more than a few frames, recognition rates rise to 98%.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0078,Uncertainty of Vowel Predictions as a Digital Biomarker for Ataxic Dysarthria,"Dysarthria is a common manifestation across cerebellar ataxias leading to impairments in communication, reduced social connections, and decreased quality of life. While dysarthria symptoms may be present in other neurological conditions, ataxic dysarthria is a perceptually distinct motor speech disorder, with the most prominent characteristics being articulation and prosody abnormalities along with distorted vowels. We hypothesized that uncertainty of vowel predictions by an automatic speech recognition system can capture speech changes present in cerebellar ataxia. Speech of participants with ataxia (N=61) and healthy controls (N=25) was recorded during the ""picture description"" task. Additionally, participants' dysarthric speech and ataxia severity were assessed on a Brief Ataxia Rating Scale (BARS). Eight participants with ataxia had speech and BARS data at two timepoints. A neural network trained for phoneme prediction was applied to speech recordings. Average entropy of vowel tokens predictions (AVE) was computed for each participant's recording, together with mean pitch and intensity standard deviations (MPSD and MISD) in the vowel segments. AVE and MISD demonstrated associations with BARS speech score (Spearman's rho=0.45 and 0.51), and AVE demonstrated associations with BARS total (rho=0.39). In the longitudinal cohort, Wilcoxon pairwise signed rank test demonstrated an increase in BARS total and AVE, while BARS speech and acoustic measures did not significantly increase. Relationship of AVE to both BARS speech and BARS total, as well as the ability to capture disease progression even in absence of measured speech decline, indicates the potential of AVE as a digital biomarker for cerebellar ataxia.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0079,Narratives: A Functional-Pragmatic Perspective,"This paper investigates narratives from a functional pragmatic view. Telling a story is a complex pattern of discourse consisting of speech actions, procedures and linguistic means like temporal organization, personal deixis, representation of direct speech and intonation. The relation between self as teller and self as actor sheds light on the constitution of image by telling a story. The method of analysis is applied to a narrative of everyday life. The purpose of telling is to participate in the experience of others and to constitute a shared communicative world.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0080,"""They Made us into a Race. We Made Ourselves into a People"": A Corpus Study of Contemporary Black American Group Identity in the Non-Fictional Writings of Ta-Nehisi Coates","This article examines representations of contemporary Black American identity in the non-fictional writings of Ta-Nehisi Coates. The dataset is a self-compiled specialized corpus of Coates's non-fictional writings from 1996 until 2018 (350 texts; 468,899 words). The study utilizes an interdisciplinary approach combining corpus linguistics and corpus pragmatics. Frequencies of five identity-related terms in the corpus (African(-)Americans, blacks, black people, black America/Americans and black community/communities) are compared diachronically; then the pragmatic prosody of the terms is analyzed via the notion of control. The findings suggest that Coates's representation of Black American group identity has shifted over time. Specifically, the terms African Americans and black America are replaced by the terms blacks and black people. The study's empirical findings, considered through the theoretical framework on Black solidarity, suggest a shift in representation of group identity in Coates's writings from an identity based on cultural and ethnic commonalities to an identity based on the shared experiences of anti-Black racism.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0081,A PERCEPTUALLY-BASED MODEL OF CHILDRENS EARLIEST PRODUCTIONS,"A model is proposed to account for processes underlying the initial extraction and representation of words. The model incorporates perceptual salience into a framework provided by autosegmental phonology. In one study, predictions of the model were tested in a corpus of utterances obtained from three children in the one-word speech period. Analyses of the corpus supported the predictions, suggesting that salience of elements such as stressed and final syllables may contribute to the form of early productions and, specifically, to the form of utterances containing filler syllables and full or partial reduplications. Because the data for this study were children's productions, and the model concerns children's representations, a second study was carried out to investigate representations somewhat more directly. That study also explored the possible influence of an additional prosodic factor on the form of early words. A word-learning task with 2-year-olds, 3-year-olds and adults assessed whether children would attend to stress pattern or segmental sequence in identifying the referent for a word. As expected, children did rely on prosody in their word choices far more frequently than did adults, suggesting that one prosodic component, stress pattern, may in some cases be prominent in a child's representation for a word. The results of the two studies provide support for the utility of the autosegmental framework, as well as additional evidence for the perceptual salience of stressed and final syllables and of stress pattern.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0082,WHEN PROSODY FAILS TO CUE SYNTACTIC STRUCTURE - 9-MONTH-OLDS SENSITIVITY TO PHONOLOGICAL VERSUS SYNTACTIC PHRASES,"According to prosodic bootstrapping accounts of syntax acquisition, language learners use the correlation between syntactic boundaries and prosodic changes (e.g., pausing, vowel lengthening, large increases or decreases in fundamental frequency) to cue the presence and arrangement of syntactic constituents. However, recent linguistic accounts suggest that prosody does not directly reflect syntactic structure but rather is governed by independent prosodic units such as phonological phrases. To examine the implications of this view for the prosodic bootstrapping hypothesis, infants in Experiment 1 were presented with sentences in which pauses were inserted either between the subject noun phrase (NP) and verb or after the verb. Half of the infants heard sentences with lexical NP subjects, in which prosodic structure is consistent with syntactic structure. The other half heard sentences with pronoun subjects, in which prosodic structure does not mirror syntactic structure. In a preferential listening paradigm, infants in the lexical NP condition listened longer to materials containing pauses between the subject an verb, the main syntactic constituents. However, in the pronoun NP condition, infants showed no difference in listening times for the two pause locations. To determine if other sentence types containing pronoun subjects potentially provide information about the syntactic constituency of these elements, infants in Experiment 2 heard yes-no questions with pronoun subjects, in which the prosodic structure reflects the constituency of the subject, Infants listened longer when pauses were inserted between the subject and verb than after the verb. Taken together, our results suggest that the prosodic information in an individual sentence is not always sufficient to assign a syntactic structure. Rather, learners must engage in active inferential processes, using cross-sentence comparisons and other types of information to arrive at the correct syntactic representation.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0083,MUSICAL PRIMING BY THE RIGHT-HEMISPHERE POST-CALLOSOTOMY,The hemispheric representation of auditory functions mediating the perception of harmony in music was investigated in two split-brain patients using a musical chord priming task. Previous experiments in normal subjects had demonstrated that the harmonic context established by a prime chord influences the accuracy of target chord intonation judgements. Only the right hemisphere of each callosotomy patient manifested the normal interaction between harmonic relatedness and intonation. The results raise the possibility that associative auditory functions which generate expectancies for harmonic progression in music are lateralized within the right hemisphere.,,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0084,INTERSPEAKER AND INTRASPEAKER VARIABILITY IN FUNDAMENTAL-FREQUENCY OF THAI TONES,"A measure was obtained of variability in fundamental frequency (F0) in citation forms of lexical tones. The language selected for investigation was Thai, a tone language with five lexical tones: mid, low, falling, high and rising. Twenty speakers participated in the experiment: 10 ''young'' male speakers and 10 ''old'' speakers, 5 male and 5 female. High-quality tape recordings were obtained of each subject's productions of a minimal set of five monosyllabic words. F0 contours were extracted by a cepstral analysis. A comparison was made of inter- and intraspeaker variability in the production of the five Thai tones. Results of analysis of variance indicated that the degree of intersubject variability in F0 was greater than intraspeaker across all five tones, that young and old speakers exhibited the same pattern of variability, and that variability in tone production differed depending on the lexical tone. The falling and rising tones exhibited smaller degrees of variability than the mid, low or high. Findings are interpreted to highlight the nature of F0 variability, the relationship of F0 variability to amount of F0 movement, and crosslinguistic differences in F0 variability as a function of prosodic structure.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0085,MACRO AND MICRO FEATURES FOR AUTOMATED PRONUNCIATION IMPROVEMENT IN THE SPELL SYSTEM,"In this paper, the analysis of macro (prosodic) and micro (segmental) features is described for a workstation designed to improve the pronunciation of English, French and Italian by non-native speakers. The SPELL workstation is intended to be a teaching device aimed at intermediate ability foreign language learners. Audio and visual aids will be used to help students improve their general intelligibility within a basic teaching paradigm called DELTA (Demonstrate, Evaluate Listening, Teach and Assess). Prosodic analysis will apply to the features of intonation, stress and rhythm. A phonological approach is used for intonation which provides a well-structured system of contrasting units that correlate with discrete linguistic functions. A more limited approach to the prosodic phonology of stress and rhythm will be taught in the SPELL system by manipulating the relatively simple acoustic features of vowel quality and segmental duration. The micro feature analysis will focus on the segmental class of vowels. A distinctive feature approach is used to characterize non-native vowel pronunciation. Acoustic properties are sought which will be speaker-independent.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0086,DETACHMENT IN CATALAN AND INFORMATION PACKAGING,"The existing functional analyses of right-detachment constructions, the afterthought approach and the topic and topicality family of proposals, are inadequate to deal with right-detachment in Catalan. This paper argues that right-detachment in Catalan plays a specific role in the structural representation of information packaging. In particular, it encodes a tailful instruction in the sense of Vallduvi (1992). Right-detachment is contrasted with left-detachment and with pronominalization. Finally, it is pointed out that the association between a particular form and a particular function need not be constant across languages: the discourse function effected by right-detachment in Catalan can be expressed exclusively by prosodic means in English.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0087,The social representation of 'women' on X platform before and after the launch of Saudi vision 2030,"This study investigates how Saudi males and females represent ""women"" on X (formerly Twitter), focusing on two distinct timeframes: 2015 (before Vision 2030) and 2022 (after Vision 2030). By integrating applied Corpus Linguistics (CL) and Critical Discourse Analysis (CDA), the research examines a corpus of 10,000 Arabic tweets (equally divided between male and female authors), thereby illuminating how broader social reforms correspond with shifts in online discourse. Specifically, we apply frequency counts, collocation analysis, and semantic prosody techniques in order to compare lexical choice, thematic focus, and evaluative stands in relation to Saudi women during both phases. The findings reveal a discernible positive shift in attitudes after the official publication of Vision 2030. In 2015, the discourse was more likely to be about ""spinsterhood,"" boycotts, and guardianship, reflecting predominantly negative or restrictive portrayals of women. By 2022, tweets became more likely to be about empowerment, achievements, and national pride, suggesting changing social attitudes that increasingly legitimize women's roles in workplaces, education, and public life. Although pockets of negativity persist-particularly in certain domains such as sports-these pockets of resistance are outnumbered by the overall trend towards more inclusive and celebratory discourses. These results highlight how top-down reforms, such as the lifting of the driving ban and the promotion of women's employment, have reshaped Saudi women's discourse. Beyond its sociolinguistic and critical discourse studies contribution, this research highlights the power of large-scale policy changes in achieving shifts in everyday language and attitudes in conservative societies.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0088,Emotion-aware cross-modal music generation based on multimodal emotion recognition,"This study presents a Multimodal Emotion-guided Music Generation Model (MEMGM) that directly translates multimodal affective states into instrumental music. Unlike prior approaches relying on textual intermediaries or discrete categories, MEMGM integrates facial expressions, speech prosody, and ECG-derived heart signals into a continuous valence-arousal-dominance (VAD) space for fine-grained emotion representation. A new MIDI-based dataset with continuous VAD annotations is constructed, enabling controllable manipulation of musical elements and nuanced mapping of emotional dynamics. The framework combines a cross-modal Transformer, multimodal BiLSTM, and gating block for robust affective recognition, followed by a plug-and-play generation module enhanced with supervised contrastive learning to ensure intra- and inter-modal consistency. Experimental results demonstrate superior performance in both emotion recognition and music generation compared to state-of-theart baselines. Subjective evaluations confirm that the generated music aligns closely with users' perceived emotions. The main contributions are: (1) an end-to-end pathway from multimodal signals to music without textual intermediaries, (2) a VAD-annotated MIDI dataset for expressive generation, and (3) a contrastive learning strategy that enhances affective alignment across modalities.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0089,Preserved appreciation of aesthetic elements of speech and music prosody in an amusic individual: A holistic approach,"We present a follow-up study on the case of a Greek amusic adult, B.Z., whose impaired performance on scale, contour, interval, and meter was reported by Paraskevopoulos, Tsapkini, and Peretz in 2010, employing a culturally-tailored version of the Montreal Battery of Evaluation of Amusia. In the present study, we administered a novel set of perceptual judgement tasks designed to investigate the ability to appreciate holistic prosodic aspects of 'expressiveness' and emotion in phrase length music and speech stimuli. Our results show that, although diagnosed as a congenital amusic, B.Z. scored as well as healthy controls (N = 24) on judging 'expressiveness' and emotional prosody in both speech and music stimuli. These findings suggest that the ability to make perceptual judgements about such prosodic qualities may be preserved in individuals who demonstrate difficulties perceiving basic musical features such as melody or rhythm. B.Z.'s case yields new insights into amusia and the processing of speech and music prosody through a holistic approach. The employment of novel stimuli with relatively fewer non-naturalistic manipulations, as developed for this study, may be a useful tool for revealing unexplored aspects of music and speech cognition and offer the possibility to further the investigation of the perception of acoustic streams in more authentic auditory conditions.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0090,Musical training alters neural processing of tones and vowels in classic Chinese poems,"Long-term rigorous musical training promotes various aspects of spoken language processing. However, it is unclear whether musical training provides an advantage in recognizing segmental and suprasegmental infor-mation of spoken language. We used vowel and tone violations in spoken unfamiliar seven-character quatrains and a rhyming judgment task to investigate the effects of musical training on tone and vowel processing by recording ERPs. Compared with non-musicians, musicians were more accurate and responded faster to incorrect than correct tones. Musicians showed larger P2 components in their ERPs than non-musicians during both tone and vowel processing, revealing increased focused attention on sounds. Both groups showed enhanced N400 and LPC for incorrect vowels (vs. correct vowels) but non-musicians showed an additional P2 effect for vowel vio-lations. Moreover, both groups showed enhanced LPC for incorrect tones (vs. correct tones) but only non -musicians showed an additional N400 effect for tone violations. These results indicate that vowel/tone pro-cessing is less effortful for musicians (vs. non-musicians). Our study suggests that long-term musical training facilitates speech tone and vowel processing in a tonal language environment by increasing the attentional focus on speech and reducing demands for detecting incorrect vowels and integration costs for tone changes.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0091,Relative influence of musical and linguistic experience on early cortical processing of pitch contours,"To assess domain specificity of experience-dependent pitch representation we evaluated the mismatch negativity (MMN) and discrimination judgments of English musicians, English nonmusicians, and native Chinese for pitch contours presented in a nonspeech context using a passive oddball paradigm. Stimuli consisted of homologues of Mandarin high rising (T2) and high level (T1) tones, and a linear rising ramp (T2L). One condition involved a between-category contrast (T1/T2), the other, a within-category contrast (T2L/T2). Irrespective of condition, musicians and Chinese showed larger MMN responses than nonmusicians; Chinese larger than musicians. Chinese, however, were less accurate than nonnatives in overt discrimination of T2L and T2. Taken together, these findings suggest that experience-dependent effects to pitch contours are domain-general and not driven by linguistic categories. Yet specific differences in long-term experience in pitch processing between domains (music vs. language) may lead to gradations in cortical plasticity to pitch contours. (c) 2008 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0092,Language-dependent pitch encoding advantage in the brainstem is not limited to acceleration rates that occur in natural speech,"Experience-dependent enhancement of neural encoding of pitch in the auditory brainstem has been observed for only specific portions of native pitch contours exhibiting high rates of pitch acceleration, irrespective of speech or nonspeech contexts. This experiment allows us to determine whether this language-dependent advantage transfers to acceleration rates that extend beyond the pitch range of natural speech. Brainstem frequency-following responses (FFRs) were recorded from Chinese and English participants in response to four, 250-ms dynamic click-train stimuli with different rates of pitch acceleration. The maximum pitch acceleration rates in a given stimulus ranged from low (0.3 Hz/ms; Mandarin Tone 2) to high (2.7 Hz/ms; 2 octaves). Pitch strength measurements were computed from the FFRs using autocorrelation algorithms with an analysis window centered at the point of maximum pitch acceleration in each stimulus. Between-group comparisons of pitch strength revealed that Chinese exhibit more robust pitch representation than English across all four acceleration rates. Regardless of language group, pitch strength was greater in response to acceleration rates within or proximal to natural speech relative to those beyond its range. Though both groups showed decreasing pitch strength with increasing acceleration rates, pitch representations of the Chinese group were more resistant to degradation. FFR spectral data were complementary across acceleration rates. These findings demonstrate that perceptually salient pitch cues associated with lexical tone influence brainstem pitch extraction not only in the speech domain, but also in auditory signals that clearly fall outside the range of dynamic pitch that a native listener is exposed to. (C) 2010 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0093,Functional ear (a)symmetry in brainstem neural activity relevant to encoding of voice pitch: A precursor for hemispheric specialization?,"Pitch processing is lateralized to the right hemisphere; linguistic pitch is further mediated by left cortical areas. This experiment investigates whether ear asymmetries vary in brainstem representation of pitch depending on linguistic status. Brainstem frequency-following responses (FFRs) were elicited by monaural stimulation of the left and right ear of 15 native speakers of Mandarin Chinese using two synthetic speech stimuli that differ in linguistic status of tone. One represented a native lexical tone (Tone 2: 12); the other, T2', a nonnative variant in which the pitch contour was a mirror image of T2 with the same starting and ending frequencies. Two 40-ms portions of f(0) contours were selected in order to compare two regions (R1, early: R2 late) differing in pitch acceleration rate and perceptual saliency. In R2, linguistic status effects revealed that T2 exhibited a larger degree of FFR rightward ear asymmetry as reflected in f(0) amplitude relative to T2'. Relative to midline (ear asymmetry = 0), the only ear asymmetry reaching significance was that favoring left ear stimulation elicited by T2'. By left- and right-ear stimulation separately, FFRs elicited by T2 were larger than T2' in the right ear only. Within T2', FFRs elicited by the earlier region were larger than the later in both ears. Within T2, no significant differences in FFRS were observed between regions in either ear. Collectively, these findings support the idea that origins of cortical processing preferences for perceptually-salient portions of pitch are rooted in early, preattentive stages of processing in the brainstem. (C) 2011 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0094,Time-driven effects on parsing during reading,"The phonological trace of perceived words starts fading away in short-term memory after a few seconds. Spoken utterances are usually 2-3 s long, possibly to allow the listener to parse the words into coherent prosodic phrases while they still have a clear representation. Results from this brain potential study suggest that even during silent reading, words are organized into 2-3 s long 'implicit' prosodic phrases. Participants read the same sentences word by word at different presentation rates. Clause-final words occurring at multiples of 2-3 s from sentence onset yielded increased positivity, irrespective of presentation rate. The effect was interpreted as a closure positive shift (CPS), reflecting insertion of implicit prosodic phrase boundaries every 2-3 s. Additionally, in participants with low working memory span, clauses over 3 s long produced a negativity, possibly indicating increased working memory load. (C) 2012 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0095,Achieving constancy in spoken word identification: Time course of talker normalization,"This event-related potential (ERP) study examines the time course of context-dependent talker normalization in spoken word identification. We found three ERP components, the N1 (100-220 ms), the N400 (250-500 ms) and the Late Positive Component (500-800 ms), which are conjectured to involve (a) auditory processing, (b) talker normalization and lexical retrieval, and (c) decisional process/lexical selection respectively. Talker normalization likely occurs in the time window of the N400 and overlaps with the lexical retrieval process. Compared with the nonspeech context, the speech contexts, no matter whether they have semantic content or not, enable listeners to tune to a talker's pitch range. In this way, speech contexts induce more efficient talker normalization during the activation of potential lexical candidates and lead to more accurate selection of the intended word in spoken word identification. (c) 2013 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0096,Cortical pitch response components show differential sensitivity to native and nonnative pitch contours,"The aim of this study is to evaluate how nonspeech pitch contours of varying shape influence latency and amplitude of cortical pitch-specific response (CPR) components differentially as a function of language experience. Stimuli included time-varying, high rising Mandarin Tone 2 (T2) and linear rising ramp (Linear), and steady-state (Flat). Both the latency and magnitude of CPR components were differentially modulated by (i) the overall trajectory of pitch contours (time-varying vs. steady-state), (ii) their pitch acceleration rates (changing vs. constant), and (iii) their linguistic status (lexical vs. non-lexical). T2 elicited larger amplitude than Linear in both language groups, but size of the effect was larger in Chinese than English. The magnitude of CPR components elicited by T2 were larger for Chinese than English at the right temporal electrode site. Using the CPR, we provide evidence in support of experience-dependent modulation of dynamic pitch contours at an early stage of sensory processing. (C) 2014 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0097,Word tones cueing morphosyntactic structure: Neuroanatomical substrates and activation time-course assessed by EEG and fMRI,"Previous studies distinguish between right hemisphere-dominant processing of prosodic/tonal information and left-hemispheric modulation of grammatical information as well as lexical tones. Swedish word accents offer a prime testing ground to better understand this division. Although similar to lexical tones, word accents are determined by words' morphosyntactic structure, which enables listeners to use the tone at the beginning of a word to predict its grammatical ending. We recorded electrophysiological and hemodynamic brain responses to words where stem tones matched or mismatched inflectional suffixes. Tones produced brain potential effects after 136 ms, correlating with subject variability in average BOLD in left primary auditory cortex, superior temporal gyrus, and inferior frontal gyrus. Invalidly cued suffixes activated the left inferior parietal lobe, arguably reflecting increased processing cost of their meaning. Thus, interaction of word accent tones with grammatical morphology yielded a rapid neural response correlating in subject variability with activations in predominantly left-hemispheric brain areas. (C) 2015 The Authors. Published by Elsevier Inc.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0098,Neural encoding of the speech envelope by children with developmental dyslexia,"Developmental dyslexia is consistently associated with difficulties in processing phonology (linguistic sound structure) across languages. One view is that dyslexia is characterised by a cognitive impairment in the ""phonological representation"" of word forms, which arises long before the child presents with a reading problem. Here we investigate a possible neural basis for developmental phonological impairments. We assess the neural quality of speech encoding in children with dyslexia by measuring the accuracy of low-frequency speech envelope encoding using EEG. We tested children with dyslexia and chronological age-matched (CA) and reading-level matched (RL) younger children. Participants listened to semantically-unpredictable sentences in a word report task. The sentences were noise-vocoded to increase reliance on envelope cues. Envelope reconstruction for envelopes between 0 and 10 Hz showed that the children with dyslexia had significantly poorer speech encoding in the 0-2 Hz band compared to both CA and RL controls. These data suggest that impaired neural encoding of low frequency speech envelopes, related to speech prosody, may underpin the phonological deficit that causes dyslexia across languages. (C) 2016 The Authors. Published by Elsevier Inc.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0099,Differential sensitivity to changes in pitch acceleration in the auditory brainstem and cortex,"The cortical pitch-specific response (CPR) is differentially sensitive to pitch contours varying in rate of accelerationtime-time-variant Mandarin Tone 2 (T2) versus constant, linear rising ramp (Linear)-as a function of language experience (Krishnan, Gandour, & Suresh, 2014). CPR and brainstem frequency following response (FFR) data were recorded concurrently from native Mandarin listeners using the same stimuli. Results showed that T2 elicited larger responses than Linear at both cortical and brainstem levels (CPR: Na-Pb, Pb-Nb; FFR). However, Pb-Nb exhibited a larger difference in magnitude between T2 and Linear than either Na-Pb or FFR. This finding highlights differential weighting of brain responses elicited by a specific temporal attribute of pitch. Consistent with the notion of a distributed, integrated hierarchical pitch processing network, temporal attributes of pitch are differentially weighted by subcortical and cortical level processing. (C) 2017 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0100,Are lexical tones musical? Native language's influence on neural response to pitch in different domains,"Language experience shapes musical and speech pitch processing. We investigated whether speaking a lexical tone language natively modulates neural processing of pitch in language and music as well as their correlation. We tested tone language (Mandarin Chinese), and non-tone language (Dutch) listeners in a passive oddball paradigm measuring mismatch negativity (MMN) for (i) Chinese lexical tones and (ii) three-note musical melodies with similar pitch contours. For lexical tones, Chinese listeners showed a later MMN peak than the non tone language listeners, whereas for MMN amplitude there were no significant differences between groups. Dutch participants also showed a late discriminative negativity (LDN). In the music condition two MMNs, corresponding to the two notes that differed between the standard and the deviant were found for both groups, and an LDN were found for both the Dutch and the Chinese listeners. The music MMNs were significantly right lateralized. Importantly, significant correlations were found between the lexical tone and the music MMNs for the Dutch but not the Chinese participants. The results suggest that speaking a tone language natively does not necessarily enhance neural responses to pitch either in language or in music, but that it does change the nature of neural pitch processing: non-tone language speakers appear to perceive lexical tones as musical, whereas for tone language speakers, lexical tones and music may activate different neural networks. Neural resources seem to be assigned differently for the lexical tones and for musical melodies, presumably depending on the presence or absence of long-term phonological memory traces.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0101,The neurocognitive signature of focus alternatives,"Focus alternatives are words/phrases that can substitute for the focused constituent of an utterance. In ""Carsten has picked [CHERRIES](F) from the tree."", (marked by pitch focus on cherries), the speaker wants to not only convey the fact that Carsten has picked cherries, but also to contrast cherries with other fruit that could have been picked, such as plums. Although focus alternatives are key to understanding the implicit aspects of an utterance, nothing is known about their neural representation. We directly contrasted neural representations of lexicosemantic similarity and focus alternative status using fMRI. Semantic relatedness was reflected in decreased activation in the bilateral superior temporal gyri. By contrast, processing of focus alternatives induced increased activations in the precuneus and the fronto-median wall, two regions previously implicated in discourse processing. These results suggest that focus alternative status is processed separately from semantic relatedness, at the level of discourse integration.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0102,Neural correlates and functional connectivity of lexical tone processing in reading,"Lexical tone processing in speech is mediated by bilateral superior temporal and inferior prefrontal regions, but little is known concerning the neural circuitries of lexical tone phonology in reading. Using fMRI, we examined the neural systems for lexical tone in visual Chinese word recognition. We found that the extraction of lexical tone phonology in print was subserved by bilateral fronto-parietal regions. Seed-to-voxel analyses showed that functionally connected cortical regions involved right inferior frontal gyrus and SMA, right middle frontal gyrus and right inferior parietal lobule, and SMA and bilateral cingulate gyri. Our results indicate that in Chinese tone reading, a bilateral network of frontal, parietal, motor, and cingulate regions is engaged, without involvement of temporal regions crucial for tone identification in auditory domain. Although neural couplings for lexical tone processing are different in speech and reading to some degree, the motor cortex seems to be a key component independent of modality.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0103,English and Mandarin native speakers? cue-weighting of lexical stress: Results from MMN and LDN,"Past research on how listeners weight stress cues such as pitch, duration and intensity has reported two inconsistent patternss: listeners' weighting conforms to 1) their native language experience (e.g., language rhythmicity, lexical tone), and 2) a general ""iambic-trochaic law"" (ITL), favouring innate sound groupings in cue perception. This study aims to tease apart the above effects by investigating the weighting of pitch, duration and intensity cues in stress-timed (Australian English) and non-stress-timed and tonal (Taiwan Mandarin) language speaking adults using a mismatch negativity (MMN) multi-feature paradigm. Results show effects that can be explained by language-specific rhythmic influence, but only partially by the ITL. Moreover, these findings revealed cross-linguistic differences indexed by both MMN and late discriminative negativity (LDN) responses at cue and syllable position levels, and thus call for more sophisticated perspectives for existing cue-weighting models.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0104,Observing gesture at learning enhances subsequent phonological and semantic processing of L2 words: An N400 study,"This study employed the N400 event-related potential (ERP) to investigate how observing different types of gestures at learning affects the subsequent processing of L2 Mandarin words differing in lexical tone by L1 English speakers. The effects of pitch gestures conveying lexical tones (e.g., upwards diagonal movements for rising tone), semantic gestures conveying word meanings (e.g., waving goodbye for to wave), and no gesture were compared. In a lexical tone discrimination task, larger N400s for Mandarin target words mismatching vs. matching Mandarin prime words in lexical tone were observed for words learned with pitch gesture. In a meaning discrimination task, larger N400s for English target words mismatching vs. matching Mandarin prime words in meaning were observed for words learned with pitch and semantic gesture. These findings provide the first neural evidence that observing gestures during L2 word learning enhances subsequent phonological and semantic processing of learned L2 words.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0105,The advantage of the music-enabled brain in accommodating lexical tone variabilities,"The perception of multiple-speaker speech is challenging. People with music training generally show more robust and faster tone perception. The present study investigated whether music training experience can facilitate tonal language speakers to accommodate speech variability in lexical tones. Native Cantonese musicians and non musicians were asked to identify Cantonese level tones from multiple speakers. Two groups were equally well in using context cues to normalize lexical tone variability at behavioral level. However, the advantage of music training was observed at cortical level. The time-domain ERP analysis suggested that musicians normalized lexical tone variability much earlier than nonmusicians (N1: 70-175 ms vs. P2: 175-280 ms). An exploratory source analysis further revealed that two groups probably relied on different cortical regions to normalize lexical tones. Left BA41 showed stronger involvement in musicians in accommodating tone variability, but right auditory cortex (including BA 41, 42 and 22) activated to a greater extend in nonmusicians.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0106,Sensory contributions to impaired prosodic processing in schizophrenia,"Background: Deficits in affect recognition are prominent features of schizophrenia. Within the auditory domain, patients show difficulty in interpreting vocal emotional cues based on intonation (prosody). The relationship of these symptoms to deficits in basic sensory processing has not been previously evaluated. Methods: Forty-three patients and 34 healthy comparison subjects were tested on two affective prosody measures: voice emotion identification and voice emotion discrimination. Basic auditory sensdory processing was measured using a tone-matching paradigm and the Distorted Tunes Test (DTT). A subset of subjects was also tested on facial affect identification and discrimination tasks. Results: Patients showed significantly impaired performance on all all emotion processing tasks. Within the patient group, a principal components analysis demonstrated significant intercorrelations between basic pitch perception and affective prosodic performance. In contrast, facial affect recognition deficits represented a distinct second component. Prosodic affect measures correlated significantly with severity of negative symptoms and impaired global outcome. Conclusions: These results demonstrate significant relationships between basic auditory processing deficits and impaired receptive prosody in schizophrenia. The separate loading of auditory and visual affective recognition measures suggests that within-modality factors may be more significant than cross-modality factors in the etiology of affect recognition deficits in schizophrenia.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0107,Subgroup differences in the lexical tone mismatch negativity (MMN) among Mandarin speakers with congenital amusia,"The association/dissociation of pitch processing between music and language is a long lasting debate. We examined this music-language relationship by investigating to what extent pitch deficits in these two domains were dissociable. We focused on a special neurodevelopmental pitch disorder congenital amusia, which primarily affects musical pitch processing. Recent research has also revealed lexical tone deficits in speech among amusics. Approximately one-third of Mandarin amusics exhibits behavioural difficulties in lexical tone perception, which is known as tone agnosia. Using mismatch negativities (MMNs), our current work probed lexical tone encoding at the pre-attentive level among the Mandarin amusics with (tone agnosics) and without (pure amusics) behavioural lexical tone deficits compared with age- and IQ-matched controls. Relative to the controls and the pure amusics, the tone agnosics exhibited reduced MMNs specifically in response to lexical tone changes. Their tone-consonant MMNs were intact and similar to those of the other two groups. Moreover, the tone MMN reduction over the left hemisphere was tightly linked to behavioural insensitivity to lexical tone changes. The current study thus provides the first psychophysiological evidence of subgroup differences in lexical tone processing among Mandarin amusics and links amusics' behavioural tone deficits to impaired pre-attentive tone processing. Despite the overall music pitch deficits, the subgroup differences in lexical tone processing in Mandarin-speaking amusics suggest dissociation of pitch deficits between music and speech. (C) 2015 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0108,The linearity of emergent spectro-temporal receptive fields in a model of auditory cortex,"The responses of cortical neurons are often characterized by measuring their spectro-temporal receptive fields (STRFs). The STRIP of a cell can be thought of as a representation of its stimulus 'preference' but it is also a filter or 'kernel' that represents the best linear prediction of the response of that cell to any stimulus. A range of in vivo STRFs with varying properties have been reported in various species, although none in humans. Using a computational model it has been shown that responses of ensembles of artificial STRFs, derived from limited sets of formative stimuli, preserve information about utterance class and prosody as well as the identity and sex of the speaker in a model speech classification system. In this work we help to put this idea on a biologically plausible footing by developing a simple model thalamo-cortical system built of conductance based neurons and synapses some of which exhibit spike-time-dependent plasticity. We show that the neurons in such a model when exposed to formative stimuli develop STRFs with varying temporal properties exhibiting a range of heterotopic integration. These model neurons also, in common with neurons measured in vivo, exhibit a wide range of non-linearities; this deviation from linearity can be exposed by characterizing the difference between the measured response of each neuron to a stimulus, and the response predicted by the STRF estimated for that neuron. The proposed model, with its simple architecture, learning rule, and modest number of neurons (< 1000), is suitable for implementation in neuromorphic analogue VLSI hardware and hence could form the basis of a developmental, real time, neuromorphic sound classification system. (C) 2008 Elsevier Ireland Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0109,Individuals with congenital amusia do not show context-dependent perception of tonal categories,"Perceptual adaptation is an active cognitive process where listeners re-analyse speech categories based on new contexts/situations/talkers. It involves top-down influences from higher cortical levels on lower-level auditory processes. Individuals with congenital amusia have impaired pitch processing with reduced connectivity between frontal and temporal regions. This study examined whether deficits in amusia would lead to impaired perceptual adaptation in lexical tone perception. Thirteen Mandarin-speaking amusics and 13 controls identified the category of target tones on an 8-step continuum ranging from rising to high-level, either in isolation or in a high-/ low-pitched context. For tones with no context, amusics exhibited reduced categorical perception than controls. While controls? lexical tone categorization demonstrated a significant context effect due to perceptual adaptation, amusics showed similar categorization patterns across both contexts. These findings suggest that congenital amusia impacts the extraction of context-dependent tonal categories in speech perception, indicating that perceptual adaptation may depend on listeners? perceptual acuity.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0110,Lexical tonal discrimination in Zapotec children. A study of the theta rhythm,"Background: Zapotec is a language used mainly in the state of Oaxaca in Mexico of tonal characteristic; homophone words with difference in fundamental frequency with different meanings. Our objective was to analyze changes in the electroencephalographic (EEG) theta rhythm during word discrimination of lexical tonal bi-syllabic homophone word samples of Zapotec. Methods: We employed electroencephalography analysis during lexical tonal discrimination in 12 healthy subjects 9-16 years of age. Results: We observed an increase in theta relative power between lexical discrimination and at rest eyes-open state in right temporal site. We also observed several significant intra- and inter-hemispheric correlations in several scalp sites, mainly in left fronto-temporal and right temporal areas when subjects were performing lexical discrimination. Conclusions: Our data suggest more engagement of neural networks of the right hemisphere are involved in Zapotec language discrimination. (C) 2015 Hospital Infantil de Mexico Federico Gomez. Published by Masson Doyma Mexico S.A.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0111,Mismatch negativity to pitch contours is influenced by language experience,"A cross-language study utilizing the mismatch negativity (MMN) evoked response was conducted to explore the influence of language experience on the preattentive cortical processing of linguistically relevant pitch contours. Chinese and English subjects were presented with Mandarin Chinese tones while the mismatch negativity (MMN) response was elicited using a passive oddball paradigm. Two oddball conditions were constructed with a common deviant, a low falling rising contour tone (T3). One condition consisted of two tones that are acoustically similar to one another (T2/T3: T2, high rising contour= standard). The other condition consisted of two tones that are acoustically dissimilar to one another (T1/T3: T1, high level= standard). These tonal pairs enabled us to assess whether different degrees of similarity between pitch movements exert a differential influence on preattentive pitch processing. Results showed that the mean MMN amplitude of the Chinese group was larger than that of the English group for the T1/T3 condition. No group differences were found for the T2/T3 condition. The mean MMN amplitude was larger for the T1/T3 relative to the T2/T3 condition for the Chinese group only. By virtue of these language group differences, we infer that early cortical processing of pitch contours may be shaped by the relative saliency of acoustic dimensions underlying the pitch patterns of a particular language. (c) 2006 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0112,Effects of native language and training on lexical tone perception: An event-related potential study,"Tone languages such as Thai use pitch differences to distinguish lexical meaning. Previous behavioral studies have reported that naive listeners can discriminate among lexical tones, but that native language background affects performance. The present study uses ERPs to determine whether native speakers of a tone language (Mandarin Chinese) and of a nontone language (English) differ in their pre-attentive discrimination among Thai lexical tones, and whether training has a different effect in these two language groups. EEGs were obtained from 10 native Mandarin Chinese speakers, 10 English and 10 Thai speakers in an oddball paradigm: The Thai syllable [k(h)a:] pronounced with a high rising or low falling tone, was presented as an infrequent deviant amidst a standard mid level tone [k(h)a] syllable, while participants watched a silent movie. Next, the Chinese and English participants completed a 2-day perceptual identification training on the mid level and low falling tones, and returned for a post training EEG. The low falling tone deviant elicited a Mismatch Negativity (MMN) in all participant groups before and after training; the high rising deviant elicited no, or a smaller, MMN, which became larger after training only in the English group. The high rising deviant also elicited a later negativity (350-650 ms) versus the mid level standard, which decreased after training in the Chinese group. These results suggest that non-Thai speakers can pre-attentively discriminate among Thai tones, but are sensitive to different physical properties of the tones, depending on their native language. English speakers are more sensitive to early pitch differences, whereas native speakers of Mandarin Chinese are more sensitive to the (later) pitch contour. (c) 2007 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0113,Lateralization of tonal and intonational pitch processing: An MEG study,"An MEG experiment was carried out in order to compare the processing of lexical-tonal and intonational contrasts, based on the tonal dialect of Roermond (the Netherlands). A set of words with identical phoneme sequences but distinct pitch contours, which represented different lexical meanings or discourse meanings (statement vs. question), were presented to native speakers as well as to a control group of speakers of Standard Dutch, a non-tone language. The stimuli were arranged in a mismatch paradigm, under three experimental conditions: in the first condition (lexical), the pitch contour differences between standard and deviant stimuli reflected differences between lexical meanings; in the second condition (intonational), the stimuli differed in their discourse meaning; in the third condition (combined), they differed both in their lexical and discourse meaning. In all three conditions, native as well as non-native responses showed a clear MMNm (magnetic mismatch negativity) in a time window from 150 to 250 ms after the divergence point of standard and deviant pitch contours. In the lexical condition, a stronger response was found over the left temporal cortex of native as well as non-native speakers. In the intonational condition, the same activation pattern was observed in the control group, but not in the group of native speakers, who showed a right-hemisphere dominance instead. Finally, in the combined (lexical and intonational) condition, brain reactions appeared to represent the summation of the patterns found in the other two conditions. In sum, the lateralization of pitch processing is condition-dependent in the native group only, which suggests that language experience determines how processes should be distributed over both temporal cortices, according to the functions available in the grammar. (C) 2010 Elsevier B.V. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0114,Word accents and morphology-ERPs of Swedish word processing,"Results indicating that high stem tones realizing word accents activate a certain class of suffixes in online processing of Central Swedish are presented. This supports the view that high Swedish word accent tones are induced onto word stems by particular suffixes rather than being associated with words in the mental lexicon. Using event-related potentials, effects of mismatch between word accents and inflectional suffixes were compared with mismatches between stem and suffix in terms of declension class. Declensionally incorrect suffixes yielded an increase in the N400, indicating problems in lexical retrieval, as well as a P600 effect, showing reanalysis. Both declensionally correct and incorrect high tone-inducing (Accent 2) suffixes combined with a mismatching low tone (Accent 1) on the stems produced P600 effects, but did not increase the N400. Suffixes usually co-occurring with Accent 1 did not yield any effects in words realized with the nonmatching Accent 2, suggesting that Accent 1 is a default accent, lacking association with any particular suffix. High tones on Accent 2 words also produced an early anterior positivity, interpreted as a P200 effect reflecting preattentive processing of the tone. (C) 2010 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0115,An event-related potential investigation of lexical pitch-accent processing in auditory Japanese,"Lexical prosody plays an important role in speech comprehension. However, the electrophysiological nature and time course of processing lexical prosody in mora-timed languages are rarely known in contrast to the wealth of knowledge in stress-timed languages and syllable-timed languages like German and French. In the present study, lexical pitch-accent processing in Japanese is investigated using event-related potentials. Participants listened to sentences with verbs either correct or incorrect with respect to pitch-accent (phonological condition), word meaning (semantic condition) or sentence type (syntactic condition). When the brain potentials of correct and incorrect sentences were compared within conditions, the phonological and semantic conditions showed a negativity and positivity (P600), while the syntactic condition displayed a P600. Furthermore, the negativity in response to pitch-accent violations (pitch-accent negativity) appeared approximately 60 ms earlier than the response to semantic violations (N400), while no significant topographical distributions were found between the two components. These results suggest that the pitch-accent negativity reflects initial phonological processing followed by lexical access and word recognition. Moreover, the P600 displayed in all conditions was interpreted as a general integration process that is common across the three domains. (C) 2011 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0116,The neural generators of the mismatch responses to Mandarin lexical tones: An MEG study,"The present magnetoencephalography study used the cortically constrained minimumnorm estimates of human brain activity to elucidate functional roles of neural generators for detecting different magnitudes of lexical tones changes. A multiple-deviant oddball paradigm was used in which the syllable ""yi"" with a low-dipping tone (T3) was the common standard sound and the same syllable with a high-level tone (T1) or a high-rising tone (T2) were the large and small deviant sounds, respectively. The data revealed a larger magnetic Mismatch field (MMNm) for large deviant in the left hemisphere. The source analysis also confirmed that the MMNm to lexical tone changes was generated in bilateral superior temporal gyri and only the large deviant revealed left lateralization. A set of frontal generators was activated at a later time and revealed differential sensitivities to the degree of deviance. The left anterior insula, the right anterior cingulate cortex, and the right ventral orbital frontal cortex were activated when detecting a large deviant, whereas the right frontal-opercular region was sensitive to the small deviant. These frontal generators were thought to be associated with various top-down mechanisms for attentional modulation. The time frequency (TF) analysis showed that large deviants yielded large theta band (5-7 Hz) activity over the left anterior scalp and the left central scalp, while small deviants yielded large alpha band activity (9-11 Hz) over the posterior scalp. The results of TF analyses implied that mechanisms of working memory and functional inhibition involved in the processes of acoustic change detection. (C) 2014 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0117,Effects of production training and perception training on lexical tone perception - A behavioral and ERP study,"The present study recorded both behavioral data and event-related brain potentials to examine the effectiveness of a perception-only training and a perception-plus-production training procedure on the intentional and unintentional perception of lexical tone by native English listeners. In the behavioral task, both the perception-only and the perception-plus-production groups improved on the tone discrimination abilities after the training session. Moreover, the participants in both groups generalized the improvements gained through the trained stimuli to the untrained stimuli. In the ERP task, the Mismatch Negativity was smaller in the post-training task than in the pre-training task. However, the two training groups did not differ in tone processing at the intentional or unintentional level after training. These results suggest that the employment of the motor system does not specifically benefit the tone perceptual skills. Furthermore, the present study investigated whether some tone pairs are more easily confused than others by native English listeners, and whether the order of tone presentation influences non-native tone discrimination. In the behavioral task, Tone2-Tone1 (rising-level) and Tone2-Tone4 (rising-falling) were the most difficult tone pairs, while Tone1-Tone2 and Tone4-Tone2 were the easiest tone pairs, even though they involved the same tone contrasts respectively. In the ERP task, the native English listeners had good discrimination when Tone2 and Tone4 were embedded in strings of Tone1, while poor discrimination when Tone1 was inserted in the context of Tone2 or Tone4. These asymmetries in tone perception might be attributed to the interference of native intonation system and can be altered by training. (C) 2015 Elsevier B.V. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0118,Toward assessment of human voice biomarkers of brain lesions through explainable deep learning,"Lesions in the brain resulting from traumatic injuries or strokes can evolve into speech dysfunction in undiagnosed patients. Employing ML-based tools to analyze the prosody or articulatory phonetics of human speech could be advantageous for early screening of undetected brain injuries. Additionally, explaining the model's decision-making process can support predictions and take appropriate measures to improve patient voice quality. However, traditional ML methods relying on low-level descriptors (LLDs) may sacrifice detailed temporal dynamics and other speech characteristics. Interpreting these descriptors can also be challenging, requiring significant effort to understand feature relationships and suitable ranges. To address these limitations, this research paper introduces xDMFCCs, a method that identifies interpretive discriminatory acoustic biomarkers from a single speech utterance, providing local and global interpretations of deep learning models in speech applications. To validate this approach, it was implemented to interpret a Convolutional Neural Network (CNN) trained on Mel-frequency Cepstral Coefficients (MFCC) for the binary classification task to differentiate between patients from control vocalizations. The ConvNet achieved promising results with a 75% f-score (75% recall, 76% precision), comparable to conventional machine learning baselines. What sets xDMFCCs apart is its explanation through a 2D time-frequency representation that preserves the complete speech signal. This representation offers a more transparent explanation for differentiating between patients and healthy controls, enhancing interpretability. This advancement enables more detailed and compelling studies in speech acoustic traits of brain lesions. Furthermore, the findings have significant implications for developing low-cost and rapid diagnostics of unnoticed brain lesions.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0119,A novel multimodal depression diagnosis approach utilizing a new hybrid fusion method,"In recent years, research has found that the impact of depression status primarily lies in patients' language expression and facial expressions. Furthermore, facial expressions and intonation in speech exhibit a natural coexistence, making facial and vocal information core recognition indicators in depression identification. It is imperative to explore the effective use of deep learning methods for multimodal depression detection. We have proposed a novel trilateral bimodal encoding model (MEN), attentional decision fusion (ADF), and feature extraction fusion strategy. We employed a hybrid fusion approach that combines early intra-modality fusion with late inter-modality fusion, for multimodal depression diagnosis. In the feature extraction fusion component, we combine different representations of the same modality before inputting them into the network for training, enhancing features relevant to depression in the data. Through our multimodal encoding network, we extract frame-level information using Convolutional Neural Networks (CNN) while considering long-term context information and dependencies with Bidirectional Long Short-Term Memory (BiLSTM). Finally, the three streams of information were effectively integrated through attention fusion representation in our Attention Decision Fusion module (ADF), for depression score regression prediction. Extensive experiments were conducted on two public datasets, AVEC2013 and AVEC2014. The average absolute error/ root mean squared error (MAE/RMSE) scores for predicting depression scores were 6.48/8.91 and 7.01/9.38, respectively. This demonstrated that our hybrid fusion method outperforms traditional early or late fusion methods in terms of performance.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0120,Development of lexical tone awareness in Chinese children with and without dyslexia,"This paper describes two studies that examined the lexical tone awareness of Chinese children both with and without dyslexia at different primary school ages. Study 1 examined the contributions of lexical tone awareness to distinguish children with and without dyslexia with respect to their Chinese character reading skills. Two hundred and seventy Chinese children participated in Study 1. Ninety of these were children with dyslexia (equally recruited from second, fourth, and sixth grades). Moreover, ninety children functioned as a chronological-age control group, and an additional ninety children functioned as a reading-level control group. The participants were tested for nonverbal intelligence, Chinese character reading, and cognitive-linguistic skills and lexical tone awareness. Our results revealed a later developmental ceiling in Chinese children with dyslexia than in those without dyslexia. Furthermore, children's lexical tone awareness could serve to distinguish children with dyslexia from typically developing children in all primary school years. Study 2 compared the lexical tone awareness and Chinese character reading skills of Chinese children with dyslexia both before and after introducing the Perceptual Training Method. The participants in this study consisted of all the participants with dyslexia from Study 1, and the measurements were the Chinese character reading test and the lexical tone awareness task from Study 1. Our results revealed that only second-grade children with dyslexia gained substantially from the training on both lexical tone awareness and character naming, whereas those in the fourth grade obtained a significant improvement only on lexical tone awareness. (C) 2017 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0121,Visual signals in text comprehension:: How to restore them when oralizing a text via a speech synthesis?,"It has been assumed theoretically and established empirically that text signals exert an influence on text memorization and comprehension. The study investigates whether the restoration of the text visual signals improve text memorization and comprehension when automatically converting a text into speech. Participants listened to a restaurant menu oralized by a text-to-speech synthesis. The visual signals used in the menu were restored either with discursive segments, with prosodic cues, or with a picture of text, displayed before or during the listening. Participants had to perform tasks assessing their text memorization and comprehension. The restoration of text visual signals exerts an influence on the participants' recall but these effects vary according to the restoration mean used and to the task. When visual signals are not restored, individuals construct an erroneous representation of the situation described in the text leading to a misinterpretation of the text meaning, whereas the discursive and prosodic restorations involve the construction of an adequate representation. (c) 2006 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0122,Newborn infants process pitch intervals,"Objective: We investigated whether the auditory system of newborn babies extracts the constancy of a pitch interval from exemplars varying in absolute pitch. Methods: Event-related brain potentials (ERP) were recorded from healthy newborn infants in an oddball paradigm consisting of frequent standard and infrequent deviant tone pairs. Tone pairs varied in absolute frequency. Standard and deviant pairs differed in the amount of pitch difference within the pairs, but not in the direction of pitch change. Results: Deviant tone pairs elicited a discriminative ERP response. Conclusions: This result suggests that the neonate auditory system represents pitch intervals similarly to adults. Significance: Adult-like processing of pitch intervals allows newborn infants to learn music, speech prosody, and to process various important auditory cues based on spectral acoustic features. (C) 2008 International Federation of Clinical Neurophysiology. Published by Elsevier Ireland Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0123,Emotional MMN: Anxiety and heart rate correlate with the ERP signature for auditory change detection,"Objective: Previous work established the mismatch negativity (MMN) as a correlate of pre-attentive auditory change detection. The present study aimed at investigating the relationship between the MMN and emotional processes associated with the detection of change. Methods: To this end, we assessed state anxiety with a questionnaire and subsequently recorded the electroencephalogram (EEG) and heart rate while participants watched a silent movie and listened to a task-irrelevant auditory oddball sequence. The oddball sequence comprised meaningless syllables of which some were deviants spoken with an angry or neutral voice. Results: The MMN to angry voice deviants was larger than that to neutral deviants and correlated positively with ensuing heart rate acceleration. Additionally, both the MMN and heart rate acceleration to angry voice deviants were increased with increasing state anxiety. A similar effect for neutral voice deviants was non-significant. Conclusion: Taken together, these results suggest that the pre-attentive processing of threat, as reflected by the MMN, is linked to an activation of the sympathetic nervous system. Moreover, this link is more strongly activated in individuals with high state anxiety. Significance: Thus, the MMN may be used as a marker for an individual's state dependent sensitivity to unattended, emotionally relevant change. (C) 2009 International Federation of Clinical Neurophysiology. Published by Elsevier Ireland Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0124,Sensory intelligence for extraction of abstract auditory rules from a speech sound stream in children with cochlear implants,"Objective: Sensory intelligence in the brain helps listeners automatically extract abstract auditory rules formed by invariant acoustic features from complex speech sound streams, presumably serving as the neural basis for speech comprehension. However, whether this intelligence is deficient in children with cochlear implants (CIs) remains unclear. Methods: Mandarin Chinese monosyllables shared a flat lexical tone contour to form an abstract auditory rule but differed in other acoustic features to construct a complex speech sound stream. The abstract rule was occasionally violated by monosyllables with a rising or falling lexical tone. Results: In normal hearing (NH) children, the abstract auditory rule could be extracted, as revealed by a mismatch negativity (MMN) and a late discriminative negativity (LDN). However, the MMN and LDN were only evoked in CI children with good hearing and speech performance. NH children with a higher speech perception or spatial hearing score had a greater MMN. The LDN was attenuated with increasing age in NH children. Conclusions: The sensory intelligence for extraction of auditory abstract rules, associated with speech perception, is deficient in CI children. This intelligence may gradually develop during childhood and adolescence. Significance: Deficient sensory intelligence in CI children may aid in understanding poor speech comprehension in complex environments. (c) 2024 International Federation of Clinical Neurophysiology. Published by Elsevier B.V. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0125,Learning phonetic categories by tracking movements,"We explore in this study how infants may derive phonetic categories from adult input that are highly variable. Neural networks in the form of self-organizing maps (SOMs; Kohonen, 1989, 1995) were used to simulate unsupervised learning of Mandarin tones. In Simulation 1, we trained the SOMs with syllable-sized continuous F-0 contours, produced by multiple speakers in connected speech, and with the corresponding velocity profiles (D1). No attempt was made to reduce the large amount of variability in the input or to add to the input any abstract features such as height and slope of the F-0 contours. In the testing phase, reasonably high categorization rate was achieved with F-0 profiles, but D1 profiles yielded almost perfect categorization of the four tones. Close inspection of the learned prototypical D1 profile clusters revealed that they had effectively eliminated surface variability and directly reflected articulatory movements toward the underlying targets of the four tones as proposed by Xu and Wang (2001). Additional simulations indicated that a further learning step was possible through which D1 prototypes with one-to-one correspondence to the tones were derived from the prototype clusters learned in Simulation 1. Implications of these findings for theories of language acquisition, speech perception and speech production are discussed. (c) 2006 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0126,Effects of prosodically modulated sub-phonetic variation on lexical competition,"Eye movements were monitored as participants followed spoken instructions to manipulate one of four objects pictured on a computer screen. Target words occurred in utterance-medial (e.g., Put the cap next to the square) or utterance-final position (e.g., Now click on the cap). Displays consisted of the target picture (e.g., a cap), a monosyllabic competitor picture (e.g., a cat), a polysyllabic competitor picture (e.g., a captain) and a distractor (e.g., a beaker). The relative proportion of fixations to the two types of competitor pictures changed as a function of the position of the target word in the utterance, demonstrating that lexical competition is modulated by prosodically conditioned phonetic variation. (C) 2006 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0127,The sound of motion in spoken language: Visual information conveyed by acoustic properties of speech,"Language is generally viewed as conveying information through symbols whose form is arbitrarily related to their meaning. This arbitrary relation is often assumed to also characterize the mental representations underlying language comprehension. We explore the idea that visuo-spatial information can be analogically conveyed through acoustic properties of speech and that such information is integrated into an analog perceptual representation as a natural part of comprehension. Listeners heard sentences describing objects, spoken at varying speaking rates. After each sentence, participants saw a picture of an object and judged whether it had been mentioned in the sentence. Participants were faster to recognize the object when motion implied by speaking rate matched the motion implied by the picture. Results suggest that visuo-spatial referential information can be analogically conveyed and represented. (c) 2006 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0128,Influences of high and low variability on infant word recognition,"Although infants begin to encode and track novel words in fluent speech by 7.5 months, their ability to recognize words is somewhat limited at this stage. In particular, when the surface form of a word is altered, by changing the gender or affective prosody of the speaker, infants begin to falter at spoken word recognition. Given that natural speech is replete with variability, only some of which determines the meaning of a word, it remains unclear how infants might ever overcome the effects of surface variability without appealing to meaning. In the current set of experiments, consequences of high and low variability are examined in preverbal infants. The source of variability, vocal affect, is a common property of infant-directed speech with which young learners have to contend. Across a series of four experiments, infants' abilities to recognize repeated encounters of words, as well as to reject similar-sounding words, are investigated in the context of high and low affective variation. Results point to positive consequences of affective variation, both in creating generalizable memory representations for words, but also in establishing phonologically precise memories for words. Conversely, low variability appears to degrade word recognition on both fronts, compromising infants' abilities to generalize across different affective forms of a word and to detect similar-sounding items. Findings are discussed in the context of principles of categorization that may potentiate the early growth of a lexicon. (C) 2007 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0129,The developmental course of lexical tone perception in the first year of life,"Perceptual reorganisation of infants' speech perception has been found from 6 months for consonants and earlier for vowels. Recently, similar reorganisation has been found for lexical tone between 6 and 9 months of age. Given that there is a close relationship between vowels and tones, this study investigates whether the perceptual reorganisation for tone begins earlier than 6 months. Non-tone language English and French infants were tested with the Thai low vs. rising lexical tone contrast, using the stimulus alternating preference procedure. Four- and 6-month-old infants discriminated the lexical tones, and there was no decline in discrimination performance across these ages. However, 9-month-olds failed to discriminate the lexical tones. This particular pattern of decline in normative tone discrimination over age indicates that perceptual reorganisation for tone does not parallel the developmentally prior decline observed in vowel perception. The findings converge with previous developmental cross-language findings on tone perception in English-language infants [Mattock, K., & Burnham, D. (2006). Chinese and English infants' tone perception: Evidence for perceptual reorganization. Infancy, 10(3)], and extend them by showing similar perceptual reorganisation for non-tone language infants learning rhythmically different non-tone languages (English and French). Crown copyright (C) 2007 Published by Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0130,Beethoven's last piano sonata and those who follow crocodiles: Cross-domain mappings of auditory pitch in a musical context,"Though auditory pitch is customarily mapped in Western cultures onto spatial verticality (high-low), both anthropological reports and cognitive studies suggest that pitch may be mapped onto a wide variety of other domains. We collected a total number of 35 pitch mappings and investigated in four experiments how these mappings are used and structured. In particular, we inquired (1) how Western subjects apply Western and non-Western metaphors to ""high"" and ""low"" pitches, (2) whether mappings applied in an abstract conceptual task are similarly applied by listeners to actual music, (3) how mappings of spatial height relate to these pitch mappings, and (4) how mappings of ""high"" and ""low"" pitch associate with other dimensions, in particular quantity, size, intensity and valence. The results show strong agreement among Western participants in applying familiar and unfamiliar metaphors for pitch, in both an abstract, conceptual task (Exp. 1) and in a music listening task (Exp. 2), indicating that diverse cross-domain mappings for pitch exist latently besides the common verticality metaphor. Furthermore. limited overlap between mappings of spatial height and pitch height was found, suggesting that, the Ubiquity of the verticality metaphor in Western usage notwithstanding, cross-domain pitch mappings are largely independent of that metaphor, and seem to be based upon other underlying dimensions. Part of the discrepancy between spatial height and pitch height is that, for pitch, ""up"" is not necessarily ""more,"" nor is it necessarily ""good."" High pitch is only ""more"" for height, intensity and brightness. It is ""less"" for mass, size and quantity. We discuss implications of these findings for music and speech prosody, and their relevance to notions of embodied cognition and of cross-domain magnitude representation. (c) 2009 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0131,Cross-modal facilitation in speech prosody,"Speech prosody has traditionally been considered solely in terms of its auditory features, yet correlated visual features exist, such as head and eyebrow movements. This study investigated the extent to which visual prosodic features are able to affect the perception of the auditory features. Participants were presented with videos of a speaker pronouncing two words, with visual features of emphasis on one of these words. For each trial, participants saw one video where the two words were identical in both pitch and amplitude, and another video where there was a difference in either pitch or amplitude that was congruent or incongruent with the visual changes. Participants were asked to decide which video contained the sound difference. Thresholds were obtained for the congruent and incongruent videos, and for an auditory-alone condition. It was found that the congruent thresholds were better than the incongruent thresholds for both pitch and amplitude changes. Interestingly, the congruent thresholds for amplitude were better than for the auditory-alone condition, which implies that the visual features improve sensitivity to loudness changes. These results demonstrate that visual stimuli can affect auditory thresholds for changes in pitch and amplitude, and furthermore support the view that visual prosodic features enhance speech processing. (C) 2009 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0132,Linguistic rhythm guides parsing decisions in written sentence comprehension,"Various recent studies attest that reading involves creating an implicit prosodic representation of the written text which may systematically affect the resolution of syntactic ambiguities in sentence comprehension. Research up to now suggests that implicit prosody itself depends on a partial syntactic analysis of the text, raising the question of whether implicit prosody contributes to the parsing process, or whether it merely interprets the syntactic analysis. The present reading experiments examine the influence of stress-based linguistic rhythm on the resolution of local lexical-syntactic ambiguities in German. Both speech production data from unprepared oral reading and eye-tracking results from silent reading demonstrate that readers favor syntactic analyses that allow for a prosodic representation in which stressed and unstressed syllables alternate rhythmically. The findings contribute evidence confirming immediate and guiding effects of linguistic rhythm on the earliest stages of syntactic parsing in reading. (C) 2011 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0133,Influences of lexical tone and pitch on word recognition in bilingual infants,"Infants' abilities to discriminate native and non-native phonemes have been extensively investigated in monolingual learners, demonstrating a transition from language-general to language-specific sensitivities over the first year after birth. However, these studies have mostly been limited to the study of vowels and consonants in monolingual learners. There is relatively little research on other types of phonetic segments, such as lexical tone, even though tone languages are very well represented across languages of the world. The goal of the present study is to investigate how Mandarin Chinese-English bilingual learners contend with non-phonemic pitch variation in English spoken word recognition. This is contrasted with their treatment of phonemic changes in lexical tone in Mandarin spoken word recognition. The experimental design was cross-sectional and three age-groups were sampled (7.5 months, 9 months and 11 months). Results demonstrated limited generalization abilities at 7.5 months, where infants only recognized words in English when matched in pitch and words in Mandarin that were matched in tone. At 9 months, infants recognized words in Mandarin Chinese that matched in tone, but also falsely recognized words that contrasted in tone. At this age, infants also recognized English words whether they were matched or mismatched in pitch. By 11 months, infants correctly recognized pitch-matched and - mismatched words in English but only recognized tonal matches in Mandarin Chinese. (C) 2012 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0134,Is it or isn't it: Listeners make rapid use of prosody to infer speaker meanings,"A visual world experiment examined the time course for pragmatic inferences derived from visual context and contrastive intonation contours. We used the construction It looks like an X pronounced with either (a) a H pitch accent on the final noun and a low boundary tone, or (b) a contrastive L + H* pitch accent and a rising boundary tone, a contour that can support contrastive inference (e.g., It LOOKSL+H* like a zebra(L-H%) ... (but it is not)). When the visual display contained a single related set of contrasting pictures (e.g. a zebra vs. a zebra-like animal), effects of LOOKSL+H* emerged prior to the processing of phonemic information from the target noun. The results indicate that the prosodic processing is incremental and guided by contextually-supported expectations. Additional analyses ruled out explanations based on context-independent heuristics that might substitute for online computation of contrast. (C) 2014 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0135,Perception of tones by infants learning a non-tone language,"This article examines the perception of tones by non-tone-language-learning (non-tone-learning) infants between 5 and 18 months in a study that reveals infants' initial sensitivity to tonal contrasts, deterioration yet plasticity of tonal sensitivity at the end of the first year, and a perceptual rebound in the second year. Dutch infants in five age groups were tested on their ability to discriminate a tonal contrast of Mandarin Chinese as well as a contracted tonal contrast. Infants are able to discriminate tonal contrasts at 5-6 months, and their tonal sensitivity deteriorates at around 9 months. However, the sensitivity rebound sat 17-18 months. Non-tone-learning infants' tonal perception is elastic, as is shown by the influence of acoustic salience and distributional learning: (1) a salient contrast may remain discriminable throughout infancy whereas a less salient one does not; (2) a bimodal distribution in tonal exposure increases non-tone-learning infants' discrimination ability during the trough in sensitivity to tonal contrasts at 11-12 months. These novel findings reveal non-tone-learning infant' U-shaped pattern in tone perception, and display their perceptual flexibility. (C) 2014 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0136,Predictive processing of novel compounds: Evidence from Japanese,"Our study argues that pre-head anticipatory processing operates at a level below the level of the sentence. A visual-world eye-tracking study demonstrated that, in processing of Japanese novel compounds, the compound structure can be constructed prior to the head if the prosodic information on the preceding modifier constituent signals that the Compound Accent Rule (CAR) is being applied. This prosodic cue rules out the single head analysis of the modifier noun, which would otherwise be a natural and economical choice. Once the structural representation for the head is computed in advance, the parser becomes faster in identifying the compound meaning. This poses a challenge to models maintaining that structural integration and word recognition are separate processes. At the same time, our results, together with previous findings, suggest the possibility that there is some degree of staging during the processing of different sources of information during the comprehension of compound nouns. (C) 2014 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0137,"Spoken word recognition in early childhood: Comparative effects of vowel, consonant and lexical tone variation","The majority of the world's languages exploit consonants, vowels and lexical tones to contrast the meanings of individual words. However, the majority of experimental research on early language development focuses on consonant-vowel languages. In the present study, the role of consonants, vowels and lexical tones in emergent word knowledge are directly compared in toddlers (2.5-3.5 years) and preschoolers (4-5 years) who were bilingual native learners of a consonant-vowel-tone language (Mandarin Chinese). Using a preferential looking paradigm, participants were presented with correct pronunciations and consonantal, vowel, and tonal variations of known words. Responses to each type of variation were assessed via gaze fixations to a visual target. When their labels were correctly pronounced, visual targets were reliably identified at both age groups. However, in toddlers, there was a high degree of sensitivity to mispronunciations due to variation in lexical tones relative to those due to consonants and vowels. This pattern was reversed in preschoolers, who were more sensitive to consonant and vowel variation than to tone variation. Findings are discussed in terms of properties of tones, vowels and consonants and the respective role of each source of variation in tone languages. (C) 2015 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0138,"It's all in the delivery: Effects of context valence, arousal, and concreteness on visual word processing","Prior research has examined how distributional properties of contexts (number of unique contexts or their informativeness) influence the effort of word recognition. These properties do not directly interrogate the semantic properties of contexts. We evaluated the influence of average concreteness, valence (positivity) and arousal of the contexts in which a word occurs on response times in the lexical decision task, age of acquisition of the word, and word recognition memory performance. Using large corpora and norming mega-studies we quantified semantics of contexts for thousands of words and demonstrated that contextual factors were predictive of lexical representation and processing above and beyond the influence shown by concreteness, valence and arousal of the word itself. Our findings indicate that lexical representations are influenced not only by how diverse the word's contexts are, but also by the embodied experiences they elicit. (C) 2016 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0139,Spoken word recognition in young tone language learners: Age-dependent effects of segmental and suprasegmental variation,"The majority of the world's languages rely on both segmental (vowels, consonants) and suprasegmental (lexical tones) information to contrast the meanings of individual words. However, research on early language development has mostly focused on the acquisition of vowel-consonant languages. Developmental research comparing sensitivity to segmental and suprasegmental features in young tone learners is extremely rare. This study examined 2- and 3-year-old monolingual tone learners' sensitivity to vowels and tones. Experiment la tested the influence of vowel and tone variation on novel word learning. Vowel and tone variation hindered word recognition efficiency in both age groups. However, tone variation hindered word recognition accuracy only in 2-year-olds, while 3-year-olds were insensitive to tone variation. Experiment 1b demonstrated that 3-year-olds could use tones to learn new words when additional support was provided, and additionally, that Tone 3 words were exceptionally difficult to learn. Experiment 2 confirmed a similar pattern of results when children were presented with familiar words. This study is the first to show that despite the importance of tones in tone languages, vowels maintain primacy over tones in young children's word recognition and that tone sensitivity in word learning and recognition changes between 2 and 3 years of age. The findings suggest that early lexical processes are more tightly constrained by variation in vowels than by tones. (C) 2016 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0140,Using prosody to infer discourse prominence in cochlear-implant users and normal-hearing listeners,"Cochlear implants (CIs) provide speech perception to adults with severe-to-profound hearing loss, but the acoustic signal remains severely degraded. Limited access to pitch cues is thought to decrease sensitivity to prosody in CI users, but co-occurring changes in intensity and duration may provide redundant cues. The current study investigates how listeners use these cues to infer discourse prominence. CI users and normal-hearing (NH) listeners were presented with sentences varying in prosody (accented vs. unaccented words) while their eye-movements were measured to referents varying in discourse status (given vs. new categories). In Experiment 1, all listeners inferred prominence when prosody on nouns distinguished categories (""SANDWICH"" -> not sandals). In Experiment 2, CI users and NH listeners presented with natural speech inferred prominence when prosody on adjectives implied contrast across both categories and properties (""PINK horse"" -> not the orange horse). In contrast, NH listeners presented with simulated CI (vocoded) speech were sensitive to acoustic differences in prosody, but did not use these cues to infer discourse status. Together, this suggests that exploiting redundant cues for comprehension varies with the demands of language processing and prior experience with the degraded signal. (C) 2017 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0141,Do you hear 'feather' when listening to 'rain'? Lexical tone activation during unconscious translation: Evidence from Mandarin-English bilinguals,"Although lexical tone is a highly prevalent phonetic cue in human languages, its role in bilingual spoken word recognition is not well understood. The present study investigates whether and how adult bilinguals, who use pitch contours to disambiguate lexical items in one language but not the other, access a tonal 1,1 when exclusively processing a non-tonal L2. Using the visual world paradigm, we show that Mandarin-English listeners automatically activated Mandarin translation equivalents of English target words such as 'rain' (Mandarin 'yu3'), and consequently were distracted by competitors whose segments and tones overlapped with the translations of English target words ('feather', also 'yu3' in Mandarin). Importantly, listeners were not distracted by competitors that overlapped with the translations of target words in all segments but not tone ('fish'; Mandarin 'yu2'), nor were they distracted by competitors that overlapped with the translations of target words in rime and tone ('wheat', Mandarin 'gu3'). These novel results demonstrate implicit access to L1 lexical representations through automatic/unconscious translation, as a result of cross-language top-down and/or lateral influence, and highlight the critical role of lexical tone activation in bilingual lexical access. (C) 2017 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0142,Voulez-vous jouer avec moi? Twelve-month-olds understand that foreign languages can communicate,"Infants understand that speech in their native language allows speakers to communicate. Is this understanding limited to their native language or does it extend to non-native languages with which infants have no experience? Twelve-month-old infants saw an actor, the Communicator, repeatedly select one of two objects. When the Communicator could no longer reach the target but a Recipient could, the Communicator vocalized a nonsense phrase either in English (infants' native language), Spanish (rhythmically different), or Russian (phonotactically different), or hummed (a non-speech vocalization). Across all three languages, native and non-native, but not humming, infants looked longer when the Recipient gave the Communicator the non-target object. Although, by 12 months, infants do not readily map non-native words to objects or discriminate most non-native speech contrasts, they understand that non-native languages can transfer information to others. Understanding language as a tool for communication extends beyond infants' native language: By 12 months, infants view language as a universal mechanism for transferring and acquiring new information.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0143,Better than native: Tone language experience enhances English lexical stress discrimination in Cantonese-English bilingual listeners,"While many second language (L2) listeners are known to struggle when discriminating non-native features absent in their first language (L1), no study has reported that L2 listeners perform better than native listeners in this regard. The present study tested whether Cantonese-English bilinguals were better in discriminating English lexical stress in individual words or pseudowords than native English listeners, even though lexical stress is absent in Cantonese. In experiments manipulating acoustic, phonotactic, and lexical cues, Cantonese-English bilingual adults exhibited superior performance in discriminating English lexical stress than native English listeners across all phonotactic/lexical conditions when the fundamental frequency (f0) cue to lexical stress was present. The findings underscore the facilitative effect of Cantonese tone language experience on English lexical stress discrimination.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0144,Universals of listening: Equivalent prosodic entrainment in tone and non-tone languages,"In English and Dutch, listeners entrain to prosodic contours to predict where focus will fall in an utterance. Here, we ask whether this strategy is universally available, even in languages with very different phonological systems (e.g., tone versus non-tone languages). In a phoneme detection experiment, we examined whether prosodic entrainment also occurs in Mandarin Chinese, a tone language, where the use of various suprasegmental cues to lexical identity may take precedence over their use in salience. Consistent with the results from Germanic languages, response times were facilitated when preceding intonation predicted high stress on the target-bearing word, and the lexical tone of the target word (i.e., rising versus falling) did not affect the Mandarin listeners' response. Further, the extent to which prosodic entrainment was used to detect the target phoneme was the same in both English and Mandarin listeners. Nevertheless, native Mandarin speakers did not adopt an entrainment strategy when the sentences were presented in English, consistent with the suggestion that L2 listening may be strained by additional functional load from prosodic processing. These findings have implications for how universal and language-specific mechanisms interact in the perception of focus structure in everyday discourse.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0145,Prosody leaks into the memories of words,"The average predictability (aka informativity) of a word in context has been shown to condition word duration (Seyfarth, 2014). All else being equal, words that tend to occur in more predictable environments are shorter than words that tend to occur in less predictable environments. One account of the informativity effect on duration is that the acoustic details of probabilistic reduction are stored as part of a word's mental representation. Other research has argued that predictability effects are tied to prosodic structure in integral ways. With the aim of assessing a potential prosodic basis for informativity effects in speech production, this study extends past work in two directions; it investigated informativity effects in another large language, Mandarin Chinese, and broadened the study beyond word duration to additional acoustic dimensions, pitch and intensity, known to index prosodic prominence. The acoustic information of content words was extracted from a large telephone conversation speech corpus with over 400,000 tokens and 6000 word types spoken by 1655 individuals and analyzed for the effect of informativity using frequency statistics estimated from a 431 million word subtitle corpus. Results indicated that words with low informativity have shorter durations, replicating the effect found in English. In addition, informativity had significant effects on maximum pitch and intensity, two phonetic dimensions related to prosodic prominence. Extending this interpretation, these results suggest that predictability is closely linked to prosodic prominence, and that the lexical representation of a word includes phonetic details associated with its average prosodic prominence in discourse. In other words, the lexicon absorbs prosodic influences on speech production.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0146,Elucidating the influences of embodiment and conceptual metaphor on lexical and non-speech tone learning,"In the contexts of language learning and music processing, hand gestures conveying acoustic information visually influence perception of speech and non-speech sounds (Connell et al., 2013; Morett & Chang, 2015). Currently, it is unclear whether this effect is due to these gestures' use of the human body to highlight relevant features of language (embodiment) or the cross-modal mapping between the visual motion trajectories of these gestures and corresponding auditory features (conceptual metaphor). To address this question, we examined identification of the pitch contours of lexical tones and non-speech analogs learned with pitch gesture, comparable dot motion, or no motion. Critically, pitch gesture and dot motion were either congruent or incongruent with the vertical conceptual metaphor of pitch. Consistent with our hypotheses, we found that identification accuracy increased for tones learned with congruent pitch gesture and dot motion, whereas it remained stable or decreased for tones learned with incongruent pitch gesture and dot motion. These findings provide the first evidence that both embodied and non-embodied visual stimuli congruent with the vertical conceptual metaphor of pitch enhance lexical and non-speech tone learning. Thus, they illuminate the influences of conceptual metaphor and embodiment on lexical and non-speech auditory perception, providing insight into how they can be leveraged to enhance language learning and music processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0147,Phonological and conceptual activation in speech comprehension,"We propose that speech comprehension involves the activation of token representations of the phonological forms of current lexical hypotheses, separately from the ongoing construction of a conceptual interpretation of the current utterance. In a series of cross-modal priming experiments, facilitation of lexical decision responses to visual target words (e.g., time) was found for targets that were semantic associates of auditory prime words (e.g., date) when the primes were isolated words, but not when the same primes appeared in sentence contexts. Identity priming (e.g., faster lexical decisions to visual date after spoken date than after an unrelated prime) appeared, however, both with isolated primes and with primes in prosodically neutral sentences. Associative priming in sentence contexts only emerged when sentence prosody involved contrastive accents, or when sentences were terminated immediately after the prime. Associative priming is therefore not an automatic consequence of speech processing. In no experiment was there associative priming from embedded words (e.g., sedate-time), but there was inhibitory identity priming (e.g., sedate-date) from embedded primes in sentence contexts. Speech comprehension therefore appears to involve separate distinct activation both of token phonological word representations and of conceptual word representations. Furthermore, both of these types of representation are distinct from the long-term memory representations of word form and meaning. (c) 2006 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0148,Age estimation using deep learning,"Age has always been an important attribute of identity. It also has been an important factor in social interaction. The posture, vocabulary, facial wrinkles and the intonation are all elements that facilitate the prediction of the user's age. Age estimation from the face by numerical analysis finds many potential applications such as the development of intelligent human-machine interfaces and improvement of safety and protection in various sectors such as transport, security and medicine. In many works, researchers are particularly interested in the face's features to regress the age. Recent advances in Artificial Intelligence (AI) and particulary Deep Learning (DL) techniques increase motivations to use this methods to estimate age. In this work, we present a novel method for age estimation from a facial images based on autoencoders. Autoencoder is an artificial neural network used for unsupervised learning of efficient coding. Its aim is to learn a representation for a set of data. The purpose of this work is to exploit the performance of autoencoders to learn features in a supervised manner to estimate user's age. We use MORPH, FG-NET datasets to test the performance of our proposed method. Experimental results show the robustness and effectiveness of the proposed method through the MAE (Men Average Error) rate showing a value of 3.34% for MORPH dataset and 3.75% for FG-NET.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0149,Processing fluency of the forms and sounds of Chinese characters,"The goal of this study is to investigate whether different types of structures and lexical tones of Chinese characters cause different processing fluency. In Experiment 1, participants' explicit affective assessments of Chinese characters with different structures, frequencies, and lexical tones were analyzed. Results indicated that participants showed explicit preferences and dispreferences to different structures and lexical tones. In Experiment 2, participants' implicit responses to different structures and lexical tones were investigated using a metaphor experimental paradigm. Results were consistent with the major findings of Experiment 1. In Experiments 3 and 4, participants' recognition of words of different structures and lexical tones were analyzed. Results revealed that participants had a better memory for Surround structure characters when stimuli were visually presented and for Tone 3 when stimuli were auditorily presented. Finally, the significance and implications of this study are discussed. (C) 2010 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0150,Synaesthesia in a logographic language: The colouring of Chinese characters and Pinyin/Bopomo spellings,"Studies of linguistic synaesthesias in English have shown a range of fine-grained language mechanisms governing the associations between colours on the one hand, and graphemes, phonemes and words on the other. However, virtually nothing is known about how synaesthetic colouring might operate in non-alphabetic systems. The current study shows how synaesthetic speakers of Mandarin Chinese come to colour the logographic units of their language. Both native and non-native Chinese speakers experienced synaesthetic colours for characters, and for words spelled in the Chinese spelling systems of Pinyin and Bopomo. We assessed the influences of lexical tone and Pinyin/Bopomo spelling and showed that synaesthetic colours are assigned to Chinese words in a non-random fashion. Our data show that Chinese-speaking synaesthetes with very different native languages can exhibit both differences and similarities in the ways in which they come to colour their Chinese words. (C) 2011 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0151,Meta-analytic evidence for the non-modularity of pitch processing in congenital amusia,"A major theme driving research in congenital amusia is related to the modularity of this musical disorder, with two possible sources of the amusic pitch perception deficit. The first possibility is that the amusic deficit is due to a broad disorder of acoustic pitch processing that has the effect of disrupting downstream musical pitch processing, and the second is that amusia is specific to a musical pitch processing module. To interrogate these hypotheses, we performed a meta-analysis on two types of effect sizes contained within 42 studies in the amusia literature: the performance gap between amusics and controls on tasks of pitch discrimination, broadly defined, and the correlation between specifically acoustic pitch perception and musical pitch perception. To augment the correlation database, we also calculated this correlation using data from 106 participants tested by our own research group. We found strong evidence for the acoustic account of amusia. The magnitude of the performance gap was moderated by the size of pitch change, but not by whether the stimuli were composed of tones or speech. Furthermore, there was a significant correlation between an individuals acoustic and musical pitch perception. However, individual cases show a double dissociation between acoustic and musical processing, which suggests that although most amusic cases are probably explainable by an acoustic deficit, there is heterogeneity within the disorder. Finally, we found that tonal language fluency does not influence the performance gap between amusics and controls, and that there was no evidence that amusics fare worse with pitch direction tasks than pitch discrimination tasks. These results constitute a quantitative review of the current literature of congenital amusia, and suggest several new directions for research, including the experimental induction of amusic behaviour through transcranial magnetic stimulation (TMS) and the systematic exploration of the developmental trajectory of this disorder. (C) 2015 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0152,"Voice actors show enhanced neural tracking of pitch, prosody perception, and music perception","Experiences with sound that make strong demands on the precision of perception, such as musical training and experience speaking a tone language, can enhance auditory neural encoding. Are high demands on the precision of perception necessary for training to drive auditory neural plasticity? Voice actors are an ideal subject population for answering this question. Voice acting requires exaggerating prosodic cues to convey emotion, character, and linguistic structure, drawing upon attention to sound, memory for sound features, and accurate sound production, but not fine perceptual precision. Here we assessed neural encoding of pitch using the frequency-following response (FFR), as well as prosody, music, and sound perception, in voice actors and a matched group of non-actors. We find that the consistency of neural sound encoding, prosody perception, and musical phrase perception are all enhanced in voice actors, suggesting that a range of neural and behavioural auditory processing enhancements can result from training which lacks fine perceptual precision. However, fine discrimination was not enhanced in voice actors but was linked to degree of musical experience, suggesting that low-level auditory processing can only be enhanced by demanding perceptual training. These findings suggest that training which taxes attention, memory, and production but is not perceptually taxing may be a way to boost neural encoding of sound and auditory pattern detection in individuals with poor auditory skills. (c) 2024 The Authors. Published by Elsevier Ltd. This is an open access article under the CC",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0153,A fuzzy decision tree-based duration model for Standard Yoruba text-to-speech synthesis,"In this paper, we present syllable-based duration modelling in the context of a prosody model for Standard Yoruba (SY) text-to-speech (TTS) synthesis applications. Our prosody model is conceptualised around a modular holistic framework. This framework is implemented using the Relational Tree (R-Tree) techniques. An important feature of our R-Tree framework is its flexibility in that it facilitates the independent implementation of the different dimensions of prosody, i.e. duration. intonation, and intensity, using different techniques and their subsequent integration. We applied the Fuzzy Decision Tree (FDT) technique to model the duration dimension. In order to evaluate the effectiveness of FDT in duration modelling. we have also developed a Classification And Regression Tree (CART) based duration model using the same speech data. Each of these models was integrated into Our R-Tree based prosody model. We performed both quantitative (i.e. Root Mean Square Error (RMSE) and Correlation (Corr)) and qualitative (i.e. intelligibility and naturalness) evaluations on the two duration models. The results show that CART models the training data more accurately than FDT. The FDT model, however, shows a better ability to extrapolate from the training data since it achieved a better accuracy for the test data set. Our qualitative evaluation results show that our FDT model produces synthesised speech that is perceived to be more natural than our CART model. In addition, we also observed that the expressiveness of FDT is much better than that of CART. That is because the representation in FDT is not restricted to a set of piece-wise or discrete constant approximation. We, therefore, conclude that the FDT approach is a practical approach for duration modelling in SY TTS applications. (c) 2006 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0154,"Combining lexical, syntactic and prosodic cues for improved online dialog act tagging","Prosody is ail important cue for identifying dialog acts. in this paper, we show that modeling the sequence of acoustic-prosodic values as n-gram features with a maximum entropy model for dialog act (DA) tagging can perform better than conventional approaches that use coarse representation of the prosodic contour through summative statistics of the prosodic contour. The proposed scheme for exploiting prosody results in,in absolute improvement of 8.7% over the use of most other widely used representations of acoustic correlates of prosody. the proposed scheme is discriminative and exploits context in the form of lexical, syntactic and prosodic cues from preceding discourse segments. Such a decoding scheme facilitates online DA tagging and offers robustness in the decoding process, unlike greedy decoding schemes that can potentially propagate errors. Our approach is different front traditional DA systems that use the entire conversation for offline dialog act decoding with the aid of a discourse model. In contrast, we use only static features and approximate the previous dialog act tags in terms of lexical, syntactic and prosodic information extracted from previous utterances. Experiments oil the Switchboard-DAMSL corpus, using only lexical, syntactic and prosodic cues from three previous utterances, yield a DA tagging accuracy of 72% compared to the best case scenario with accurate knowledge of previous DA tags (oracle), which results in 74% accuracy. (C) 2009 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0155,A three-stage approach to the automated scoring of spontaneous spoken responses,"This paper presents a description and evaluation of Speech Rater(SM), a system for automated scoring of non-native speakers' spoken English proficiency, based on tasks which elicit spontaneous monologues on particular topics. This system builds on much previous work in the automated scoring of test responses, but differs from previous work in that the highly unpredictable nature of the responses to this task type makes the challenge of accurate scoring much more difficult. SpeechRater uses a three-stage architecture. Responses are first processed by a filtering model to ensure that no exceptional conditions exist which might prevent them from being scored by SpeechRater. Responses not filtered out at this stage are then processed by the scoring model to estimate the proficiency rating which a human might assign to them, on the basis of features related to fluency, pronunciation, vocabulary diversity, and grammar. Finally, an aggregation model combines an examinee's scores for multiple items to calculate a total score, as well as an interval in which the examinee's score is predicted to reside with high confidence. SpeechRater's current level of accuracy and construct representation have been deemed sufficient for low-stakes practice exercises, and it has been used in a practice exam for the TOEFL since late 2006. In such a practice environment, it offers a number of advantages compared to human raters, including system load management, and the facilitation of immediate feedback to students. However, it must be acknowledged that SpeechRater presently fails to measure many important aspects of speaking proficiency (such as intonation and appropriateness of topic development), and its agreement with human ratings of proficiency does not yet approach the level of agreement between two human raters. (C) 2010 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0156,Enriching machine-mediated speech-to-speech translation using contextual information,"Conventional approaches to speech-to-speech (S2S) translation typically ignore key contextual information such as prosody, emphasis, discourse state in the translation process. Capturing and exploiting such contextual information is especially important in machine-mediated S2S translation as it can serve as a complementary knowledge source that can potentially aid the end users in improved understanding and disambiguation. In this work, we present a general framework for integrating rich contextual information in S2S translation. We present novel methodologies for integrating source side context in the form of dialog act (DA) tags, and target side context using prosodic word prominence. We demonstrate the integration of the DA tags in two different statistical translation frameworks, phrase-based translation and a bag-of-words lexical choice model. In addition to producing interpretable DA annotated target language translations, we also obtain significant improvements in terms of automatic evaluation metrics such as lexical selection accuracy and BLEU score. Our experiments also indicate that finer representation of dialog information such as yes no questions, wit-questions and open questions are the most useful in improving translation quality. For target side enrichment, we employ factored translation models to integrate the assignment and transfer of prosodic word prominence (pitch accents) during translation. The factored translation models provide significant improvement in assignment of correct pitch accents to the target words in comparison with a post-processing approach. Our framework is suitable for integrating any word or utterance level contextual information that can be reliably detected (recognized) from speech and/or text. (C) 2011 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0157,Linking bottom-up intonation stylization to discourse structure,"A new approach for intonation stylization that enables the extraction of an intonation representation from prosodically unlabeled data is introduced. This approach yields global and local intonation contour classes arising from a contour-based, parametric and superpositional intonation stylization. Based on findings about the linguistic interpretation of the contour classes derived from corpus statistics and perception experiments, we created simple prediction models for the partial generation of intonation contours from discourse structure defined by discourse segment boundaries and the information status of nouns within these segments. The predicted intonation contours were evaluated by human judgments of adequacy that yielded a high accordance. (C) 2014 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0158,Hierarchical representation and estimation of prosody using continuous wavelet transform,"Prominences and boundaries are the essential constituents of prosodic structure in speech. They provide for means to chunk the speech stream into linguistically relevant units by providing them with relative saliences and demarcating them within utterance structures. Prominences and boundaries have both been widely used in both basic research on prosody as well as in text-to-speech synthesis. However, there are no representation schemes that would provide for both estimating and modelling them in a unified fashion. Here we present an unsupervised unified account for estimating and representing prosodic prominences and boundaries using a scale-space analysis based on continuous wavelet transform. The methods are evaluated and compared to earlier work using the Boston University Radio News corpus. The results show that the proposed method is comparable with the best published supervised annotation methods. (C) 2016 Elsevier Ltd. All rights reserved",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0159,Annotating and modeling empathy in spoken conversations,"Empathy, as defined in behavioral sciences, expresses the ability of human beings to recognize, understand and react to emotions, attitudes and beliefs of others. In this paper, we address two related problems in automatic affective behavior analysis: the design of the annotation protocol and the automatic recognition of empathy from human human dyadic spoken conversations. We propose and evaluate an annotation scheme for empathy inspired by the modal model of emotions. The annotation scheme was evaluated on a corpus of real-life, dyadic spoken conversations. In the context of behavioral analysis, we designed an automatic segmentation and classification system for empathy. Given the different speech and language levels of representation where empathy may be communicated, we investigated features derived from the lexical and acoustic spaces. The feature development process was designed to support both the fusion and automatic selection of relevant features from a high dimensional space. The automatic classification system was evaluated on call center conversations where it showed significantly better performance than the baseline. (C) 2017 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0160,Developmental changes in brain response to speech perception in late-talking children: A longitudinal MMR study,"This study used a longitudinal design to examine the development of mismatch responses (MMRs) to Mandarin lexical tones, an index of neural speech discriminative responses, in late talkers and typical controls at 3, 5, and 6 years of age. Lexical tones are phonetic suprasegments that distinguish the lexical meanings of syllables in tonal languages. The 2 year-old late talkers were later divided into persistent language delay and late bloomer groups according to their performance on standardized language tests at 4 years. Results showed that children with persistent language delay demonstrated more positive mismatch responses than the typical controls at 3 years of age. At the age of 5, no group difference were found in the amplitude of MMRs, but the maturation of MMRs could be observed in the change of topography, with more prominent negative response in the frontal sites only in the typical group. Correlations were found between the index of MMRs at 3 years and children's language performance outcome at 6 years. Our results indicate that the development of fine-grained tone representations is delayed in late-talking children between 3 and 5 years and may be one of the underlying mechanisms which associated with later language performance. (C) 2016 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0161,Adult-like processing of time-compressed speech by newborns: A NIRS study,"Humans can adapt to a wide range of variations in the speech signal, maintaining an invariant representation of the linguistic information it contains. Among them, adaptation to rapid or time-compressed speech has been well studied in adults, but the developmental origin of this capacity remains unknown. Does this ability depend on experience with speech (if yes, as heard in utero or as heard postnatally), with sounds in general or is it experience-independent? Using near-infrared spectroscopy, we show that the newborn brain can discriminate between three different compression rates: normal, i.e. 100% of the original duration, moderately compressed, i.e. 60% of original duration and highly compressed, i.e. 30% of original duration. Even more interestingly, responses to normal and moderately compressed speech are similar, showing a canonical hemodynamic response in the left temporoparietal, right frontal and right temporal cortex, while responses to highly compressed speech are inverted, showing a decrease in oxyhemoglobin concentration. These results mirror those found in adults, who readily adapt to moderately compressed, but not to highly compressed speech, showing that adaptation to time-compressed speech requires little or no experience with speech, and happens at an auditory, and not at a more abstract linguistic level. (C) 2016 The Authors. Published by Elsevier Ltd.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0162,Datasets on the production and perception of underlying and epenthetic glottal stops in Maltese,"This article provides some supplementary analysis data of speech production and perception of glottal stops in the Semitic language Maltese. In Maltese, a glottal stop can occur as a phoneme, but also as a phonetic marker of vowel-initial words (as in the case with Germanic languages like English). Data from four experiments are provided, which will allow other researchers to reproduce the results and apply their own data-analysis techniques to these data for further data exploration. A production experiment (Experiment 1) investigates how often the glottal marking of vowel-initial words occurs (causing vowel-initial words to be ambiguous with words starting with a glottal stop as a phoneme) and whether the glottal gesture for this marking can be differentiated from an underlying (phonemic) glottal stop in its acoustic properties. Experiments 2 to 4 investigate how and to what extent Maltese listeners perceive glottal markings as lexical (phonemic) or epenthetic (phonetic), using a two-alternative forced choice task (Experiment 2), a visual-world eye tracking task with printed target words (Experiment 3) and a gating task (Experiment 4). A full account of theoretical consequences of these data can be found in the full length article entitled ""The glottal stop between segmental and suprasegmental processing: The case of Maltese""[1]. (C) 2020 The Author(s). Published by Elsevier Inc.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0163,"A dataset for classifying phrases and sentences into statements, questions, or exclamations based on sound pitch","Speech is the most fundamental and sophisticated channel of human communication, and breakthroughs in Natural Language Processing (NLP) have substantially raised the quality of human-computer interaction. In particular, new wave of deep learning methods have significantly advanced human speech recognition by obtaining fine-grained acoustic cues including pitch, an acoustic feature that can be a critical ingredient in understanding communicative intent. Pitch variation is in particular important for prosodic classification tasks (i.e., statements, questions, and exclamations), which is crucial in tonal and low resource languages such as Kurdish, where intonation holds significant semantic information. This paper presents the dataset of the Statements, Questions, or Exclamations Based on Sound Pitch (SQEBSP) which contains 12,660 professionally-recorded speech audio clips by 431 native Kurdish speakers who reside in the Kurdistan Region of Iraq. Regarding utterances, 10 new phrases were articulated by each speaker per three prosodic categories: statements, questions, and exclamations. All utterances were digitized at 16 kHz and then manually checked for correctness concerning pitch-based classification. The dataset contains equal representation from all three classes, about 4200 samples per class, and metadata such as speaker gender, age group, and sentence identifiers. The original audio files, alongside resources like Mel-Frequency Cepstral Coefficients (MFCCs) and waveform visualizations, can be found on Mendeley Data. The dataset offered has significant advantages for formulating and testing pitch-based speech classification algorithms, furthers the work on pronunciation modelling for languages lacking sufficient resources. It furthermore, aids in developing speech technologies sensitive to dialects. (c) 2025 Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/)",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0164,Novel feature representation using single frequency filtering and nonlinear energy operator for speech emotion recognition,"In this paper, the intrinsic characteristics of speech modulations are estimated to propose the instant modulation spectral features for efficient emotion recognition. This feature representation is based on single frequency filtering (SFF) technique and higher order nonlinear energy operator. The speech signal is decomposed into frequency sub-bands using SFF, and associated nonlinear energies are estimated with higher order nonlinear energy operator. Then, the feature vector is realized using cepstral analysis. The high-resolution property of SFF technique is exploited to extract the amplitude envelope of the speech signal at a selected frequency with good time-frequency resolution. The fourth order nonlinear energy operator provides noise robustness in estimating the modulation components. The proposed feature set is tested for the emotion recognition task using the i-vector model with the probabilistic linear discriminant scoring scheme, support vector machine and random forest classifiers. The results demonstrate that the performance of this feature representation is better than the widely used spectral and prosody features, achieving detection accuracy of 85.75%, 59.88%, and 65.78% on three emotional databases, EMODB, FAUAIBO, and IEMOCAP, respectively. Further, the proposed features are found to be robust in the presence of additive white Gaussian and vehicular noises. (C) 2021 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0165,Developing children's ASR system under low-resource conditions using end-to-end architecture,"The work presented in this paper aims at enhancing the performance of end -to -end (E2E) speech recognition task for children's speech under low resource conditions. For majority of the languages, there is hardly any speech data from child speakers. Furthermore, even the available children's speech corpora are limited in terms of the number of hours of data. On the other hand, large amounts of adults' speech data are freely available for research as well as commercial purposes. As a consequence, developing an effective E2E automatic speech recognition (ASR) system for children becomes a very challenging task. One may develop an ASR system using adults' speech and then use it to transcribe children's data, but this leads to very poor recognition rates due to the stark differences in the acoustic attributes of adults' and children's speech. In order to overcome these hurdles and to develop a robust children's ASR system employing E2E architecture, we have resorted to several out -of -domain and in -domain data augmentation techniques. For out -of -domain data augmentation, we have explicitly modified adults' speech to render it acoustically similar to that of children's speech before pooling into training. On the other hand, in the case of in -domain data augmentation, we have slightly modified the pitch and duration of children's speech in order to create more data capturing greater diversity. Data augmentation approaches helps in mitigating the ill-effects resulting from the scarcity of data from child domain to a certain extent. This, in turn, reduces the error rates by a large margin. In addition to data augmentation, we have also studied the efficacy of Gamma -tone frequency cepstral coefficients (GFCC) and frequency domain linear prediction (FDLP) technique along with the most commonly used Mel -frequency cepstral coefficients (MFCC) for front-end speech parameterization. Both MFCC as well as GFCC capture and model the spectral envelope of speech. On the other hand, application of linear prediction on the frequency domain representation of speech signal helps to effectively capture the temporal envelope during front-end feature extraction. Employing FDLP features that model the temporal envelope provides important cues for the perception and understanding of stop bursts and, at times, complete phonemes. This motivated us to perform a comparative experimental study of the effectiveness of the three aforementioned front-end acoustic features. In our experimental explorations, the use of proposed data augmentation in combination of FDLP features has shown a relative improvement in character error rate by 67.6% over the baseline system. The combination of data augmentation with MFCC or GFCC features is observed to result in lower recognition performances.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0166,Spectral and temporal cues for speech recognition: Implications for auditory prostheses,"Features of stimulation important for speech recognition in people with normal hearing and in people using implanted auditory prostheses include spectral information represented by place of stimulation along the tonotopic axis and temporal information represented in low-frequency envelopes of the signal. The relative contributions of these features to speech recognition and their interactions have been studied using vocoder-like simulations of cochlear implant speech processors presented to listeners with normal hearing. In these studies, spectral/place information was manipulated by varying the number of channels and the temporal-envelope information was manipulated by varying the lowpass cutoffs of the envelope extractors. Consonant and vowel recognition in quiet reached plateau at 8 and 12 channels and lowpass cutoff frequencies of 16 Hz and 4 Hz, respectively. Phoneme (especially vowel) recognition in noise required larger numbers of channels. Lexical tone recognition required larger numbers of channels and higher lowpass cutoff frequencies. There was a tradeoff between spectral/place and temporal-envelope requirements. Most current auditory prostheses seem to deliver adequate temporal-envelope information, but the number of effective channels is suboptimal, particularly for speech recognition in noise. lexical tone recognition, and music perception. (C) 2007 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0167,Lexical tone recognition with spectrally mismatched envelopes,"It has been shown that frequency-place mismatch has detrimental effects on English speech recognition. The present study investigated the effects of mismatched spectral distribution of envelopes on Mandarin Chinese tone recognition using a noise-excited vocoder. In Experiment 1, speech samples were processed to simulate a cochlear implant with various insertion depths. The carrier bands were shifted basally relative to the analysis bands by 1-7 mm in the cochlea. Nine normal-hearing Mandarin Chinese listeners participated in this experiment. Basal shift of the carriers only slightly affected tone recognition. The resistance of tone recognition to spectral shift can be attributed to the overall amplitude contour cues that are independent from spectral manipulations. Experiment 2 examined the effects of frequency compression, where widened analysis bands by 2, 6, and 10 mm were compressively allocated to narrower carrier bands. Five of the 9 subjects participated in Experiment 2. It appears that the expanded frequency information especially on the low frequency end can compensate for the distortion from frequency compression. Thus, spectral shift might not pose a severe problem for tone recognition, and allocation of wider frequency range to include more low frequency information might be beneficial for tone recognition. (c) 2008 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0168,Perception of Mandarin Chinese with cochlear implants using enhanced temporal pitch cues,"A cochlear implant (Cl) signal processing strategy named F0 modulation (F0mod) was compared with the advanced combination encoder (ACE) strategy in a group of four post-lingually deafened Mandarin Chinese speaking Cl listeners. F0 provides an enhanced temporal pitch cue by amplitude modulating the multichannel electrical stimulation pattern at the fundamental frequency (F0) of the incoming speech signal. Word and sentence recognition tests were carried out in quiet and in noise. The responses for the word-recognition test were further segmented into phoneme and tone scores. Off-line implementations of ACE and F0mod were used, and electrical stimulation patterns were directly streamed to the CI subject's implant. To focus on the feasibility of enhanced temporal cues for tonal language perception, idealized PO information that was extracted from speech tokens in quiet was used in the F0mod processing of speech-in-noise mixtures. The results indicated significantly better lexical tone perception with the F0mod strategy than with ACE for the male voice (p < 0.05). No significant differences in sentence recognition were found between F0mod and ACE. (C) 2012 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0169,Contribution of bimodal hearing to lexical tone normalization in Mandarin-speaking cochlear implant users,"Native Mandarin normal-hearing (NH) listeners can easily perceive lexical tones even under conditions of great voice pitch variations across speakers by using the pitch contrast between context and target stimuli. It is however unclear whether cochlear implant (Cl) users with limited access to pitch cues can make similar use of context pitch cues for tone normalization. In this study, native Mandarin NH listeners and pre-lingually deafened unilaterally implanted CI users were asked to recognize a series of Mandarin tones varying from Tone 1 (high-flat) to Tone 2 (mid-rising) with or without a preceding sentence context. Most of the Cl subjects used a hearing aid (HA) in the non-implanted ear (i.e., bimodal users) and were tested both with CI alone and CI + HA. In the test without context, typical S-shaped tone recognition functions were observed for most CI subjects and the function slopes and perceptual boundaries were similar with either CI alone or CI + HA. Compared to NH subjects, CI subjects were less sensitive to the pitch changes in target tones. In the test with context, NH subjects had more (resp. fewer) Tone-2 responses in a context with high (resp. low) fundamental frequencies, known as the contrastive context effect. For Cl subjects, a similar contrastive context effect was found statistically significant for tone recognition with CI + HA but not with CI alone. The results suggest that the pitch cues from CIs may not be sufficient to consistently support the pitch contrast processing for tone normalization. The additional pitch cues from aided residual acoustic hearing can however provide Cl users with a similar tone normalization capability as NH listeners. (C) 2014 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0170,Effects of steep high-frequency hearing loss on speech recognition using temporal fine structure in low-frequency region,"The present study examined the effects of steep high-frequency sensorineural hearing loss (SHF-SNHL) on speech recognition using acoustic temporal fine structure (TFS) in the low-frequency region where the absolute thresholds appeared to be normal. In total, 28 participants with SHF-SNHL were assigned to 3 groups according to the cut-off frequency (1, 2, and 4 kHz, respectively) of their pure-tone absolute thresholds. Fourteen age-matched normal-hearing (NH) individuals were enrolled as controls. For each Mandarin sentence, the acoustic TFS in 10 frequency bands (each 3-ERB wide) was extracted using the Hilbert transform and was further lowpass filtered at 1, 2, and 4 kHz. Speech recognition scores were compared among the NH and 1-, 2-, and 4-kHz SHF-SNHL groups using stimuli with varying bandwidths. Results showed that speech recognition with the same TFS-speech stimulus bandwidth differed significantly in groups and filtering conditions. Sentence recognition in quiet conditions was better than that in noise. Compared with the NH participants, nearly all the SHF-SNHL participants showed significantly poorer sentence recognition within their frequency regions with ""normal hearing"" (defined clinically by normal absolute thresholds) in both quiet and noisy conditions. These may result from disrupted auditory nerve function in the ""normal hearing"" low-frequency regions. (C) 2015 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0171,Voice emotion perception and production in cochlear implant users,"Voice emotion is a fundamental component of human social interaction and social development. Unfortunately, cochlear implant users are often forced to interface with highly degraded prosodic cues as a result of device constraints in extraction, processing, and transmission. As such, individuals with cochlear implants frequently demonstrate significant difficulty in recognizing voice emotions in comparison to their normal hearing counterparts. Cochlear implant-mediated perception and production of voice emotion is an important but relatively understudied area of research. However, a rich understanding of the voice emotion auditory processing offers opportunities to improve upon CI biomedical design and to develop training programs benefiting CI performance. In this review, we will address the issues, current literature, and future directions for improved voice emotion processing in cochlear implant users. (C) 2017 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0172,Vowel and tone recognition in quiet and in noise among Mandarin-speaking amusics,"Music and language are two intricately linked communication modalities in humans. A deficit in music pitch processing as manifested in the condition of congenital amusia has been related to difficulties in lexical tone processing for both tone and non-tonal languages. However, it is still unclear whether amusia also affects the perception of vowel phonemes in quiet and in noise. In this study, we examined vowel-plustone identification in quiet and noise conditions among Mandarin-speaking amusics with and without speech tone difficulties (tone agnosics and pure amusics, respectively), and IQ- and age-matched controls. Overall, pure amusics showed vowel and tone identification comparable to the controls in both quiet and noise conditions. Compared to pure amusics and controls, tone agnosics showed deficits in tone perception in both quiet and noise conditions. More importantly, their vowel perception was lower than pure amusics and controls in noise conditions, e.g., at a signal-to-noise ratio of -4 dB, although they showed normal-like performance in quiet and at a signal-to-noise ratio of -8 dB. These results suggest that when amusia affected speech tone processing (e.g., tone agnosics), it could also compromise vowel processing in noise. However, amusia alone does not affect tone or vowel perception in Mandarin Chinese either in quiet or in noise. Overall, the current study highlights the necessity of taking heterogeneity within the amusic group into account when considering the related speech deficits in this group. (C) 2018 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0173,"Effect of blindness on mismatch responses to Mandarin lexical tones, consonants, and vowels","According to the hypothesis of auditory compensation, blind listeners are more sensitive to auditory input than sighted listeners. In the current study, we employed the passive oddball paradigm to investigate the effect of blindness on listeners' mismatch responses to Mandarin lexical tones, consonants, and vowels. Twelve blind and twelve sighted age- and verbal IQ-matched adults with normal hearing participated in this study. Our results indicated that blind listeners possibly had a more efficient pre-attentive processing (shorter MMN peak latency) of lexical tones in the tone-dominant hemisphere (i.e., the right hemisphere); and that they exhibited greater sensitivity (larger MMN amplitude) when processing phonemes (consonants and/or vowels) at the pre-attentive stage in both hemispheres compared with sighted individuals. However, we observed longer MMN and P3a peak latencies during phoneme processing in the blind versus control participants, indicating that blind listeners may be slower in terms of pre-attentive processing and involuntary attention switching when processing phonemes. This could be due to a lack of visual experience in the production and perception of phonemes. In a word, the current study revealed a two-sided influence of blindness on Mandarin speech perception. (C) 2018 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0174,Deficient sensory and cognitive processing in children with cochlear implants: An event-related potential study,"Compared with children having normal hearing (NH), those with cochlear implants (CIs) perform poorly in spoken language comprehension which involves both low-level acoustic encoding and higher-level cognitive processing. Here, we performed an electroencephalography study to portray this brain dynamics of speech perception in CI children. We presented a Mandarin Chinese monosyllable or four-syllable idiom to CI and NH children, and infrequently varied its lexical tone to form a novel monosyllable or pseudo idiom in an oddball paradigm. The tone contrast embedded in the monosyllables evoked similar mismatch negativities (MMNs) in CI and NH children at an early stage (similar to 200 ms). However, the amplitude of the MMN evoked by the tone contrast in the idiom context was significantly lower in CI children than in NH children. Furthermore, robust late discriminative negativity (LDN) at a late stage (500 ms) was found only in NH children, but not in CI children. The MMN and LDN findings indicate deficits of low-level acoustic encoding in a complex context (such as an idiom) and higher-level cognitive processing in CI children, respectively. Both deficient sensory and cognitive processing may contribute to the speech perception difficulties in CI children. (C) 2021 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0175,Auditory cortex and beyond: Deficits in congenital amusia,"Congenital amusia is a neuro-developmental disorder of music perception and production, with the observed deficits contrasting with the sophisticated music processing reported for the general population. Musical deficits within amusia have been hypothesized to arise from altered pitch processing, with impairments in pitch discrimination and, notably, short-term memory. We here review research investigating its behavioral and neural correlates, in particular the impairments at encoding, retention, and recollection of pitch information, as well as how these impairments extend to the processing of pitch cues in speech and emotion. The impairments have been related to altered brain responses in a distributed fronto-temporal network, which can be observed also at rest. Neuroimaging studies revealed changes in connectivity patterns within this network and beyond, shedding light on the brain dynamics underlying auditory cognition. Interestingly, some studies revealed spared implicit pitch processing in congenital amusia, showing the power of implicit cognition in the music domain. Building on these findings, together with audiovisual integration and other beneficial mechanisms, we outline perspectives for training and rehabilitation and the future directions of this research domain.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0176,Perception of voice cues and speech-in-speech by children with prelingual single-sided deafness and a cochlear implant,"Voice cues, such as fundamental frequency (F0) and vocal tract length (VTL), help listeners identify the speaker's gender, perceive the linguistic and emotional prosody, and segregate competing talkers. Postlingually implanted adult cochlear implant (CI) users seem to have difficulty in perceiving and making use of voice cues, especially of VTL. Early implanted child CI users, in contrast, perceive and make use of both voice cues better than CI adults, and in patterns similar to their peers with normal hearing (NH). In our study, we investigated the perception and use of voice cues in children with single-sided deafness (SSD) who received their CI at an early age (SSD+CI), in an attempt to bridge the gap between these two groups. The SSD+CI children have access to bilateral auditory information and often receive their CI at an early age, similar to CI children. They may also have dominant acoustic representations, similar to CI adults who acquired hearing loss at a later age. As such, the current study aimed to investigate the perception and use of voice cues by a group of nine early-implanted children with prelingual SSD. The study consisted of three experiments: F0 and VTL discrimination, voice gender categorization, and speech-in-speech perception. In each experiment, the results of the SSD group are compared to children and adults with CIs (for their CI ear) and with typical hearing (for their NH ear). Overall, the SSD+CI children had poorer VTL detection thresholds with their CI compared to their NH ear, while their F0 perception was similar across ears. Detection thresholds for both F0 and VTL with their CI ear was comparable to those of bilaterally implanted CI children, suggesting that SSD+CI children do not only rely on their NH ear, but actually make use of their CI. SSD+CI children relied more heavily on F0 cues than on VTL cues for voice gender categorization, with cue weighting patterns comparable to those of CI adults. In contrast to CI children, the SSD+CI children showed limited speech perception benefit based on F0 and VTL differences between the target and masker speaker, which again corresponded to the results of CI adults. Altogether, the SSD+CI children make good use of their CI, despite a good-hearing ear, however, the perceptual patterns seem to fall in-between those of CI children and CI adults. Perhaps a combination of childhood neuroplasticity, limited experience with relying only on the CI, and a dominant acoustic representation of voice gender explain these results.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0177,Effects of fine structure and extended low frequencies in pediatric cochlear implant recipients,"Objective: In recent years, new speech coding strategies have been developed with the aim of improving the transmission of temporal fine structure to cochlear implant recipients. This study reports on the implementation of one such strategy (fine structure processing, FSP) in children. Methods: This was a prospective study investigating the upgrade tc a new speech processor. The upgrade used a repeated measures design with an alternating order of conditions (A-B-A-B design). Twelve pre- and perilingually deaf children with MED-EL C40+ cochlear implants were enrolled in the study. Patients were upgraded from their Tempo+ speech processor, which used continuous interleaved sampling (CIS) in combination with a frequency spectrum of 200-8500 Hz, to an Opus speech processor, which used FSP with an extended frequency spectrum of 70-8500 Hz. The primary means of testing was an HSM (Hochmair, Schulz and Moser) sentence test at 65 and 80 dB in quiet. In addition, the ""Mainzer Kindersprachtest"" (Mainz audiometric speech test for children) was applied at 65 and 70 dB. Results: When the new FSP speech processor was used together with the extended low frequency range, HSM sentence tests at 65 and 80 dB resulted in scores indicating statisticially significant improvements of 7.1 and 9.9 percentage points, respectively. Scores in the ""Mainzer Kindersprachtest"" at 65 and 70 dB indicated statistically significant improvements of 9.3 and 6.1 percentage points, respectively. Conclusions: The present study clearly shows that children benefit from the fine structure speech coding strategy in combination with an extended frequency spectrum in tire low frequencies, as is offered by the Opus speech processors. This should be taken into consideration when fitting pre- and perilingually deaf children implanted almost a decade previously. (C) 2011 Elsevier Ireland Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0178,Mandarin Chinese speech recognition by pediatric cochlear implant users,"Objectives: Because of difficulties associated with pediatric speech testing, most pediatric cochlear implant (Cl) speech studies necessarily involve basic and simple perceptual tasks. There are relatively few studies regarding Mandarin-speaking pediatric Cl users' perception of more difficult speech materials (e.g., words and sentences produced by multiple talkers). Difficult speech materials and tests necessarily require older pediatric Cl users, who may have different etiologies of hearing loss, duration of deafness, Cl experience. The present study investigated how pediatric Cl patient demographics influence speech recognition performance with relatively difficult test materials and methods. Methods: In this study, open-set recognition of multi-talker (two males and two females) Mandarin Chinese disyllables and sentences were measured in 37 Mandarin-speaking pediatric Cl users. Subjects were grouped according to etiology of deafness and previous acoustic hearing experience. Group 1 subjects were all congenitally deafened with little-to-no acoustic hearing experience. Group 2 subjects were not congenitally deafened and had substantial acoustic hearing experience prior to implantation. Multiple linear regression analyses were performed within each group using subject demographics such as age at implantation and age at testing. Results: Pediatric Cl performance was generally quite good. For Group 1, mean performance was 82.3% correct for disyllables and 82.8% correct for sentences. For Group 2, mean performance was 76.6% correct for disyllables and 84.4% correct for sentences. For Group 1, multiple linear regression analyses showed that age at implantation predicted disyllable recognition, and that age at implantation and age at testing predicted sentence recognition. For Group 2, neither age at implantation nor age at testing predicted disyllable or sentence recognition. Performance was significantly better with the female than with the male talkers. Conclusions: Consistent with previous studies' findings, early implantation provided a significant advantage for profoundly deaf children. Performance for both groups was generally quite good for the relatively difficult materials and tasks, suggesting that open-set word and sentence recognition may be useful in evaluating speech performance with older pediatric Cl users. Differences in disyllable recognition between Groups 1 and 2 may reflect differences in adaptation to electric stimulation. The Group 1 subjects developed speech patterns exclusively via electric stimulation, while the Group 2 subjects adapted to electric stimulation relative to previous acoustic patterns. (C) 2011 Elsevier Ireland Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0179,Tone and sentence perception in young Mandarin-speaking children with cochlear implants,"Objectives: The purpose of this study was to examine the outcomes of cochlear implantation in young children in terms of (1) perception of lexical tones in quiet, (2) perception of sentences in quiet and in noise, (3) the effects of five demographic variables (i.e., preoperative hearing level, age at implantation, duration of cochlear implants use, maternal educational level, and whether a child underwent a hearing aid trial before implantation) on lexical tone perception and sentence perception, and (4) the relationship between lexical tone perception and sentence perception. Methods: 96 participants, aged from 2.41 years to 7.09 years, were recruited in mainland China. The children exhibited normal cognitive abilities and received unilateral implants at an average age of 2.72 years, with ages ranging from 0.69 to 5 years of age. Results: The mean score for tone identification was 77% (SD = 13%; chance level = 50%). Tone 2/tone 3 was the most difficult tone contrast to identify. Children with a longer duration of CI use and whose mothers had more years of education tended to perform better in sentence perception in quiet and in noise. Having undergone a hearing aid trial before implantation and more residual hearing were additional factors contributing to better sentence perception in noise. The only demographical variable that related to tone perception in quiet was duration of CI. In addition, while there was a modest correlation between tone perception and sentence perception in quiet (r(s) = 0.47, p < 0.001), the correlation between tone perception in quiet and sentence perception in noise was much weaker (r(s) = 0.28, p < 0.05). Conclusions: The findings suggested that most young children who had been implanted before 5 years of age and had 1-3 years of implant use did not catch up with their aged peers with normal hearing in tone perception and sentence perception. The weak to moderate correlation between tone perception in quiet and sentence perception might imply that the improvement of tone perception in quiet may not necessarily contribute to sentence perception, especially in noise condition. (C) 2014 Elsevier Ireland Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0180,Mandarin lexical tones identification among children with cochlear implants or hearing aids,"Objectives: Mandarin Chinese is a lexical tone language that has four tones, with a change in tone denoting a change in lexical meaning. There are few studies regarding lexical tone identification abilities in deafened children using either cochlear implants (CIs) or hearing aids (HAs). Furthermore, no study has compared the lexical tone identification abilities of deafened children with their hearing devices turned on and off. The present study aimed to investigate the lexical tone identification abilities of deafened children with CIs or HAs. Methods: Forty prelingually deafened children (20 with CIs and 20 with HAs) participated in the study. In the HA group, 20 children were binaurally aided. In the CI group, all of the children were unilaterally implanted. All of the subjects completed a computerized lexical tone pairs test with their hearing devices turned on and off. The correct answers of all items were recorded as the total score and the correct answers of the tone pairs were recorded as subtotal scores. Results: No significant differences in the tone pair identification scores were found between the Cl group and HA group either with the devices turned on or off (t = 1.62, p = 0.11; t = 1.863, p = 0.07, respectively). The scores in the aided condition were higher than in the unaided condition regardless of the device used (t = 22.09, p < 0.001, in the HA group; t = 20.20, p < 0.001, in the CI group). Significantly higher scores were found in the tone pairs that contained tone 4. Age at fitting of the devices was correlated with tone identification abilities in both the CI and HA groups. Other demographic factors were not correlated with tone identification ability. Conclusions: The hearing device, whether a hearing aid or cochlear implant, is beneficial for tone identification. The lexical tone identification abilities were similar regardless of whether the subjects wore a HA or CI. Lexical tone pairs with different durations and dissimilar tone contour patterns are more easily identified. Receiving devices at earlier age tends to produce better lexical tone identification abilities in prelingually deafened children. (C) 2014 Elsevier Ireland Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0181,Development and validation of a new Mandarin tone identification test,"Objectives: The objectives of this study were to develop a new Mandarin tone identification test (MTIT) to assess the Mandarin tone identification ability of children with hearing impairment (HI) and at age around 7 years; and to evaluate the reliability and sensitivity of the MTIT. Methods: The word materials to be used in the MTIT were developed in Phase I. Monosyllables were chosen to represent the daily repertoire of young children and to avoid the influence of co-articulation and intonation. Each test stimulus set contained four words, with one target, one containing contrastive tone, and two unrelated distracters. All words were depicted using simple pictures, and the test targets in quiet or in noise were presented using recorded stimuli on a custom software. Phase II evaluated the reliability and sensitivity of the MTIT. Participants were 50 normal-hearing native-Mandarin speakers around 7 years of age. Results: In Phase I, the MTIT was developed as described above. The final test consists of 51 words that are within the vocabulary repertoire of children aged 7 years. In Phase II, with the Mandarin tone identification scores collected from 50 children, the repeated measure ANOVA showed a significant main effect of S/N on MTIT performance (p < 0.001). Pairwise comparisons revealed a significant difference in performance across the five S/N conditions (p < 0.01) when S/N varied from -30 to -10 dB. Cronbach's alpha at -15 dB S/N was 0.66, suggesting satisfactory internal consistency reliability. A paired-samples t-test showed that there was no significant difference between the test-retest scores across the five S/N conditions (p > 0.05). Conclusions: Compared with the available Mandarin tone identification tools, MTIT systematically evaluated the tone identification performance in noisy environment for normal hearing children at age around 7 years. Results also showed satisfactory internal consistency reliability, good test-retest reliability and good sensitivity. In the near future, MTIT could be used to evaluate tone perception ability of children with hearing impairment and help to design hearing rehabilitation strategies for this population at the age critical for their language learning. (C) 2014 Elsevier Ireland Ltd. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0182,Behavioral assessment of auditory processing disorder in children with non-syndromic cleft lip and/or palate,"Objective: Peripheral hearing disorders have been frequently described in children with non-syndromic cleft lip and/or palate (NSCL/P). However, auditory processing problems are rarely considered for children with NSCL/P despite their poor academic performance in general compared to their craniofacially normal peers. This study aimed to compare auditory processing skills, using behavioral assessment techniques, in school age children with and without NSCL/P. Methods: One hundred and forty one Mandarin-speaking children with NSCL/P aged from 6.00 to 15.67 years, and 60 age-matched, craniofacially normal children, were recruited. Standard hearing health tests were conducted to evaluate peripheral hearing. Behavioral auditory processing assessment included adaptive tests of temporal resolution (ATTR), and the Mandarin pediatric lexical tone and disyllabic-word picture identification test in noise (MAPPID-N). Results: Age effects were found in children with cleft disorder but not in the control group for gap detection thresholds with ATTR narrow band noise in the across-channel stimuli condition, with a significant difference in test performance between the 6 to 8 year group and 12 to 15 year group of children with NSCL/P. For MAPPID-N, the bilateral cleft lip and palate subgroup showed significantly poorer SNR-50% scores than the control group in the condition where speech was spatially separated from noise. Also, the cleft palate participants showed a significantly smaller spatial separation advantage for speech recognition in noise compared to the control group children. Conclusion: ATM gap detection test results indicated that maturation for temporal resolution abilities was not achieved in children with NSCL/P until approximately 8 years of age compared to approximately 6 years for craniofacially normal children. For speech recognition in noisy environments, poorer abilities to use timing and intensity cues were found in children with cleft palate and children with bilateral cleft lip and palate compared to craniofacially normal children. Consequently, it is worthwhile to consider the potential for auditory processing disorder in when assessing the auditory status of children with NSCL/P. (C) 2014 Elsevier Ireland Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0183,A mismatch negativity study in Mandarin-speaking children with sensorineural hearing loss,"Objective: a) To examine the effects of sensorineural hearing loss on the discriminability of linguistic and non-linguistic stimuli at the cortical level, and b) to examine whether the cortical responses differ based on the chronological age at intervention, the degree of hearing loss, or the acoustic stimulation mode in children with severe and profound hearing loss. Methods: Mismatch negativity (MMN) responses were collected from 43 children with severe and profound bilateral sensorineural hearing loss, and 20 children with normal hearing (age: 3-6 years). In the non-verbal stimulation condition, pure tones with frequencies of 1 kHz and 1.1 kHz were used as the standard and the deviant respectively. In the verbal stimulation condition, the Chinese mandarin tokens/ ba2/and/ba4/were used as the standard and the deviant respectively. Latency and amplitude of the MMN responses were collected and analyzed. Results: Overall, children with hearing loss showed longer latencies and lower amplitudes of the MMN responses to both non-verbal and verbal stimulations. The latency of the verbal/ba2/-/ba4/pair was longer than that of the nonverbal 1 kHz-1.1 kHz pair in both groups of children. Conclusions: Children with hearing loss, especially those who received intervention after 2 years of age, showed substantial weakness in the neural responses to lexical tones and pure tones. Thus, the chronological age when the children receive hearing intervention may have an impact on the effectiveness of discriminating between verbal and non-verbal signals. (C) 2016 Elsevier Ireland Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0184,Spoken word recognition in noise in Mandarin-speaking pediatric cochlear implant users,"Objective: The purpose of the present study was to compare spoken word recognition performance in the presence of speech spectrum-shaped noise and four-talker babbles in Mandarin-speaking children with cochlear implants (CIs). Methods: Participants included 33 children with unilateral CIs (with a mean age of 10.4 +/- 2.9 years old and a mean length of CI use of 7.5 +/- 3.0 years). The Standard Chinese version of Lexical Neighborhood Test was implemented in quiet, speech-spectrum-shaped noise (SSN), and four-talker babble (FTB). The signal-to-noise ratios (SNRs) were set at +5 and + 10 dB for both types of maskers. Participants responded by verbally repeating each word they heard and the response was scored as the percentage accuracy of recognition performance. A Generalized Linear Model (GLM) fitting, correlational tests, and a two-way repeated-measures ANOVA were conducted on the percent-correct data. Results: Word recognition in quiet was on average 74.5% correct but dropped to 57.3% and 48.8% correct for SSN and FTB at 10 dB SNR, respectively, and 44.4% and 32.6% correct for SSN and FTB at 5 dB SNR, respectively. In both quiet and noise conditions, the participants showed lower recognition accuracy for the hard words than for the easy words. Disyllabic words were recognized with higher accuracy rates than were the monosyllabic words. The GLM analysis revealed that all four tested factors (masker type, SNR, lexical neighborhood feature, and lexical type) showed significant impacts on word recognition in children with CIs. Word recognition scores in the two types of maskers were significantly correlated for the disyllabic words at both SNRs and monosyllabic words at 10 dB SNR. Conclusions: The present study demonstrated that the lexical features such as the lexical neighborhood characteristics and lexical type had significant effects on speech recognition performance in both quiet and noise conditions in pediatric CI users. Children with years of experience of CI use still encountered remarkable difficulties in everyday listening environment although their speech recognition in quiet reached relatively desired level. Fluctuating noise, such as speech babbles, caused greater challenge than steady-state noise for speech recognition in children with CIs.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0185,Tone perception development in Mandarin-speaking children with cochlear implants,"Objective: The purpose of this study was to examine the longer-term effects of cochlear implant (CI) use on tone perception by evaluating improvement in Mandarin tone recognition in children with CIs 2 and 3 years post CI activation, and to explore the effects of implant age, chronological age and duration of CI use on the development of tone perception.Methods: Tone perception was assessed in 29 bilateral profound hearing impaired children (mean chronological age = 4.6 years, SD = 0.7 years) with unilateral CIs at 24 and 36 months after CI activation using the tone perception subtest in the Mandarin Early Speech Perception (MESP-T) test.Results: Children's tone recognition for tone pairs and individual tones improved significantly between 2 and 3 years post CI use, showing an increase in average tone recognition score from 73.2% to 81.8%, which was significantly higher than chance level (i.e. 50%). There was no significant correlation between tone recognition ability and either implant age or chronological age at two evaluation points. Further analysis revealed that the tone recognition score for tone pair 2-3 was significantly lower than that for other tone pairs except for tone pair 2-4.Conclusions: Longer CI experience can significantly improve tone recognition ability in CI children between 2 and 3 years post CI activation.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0186,Lexical tone recognition in multi-talker babbles and steady-state noise by Mandarin-speaking children with unilateral cochlear implants or bimodal hearing,"Background and objectives: Lexical tone presents challenges to cochlear implant (CI) users especially in noise conditions. Bimodal hearing utilizes residual acoustic hearing in the contralateral side and may offer benefits for tone recognition in noise. The purpose of the present study was to evaluate tone recognition in both steady-state noise and multi-talker babbles by the prelingually-deafened, Mandarin-speaking children with unilateral CIs or bimodal hearing. Methods: Fifty-three prelingually-deafened, Mandarin-speaking children who received CIs participated in this study. Twenty-two of them were unilateral CI users and 31 wore a hearing aid (HA) in the contralateral ear (i.e., bimodal hearing). All subjects were tested for Mandarin tone recognition in quiet and in two types of maskers: speech-spectrum-shaped noise (SSN) and two-talker babbles (TTB) at four signal-to-noise ratios (-6, 0, +6, and +12 dB). Results: While no differences existed in tone recognition in quiet between the two groups, the Bimodal group outperformed the Unilateral CI group under noise conditions. The differences between the two groups were significant at SNRs of 0, +6, and +12 dB in the SSN conditions (all p < 0.05), and at SNRs of +6 and +12 dB of TTB conditions (both p < 0.01), but not significant at other conditions (p > 0.05). The TTB exerted a greater masking effect than the SSN for tone recognition in the Unilateral CI group as well as in the Bimodal group at all SNRs tested (all p < 0.05). Among demographic or audiometric variables, only age at implantation showed a weak but significant correlation with the mean tone recognition performance under the SSN conditions (r = -0.276, p = 0.045). However, when Bonferroni correction was applied to the correlation analysis results, the weak correlation became not significant. Conclusion: Prelingually-deafened children with CIs face challenges in tone perception in noisy environments, especially when the noise is fluctuating in amplitude such as the multi-talker babbles. Wearing a HA on the contralateral side when residual hearing permits is beneficial for tone recognition in noise.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0187,The roles of pitch type and lexicality in the hemispheric lateralization for lexical tone processing: An ERP study,"Previous studies proposed different views to explain the hemispheric lateralization of lexical tone processing. But how the acoustic and phonological information modulates it remains unclear. The acoustic information refers to the physical acoustic features of lexical tones, and the phonological information means the different word meanings differentiated by lexical tones. In the present study, we adopted the active oddball paradigm to explore the effects of pitch type and lexicality on native Cantonese speakers' lexical tone processing with the event related potential (ERP) technique. We used Cantonese level and contour tones (pitch type) to examine the role of acoustic information and real words and pseudowords (lexicality) to detect the phonological information's effect. The results showed that the pitch type and lexicality affected the N2b amplitudes between the left and right hemispheres interactively, while they did not play roles in P3b amplitudes. The results indicated that the acoustic and phonological information modulated the hemispheric lateralization of lexical tone processing interactively only in the early stage (N2b time window) but not in the later stage (P3b time window). The findings suggested a two-stage model interprets the hemispheric lateralization in lexical tone processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0188,Full-form vs. combinatorial processing of Chinese compound words: Evidence from mismatch negativity,"This study examined whether Chinese spoken compound words are processed via full-form access or combination through morphemes by recording mismatch negativity (MMN). MMN has been shown to be larger for linguistic units that involves full-form access (lexical MMN enhancement) and smaller for separate but combinable units (combinatorial MMN reduction). Chinse compound words were compared against pseudocompounds, which do not have full-form representations in the long-term memory and are ""illegal"" combinations. All stimuli were disyllabic (bimorphemic). Word frequency was manipulated with the prediction that low-frequency compounds are more likely processed combinatorially, while high-frequency ones are more likely accessed in full forms. The results showed that low-frequency words elicited smaller MMNs than pseudocompounds, which supported the prediction of combinatorial processing. However, neither MMN enhancement nor reduction was found for high -frequency words. These results were interpreted within the dual-route model framework that assumes simulta-neous access to words and morphemes.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0189,Toddlers' sensitivity to segmental and suprasegmental mispronunciations of familiar words,"Recent research has shown that children as young as 19 months demonstrate graded sensitivity to mispronunciations in consonant onsets and vowels in word recognition tasks. This is evident in their progressively diminishing attention to relevant objects (e.g., a dog) as mispronunciations increasingly deviate from the correct word form (such as /dog/ changing to /gog/, /kog/, or /sog/). Despite these sensitivities, uncertainties remain about their broad generalizability, especially regarding the differences between word onsets and codas, and between lexical segmental (consonants and vowels) and supra-segmental (e.g., lexical stress and tones) elements. The present study aimed to fill these gaps. Using the intermodal preferential paradigm, we conducted two experiments to evaluate toddlers' responses to coda and lexical tone mispronunciations. Our results revealed a linear decline in toddlers' attention to familiar objects as mispronunciations became more severe, suggesting that by 19-20 months, infants' lexical representations encompass detailed phonetic information of both segmental and supra-segmental categories. Moreover, our results indicate that toddlers utilize these details in lexical processing. Such findings offer a more comprehensive understanding of the phonetic structures within toddlers' early lexical representations, sheding light on the mechanisms toddlers use in processing various word positions, across different acoustic dimensions, and in multiple languages.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0190,Developmental trajectories of non-native tone perception differ between monolingual and bilingual infants learning a pitch accent language,"The developmental trajectories of tone perception among tone and non-tone language learning infants have received wide attention and discussion in recent decades under the perceptual attunement framework. Nevertheless, tone perception in infants from pitch accent and bilingual language backgrounds has not been well understood. The present study examined monolingual and bilingual Norwegian-learning infants' discrimination of two Cantonese tone contrasts at 5 and 10 months, ages corresponding to the onset and offset of perceptual attunement. Results showed that while monolingual infants were sensitive to the salient contrast, bilingual infants showed sensitivity to both contrasts at 10 months. In sum, infant age and bilingual language background affected discrimination. Pitch accent language experience or contrast salience may also play a role. The finding that early bilingual experience facilitated tone perception is of particular interest. It suggests that infant perception could be enhanced by a more complex linguistic environment on a broader level. As this was observed only at 10 months, cumulative exposure may be required for infants in a complex bilingual environment. Future studies should disambiguate explanations generated from the current finding, ranging from neurocognitive plasticity to perceptual salience, and from experience-dependent to independent possibilities.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0191,Phonetic-enriched text representation for Chinese sentiment analysis with reinforcement learning,"The Chinese pronunciation system offers two characteristics that distinguish it from other languages: deep phonemic orthography and intonation variations. In this paper, we hypothesize that these two important properties can play a major role in Chinese sentiment analysis. In particular, we propose two effective features to encode phonetic information and, hence, fuse it with textual information. With this hypothesis, we propose Disambiguate Intonation for Sentiment Analysis (DISA), a network that we develop based on the principles of reinforcement learning. DISA disambiguates intonations for each Chinese character (pinyin) and, hence, learns precise phonetic representations. We also fuse phonetic features with textual and visual features to further improve performance. Experimental results on five different Chinese sentiment analysis datasets show that the inclusion of phonetic features significantly and consistently improves the performance of textual and visual representations and surpasses the state-of-the-art Chinese character-level representations.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0192,Predictors of spoken language learning,"We report two sets of experiments showing that the large individual variability in language learning success in adults can be attributed to neurophysiological, neuroanatomical, cognitive, and perceptual factors. In the first set of experiments, native English-speaking adults learned to incorporate lexically meaningfully pitch patterns in words. We found those who were successful to have higher activation in bilateral auditory cortex, larger volume in Heschl's Gyrus, and more accurate pitch pattern perception. All of these measures were performed before training began. In the second set of experiments, native English-speaking adults learned a phonological grammatical system governing the formation of words of an artificial language. Again, neurophysiological, neuroanatomical, and cognitive factors predicted to an extent how well these adults learned. Taken together, these experiments suggest that neural and behavioral factors can be used to predict spoken language learning. These predictors can inform the redesign of existing training paradigms to maximize learning for learners with different learning profiles. Learning outcomes: Readers will be able to: (a) understand the linguistic concepts of lexical tone and phonological grammar, (b) identify the brain regions associated with learning lexical tone and phonological grammar, and (c) identify the cognitive predictors for successful learning of a tone language and phonological rules. (C) 2011 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0193,Are tones phones?,"The psycholinguistic status of lexical tones and phones is indexed via phonological and tonological awareness (PA and TA, respectively) using Thai speech. In Experiment 1 (Thai participants, alphabetic script and orthographically explicit phones/tones), PA was better than TA in children and primary school-educated adults, and TA improved to PA levels only in tertiary-educated adults. In Experiment 2 (Cantonese participants, logographic script and no orthographically explicit phones/tones), children and primary-educated adults had better PA than TA, and PA and TA were equivalent in tertiary-educated adults, but were nevertheless still below the level of their Thai counterparts. Experiment 3 (English-language participants, alphabetic script and nontonal) showed better PA than TA. Regression analyses showed that both TA and PA are predicted by reading ability for Thai children but by general nonorthographic age-related variables for Cantonese children, whereas for English children reading ability predicts PA but not TA. The results show a phone > tone perceptual advantage over both age and languages that is affected by availability of orthographically relevant information and metalinguistic maturity. More generally, both the perception and the psycholinguistic representation of phones and tones differ. (C) 2010 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0194,Processing of lexical stress cues by young children,"Although infants learn an impressive amount about their native-language phonological system by the end of the first year of life, after the first year children still have much to learn about how acoustic dimensions cue linguistic categories in fluent speech. The current study investigated what children have learned about how the acoustic dimension of pitch indicates the location of the stressed syllable in familiar words. Preschoolers (2.5- to 5-year-olds) and adults were tested on their ability to use lexical-stress cues to identify familiar words. Both age groups saw pictures of a bunny and a banana and heard versions of ""bunny"" and ""banana"" in which stress either was indicated normally with convergent cues (pitch, duration, amplitude, and vowel quality) or was manipulated such that only pitch differentiated the words' initial syllables. Adults (n = 48) used both the convergent cues and the isolated pitch cue to identify the target words as they unfolded. Children (n = 206) used the convergent stress cues but not pitch alone in identifying words. We discuss potential reasons for children's difficulty in exploiting isolated pitch cues to stress despite children's early sensitivity to pitch in language. These findings contribute to a view in which phonological development progresses toward the adult state well past infancy. (C) 2014 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0195,Can bilingual children turn one language off? Evidence from perceptual switching,"Bilinguals have the sole option of conversing in one language in spite of knowing two languages. The question of how bilinguals alternate between their two languages, activating and deactivating one language, is not well understood. In the current study, we investigated the development of this process by researching bilingual children's abilities to selectively integrate lexical tone based on its relevance in the language being used. In particular, the current study sought to determine the effects of global conversation-level cues versus local (within-word phonotactic) cues on children's tone integration in newly learned words. Words were taught to children via a conversational narrative, and word recognition was investigated using the intermodal preferential-looking paradigm. Children were tested on recognition of words with stimuli that were either matched or. mismatched in tone in both English and Mandarin conversations. Results demonstrated that 3- to 4-year-olds did not adapt their interpretation of lexical tone changes to the language being spoken. In contrast, 4- to 5-year-olds were able to do so when supported by informative within-word cues. Results suggest that preschool children are capable of selectively activating a single language given word-internal cues to language. (C) 2016 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0196,Lexical prosody beyond first-language boundary: Chinese lexical tone sensitivity predicts English reading comprehension,"This 1-year longitudinal study examined the role of Cantonese lexical tone sensitivity in predicting English reading comprehension and the pathways underlying their relation. Multiple measures of Cantonese lexical tone sensitivity, English lexical stress sensitivity, Cantonese segmental phonological awareness, general auditory sensitivity, English word reading, and English reading comprehension were administered to 133 Cantonese-English unbalanced bilingual second graders. Structural equation modeling analysis identified transfer of Cantonese lexical tone sensitivity to English reading comprehension. This transfer was realized through a direct pathway via English stress sensitivity and also-an indirect pathway via English word reading. These results suggest that prosodic sensitivity is an important factor influencing English reading comprehension and that it needs to be incorporated into theoretical accounts of reading comprehension across languages. (C) 2016 Elsevier Inc All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0197,The relationship between children's sensitivity to dominant and non-dominant patterns of lexical stress and reading accuracy,"This study reports on a new task for assessing children's sensitivity to lexical stress for words with different stress patterns and demonstrates that this task is useful in examining predictors of reading accuracy during the elementary years. In English, polysyllabic words beginning with a strong syllable exhibit the most common or dominant pattern of lexical stress (e.g., ""coconut""), whereas polysyllabic words beginning with a weak syllable exhibit a less common non-dominant pattern (e.g., ""banana""). The new Aliens Talking Underwater task assesses children's ability to match low-pass filtered recordings of words to pictures of objects. Via filtering, phonetic detail is removed but prosodic contour information relating to lexical stress is retained. In a series of two-alternative forced choice trials, participants see a picture and are asked to choose which of two filtered recordings matches the name of that picture; one recording exhibits the correct lexical stress of the target word, and the other recording reverses the pattern of stress over the initial two syllables of the target word rendering it incorrect. Target words exhibit either dominant stress or non-dominant stress. Analysis of data collected from 192 typically developing children aged 5 to 12 years revealed that sensitivity to non dominant lexical stress was a significant predictor of reading accuracy even when age and phonological awareness were taken into account. A total of 76.3% of variance in children's reading accuracy was explained by these variables. Crown Copyright (C) 2016 Published by Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0198,"Vowels, consonants, and lexical tones: Sensitivity to phonological variation in monolingual Mandarin and bilingual English-Mandarin toddlers","Although bilingual learners represent the linguistic majority, much less is known about their lexical processing in comparison with monolingual learners. In the current study, bilingual and monolingual toddlers were compared on their ability to recognize familiar words. Children were presented with correct pronunciations and mispronunciations, with the latter involving a vowel, consonant, or tone substitution. A robust ability to recognize words when their labels were correctly pronounced was observed in both groups. Both groups also exhibited a robust ability to reject vowel, tone, and consonant mispronunciations as possible labels for familiar words. However, time course analyses revealed processing differences based on language background; relative to Mandarin mono-linguals, Mandarin-English bilingual toddlers demonstrated reduced efficiency in recognizing correctly pronounced words. With respect to mispronunciations, Mandarin-English bilingual learners demonstrated reduced sensitivity to tone mispronunciations relative to Mandarin monolingual toddlers. Moreover, the relative cost of mispronunciations differed for monolingual and bilingual toddlers. Monolingual toddlers demonstrated least sensitivity to consonants followed by vowels and tones, whereas bilingual toddlers demonstrated least sensitivity to tone, followed by consonants and then by vowels. Time course analyses revealed that both groups were sensitive to vowel and consonant variation. Results reveal both similarities and differences in monolingual and bilingual learners' processing of familiar words in Mandarin Chinese. (C) 2017 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0199,Statistical learning of speech sounds is most robust during the period of perceptual attunement,"Although statistical learning has been shown to be a domain general mechanism, its constraints, such as its interactions with perceptual development, are less well understood and discussed. This study is among the first to investigate the distributional learning of lexical pitch in non-tone-language-learning infants, exploring its interaction with language-specific perceptual attunement during the first 2 years after birth. A total of 88 normally developing Dutch infants of 5, 11, and 14 months were tested via a distributional learning paradigm and were familiarized on a unimodal or bimodal distribution of high-level versus high-falling tones in Mandarin Chinese. After familiarization, they were tested on a tonal contrast that shared equal distributional information in either modality. At 5 months, infants in both conditions discriminated the contrast, whereas 11-month-olds showed discrimination only in the bimodal condition. By 14 months, infants failed to discriminate the contrast in either condition. Results indicate interplay between infants' long-term linguistic experience throughout development and short-term distributional learning during the experiment, and they suggest that the influence of tonal distributional learning varies along the perceptual attunement trajectory, such that opportunities for distributional learning effects appear to be constrained in the beginning and at the end of perceptual attunement. The current study contributes to previous research by demonstrating an effect of age on learning from distributional cues. (C) 2017 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0200,"Developmental change in tone perception in Mandarin monolingual, English monolingual, and Mandarin-English bilingual infants: Divergences between monolingual and bilingual learners","Most languages use lexical tone to discriminate the meanings of words. There has been recent interest in tracking the development of tone categories during infancy. These studies have focused largely on monolingual infants learning either a tone language or a non-tone language. It remains to be seen how bilingual infants learning one tone language (e.g., Mandarin) and one non-tone language (e.g., English) discriminate tones. Here, we examined infants' discrimination of two Mandarin tones pairs: one salient and one subtle. Discrimination was investigated in three groups: Mandarin-English bilinguals, English monolinguals, and Mandarin monolinguals at 6 months and 9 months of age in a cross-sectional design. Results demonstrated relatively strong Mandarin tone discrimination in Mandarin monolinguals, with salient tone discrimination at 6 months and both salient and subtle tone discrimination at 9 months. English monolinguals discriminated neither contrast at 6 months but discriminated the salient contrast at 9 months. Surprisingly, there was no evidence for tone discrimination in Mandarin-English bilingual infants. In a second experiment, 12- and 13-month-old Mandarin-English bilingual and English monolingual infants were tested to determine whether bilinguals would demonstrate tone sensitivity at a later age. Results revealed a lack of tone sensitivity at 12 or 13 months in bilingual infants, yet English monolingual infants were sensitive to both salient and subtle Mandarin tone contrasts at 12 or 13 months. Our findings provide evidence for age-related convergence in Mandarin tone discrimination in English and Mandarin monolingual infants and for a distinct pattern of tone discrimination in bilingual infants. Theoretical implications for phonetic category acquisition are discussed. (C) 2018 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0201,Narrowing in face and speech perception in infancy: Developmental change in the relations between domains,"Although prior research has established that perceptual narrowing reflects the influence of experience on the development of face and speech processing, it is unclear whether narrowing in the two domains is related. A within-participant design (N= 72) was used to investigate discrimination of own- and other-race faces and native and non-native speech sounds in 3-, 6-, 9-, and 12-month-old infants. For face and speech discrimination, whereas 3-month-olds discriminated own-race faces and native speech sounds as well as other-race faces and non-native speech sounds, older infants discriminated only own-race faces and native speech sounds. Narrowing in face and narrowing in speech were not correlated at 6 months, negatively correlated at 9 months, and positively correlated at 12 months. The findings reveal dynamic developmental changes in the relation between modalities during the first year of life. (C) 2018 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0202,"Vowel, consonant, and tone variation exert asymmetrical effects on spoken word recognition: Evidence from 6-year-old monolingual and bilingual learners of Mandarin","Most children are raised in a bilingual environment. However, compared with monolingual language acquisition, relatively little is known about how bilingual children acquire native phonology. Moreover, much less is known about how children acquire knowledge of tone languages in comparison with consonant-vowel languages such as English. In this study, 6-year-old Mandarin monolingual and English-Mandarin bilingual learners were tested on their sensitivity to vowel, consonant, and lexical tone mispronunciations of known words in a preferential looking task (N = 48; Experiment 1) and in an explicit judgment task (N = 48; Experiment 2). Results demonstrated that in Experiment 1 both monolingual and bilingual participants were sensitive to vowel mispronunciations. Bilingual children were also sensitive to consonant and tone mispronunciations. When sensitivity to mispronunciations was compared with correct pronunciations, monolingual and bilingual participants were similarly sensitive to consonant mispronunciations and similarly insensitive to tones, but they demonstrated subtle variance in vowel sensitivity. In Experiment 2, both groups demonstrated reduced sensitivity to tone mispronunciations relative to vowel and consonant mispronunciations. Therefore, in both implicit and explicit tasks, participants demonstrated asymmetrical sensitivity to segments (vowels and consonants) and tones. Our findings are discussed in terms of influences of phonological variation and language background on children's word knowledge. (C) 2019 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0203,Tonal interference in word learning? A comparison of Cantonese and French,"Most languages of the world use lexical tones to contrast words. Thus, understanding how individuals process tones when learning new words is fundamental for a better understanding of the mechanisms underlying word learning. The current study asked how tonal information is integrated during word learning. We investigated whether variability in tonal information during learning can interfere with the learning of new words and whether this is language and age dependent. Cantonese- and French -learning 30month -olds (N = 97) and Cantonese- and French-speaking adults (N = 50) were tested with an eye -tracking task on their ability to learn phonetically different pairs of novel words in two learning conditions: a 1 -tone condition in which each object was named with a single label and a 3 -tone condition in which each object was named with three different labels varying in tone. We predicted learning in all groups in the 1 -tone condition. For the 3tone condition, because tones are part of the phonological system of Cantonese but not of French, we expected the Cantonese groups to either fail (toddlers) or show lower performance than in the 1tone condition (adults), whereas the French groups might show less sensitivity to this manipulation. The results show that all participants learned in the 1 -tone condition and were sensitive to tone variation to some extent. Learning in the 3 -tone condition was impeded in both groups of toddlers. We argue that tonal interference in word learning likely comes from the phonological level in the Cantonese groups and from the acoustic level in the French groups. (c) 2024 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/ licenses/by/4.0/).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0204,Preference and processing: The role of speech affect in early spoken word recognition,"Infants prefer to listen to happy speech. To assess influences of speech affect on early lexical processing, 7.5- and 10.5-month-old infants were familiarized with one word spoken with happy affect and another with neutral affect and then tested on recognition of these words in fluent passages. Infants heard all passages either with happy affect or with neutral affect. Contrary to initial expectations that positive affect would facilitate word recognition, younger infants recognized familiarized words only when affect matched across familiarization and testing. Older infants displayed a more mature pattern of word recognition, recognizing words across variations in affect regardless of the direction of change when the task was somewhat simplified. However, younger infants continued to be limited by affective matching in the simplified task. Early processing advantages thus do not necessarily follow listening preferences. Rather, infants' early lexical representations appear to be dominated by covarying properties of experienced exemplars, whether or not these are ultimately relevant for lexical distinctions. (C) 2004 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0205,Phonological phrase boundaries constrain lexical access II. Infant data,"The location of phonological phrase boundaries was shown to affect lexical access by English-learning infants of 10 and 13 months of age. Experiments 1 and 2 used the head-turn preference procedure: infants were familiarized with two bisyllabic words, then presented with sentences that either contained the familiarized words or contained both their syllables separated by a phonological phrase boundary. Ten-month-olds did not show any listening preference, whereas 13-month-olds listened significantly longer to sentences containing the familiarized words. Experiments 3 and 4 relied on a variant of the conditioned head-turning technique. In a first session, infants were trained to turn their heads for an isolated bisyllabic word. In the second session, they were exposed to the same sentences as above. Both 10- and 12.5-month-old infants turned significantly more often when the target word truly appeared in the sentence. These results suggest that phonological phrase boundaries constrain on-line lexical access in infants. (C) 2004 Published by Elsevier Inc.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0206,Phonological knowledge guides 2-year-olds' and adults' interpretation of salient pitch contours in word learning,"Phonology provides a system by which a limited number of types of phonetic variation can signal communicative intentions at Multiple levels of linguistic analysis. Because phonologies vary from language to language, acquiring the phonology of a language demands learning to attribute phonetic variation appropriately. Here, we studied the case of pitch-contour variation. In English, pitch contour does not differentiate words, but serves other functions, like marking yes/no questions and conveying emotions. We show that, in accordance with their phonology, English-speaking adults and 2-year-olds do not interpret salient pitch contours as inherent to novel words. We taught participants a new word with consistent segmental and pitch characteristics, and then tested word recognition for trained and deviant pronunciations using an eyegaze-based procedure. Vowel-quality mispronunciations impaired recognition, but large changes in pitch contour did not. By age 2, children already apply their knowledge of English phonology to interpret phonetic consistencies in their experience with words. (C) 2009 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0207,The roles of tonal and segmental information in Mandarin spoken word recognition: An eyetracking study,"We used eyetracking to examine how tonal versus segmental information influence spoken word recognition in Mandarin Chinese. Participants heard an auditory word and were required to identify its corresponding picture from an array that included the target item (chuang2 'bed'), a phonological competitor (segmental: chuang1 'window'; cohort: chuan2 'ship'; rhyme: huang2 'yellow': tonal: niu2 'cow'), and two phonologically unrelated distractors. Growth curve analysis was used to characterize the trajectory of looks to target and competitor items during word processing. We found similar model fits for the segmental and cohort conditions characterized by slower eye movements to correct targets compared to baseline, suggesting that tonal and segmental information are accessed concurrently and play comparable roles in constraining activation. These findings are discussed with respect to current models of spoken word recognition that have not previously accounted for the role of tone. (C) 2010 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0208,Stress matters: Effects of anticipated lexical stress on silent reading,"This paper presents findings from two eye-tracking studies designed to investigate the role of metrical prosody in silent reading. In Experiment 1, participants read stress-alternating noun-verb or noun-adjective homographs (e.g. PREsent, preSENT) embedded in limericks, such that the lexical stress of the homograph, as determined by context, either matched or mismatched the metrical pattern of the limerick. The results demonstrated a reading cost when readers encountered a mismatch between the predicted and actual stress pattern of the word. Experiment 2 demonstrated a similar cost of a mismatch in stress patterns in a context where the metrical constraint was mediated by lexical category rather than by explicit meter. Both experiments demonstrated that readers are slower to read words when their stress pattern does not conform to expectations. The data from these two eye-tracking experiments provide some of the first on-line evidence that metrical information is part of the default representation of a word during silent reading. (C) 2010 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0209,Intonation facilitates contrast resolution: Evidence from Japanese adults and 6-year olds,"Two eye-tracking experiments tested how pitch prominence on a prenominal adjective affects contrast resolution in Japanese adult and 6-year old listeners. Participants located two animals in succession on displays with multiple colored animals. In Experiment 1, adults' fixations to the contrastive target (pink cat -> GREEN cat) were facilitated by a pitch expansion on the adjective while infelicitous pitch expansion (purple rabbit ->. ORANGE monkey) led to a garden-path effect, i.e., frequent fixations to the incorrect target (orange rabbit). In 6-year olds, only the facilitation effect surfaced. Hypothesizing that the interval between the two questions may not have given enough time for children to overcome their tendency to perseverate on the first target. Experiment 2 used longer intervals and confirmed a garden-path effect in 6-year olds. These results demonstrate that Japanese 6-year olds can make use of contrast-marking pitch prominence when time allows an establishment of proper discourse representation. (C) 2011 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0210,When does native language input affect phonetic perception? The precocious case of lexical tone,"Previous studies have suggested that the perception of vowels and consonants changes from language-universal to language-specific between 6 and 12 months of age. This report suggests that language-specific perception emerges even earlier for lexical tones. Experiment 1 tested English-learners' perception of Cantonese tones, replicating declines in tone discrimination from 4 to 9 months of age. Experiment 2 tested infants learning non-native versus native tone systems (Mandarin-learners versus Cantonese-learners). All Chinese-learners discriminated the tones, but showed language-specific differences in tone preferences at both ages. Indeed, English-, Mandarin-, and Cantonese-learning 4-month-olds all exhibited distinct preferences. With other work, this shows that language-specific speech perception emerges over a more complex and extended schedule than previously thought: first for lexical stress aid tone (<5 months), then vowels (6-8 months), consonants (8.5-12 months), and finally phoneme duration (18 months). Acoustic salience likely plays an important role in determining the timing of phonetic development. (C) 2012 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0211,The temporal dynamics of perceptual uncertainty: eye movement evidence from Cantonese segment and tone perception,"Two visual world eyetracking experiments investigated how acoustic cue value and statistical variance affect perceptual uncertainty during Cantonese consonant (Experiment 1) and tone perception (Experiment 2). Participants heard low- or high-variance acoustic stimuli. Euclidean distance of fixations from target and competitor pictures over time was analysed using Generalised Additive Mixed Modelling. Distance of fixations from target and competitor pictures varied as a function of acoustic cue, providing evidence for gradient, nonlinear sensitivity to cue values. Moreover, cue value effects significantly interacted with statistical variance, indicating that the cue distribution directly affects perceptual uncertainty. Interestingly, the time course of effects differed between target distance and competitor distance models. The pattern of effects over time suggests a global strategy in response to the level of uncertainty: as uncertainty increases, verification looks increase accordingly. Low variance generally creates less uncertainty, but can lead to greater uncertainty in the face of unexpected speech tokens. (C) 2016 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0212,Mapping non-native pitch contours to meaning: Perceptual and experiential factors,"Infants show interesting patterns of flexibility and constraint early in word learning. Here, we explore perceptual and experiential factors that drive associative learning of labels that differ in pitch contour. Contrary to the salience hypothesis proposed in Experiment 1, English-learning 14-month-olds failed to map acoustically distinctive level and dipping labels to novel referents, even though they discriminated the labels when no potential referents were present. Conversely, infants readily mapped the less distinctive rising and dipping labels. In Experiment 2, we found that the degree of pitch variation in labels also does not account for learning. Instead, English-learning infants only learned if one of the labels had a rising pitch contour. We argue that experience with hearing and/or producing native language prosody may lead infants to initially over-interpret the role rising pitch plays in differentiating words. Together, our findings suggest that multiple factors contribute to whether specific acoustic forms will function as candidate object labels.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0213,Graded phonological neighborhood effects on lexical retrieval: Evidence from Mandarin Chinese,"How phonological neighborhood affects lexical retrieval can shed important light on lexical organization and processing. Yet these effects are unclear, particularly in Mandarin Chinese. This is likely because the working definition of phonological neighbors (i.e., the one-phoneme edit rule) used in Indo-European languages inadequately characterizes the phonological similarity among Mandarin words, which have simpler syllable structures and lexical tones. The current study proposes a graded Mandarin phonological neighborhood and investigates the impacts of near-to-distant Mandarin phonological neighbors on lexical retrieval. In Study 1, we investigated how Mandarin phonological similarity is influenced by the editing of lexical tone, constituent (onset/rime, initial/ final) and phoneme. Native Mandarin speakers rated the similarity between the edited monosyllabic words. We found that constituent-edit neighbors were rated as the most dissimilar, followed by phoneme-edit neighbors, while tone-edit neighbors were the most similar. In Study 2, we calculated the constituent-, phoneme- and toneedit phonological neighborhood densities and frequencies for 4,706 monosyllabic Mandarin words. We then utilized extant datasets to examine how the density and frequency of neighbors at varied distances, as well as of homophonic neighbors, impact response latencies in word naming, visual lexical decision, and picture naming tasks. The results showed that graded phonological neighbors had differential impacts on lexical retrieval efficiency: distant (constituent-edit) neighbors facilitated word retrieval, while near (phoneme-, tone-edit and homophonic) neighbors had inhibitory effects. We discuss these findings within an interactive activation and competition framework and suggest future directions to study the representation and processing of the Mandarin phonological lexicon.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0214,Left-edge boundary tone and main clause verb effects on syntactic processing in embedded clauses - An ERP study,"We examined the effects of main clause verb pragmatics and left-edge boundary tones on syntactic processing in Swedish embedded clauses, using listener judgments and Event-Related Potentials. When the syntactic structure did not match the expectation based on the occurrence of a left-edge boundary tone, the acceptance rate decreased significantly, and a biphasic positive effect with an early peak (P345) and a late peak (P600) showed increased processing load. A larger continuous positive effect (P600) was obtained by changing an assertive main clause verb to a nonassertive verb. thereby modifying the lexical pragmatic context of the embedded clause. Increased positivity was also seen at the left-edge boundary tone when it mismatched a preceding nonassertive verb. We conclude that left-edge boundary tones are used in addition to verb pragmatics to guide the syntactic processing of embedded clauses in Swedish, and that pragmatic and prosodic information is integrated immediately. (c) 2008 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0215,An acoustic study of vowels and coarticulation as a function of utterance type: A case of acquired apraxia of speech,"This case study examines vowel production and coarticulation patterns in AOS as a function of utterance type. A female speaker with AOS, and a sex-matched control participant repeated target vowels in three conditions: isolated Word statement frames and, question frames. The first two formant frequencies (F1 and F2) of the target vowels were measured at their onset and temporal midpoints. The vowel spaces of both speakers were examined using a two-dimensional vowel chart; temporal midpoint formant frequency values of the first formant (171) were plotted against the difference between F2 and F1 (F2-F1) as a representation of the constricted versus open (F-I) and fronted versus backed (F2-F1) quality of vowels. The speaker with AOS produced vowels that deviated to a posterior/backed distribution within the vowel space. Coarticulation patterns were also examined using F2 locus equations. The speaker with AOS exhibited reduced coarticulation patterns in all three conditions, when compared to the control speaker. Furthermore, these effects were amplified with increasing motoric complexity. The data elucidate the nature of the compromised speech control system in a speaker with AOS. (C) 2009 Elsevier Ltd. All rights reserved",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0216,Second language experience modulates neural specialization for first language lexical tones,"Recent neuroimaging studies have revealed distinct functional roles of left and right temporal lobe structures in the processing of lexical tones in Chinese. In the present study, we ask whether knowledge of a second language (English) modulates this pattern of activation in the perception of tonal contrasts. Twenty-four native Chinese speakers were recruited from undergraduate and graduate students at Beijing Normal University, China. Participants listened to blocks of computationally manipulated /ba/ syllables which were varied to form within- and across-category deviants at equal acoustic intervals from a standard tone while their cortical blood oxygenation was measured by functional near-infrared spectroscopy (fNIRS). Blocks were analyzed for peak blood oxygenation (HbO) levels, and several linear models were estimated for these data, including effects of deviant tone type (within- or across-category), behavioral differences in tone identification, age of earliest exposure to English (spoken), and proficiency in English. Functional changes in HbO indicated a significantly greater response to within-category contrasts in right STG, consistent with previous findings. However, the effect of deviant type in left MTG was significantly modulated by the age of participants' earliest English exposure: Average across-category activation exceeded within-category activation only for participants exposed to English after 13 years of age. While previous research has established the importance of left MTG in the categorical perception of lexical tones, our findings suggest that the functional specialization of this region is sensitive to second language experience, even in the processing of native language. (C) 2014 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/3.0/).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0217,ERP correlates of prosody and syntax interaction in case of embedded sentences,"Understanding spoken language depends on processing the delicate combination of grammatical structure, meaning and prosody of utterances. Previous studies have established that prosody influences the processing of sentences when the grammatical structure is ambiguous, however it is unclear how closely prosody and Syntax are related when there is no ambiguity. In an event-related brain potential (ERP) study, we investigated the processing of embedded normal and pseudosentences in which all function and content words were replaced by meaningless words. Sentences could have either natural prosodic structure or incongruent prosodic structure, where the prosody deviated from the one expected based on the syntactic structure, but otherwise the sentences were unambiguous. The resulting ERP components (CPS) showed that the constructiOn of prosodic structure was similar in normal and pseudosentences, thus suggesting that prosody has an abstract, recursive representation, independent of other linguistic information. Moreover, we found evidence that the incongruent prosody was not only detected (shown by the RAN), but it induced neural reintegration processes (shown by the P600) in spite of the syntactic structure of sentences being intact. These results suggest that the prosodic structure is a mandatory constituent of sentence structure building whenever it is present. (C) 2015 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0218,Neural systems for auditory perception of lexical tones,"Previous neuroimaging research on cognitive processing of speech tone has generated dramatically different patterns of findings. Even at the basic perception level, brain mapping studies of lexical tones have yielded inconsistent results. Apart from the data inconsistency problem, experimental materials in past studies of tone perception carried little or minimal lexical semantics, an important dimension that should not be dispensed with because speech tones serve to distinguish lexical meanings. The present study sought to examine the neural correlates of the perception of speech tone using lexically meaningful experimental stimuli. A simple lexical tone perception task was devised in which native Mandarin speakers were asked to judge whether or not the two syllables of an auditorily presented Chinese bisyllabic word had the same tone. We selected bisyllabic words as experimental stimuli because Chinese monosyllables often convey little or very vague meanings due to rampant homophony. We found that the left inferior frontal gyrus, the right middle temporal gyrus and bilateral superior temporal gyri are responsible for basic perception of linguistic pitches. Our interpretation of the data sees the left superior temporal gyrus as engaged in primary acoustic analysis of the auditory stimuli, while the right middle superior temporal gyrus and the left inferior frontal region are involved in both tonal and semantic processing of the language stimuli. (C) 2015 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0219,Learning transforms functional organization for Mandarin lexical tone discrimination in the brain: Evidence from a MEG experiment on second language learning,"Brain neuroplasticity refers to anatomical changes and functional reorganization in the brain as the results of daily experience and new learning, and thus, should have profound implications for the changes in brain processing during and after bilingual learning. By using a MEG experiment of magnetic mismatch negativity (MMN)/magnetic mismatch field (MMNm) paradigm, the present study examined the brain wave patterns of beginning learners of learning Chinese as a second language (CSL), who enrolled in a short term digital learning program, and were found to have improved sensitivity in discrimination for lexical tones by developing a left lateralized dominance for processing Chinese lexical tones, just like native Mandarin speakers. In the experiment, the measurements of MMNm, with the contrast between large deviant T3/T1 and between small deviant T3/T2, were computed to index the changed patterns of brain processing before and after learning. Results of the MMNm data indicated that activation for large deviant contrast T3/T1 elicited earlier and significantly larger MMN than that of the small deviant contrast T3/T2, which is in line with previous studies done on native Chinese speakers. More importantly, in the comparison of pre-test and post-test time course, the amplitude of MMNm showed a significant increase in the left hemisphere after learning. The evidence indicated clearly a learning effect in the perception of linguistic features and a left lateralized neural network of processing Mandarin lexical tones. Moreover, in the source analysis, the left lateralized patterns of prefrontal generator (i.e., insula) and generator of auditory cortex (i.e., Heschl's Gyrus) changed after learning, which is also consistent with the native Mandarin speaker's response patterns. In sum, the present study provides neural evidence for a functional reorganization in the brains of second language learners, transforming the CSL learners to behave like native speakers in the perception of lexical tones even after just a short-term and non-specific training program. (C) 2016 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0220,Does phonological rule of tone substitution modulate mismatch negativity?,"This study examined whether the phonological substitution rule of tone sandhi modulates tone perception in the preattentive stage. Tone sandhi is commonly present in East Asian languages. An example from Mandarin is the Tone tone 3 sandhi rule: T3 is pronounced as T2 when followed by another T3 (33 -> 23). Previous mismatch negativity (MMN) studies in Mandarin have reported a smaller amplitude or longer latency in standard-deviant pair consisting of T2 and T3 (T2-T3) than in Tl-T3. The most widely accepted explanation for this is that T2 and T3 have steeper pitch slopes than Tl. This study tested an alternative account based on the phonological rule that the frequent substitution that occurs between T2 and T3 results in reduced MMN. In Experiment 1, we first tried to replicate the finding in Mandarin. In Experiment 2, using both unskilled and skilled speakers, we tested a sandhi tone pair of very different pitch slopes in Taiwanese. Delayed peak latency of sandhi pair was evident in both languages but only in skilled speakers. Our results did not support the shared-pitch-slope account and were instead consistent with the argument that a language-specific phonological rule could modulate preattentive tone processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0221,Prosodically controlled derivations in the mental lexicon,"Swedish morphemes are classified as prosodically specified or prosodically unspecified, depending on lexical or phonological stress, respectively. Here, we investigate the allomorphy of the suffix -(i)sk, which indicates the distinction between lexical and phonological stress; if attached to a lexically stressed morpheme, it takes a non-syllabic form (-sk), whereas if attached to a phonologically stressed morpheme, an epenthetic vowel is inserted (-isk). Using mismatch negativity (MMN), we explored the neural processing of this allomorphy across lexically stressed and phonologically stressed morphemes. In an oddball paradigm, participants were occasionally presented with congruent and incongruent derivations, created by the suffix -(i)sk, within the repetitive presentation of their monomorphemic stems. The results indicated that the congruent derivation of the lexically stressed stem elicited a larger MMN than the incongruent sequences of the same stem and the derivational suffix, whereas after the phonologically stressed stem a non-significant tendency towards an opposite pattern was observed. We argue that the significant MMN response to the congruent derivation in the lexical stress condition is in line with lexical MMN, indicating a holistic processing of the sequence of lexically stressed stem and derivational suffix. The enhanced MMN response to the incongruent derivation in the phonological stress condition, on the other hand, is suggested to reflect combinatorial processing of the sequence of phonologically stressed stem and derivational suffix. These findings bring a new aspect to the dual-system approach to neural processing of morphologically complex words, namely the specification of word stress.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0222,Do subsyllabic units play a role in Mandarin spoken word recognition? Evidence from phonotactic processing,"This study investigates the roles of Mandarin subsyllabic units in spoken word recognition by examining the neural processing of two phonotactic anomalies: (1) segmental gaps, which contain a non-existing combination of segments (e.g., *[ki1]); and (2) tonal gaps, which refer to a nonword comprised of possible segment combinations with an incongruous tone (e.g., *[tau2]; cf. [tau1] ""knife""). Event-related potentials were recorded while participants performed an auditory lexical decision task. The response to segmental gaps differed from the other stimuli types in the amplitudes and scalp distributions of several components, including the P350, the N400, and the late positive complex. The P350 effect occurred around 370 ms before the entire syllable was revealed, indicating that lexical processing is not based solely on syllable representations. Furthermore, the overall differences between segmental and tonal gaps suggest that tones and vowels are dissociable. These results thus provide converging evidence for the view that Mandarin syllables are processed incrementally through phonemes.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0223,The dissociation of grammatical category from multiple levels in the neural representation of words: stress typicality effect among Chinese EFL learners,"This study aimed to dissociate grammatical category from multiple levels during the processing of stress typicality, with a focus on understanding how class information is represented among Chinese speakers who learn English as a foreign language (EFL). Disyllabic English words were used as stimuli and three event-related potentials (ERPs) components, including P200, N400, and LPC (late positive component), were analyzed across two tasks that varied in their direct utilization of grammatical cue: lexical decision task in Experiment 1 and grammatical classification task in Experiment 2. Our findings indicate differences between words exhibiting distinct stress and grammatical patterns, suggesting that prosodic and grammatical cues are dissociated early around 200 ms, and continue to influence lexical access into later time windows. Additionally, the direct tapping of grammatical cues appears to impact how classes are processed, as differences between tasks were observed. In summary, our results reveal that grammatical class could be represented at the orthographic level and dissociated from prosody at an early stage. Furthermore, the representation of grammatical class among Chinese EFL learners may be independent of semantics.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0224,The underlying mechanism of deficits of speech comprehension and hallucinations in Chinese patients with schizophrenia,"Sentence context and fundamental frequency (F0) contours are important factors to speech perception and comprehension. In Chinese-Mandarin, lexical tones can be distinguished by the F0 contours. Previous studies found healthy people could use the cue of context to recover the phonological representations of lexical tones from the altered tonal patterns to comprehend the sentences in quiet condition, but can not in noise environment. Lots of research showed that patients with schizophrenia have deficits of speech perception and comprehension. However, it is unclear how context and F0 contours influence speech perception and comprehension in patients with schizophrenia. This study detected the contribution of context and lexical tone to sentence comprehension in four types of sentences by manipulating the context and F0 contours in 32 patients with schizophrenia and 33 healthy controls. The results showed that (1) in patients with schizophrenia, the interaction between context and F0 contour was not significant, which was significant in healthy controls; (2) the scores of sentences with two types of sentences with flattened F0 contours were negatively correlated with hallucination trait scores; (3) the patients with schizophrenia showed significantly lower scores on the intelligibility of sentences in all conditions, which were negatively correlated with PANSS-P. The patients with schizophrenia couldn't use the cue of context to recover the phonological representations of lexical tones from the altered tonal patterns when they comprehend the sentences, inner noise may be the underlying mechanism for the deficits of speech perception and comprehension.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0225,Effects of Muscle Tension Dysphonia on Tone Phonation: Acoustic and Perceptual Studies in Vietnamese Female Teachers,"Muscle tension dysphonia (MTD) is a hyperfunctional voice disorder commonly seen in professional voice users. To date, published acoustic studies of this disorder have mainly focused on nontonal language speakers, and no publication has documented its impact on lexical tone characteristics. In this study, we examined whether and how this voice disorder affected acoustically and perceptually the characteristics of tones in Vietnamese teachers. Voice data were obtained from 42 Vietnamese female primary school teachers diagnosed with MTD and 30 vocally healthy teachers. Tonal data were analyzed using Computerized Speech Lab (CSL-4300B) and Speech Analyzer. Parameters analyzed included the two most important acoustic cues in Vietnamese tones, that is, tonal fundamental frequency (F-0) and laryngealization. Tonal F-0 was assessed using a factorial analysis of variance with group and career durations as independent variables. Tonal samples were also perceptually assessed by a panel of native speakers of the same dialect. The results showed that MTD lowered tonal F-0 in high tones and tones with extensive fundamental frequency variation. There was also a significant main effect for career duration; in MTD group, tonal F-0 was lower in teachers with longer career duration. The teachers with MTD showed different patterns of laryngealization compared with the control group. Tone perception was poorer for tones with extensive fundamental frequency variation and without a typical phonation type. The results in this group of teachers supported our hypothesis that MTD impairs lexical tone phonation.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0226,Victims or non-humans: Exploring the semantic preference of refugees in Spanish news articles,"This paper explores the discursive representation of refugees in a 1.8-million-word corpus of Spanish news articles collected from the digital libraries of El Mundo and El Pais. Through a corpus-assisted methodology, synchronic and diachronic analyses have been conducted in order to examine the semantic preference of the lemma refugiado over the 2010-16 period. The results show a semantic preference of refugiado for two major semantic sets, namely victimization and dehumanization. Indeed, the preference tends to fluctuate between these two extremes over the years analyzed: either they are victims and their human condition is highlighted or they are dehumanized. Both sets of collocates represent about 50 percent of the total semantic preference of refugiado. (C) 2019 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0227,Production rather than observation: comparison between the roles of embodiment and conceptual metaphor in L2 lexical tone learning,"Background: It remains unclear how embodiment and conceptual metaphor play a role in second language (L2) lexical tone learning. Aims: The present study aimed to examine the roles of pitch gesture production (PP), pitch feature observation (PO), and word-picture association (WA) approach in L2 lexical tone learning. S ample: Participants were 90 undergraduate students with Mandarin as their native language. Methods: Participants learned Thai lexical tones via the three approaches and completed tone discrimination, tone identification, and word-picture matching tasks. Results: The PP performed better than the PO and WA in discriminating between and identifying specific tones. The PP was more accurate than the WA in word-picture matching. Conclusions: The pitch gesture production's embodiment was superior to the pitch feature observation's conceptual metaphor in learning L2 lexical tones. However, its role was affected by lexical tones' pitch features, test tasks, and learners' tonal experience.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0228,Kera tone and voicing interaction,"This paper sets out to investigate the relative role of Voice Onset Time (VOT) and fundamental frequency ill a language with lexical tone. By comparing English and the Chadic language Kera. this study explores the interaction between the production and perception of voicing, and the role played by voicing, in the phonological structure of the language. The major finding is that the role of VOT and F0 in the perception of voicing differs dramatically between English and Kera subjects with English subjects relying mainly oil VOT and Kera subjects mainly on F0. The phonological implications of these results for Kera are that tone is contrastive while voicing, is not, which is the reverse of English. I will propose that Kera has no [voice] feature. and that the variation in VOT plays a role as a supporting cue to tone, although the exact amount of VOT usage in Kera production and perception differs depending oil the amount of contact with French. The dominance of tone suggests that despite claims that Kera exhibits long-distance voice spreading, what spreads is in fact tone rather than voice. (C) 2008 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0229,Prosodic subcategories in Japanese,"Research on Japanese prosody, especially on the pitch accent system of the language, has for a long time found that a single domain ""phonological phrase"" is not sufficient. Rather: two domains need to be distinguished, which go by various names (Minor vs. Major Phrase, Accentual vs. Intermediate Phrase). While empirically well-founded, these developments, together with similar findings in other languages, have resulted in a dissolution of the originally tightly organized universal prosodic hierarchy into a collection of many prosodic types, each instantiated here and there in different languages, but never simultaneously realized within a single language. Two strands of recent work, that of Selkirk (2009:205-219, 2011a), and of Ito and Mester (2007, 2009a, 2009b) converge on a common theme from different directions. On the one hand; Selkirk has developed a vastly simplified approach to the syntax prosody mapping which distinguishes only three levels (word, phrase, and clause) where syntactic constituents are systematically made to correspond to phonological domains (""Match Theory""). On the other hand, Ito and Mester have argued that the empirically necessary subcategories (such as Minor vs. Major Phrase) need to be understood not as additional categories existing in their own right, but rather as instances of recursively deployed basic categories. This paper carries forward this line of prosodic hierarchy research and shows that the recursion-based conception implemented within Match Theory allows for a conceptually and empirically cleaner understanding of the phonological facts and generalizations in Japanese as well as for an understanding of the respective roles of syntax and phonology in determining prosodic constituent structure organization, and the limitation in types of distinctions in prosodic category that are made in phonological representation. Finally, a formal constraint-based OT analysis is developed that provides an account of the varying tonal and accentual structures in syntactic collocations of varying sizes and structures. (C) 2012 Elsevier B.V. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0230,Prosodic correlates of discourse boundaries and hierarchy in discourse production,"A well-formed discourse is more than just a series of well-formed sentences. While often left implicit, this structure to discourse is sometimes overtly cued. And though most attention in this area has focused on lexicalized cues like discourse markers, prosody can also convey information about the structure of discourse. This paper presents the results of a production study examining prosodic correlates of discourse structure in readings of a newspaper article. Prosodic measures of pause duration, pitch, intensity and speech rate were found to significantly correlate with discourse structural measures of boundary size, discourse coordination/subordination, and their interaction. This interaction effect shows that the effect of boundary size on an utterance's prosody often depends on whether that utterance is coordinated or subordinated, and vice versa. These results expand our understanding of how prosody correlates with discourse structure, setting the stage for follow-up perception studies of what prosodic variation listeners use in discourse interpretation. (C) 2013 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0231,"On polarity emphasis, assertion and mood in Vietnamese and English","The paper presents data from several languages chiefly, Vietnamese and English in support of two empirical claims concerning the syntax of polarity elements, assertion and mood (illocutionary force). The proposal draws on and develops Klein's (1998) arguments for a decomposition of Finiteness: whereas Klein originally proposed that finiteness should be understood as involving at least two independent components tense and assertion (validity) this is elaborated to three in the present analysis, with polarity added as a distinct projection intermediate between the other two projections, to the left of Outer Aspect. Contrastive intonation polarity emphasis is argued to be able to target either polarity or assertion, by default the former; cf. Battlori and Hernanz (2011). With regard to assertion itself, it is shown that these features are projected rather low in Vietnamese phrase-structure, immediately to the left of the predicate-phrase. It is further claimed on the basis of evidence from imperative, interrogative and modal constructions that this low structural position hosts many other illocutionary features in Vietnamese (notwithstanding the evidence of Romance and Germanic languages, which seem to support a much higher position for such features on the left periphery of the clause). The paper considers the theoretical implications of this apparently parametric contrast in the context of current Minimalist theorizing. (C) 2013 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0232,Whispered Mandarin has no production-enhanced cues for tone and intonation,"It is often assumed, explicitly or implicitly, that speakers generate special cues in whispered tone and intonation to make up for the absence of fundamental frequency. The present study examined this assumption with one production and three perception experiments. The production experiment compared duration, intensity, formants and spectral tilt of phonated and whispered Mandarin monosyllabic utterances with four lexical tones spoken as either statements or questions. For tones, no acoustic properties were found to occur only in whispered but not in phonated utterances. For intonation, some spectral tilt measurements differed between the two phonation types. The two tone perception experiments used phonated and whispered utterances as well as amplitude-modulated noise based on those utterances as stimuli. Results show that once turned into amplitude-modulated noise, phonated and whispered tones had similar identification patterns, indicating that the non-F-0 tonal cues in whispers were already in phonated speech. The intonation perception experiment used original utterances as stimuli and showed a substantial drop in overall identification rate and an overwhelming bias towards statement. Thus the spectral tilt differences found in the acoustic analysis were not helpful for intonation perception. Possible reasons for the lack of effective enhancement in whispered speech were discussed. (C) 2018 Elsevier B.V. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0233,Semantic dependency and local convolution for enhancing naturalness and tone in text-to-speech synthesis,"Self-attention-based networks have become increasingly popular due to their exceptional performance in parallel training and global context modeling. However, it may fall short of capturing local dependencies, particularly in datasets with strong local correlations. To address this challenge, we propose a novel method that utilizes semantic dependency to extract linguistic information from the original text. The semantic relationship between nodes serves as prior knowledge to refine the self-attention distribution. Additionally, to better fuse local contextual information, we introduce a one-dimensional convolution neural network to generate the query and value matrices in the self-attention mechanism, taking advantage of the strong correlation between input characters. We apply this variant of the self-attention network to text-to-speech tasks and propose a non-autoregressive neural text-to-speech model. To enhance pronunciation accuracy, we separate tones from phonemes as independent features in model training. Experimental results show that our model yields good performance in speech synthesis. Specifically, the proposed method significantly improves the processing of pause, stress, and intonation in speech.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0234,ERP correlates of pre-attentive processing of Cantonese lexical tones: The effects of pitch contour and pitch height,"The pre-attentive processing of Cantonese tones was studied with an auditory passive oddball paradigm. Event-related potentials to standard and deviant auditory stimuli were recorded as participants watched a silent movie attentively. The standards and deviants differed in either pitch level or pitch contour. Mismatch negativity (MMN) and P3a were elicited by all types of deviant tones, suggesting that lexical tone was processed pre-attentively. In addition, the size and latency of MMN were sensitive to the size of pitch level change, while the latency of P3a captured the presence of pitch contour change. These results indicate that pitch contour and pitch height are two important dimensions in sensory processing of lexical tones. (C) 2010 Elsevier Ireland Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0235,Right hemisphere advantage in processing Cantonese level and contour tones: Evidence from dichotic listening,"The brain lateralization pattern of Cantonese tonal processing was examined with the dichotic listening (DL) paradigm. Three factors were manipulated systematically in the study. First, the processing of level tones was compared with that of contour tones. Second, the influence of a linguistic context in tonal processing was studied by contrasting the patterns of brain lateralization for real syllables, pseudo-syllables, and hums. Finally, the discrimination and the identification tasks were used to test how processing depth might modulate the results obtained. A right hemisphere advantage (RHA) was obtained regardless of tone type, stimulus type, and task. In addition, the performance on level tones was in general better than that on contour tones. These findings suggest that Cantonese speakers are highly sensitive to the acoustic features of lexical tones, which supports the acoustic view about tonal processing. (C) 2013 Elsevier Ireland Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0236,Non-speech and speech pitch perception among Cantonese-speaking children with autism spectrum disorder: An ERP study,"The present study compared how Cantonese-speaking children with Autism Spectrum Disorder (ASD) and their typically developing counterparts perceived speech pitch and non-speech pitch information using ERP measurements. Sixteen children with ASD (mean age = 10.42 years, SD = 2.12 years) and sixteen normal controls (mean age = 9.48 years, SD = .86 years) participated in two experiments, in which Cantonese lexical tone contrasts and non-speech pitch variations were presented to children following an oddball paradigm when they watched a silent movie. The results showed that: 1) When processing speech pitch contour, the two groups did not differ in the amplitude of mismatch response (p-MMR), while typically developing controls showed larger mismatch negativity (MMN) responses than children with ASD. In the processing of speech pitch height, more positive p-MMR was observed among children with ASD than among normal controls and stronger MMN was found for typically developing children than for children with ASD. 2) For the processing of non-speech pitch, MMN rather than p-MMR was observed and the two groups did not differ significantly with each other in the amplitudes of MMN. These results indicated that Cantonese-speaking children with ASD manifested impaired ability when processing speech pitch information (i.e., lexical tone), which was in line with previous research. However, they did not show the advantage in processing non-speech or auditory pitch information, which was not in agreement with the previous studies. Results were discussed from the perspective of how language background (i.e., Cantonese) might shape the perceptive abilities of children.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0237,ASR emotional speech: Clarifying the issues and enhancing performance,"There are multiple reasons to expect that recognising the verbal content of emotional speech will be a difficult problem, and recognition rates reported in the literature are in fact low. Including information about prosody improves recognition rate for emotions simulated by actors, but its relevance to the freer patterns of spontaneous speech is unproven. This paper shows that recognition rate for spontaneous emotionally coloured speech can be improved by using a language model based on increased representation of emotional utterances. The models are derived by adapting an already existing corpus, the British National Corpus (BNC). An emotional lexicon is used to identify emotionally coloured words, and sentences containing these words are recombined with the BNC to form a corpus with a raised proportion of emotional material. Using a language model based on that technique improves recognition rate by about 20%. (c) 2005 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0238,On the localness modeling for the self-attention based end-to-end speech synthesis,"Attention based end-to-end speech synthesis achieves better performance in both prosody and quality compared to the conventional ""front-end""-""back-end"" structure. But training such end-to-end framework is usually time-consuming because of the use of recurrent neural networks. To enable parallel calculation and long-range dependency modeling, a solely self-attention based framework named Transformer is proposed recently in the end-to-end family. However, it lacks position information in sequential modeling, so that the extra position representation is crucial to achieve good performance. Besides, the weighted sum form of self-attention is conducted over the whole input sequence when computing latent representation, which may disperse the attention to the whole input sequence other than focusing on the more important neighboring input states, resulting in generation errors. In this paper, we introduce two localness modeling methods to enhance the self-attention based representation for speech synthesis, which maintain the abilities of parallel computation and global-range dependency modeling in self-attention while improving the generation stability. We systematically analyze the solely self-attention based end-to-end speech synthesis framework, and unveil the importance of local context. Then we add the proposed relative-position-aware method to enhance local edges and experiment with different architectures to examine the effectiveness of localness modeling. In order to achieve query-specific window and discard the hyper-parameter of the relative-position-aware approach, we further conduct Gaussian-based bias to enhance localness. Experimental results indicate that the two proposed localness enhanced methods can both improve the performance of the self-attention model, especially when applied to the encoder part. And the query-specific window of Gaussian bias approach is more robust compared with the fixed relative edges. (c) 2020 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0239,Hemispheric roles in the perception of speech prosody,"Speech prosody is processed in neither a single region nor a specific hemisphere, but engages multiple areas comprising a large-scale spatially distributed network in both hemispheres. It remains to be elucidated whether hemispheric lateralization is based on higher-level prosodic representations or lower-level encoding of acoustic cues, or both. A cross-language (Chinese; English) fMRI study was conducted to examine brain activity elicited by selective attention to Chinese intonation (I) and tone (T) presented in three-syllable (I3, T3) and one-syllable (I1, T1) utterance pairs in a speeded response, discrimination paradigm. The Chinese group exhibited greater activity than the English in a left inferior parietal region across tasks (I1, I3, T1, T3). Only the Chinese group exhibited a leftward asymmetry in inferior parietal and posterior superior temporal (I1, I3, T1, T3), anterior temporal (I1, I3, T1, T3), and frontopolar (I1, I3) regions. Both language groups shared a rightward asymmetry in the mid portions of the superior temporal sulcus and middle frontal gyrus irrespective of prosodic unit or temporal interval. Hemispheric laterality effects enable us to distinguish brain activity associated with higher-order prosodic representations in the Chinese group from that associated with lower-level acoustic/auditory processes that are shared among listeners regardless of language experience. Lateralization is influenced by language experience that shapes the internal prosodic representation of an external auditory signal. We propose that speech prosody perception is mediated primarily by the RH, but is left-lateralized to task-dependent regions when language processing is required beyond the auditory analysis of the complex sound. (C) 2004 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0240,Identification of emotional intonation evaluated by fMRI,"During acoustic communication among human beings, emotional information can be expressed both by the propositional content of verbal utterances and by the modulation of speech melody (affective prosody). It is well established that linguistic processing is bound predominantly to the left hemisphere of the brain. By contrast, the encoding of emotional intonation has been assumed to depend specifically upon right-sided cerebral structures. However, prior clinical and functional imaging studies yielded discrepant data with respect to interhemispheric lateralization and intrahemispheric localization of brain regions contributing to processing of affective prosody. In order to delineate the cerebral network engaged in the perception of emotional tone, functional magnetic resonance imaging (fMRI) was performed during recognition of prosodic expressions of five different basic emotions (happy, sad, angry, fearful, and disgusted) and during phonetic monitoring of the same stimuli. As compared to baseline at rest, both tasks yielded widespread bilateral hemodynamic responses within frontal, temporal, and parietal areas, the thalamus, and the cerebellum. A comparison of the respective activation maps, however, revealed comprehension of affective prosody to be bound to a distinct right-hemisphere pattern of activation, encompassing posterior superior temporal sulcus (Brodmann Area [BA] 22), dorsolateral (BA 44/45), and orbitobasal (BA 47) frontal areas. Activation within left-sided speech areas, in contrast, was observed during the phonetic task. These findings indicate that partially distinct cerebral networks subserve processing of phonetic and intonational information during speech perception. (C) 2004 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0241,Cerebral mechanisms of prosodic integration: evidence from connected speech,"Using functional Magnetic Resonance Imaging (fMRI) and long connected speech stimuli, we addressed the question of neuronal networks involved in prosodic integration by comparing (1) differences in brain activity when hearing connected speech stimuli with high and low degrees of prosodic expression; (2) differences in brain activity in two different diotic listening conditions (normal speech delivery to both ears, i.e., NN; and low-pass-filtered speech delivery to both ears, i.e., FF); and (3) effects of high and low degrees of prosodic information in the NN and FIT conditions. Twelve right-handed French men listened passively to the stimuli. Each stimulus induced a specific cerebral network, the flat one weakening activations, which were mainly reduced to the bilateral STG for both listening conditions. High degrees of prosodic information were found to trigger right specific activations in a wider neuronal network involved in speech integration (such as BA44, BA21-22 and BA39-40) than low degrees of prosodic information did. More precisely, the right BA44 was found to be specifically involved in the process of F-0 modulations, which are the main acoustic correlate of prosody. Not only do the results achieved in the present experiment using 30-s-long connected speech stimuli show the involvement of a bilateral neuronal network but they also strongly suggest that high degrees of prosodic information elicit activations in a wider neuronal network involved in speech perception than low degrees of prosodic information do. (C) 2004 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0242,Processing of infant-directed speech by adults,"Adults typically address infants in a special speech mode called infant-directed speech (IDS). IDS is characterized by a special prosody (i.e., higher pitched, slower and hyperarticulated) and a special lexicon (""baby talk""). Here we investigated which areas of the adult brain are involved in processing IDS, which aspects of IDS (prosodic or lexical) are processed, to what extent the experience of being a parent affects the way adults process IDS, and the effects of gender and personality on IDS processing. Using functional magnetic resonance imaging, we found that mothers with preverbal infants showed enhanced activation in the auditory dorsal pathway of the language areas, regardless of whether they listened to the prosodic or lexical component of IDS. We also found that extroverted mothers showed higher cortical activation in speech-related motor areas than did mothers with lower extroverted personality scores. Increased cortical activation levels were not found for fathers, non-parents, or mothers with older children. (C) 2010 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0243,Left hemisphere lateralization for lexical and acoustic pitch processing in Cantonese speakers as revealed by mismatch negativity,"For nontonal language speakers, speech processing is lateralized to the left hemisphere and musical processing is lateralized to the right hemisphere (i.e., function-dependent brain asymmetry). On the other hand, acoustic temporal processing is lateralized to the left hemisphere and spectral/pitch processing is lateralized to the right hemisphere (i.e., acoustic-dependent brain asymmetry). In this study, we examine whether the hemispheric lateralization of lexical pitch and acoustic pitch processing in tonal language speakers is consistent with the patterns of function- and acoustic-dependent brain asymmetry in nontonal language speakers. Pitch contrast in both speech stimuli (syllable /ji/ in Experiment 1) and nonspeech stimuli (harmonic tone in Experiment 1; pure tone in Experiment 2) was presented to native Cantonese speakers in passive oddball paradigms. We found that the mismatch negativity (MMN) elicited by lexical pitch contrast was lateralized to the left hemisphere, which is consistent with the pattern of function-dependent brain asymmetry (i.e., left hemisphere lateralization for speech processing) in nontonal language speakers. However, the MMN elicited by acoustic pitch contrast was also left hemisphere lateralized (harmonic tone in Experiment 1) or showed a tendency for left hemisphere lateralization (pure tone in Experiment 2), which is inconsistent with the pattern of acoustic-dependent brain asymmetry (i.e., right hemisphere lateralization for acoustic pitch processing) in nontonal language speakers. The consistent pattern of function-dependent brain asymmetry and the inconsistent pattern of acoustic-dependent brain asymmetry between tonal and nontonal language speakers can be explained by the hypothesis that the acoustic-dependent brain asymmetry is the consequence of a carryover effect from function-dependent brain asymmetry. Potential evolutionary implication of this hypothesis is discussed. (C) 2013 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0244,A neural mechanism for recognizing speech spoken by different speakers,"Understanding speech from different speakers is a sophisticated process, particularly because the same acoustic parameters convey important information about both the speech message and the person speaking. How the human brain accomplishes speech recognition under such conditions is unknown. One view is that speaker information is discarded at early processing stages and not used for understanding the speech message. An alternative view is that speaker information is exploited to improve speech recognition. Consistent with the latter view, previous research identified functional interactions between the left- and the right-hemispheric superior temporal sulcus/gyrus, which process speech- and speaker-specific vocal tract parameters, respectively. Vocal tract parameters are one of the two major acoustic features that determine both speaker identity and speech message (phonemes). Here, using functional magnetic resonance imaging (fMRI), we show that a similar interaction exists for glottal fold parameters between the left and right Heschl's gyri. Glottal fold parameters are the other main acoustic feature that determines speaker identity and speech message (linguistic prosody). The findings suggest that interactions between left- and right-hemispheric areas are specific to the processing of different acoustic features of speech and speaker, and that they represent a general neural mechanism when understanding speech from different speakers. (C) 2014 Elsevier Inc. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0245,Functionally integrated neural processing of linguistic and talker information: An event-related fMRI and ERP study,"Speech signals contain information of both linguistic content and a talker's voice. Conventionally, linguistic and talker processing are thought to be mediated by distinct neural systems in the left and right hemispheres respectively, but there is growing evidence that linguistic and talker processing interact in many ways. Previous studies suggest that talker-related vocal tract changes are processed integrally with phonetic changes in the bilateral posterior superior temporal gyrus/superior temporal sulcus (STG/STS), because the vocal tract parameter influences the perception of phonetic information. It is yet unclear whether the bilateral STG is also activated by the integral processing of another parameter-pitch, which influences the perception of lexical tone information and is related to talker differences in tone languages. In this study, we conducted separate functional magnetic resonance imaging (fMRI) and event-related potential (ERP) experiments to examine the spatial and temporal loci of interactions of lexical tone and talker-related pitch processing in Cantonese. We found that the STG was activated bilaterally during the processing of talker changes when listeners attended to lexical tone changes in the stimuli and during the processing of lexical tone changes when listeners attended to talker changes, suggesting that lexical tone and talker processing are functionally integrated in the bilateral STG. It extends the previous study, providing evidence for a general neural mechanism of integral phonetic and talker processing in the bilateral STG. The ERP results showinteractions of lexical tone and talker processing 500-800 ms after auditory word onset (a simultaneous posterior P3b and a frontal negativity). Moreover, there is some asymmetry in the interaction, such that unattended talker changes affect linguistic processing more than vice versa, which may be related to the ambiguity that talker changes cause in speech perception and/or attention bias to talker changes. Our findings have implications for understanding the neural encoding of linguistic and talker information. (C) 2015 Elsevier Inc. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0246,Perturbation of left posterior prefrontal cortex modulates top-down processing in sentence comprehension,"Communication is an inferential process. In particular, language comprehension constantly requires top-down efforts, as often multiple interpretations are compatible with a given sentence. To assess top-down processing in the language domain, our experiment employed ambiguous sentences that allow for multiple interpretations (e.g., The client sued the murderer with the corrupt lawyer., where the corrupt lawyer could either belong to The client or the murderer). Interpretation thus depended on whether participants chunk the words of the sentence into short or long syntactic phrases. In principle, bottom-up acoustic information (i.e., the presence or absence of an intonational phrase boundary at the offset of the murderer) indicates one of the two possible interpretations. Yet, acoustic information often indicates interpretations that require words to be chunked into overly long phrases that would overburden working memory. Processing is biased against these demands, reflected in a top-down preference to chunk words into short rather than long phrases. It is often proposed, but also hotly debated, that the ability to chunk words into short phrases is subserved by the left inferior frontal gyrus (IFG). Here, we employed focal repetitive transcranial magnetic stimulation to perturb the left IFG, which resulted in a further decrease of the aptitude to tolerate long phrases, indicating the inability of the left IFG to assist the chunking of words into phrases. In contrast, the processing of auditory information was not affected. Our findings support a causal topdown role of the left inferior frontal gyrus in the chunking of words into phrases.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0247,Speech frequency-following response in human auditory cortex is more than a simple tracking,"The human auditory cortex is recently found to contribute to the frequency following response (FFR) and the cortical component has been shown to be more relevant to speech perception. However, it is not clear how cortical FFR may contribute to the processing of speech fundamental frequency (F0) and the dynamic pitch. Using intracranial EEG recordings, we observed a significant FFR at the fundamental frequency (F0) for both speech and speech-like harmonic complex stimuli in the human auditory cortex, even in the missing fundamental condition. Both the spectral amplitude and phase coherence of the cortical FFR showed a significant harmonic preference, and attenuated from the primary auditory cortex to the surrounding associative auditory cortex. The phase coherence of the speech FFR was found significantly higher than that of the harmonic complex stimuli, especially in the left hemisphere, showing a high timing fidelity of the cortical FFR in tracking dynamic F0 in speech. Spectrally, the frequency band of the cortical FFR was largely overlapped with the range of the human vocal pitch. Taken together, our study parsed the intrinsic properties of the cortical FFR and reveals a preference for speech-like sounds, supporting its potential role in processing speech intonation and lexical tones.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0248,Classification of emotion categories based on functional connectivity patterns of the human brain,"Neurophysiological and psychological models posit that emotions depend on connections across wide-spread cor-ticolimbic circuits. While previous studies using pattern recognition on neuroimaging data have shown differences between various discrete emotions in brain activity patterns, less is known about the differences in functional connectivity. Thus, we employed multivariate pattern analysis on functional magnetic resonance imaging data (i) to develop a pipeline for applying pattern recognition in functional connectivity data, and (ii) to test whether connectivity patterns differ across emotion categories. Six emotions (anger, fear, disgust, happiness, sadness, and surprise) and a neutral state were induced in 16 participants using one-minute-long emotional narratives with natural prosody while brain activity was measured with functional magnetic resonance imaging (fMRI). We computed emotion-wise connectivity matrices both for whole-brain connections and for 10 previously defined functionally connected brain subnetworks and trained an across-participant classifier to categorize the emotional states based on whole-brain data and for each subnetwork separately. The whole-brain classifier performed above chance level with all emotions except sadness, suggesting that different emotions are characterized by differences in large-scale connectivity patterns. When focusing on the connectivity within the 10 subnetworks, classification was successful within the default mode system and for all emotions. We thus show preliminary evidence for consistently different sustained functional connectivity patterns for instances of emotion categories particularly within the default mode system.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0249,Cortical tracking of lexical speech units in a multi-talker background is immature in school-aged children,"Children have more difficulty perceiving speech in noise than adults. Whether this difficulty relates to an immature processing of prosodic or linguistic elements of the attended speech is still unclear. To address the impact of noise on linguistic processing per se, we assessed how babble noise impacts the cortical tracking of intelligible speech devoid of prosody in school-aged children and adults. Twenty adults and twenty children (7-9 years) listened to synthesized French monosyllabic words presented at 2.5 Hz, either randomly or in 4-word hierarchical structures wherein 2 words formed a phrase at 1.25 Hz, and 2 phrases formed a sentence at 0.625 Hz, with or without babble noise. Neuromagnetic responses to words, phrases and sentences were identified and source-localized. Children and adults displayed significant cortical tracking of words in all conditions, and of phrases and sentences only when words formed meaningful sentences. In children compared with adults, the cortical tracking was lower for all linguistic units in conditions without noise. In the presence of noise, the cortical tracking was similarly reduced for sentence units in both groups, but remained stable for phrase units. Critically, when there was noise, adults increased the cortical tracking of monosyllabic words in the inferior frontal gyri and supratemporal auditory cortices but children did not. This study demonstrates that the difficulties of school-aged children in understanding speech in a multi-talker background might be partly due to an immature tracking of lexical but not supra-lexical linguistic units.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0250,Temporal neural dynamics of understanding communicative intentions from speech prosody,"Understanding the correct intention of a speaker is critical for social interaction. Speech prosody is an important source for understanding speakers' intentions during verbal communication. However, the neural dynamics by which the human brain translates the prosodic cues into a mental representation of communicative intentions in real time remains unclear. Here, we recorded EEG (electroencephalograph) while participants listened to dialogues. The prosodic features of the critical words at the end of sentences were manipulated to signal either suggestion, warning, or neutral intentions. The results showed that suggestion and warning intentions evoked enhanced late positive event-related potentials (ERPs) compared to the neutral condition. Linear mixed-effects model (LMEM) regression and representational similarity analysis (RSA) analyses revealed that these ERP effects were distinctively correlated with prosodic acoustic analysis, emotional valence evaluation, and intention interpretation in different time windows; The onset latency significantly increased as the processing level of abstractness and communicative intentionality increased. Neural representations of intention and emotional information emerged and parallelly persisted over a long time window, guiding the correct identification of communicative intention. These results provide new insights into understanding the structural components of intention processing and their temporal neural dynamics underlying communicative intention comprehension from speech prosody in online social interactions.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0251,The where and when of linguistic word-level prosody,"uDespite its presence in all natural languages prosodic processing remains under-researched in cognitive science. Hemispheric specialisation for linguistic word-level prosody, specifically, sensitivity to stress typicality was examined using dichotic listening. In Experiment 1, participants named targets and in Experiment 2 participants classified targets as nouns or verbs. In both studies stress typicality effects emerged in the left hemisphere only. These results suggest that: (1) the left hemisphere may be responsible for conveying accurate stress patterns prior to lexical access, (2) supra-segmental information reduces the set of potential candidates during lexical access, and (3) prosody and grammatical category interact in the language processing system. (c) 2007 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0252,Speech-independent production of communicative gestures: Evidence from patients with complete callosal disconnection,"Recent neuropsychological, psycholinguistic, and evolutionary theories on language and gesture associate communicative gesture production exclusively with left hemisphere language production. An argument for this approach is the finding that right-handers with left hemisphere language dominance prefer the right hand for communicative gestures. However, several studies have reported distinct patterns of hand preferences for different gesture types, such as deictics, batons, or physiographs, and this calls for an alternative hypothesis. We investigated hand preference and gesture types in spontaneous gesticulation during three semi-standardized interviews of three right-handed patients and one left-handed patient with complete callosal disconnection, all with left hemisphere dominance for praxis. Three of them, with left hemisphere language dominance, exhibited a reliable left-hand preference for spontaneous communicative gestures despite their left hand agraphia and apraxia. The fourth patient, with presumed bihemispheric language representation, revealed a consistent right-hand preference for gestures. All four patients displayed batons, tosses, and shrugs more often with the left hand/shoulder, but exhibited a right hand preference for pantomime gestures. We conclude that the hand preference for certain gesture types cannot be predicted by hemispheric dominance for language or by handedness. We found distinct hand preferences for specific gesture types. This suggests a conceptual specificity of the left and right hand gestures. We propose that left hand gestures are related to specialized right hemisphere functions, such as prosody or emotion, and that they are generated independently of left hemisphere language production. Our findings challenge the traditional neuropsychological and psycholinguistic view on communicative gesture production. (C) 2007 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0253,Processing melodic contour and speech intonation in congenital amusics with Mandarin Chinese,"Congenital amusia is a disorder in the perception and production of musical pitch. It has been suggested that early exposure to a tonal language may compensate for the pitch disorder (Peretz, 2008). If so, it is reasonable to expect that there would be different characterizations of pitch perception in music and speech in congenital amusics who speak a tonal language, such as Mandarin. In this study, a group of 11 adults with amusia whose first language was Mandarin were tested with melodic contour and speech intonation discrimination and identification tasks. The participants with amusia were impaired in discriminating and identifying melodic contour. These abnormalities were also detected in identifying both speech and non-linguistic analogue derived patterns for the Mandarin intonation tasks. In addition, there was an overall trend for the participants with amusia to show deficits with respect to controls in the intonation discrimination tasks for both speech and non-linguistic analogues. These findings suggest that the amusics' melodic pitch deficits may extend to the perception of speech, and could potentially result in some language deficits in those who speak a tonal language. (C) 2010 Elsevier Ltd. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0254,Emotional prosody in congenital amusia: Impaired and spared processes,"Congenital amusia is a lifelong deficit of music processing, in particular of pitch processing. Most research investigating this neurodevelopmental disorder has focused on music perception, but pitch also has a critical role for intentional and emotional prosody in speech. Two previous studies investigating amusics' emotional prosody recognition have shown either some deficit or no deficit (compared to controls). However, these previous studies have used only long sentence stimuli, which allow for limited control over acoustic content. Here, we tested amusic individuals for emotional prosody perception in sentences and vowels. For each type of material, participants performed an emotion categorization task, followed by intensity ratings of the recognized emotion. Compared to controls, amusic individuals had similar recognition of emotion in sentences, but poorer performance in vowels, especially when distinguishing sad and neutral stimuli. These lower performances in amusics were linked with difficulties in processing pitch and spectro-temporal parameters of the vowel stimuli. For emotion intensity, neither sentence nor vowel ratings differed between participant groups, suggesting preserved implicit processing of emotional prosody in amusia. These findings can be integrated into previous data showing preserved implicit processing of pitch and emotion in amusia alongside deficits in explicit recognition tasks. They are thus further supporting the hypothesis of impaired conscious analysis of pitch and timbre in this neurodevelopmental disorder.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0255,Setting the tone: An ERP investigation of the influences of phonological similarity on spoken word recognition in Mandarin Chinese,"We investigated the influences of phonological similarity on the time course of spoken word processing in Mandarin Chinese. Event related potentials were recorded while adult native speakers of Mandarin (N=19) judged whether auditory,words matched or mismatched visually presented pictures. Mismatching words were of the following nature: segmental (e.g., picture: hua1 'flower'; sound: hua4 'painting'); cohort (e.g., picture: hua1 'flower'; sound: hui1 'gray'); rhyme (e.g., picture: hua1 'flower'; sound: gua1 'melon'); tonal (e.g., picture: hua1 'flower': sound: jing1 'whale'); unrelated (e.g., picture: hua1 'flower'; sound: lang2 'wolf'). Expectancy violations in the segmental condition showed an early-going modulation of components (starting at 250 ms post-stimulus onset), suggesting that listeners used tonal information to constrain word recognition as soon as it became available, just like they did with phonemic information in the cohort condition. However, effects were less persistent and more left-lateralized in the segmental than cohort condition, suggesting dissociable cognitive processes underlie access to tonal versus phonemic information. Cohort versus rhyme mismatches showed distinct patterns of modulation which were very similar to what has been observed in English, suggesting onsets and rimes are weighted similarly across the two languages. Last, we did not observe effects for whole-syllable mismatches above and beyond those for mismatches in individual components, suggesting the syllable does not merit a special status in Mandarin spoken word recognition. These results are discussed with respect to modifications needed for existing models to accommodate the tonal languages spoken by a large proportion of the world's speakers. (C) 2012 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0256,"Mismatch responses to lexical tone, initial consonant, and vowel in Mandarin-speaking preschoolers","The present study investigates how age, phonological saliency, and deviance size affect the presence of mismatch negativity (MMN) and positive mismatch response (P-MMR). This work measured the auditory mismatch responses to Mandarin lexical tones, initial consonants, and vowels in 4- to 6-year-old preschoolers using the multiple-deviant oddball paradigm. The data showed the coexistence of MMN and P-MMR in the same age group when responding to the three types of syllabic features in Mandarin. The transition from a predominantly positive response to a predominantly negative response supported the multiple MMN mechanisms. Congruent with the phonological saliency hypothesis and the phonetic acquisition order of Mandarin in behavioral studies, for the compulsory elements of Mandarin syllables, lexical tones, and vowels, the larger deviants elicited adult-like MMNs, whereas the smaller deviants elicited P-MMRs. The optional elements of the Mandarin syllables, the initial consonant, only elicited P-MMR in preschoolers. These findings suggest that MMN and P-MMR index different functional characteristics and may provide information on when and how children's speech perception becomes automatic at different developmental stages. (C) 2012 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0257,Access to lexical meaning in pitch-flattened Chinese sentences: An fMRI study,"Chinese is a tonal language in which variation in pitch is used to distinguish word meanings. Thus, in order to understand a word, listeners have to extract the pitch patterns in addition to its phonemes. Can the correct word meaning still be accessed in sentence contexts if pitch patterns of words are altered? If so, how is this accomplished? The present study attempts to address such questions with event-related functional magnetic resonance imaging (fMRI). Native speakers of Mandarin Chinese listened to normal and pitch-flattened (monotone) speech inside the scanner. The behavioral results indicated that they rated monotone sentences as intelligible as normal sentences, and performed equally well in a dictation test on the two types of sentences. The fMRI results showed that both types of sentences elicited similar activation in the left insular, middle and inferior temporal gyri, but the monotone sentences elicited greater activation in the left planum temporale (PT) compared with normal sentences. These results demonstrate that lexical meaning can still be accessed in pitch-flattened Chinese sentences, and that this process is realized by automatic recovery of the phonological representations of lexical tones from the altered tonal patterns. Our findings suggest that the details of spoken pitch patterns are not essential for adequate lexical-semantic processing during sentence comprehension even in tonal languages like Mandarin Chinese, given that listeners can automatically use additional neural and cognitive resources to recover distorted tonal patterns in sentences. (c) 2012 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0258,Hemispheric lateralization for early auditory processing of lexical tones: Dependence on pitch level and pitch contour,"In Mandarin Chinese, a tonal language, pitch level and pitch contour are two dimensions of lexical tones according to their acoustic features (i.e., pitch patterns). A change in pitch level features a step change whereas that in pitch contour features a continuous variation in voice pitch. Currently, relatively little is known about the hemispheric lateralization for the processing of each dimension. To address this issue, we made whole-head electrical recordings of mismatch negativity in native Chinese speakers in response to the contrast of Chinese lexical tones in each dimension. We found that pre-attentive auditory processing of pitch level was obviously lateralized to the right hemisphere whereas there is a tendency for that of pitch contour to be lateralized to the left. We also found that the brain responded faster to pitch level than to pitch contour at a pre-attentive stage. These results indicate that the hemispheric lateralization for early auditory processing of lexical tones depends on the pitch level and pitch contour, and suggest an underlying inter-hemispheric interactive mechanism for the processing. (C) 2013 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0259,"The interaction of lexical tone, intonation and semantic context in on-line spoken word recognition: An ERP study on Cantonese Chinese","In two ERP experiments, we investigate the on-line interplay of lexical tone, intonation and semantic context during spoken word recognition in Cantonese Chinese. Experiment 1 shows that lexical tone and intonation interact immediately. Words with a low lexical tone at the end of questions (with a rising question intonation) lead to a processing conflict. This is reflected in a low accuracy in lexical identification and in a P600 effect compared to the same words at the end of a statement. Experiment 2 shows that a strongly biasing semantic context leads to much better lexical-identification performance for words with a low tone at the end of questions and to a disappearance of the P600 effect. These results support the claim that semantic context plays a major role in disentangling the tonal information from the intonational information, and thus, in resolving the on-line conflict between intonation and tone. However, the ERP data indicate that the introduction of a semantic context does not entirely eliminate on-line processing problems for words at the end of questions. This is revealed by the presence of an N400 effect for words with a low lexical tone and for words with a high-mid lexical tone at the end of questions. The ERP data thus show that, while semantic context helps in the eventual lexical identification, it makes the deviation of the contextually expected lexical tone from the actual acoustic signal more salient. (C) 2013 Elsevier Ltd. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0260,Cortical pitch response components index stimulus onset/offset and dynamic features of pitch contours,"Voice pitch is an important information-bearing component of language that is subject to experience dependent plasticity at both early cortical and subcortical stages of processing. We have already demonstrated that pitch onset component (Na) of the cortical pitch response (CPR) is sensitive to flat pitch and its salience. In regards to dynamic pitch, we do not yet know whether the multiple pitch-related transient components of the CPR reflect specific temporal attributes of such stimuli. Here we examine the sensitivity of the multiple transient components of CPR to changes in pitch acceleration associated with the Mandarin high rising lexical tone. CPR responses from Chinese listeners were elicited by three citation forms varying in pitch acceleration and duration. Results showed that the pitch onset component (Na) was invariant to changes in acceleration. In contrast, Na-Pb and Pb-Nb showed a systematic increase in the interpeak latency and decrease in amplitude with increase in pitch acceleration that followed the time course of pitch change across the three stimuli. A strong correlation with pitch acceleration was observed for these two components only - a putative index of pitch-relevant neural activity associated with the more rapidly-changing portions of the pitch contour. Pc-Nc marks unambiguously the stimulus offset. We therefore propose that in the early stages of cortical sensory processing, a series of neural markers flag different temporal attributes of a dynamic pitch contour: onset of temporal regularity (Na); changes in temporal regularity between onset and offset (Na-Pb, Pb-Nb); and offset of temporal regularity (Pc-Nc). At the temporal electrode sites, the stimulus with the most gradual change in pitch acceleration evoked a rightward asymmetry. Yet within the left hemisphere, stimuli with more gradual change were indistinguishable. These findings highlight the emergence of early hemispheric preferences and their functional roles as related to sensory and cognitive properties of the stimulus. (C) 2014 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0261,A music perception disorder (congenital amusia) influences speech comprehension,"This study investigated the underlying link between speech and music by examining whether and to what extent congenital amusia, a musical disorder characterized by degraded pitch processing, would impact spoken sentence comprehension for speakers of Mandarin, a tone language. Sixteen Mandarin-speaking amusics and 16 matched controls were tested on the intelligibility of news-like Mandarin sentences with natural and flat fundamental frequency (F-0) contours (created via speech resynthesis) under four signal-to-noise (SNR) conditions (no noise, +5, 0, and -5 dB SNR). While speech intelligibility in quiet and extremely noisy conditions (SNR= -5 dB) was not significantly compromised by flattened F-0, both amusic and control groups achieved better performance with natural-F-0 sentences than flat-F-0 sentences under moderately noisy conditions (SNR= +5 and 0 dB). Relative to normal listeners, amusics demonstrated reduced speech intelligibility in both quiet and noise, regardless of whether the F-0 contours of the sentences were natural or flattened. This deficit in speech intelligibility was not associated with impaired pitch perception in amusia. These findings provide evidence for impaired speech comprehension in congenital amusia, suggesting that the deficit of amusics extends beyond pitch processing and includes segmental processing. (C) 2014 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0262,Neural responses to category ambiguous words,"Category ambiguous words (like hug and swing) have the potential to complicate both learning and processing of language. However, uses of such words may be disambiguated by acoustic differences that depend on the category of use. This article uses an event-related potential (ERP) technique to ask whether adult native speakers of English show neural sensitivity to those differences. The results indicate that noun and verb tokens of ambiguous words produce differences in the arriplitude of the ERP response over left anterior sites as early as 100 ms following stimulus onset and persisting for over 400 ms. Nonsense words extracted from noun and verb contexts do not show such differences. These findings suggest that the acoustic differences between noun and verb tokens of ambiguous words are perceived and processed by adults and may be part of the lexical representation of the word. (c) 2015 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0263,Affective evaluation of simultaneous tone combinations in congenital amusia,"Congenital amusia is a neurodevelopmental disorder characterized by impaired pitch processing. Although pitch simultaneities are among the fundamental building blocks of Western tonal music, affective responses to simultaneities such as isolated dyads varying in consonance/dissonance or chords varying in major/minor quality have rarely been studied in amusic individuals. Thirteen amusics and thirteen matched controls enculturated to Western tonal music provided pleasantness ratings of sine-tone dyads and complex-tone dyads in piano timbre as well as perceived happiness/sadness ratings of sine-tone triads and complex-tone triads in piano timbre. Acoustical analyses of roughness and harmonicity were conducted to determine whether similar acoustic information contributed to these evaluations in amusics and controls. Amusic individuals' pleasantness ratings indicated sensitivity to consonance and dissonance for complex-tone (piano timbre) dyads and, to a lesser degree, sine-tone dyads, whereas controls showed sensitivity when listening to both tone types. Furthermore, amusic individuals showed some sensitivity to the happiness-major association in the complex-tone condition, but not in the sinetone condition. Controls rated major chords as happier than minor chords in both tone types. Linear regression analyses revealed that affective ratings of dyads and triads by amusic individuals were predicted by roughness but not harmonicity, whereas affective ratings by controls were predicted by both roughness and harmonicity. We discuss affective sensitivity in congenital amusia in view of theories of affective responses to isolated chords in Western listeners. (C) 2015 The Authors. Published by Elsevier Ltd.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0264,Neural responses towards a speaker's feeling of (un)knowing,"During interpersonal communication, listeners must rapidly evaluate verbal and vocal cues to arrive at an integrated meaning about the utterance and about the speaker, including a representation of the speaker's 'feeling of knowing' (i.e., how confident they are in relation to the utterance). In this study, we investigated the time course and neural responses underlying a listener's ability to evaluate speaker confidence from combined verbal and vocal cues. We recorded real-time brain responses as listeners judged statements conveying three levels of confidence with the speaker's voice (confident, close-to-confident, unconfident), which were preceded by meaning-congruent lexical phrases (e.g. I am positive, Most likely, Perhaps). Event-related potentials to utterances with combined lexical and vocal cues about speaker confidence were compared to responses elicited by utterances without the verbal phrase in a previous study (Jiang and Pell, 2015). Utterances with combined cues about speaker confidence elicited reduced, N1, P2 and N400 responses when compared to corresponding utterances without the phrase. When compared to confident statements, close-to-confident and unconfident expressions elicited reduced N1 and P2 responses and a late positivity from 900 to 1250 ms; unconfident and close-to-confident expressions were differentiated later in the 1250-1600 ms time window. The effect of lexical phrases on confidence processing differed for male and female participants, with evidence that female listeners incorporated information from the verbal and vocal channels in a distinct manner. Individual differences in trait empathy and trait anxiety also moderated neural responses during confidence processing. Our findings showcase the cognitive processing mechanisms and individual factors governing how we infer a speaker's mental (knowledge) state from the speech signal. (C) 2015 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0265,Cortical processing of phonetic and emotional information in speech: A cross-modal priming study,"The current study employed behavioral and electrophysiological measures to investigate the timing, localization, and neural oscillation characteristics of cortical activities associated with phonetic and emotional information processing of speech. The experimental design used a cross-modal priming paradigm in which the normal adult participants were presented a visual prime followed by an auditory target. Primes were facial expressions that systematically varied in emotional content (happy or angry) and mouth shape (corresponding to /a/ or /i/ vowels). Targets were spoken words that varied by emotional prosody (happy or angry) and vowel (/a/ or /i/). In both the phonetic and prosodic conditions, participants were asked to judge congruency status of the visual prime and the auditory target. Behavioral results showed a congruency effect for both percent correct and reaction time. Two ERP responses, the N400 and late positive response (LPR), were identified in both conditions. Source localization and inter-trial phase coherence of the N400 and LPR components further revealed different cortical contributions and neural oscillation patterns for selective processing of phonetic and emotional information in speech. The results provide corroborating evidence for the necessity of differentiating brain mechanisms underlying the representation and processing of co-existing linguistic and paralinguistic information in spoken language, which has important implications for theoretical models of speech recognition as well as clinical studies on the neural bases of language and social communication deficits. (C) 2016 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0266,Musical experience facilitates lexical tone processing among Mandarin speakers: Behavioral and neural evidence,"Music and speech share many sound attributes. Pitch, as the percept of fundamental frequency, often occupies the center of researchers' attention in studies on the relationship between music and speech. One widely held assumption is that music experience may confer an advantage in speech tone processing. The cross-domain effects of musical training on non-tonal language speakers' linguistic pitch processing have been relatively well established. However, it remains unclear whether musical experience improves the processing of lexical tone for native tone language speakers who actually use lexical tones in their daily communication. Using a passive oddball paradigm, the present study revealed that among Mandarin speakers, musicians demonstrated enlarged electrical responses to lexical tone changes as reflected by the increased mismatch negativity (MMN) amplitudes, as well as faster behavioral discrimination performance compared with age- and IQ-matched nonmusicians. The current results suggest that in spite of the preexisting long-term experience with lexical tones in both musicians and non musicians, musical experience can still modulate the cortical plasticity of linguistic tone processing and is associated with enhanced neural processing of speech tones. Our current results thus provide the first electrophysiological evidence supporting the notion that pitch expertise in the music domain may indeed be transferable to the speech domain even for native tone language speakers. (C) 2016 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0267,Online processing of tone and intonation in Mandarin: Evidence from ERPs,"Event-related potentials (ERPs) were used to investigate the online processing of tone and intonation in Mandarin at the attentive stage. We examined the behavioral and electrophysiological responses of native Mandarin listeners to Mandarin sentences, which contrast in final tones (rising Tone2 or falling Tone4) and intonations (Question or Statement). A clear P300 effect was observed for question-statement contrast in sentences ending with Tone4, but no ERP effect was found for question-statement contrast in sentences ending with Tone2. Our results provide ERP evidence for the interaction of tone and intonation in Mandarin, confirming the findings with behavioral metalinguistic data that native Mandarin listeners can distinguish between question intonation and statement intonation when the intonation is associated with a final Tone4, but fail to do so when the intonation is associated with a final Tone2. Our study extended the understanding of online processing of tone and intonation (1) from the pre-attentive stage to the attentive stage and (2) within a larger domain (i.e. multi-word utterances) than a single word utterance. (C) 2016 Elsevier Ltd. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0268,Neural correlates of the affective properties of spontaneous and volitional laughter types,"Previous investigations of vocal expressions of emotion have identified acoustic and perceptual distinctions between expressions of different emotion categories, and between spontaneous and volitional (or acted) variants of a given category. Recent work on laughter has identified relationships between acoustic properties of laughs and their perceived affective properties (arousal and valence) that are similar across spontaneous and volitional types (Bryant & Aktipis, 2014; Lavan et al., 2016). In the current study, we explored the neural correlates of such relationships by measuring modulations of the BOLD response in the presence of itemwise variability in the subjective affective properties of spontaneous and volitional laughter. Across all laughs, and within spontaneous and volitional sets, we consistently observed linear increases in the response of bilateral auditory cortices (including Heschrs gyrus and superior temporal gyrus [STG]) associated with higher ratings of perceived arousal, valence and authenticity. Areas in the anterior medial prefrontal cortex (amPFC) showed negative linear correlations with valence and authenticity ratings across the full set of spontaneous and volitional laughs; in line with previous research (McGettigan et al., 2015; Szameitat etal., 2010), we suggest that this reflects increased engagement of these regions in response to laughter of greater social ambiguity. Strikingly, an investigation of higher-order relationships between the entire laughter set and the neural response revealed a positive quadratic profile of the BOLD response in right-dominant STG (extending onto the dorsal bank of the STS), where this region responded most strongly to laughs rated at the extremes of the authenticity scale. While previous studies claimed a role for right STG in bipolar representation of emotional valence, we instead argue that this may in fact exhibit a relatively categorical response to emotional signals, whether positive or negative.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0269,Neural bases of congenital amusia in tonal language speakers,"Congenital amusia is a lifelong neurodevelopmental disorder of fine-grained pitch processing. In this fMRI study, we examined the neural bases of congenial amusia in speakers of a tonal language Cantonese. Previous studies on non-tonal language speakers suggest that the neural deficits of congenital amusia lie in the music selective neural circuitry in the right inferior frontal gyrus (IFG). However, it is unclear whether this finding can generalize to congenital amusics in tonal languages. Tonal language experience has been reported to shape the neural processing of pitch, which raises the question of how tonal language experience affects the neural bases of congenital amusia. To investigate this question, we examined the neural circuitries sub-serving the processing of relative pitch interval in pitch-matched Cantonese level tone and musical stimuli in 11 Cantonese-speaking amusics and 11 musically intact controls. Cantonese-speaking amusics exhibited abnormal brain activities in a widely distributed neural network during the processing of lexical tone and musical stimuli. Whereas the controls exhibited significant activation in the right superior temporal gyrus (STG) in the lexical tone condition and in the cerebellum regardless of the lexical tone and music conditions, no activation was found in the amusics in those regions, which likely reflects a dysfunctional neural mechanism of relative pitch processing in the amusics. Furthermore, the amusics showed abnormally strong activation of the right middle frontal gyrus and precuneus when the pitch stimuli were repeated, which presumably reflect deficits of attending to repeated pitch stimuli or encoding them into working memory. No significant group difference was found in the right IFG in either the whole-brain analysis or region-of-interest analysis. These findings imply that the neural deficits in tonal language speakers might differ from those in non-tonal language speakers, and overlap partly with the neural circuitries of lexical tone processing (e.g. right STG).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0270,Testing native language neural commitment at the brainstem level: A cross-linguistic investigation of the association between frequency-following response and speech perception,"A current topic in auditory neurophysiology is how brainstem sensory coding contributes to higher-level perceptual, linguistic and cognitive skills. This cross-language study was designed to compare frequency following responses (FFRs) for lexical tones in tonal (Mandarin Chinese) and non-tonal (English) language users and test the correlational strength between FFRs and behavior as a function of language experience. The behavioral measures were obtained in the Garner paradigm to assess how lexical tones might interfere with vowel category and duration judgement. The FFR results replicated previous findings about between-group differences, showing enhanced pitch tracking responses in the Chinese subjects. The behavioral data from the two subject groups showed that lexical tone variation in the vowel stimuli significantly interfered with vowel identification with a greater effect in the Chinese group. Moreover, the FFRs for lexical tone contours were significantly correlated with the behavioral interference only in the Chinese group. This pattern of language-specific association between speech perception and brainstem-level neural phase-locking of linguistic pitch information provides evidence for a possible native language neural commitment at the subcortical level, highlighting the role of experience dependent brainstem tuning in influencing subsequent linguistic processing in the adult brain.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0271,Roles of posterior parietal and dorsal premotor cortices in relative pitch processing: Comparing musical intervals to lexical tones,"Humans use time-varying pitch patterns to convey information in music and speech. Recognition of musical melodies and lexical tones relies on relative pitch (RP), the ability to identify intervals between two pitches. RP processing in music is usually more fine-grained than that in tonal languages. In Western music, there are twelve pitch categories within an octave, whereas there are only three level (non-glide) lexical tones in Taiwanese (or Taiwanese Hokkien, a tonal language). The present study aimed at comparing the neural substrates underlying RP processing of musical melodic intervals with that of level lexical tones in Taiwanese. Functional magnetic resonance imaging data from fourteen participants with good RP were analyzed. The results showed that imagining the sounds of visually presented musical intervals was associated with enhanced activity in the central subregion of the right dorsal premotor cortex (dPMC), right posterior parietal cortex (PPC), and right dorsal precuneus compared to auditory imagery of visually presented Taiwanese bi-character words with level lexical tones. During the sound-congruence-judgement task (auditory imagery of musical intervals or bi-character words, and subsequently judging if the imagined sounds were melodically congruent with heard sounds), the contrast of the musical minus linguistic conditions yielded activity in the bilateral dPMC-PPC network and dorsal precuneus, with the dPMC activated in the rostral subregion. The central dPMC and PPC may mediate the attention-based maintenance of pitch intervals, whereas the dorsal precuneus may support attention control and the spatial/sensorimotor processing of the fine-grained pitch structures of music. When judging the congruence between the imagined and heard musical intervals, the bilateral rostral dPMC may play a role in attention control, working memory, evaluation of motor activities, and monitoring mechanisms. Based on the findings of this study and recent studies of amusia, we suggest that higher order cognitive operations are critical to the more fine-grained pitch processing of musical melodies compared to lexical tones.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0272,Induced gamma oscillations index individual differences in speech sound perception and production,"Auditory neuroscience has provided strong evidence that neural oscillations synchronize to the rhythm of speech stimuli, and oscillations at different frequencies have been linked to processing of different language structures. The present study aims to examine how these ubiquitous neurophysiological attributes may inform us about the brain processes that underpin individual differences in speech perception and production, which in turn elucidate the specific functions of neural oscillations in the domain of speech processing. To this end, we recorded electrophysiological responses to a lexical tone contrast in a passive auditory oddball paradigm from two groups of healthy tone-language speakers who were equal in perceptual discriminability but differed in response latency and production distinctiveness of the tone contrast. Time-frequency analysis was applied to the EEG data, and decomposed into theta (4-7 Hz), beta (12-30 Hz), and gamma (30-50 Hz) frequency bands. Results show that listeners with longer discrimination RT and less distinctive production showed significantly higher induced (non-phase-locked) gamma during tone processing. Moreover, among speakers with less distinctive production, individual differences in induced gamma were significantly correlated with discrimination latency and production distinction. Based on the present findings, we propose that differences in gamma oscillations reflect differential sensory/perceptual computations during acoustic encoding, impacting the quality of perceptual representations, which further mediates individual differences in speech perception and production.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0273,Electrophysiological evidence for the integral nature of tone in Mandarin spoken word recognition,"Current models of spoken word recognition have been predominantly based on studies of Indo-European languages. As a result, less is known about the recognition processes involved in the perception of tonal languages (e.g., Mandarin Chinese), and the role of lexical tone in speech perception. One view is that words in tonal languages are processed phonologically through individual segments, while another view is that they are processed lexically as a whole. Moreover, a recent study claimed to be the first to discover an early phonological processing stage in Mandarin (Huang et al., 2014). There seems to be a lack of investigations concerning tonal languages, as no clear conclusions have been reached about the nature of tonal processes, or a model of spoken word recognition that best incorporates lexical tone. The current study addressed these issues by presenting 18 native Mandarin speakers with aural sentences with medial target words. These either matched or mismatched the preceding visually presented sentences with medial target words (e.g, /jia1/home). Violation conditions involved target words that differed in the following ways: tone violation, where only the tone was different (e.g., /jia4/""price""), onset violation, where only the onset was different (e.g., /xia1/""shrimp""), and syllable violation, where both the tone and the onset were different (e.g., /tang2/""candy""). We did not find evidence for an early phonological processing stage in Mandarin. Instead, our findings indicate that Mandarin syllables are processed incrementally through phonological segments and that tone is strongly associated with lexical access. These results are discussed with respect to modifications for existing models in spoken word recognition to incorporate the processes involved with tonal language recognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0274,The roles of lexical tone and rime during Mandarin sentence comprehension: An event-related potential study,"This study used event-related potential (ERP) recording to examine the role of lexical tone and rime in Mandarin Chinese spoken sentence comprehension. A violation paradigm was adopted, such that selected target syllables in the sentences were replaced with tone-violated, rime-violated, or double-violated syllables. Participants judged whether each sentence was congruent. The behavioral results confirmed previous findings: Tone violation was more difficult to detect than rime violation. The ERP results showed that rime and double violations, but not tone violation, elicited a larger N400 than the original condition. Similarly, tone and rime violations elicited a larger P600 than the original condition, and the effect started and ended 50 ms earlier in the tone-violation type. Interestingly, the double-violation type differed significantly from the original type only in the posterior electrodes, suggesting a weaker P600 effect than the tone- and rime-violation types. The differences in ERP effects between rime and tone processing indicate that rime played a more important role in semantic access, while tone played a more important role in error recovery. A model of Chinese speech perception was proposed to accommodate the different roles of lexical tone and rime at different processing stages during sentence comprehension.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0275,"Integral perception, but separate processing: The perceptual normalization of lexical tones and vowels","In tonal languages, speech variability arises in both lexical tone (i.e., suprasegmentally) and vowel quality (segmentally). Listeners can use surrounding speech context to overcome variability in both speech cues, a process known as extrinsic normalization. Although vowels are the main carriers of tones, it is still unknown whether the combined percept (lexical tone and vowel quality) is normalized integrally or in partly separate processes. Here we used electroencephalography (EEG) to investigate the time course of lexical tone normalization and vowel normalization to answer this question. Cantonese adults listened to synthesized three-syllable stimuli in which the identity of a target syllable ? ambiguous between high vs. mid-tone (Tone condition) or between /o/ vs. /u/ (Vowel condition) ? was dependent on either the tone range (Tone condition) or the formant range (Vowel condition) of the first two syllables. It was observed that the ambiguous tone was more often interpreted as a high-level tone when the context had a relatively low pitch than when it had a high pitch (Tone condition). Similarly, the ambiguous vowel was more often interpreted as /o/ when the context had a relatively low formant range than when it had a relatively high formant range (Vowel condition). These findings show the typical pattern of extrinsic tone and vowel normalization. Importantly, the EEG results of participants showing the contrastive normalization effect demonstrated that the effects of vowel normalization could already be observed within the N2 time window (190?350 ms), while the first reliable effect of lexical tone normalization on cortical processing was observable only from the P3 time window (220?500 ms) onwards. The ERP patterns demonstrate that the contrastive perceptual normalization of lexical tones and that of vowels occur at least in partially separate time windows. This suggests that the extrinsic normalization can operate at the level of phonemes and tonemes separately instead of operating on the whole syllable at once.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0276,Impaired emotion perception and categorization in semantic aphasia,"According to a constructionist model of emotion, conceptual knowledge plays a foundational role in emotion perception; reduced availability of relevant conceptual knowledge should therefore impair emotion perception. Conceptual deficits can follow both degradation of semantic knowledge (e.g., semantic 'storage' deficits in semantic dementia) and deregulation of retrieval (e.g., semantic 'access' deficits in semantic aphasia). While emotion recognition deficits are known to accompany degraded conceptual knowledge, less is known about the impact of semantic access deficits. Here, we examined emotion perception and categorization tasks in patients with semantic aphasia, who have difficulty accessing semantic information in a flexible and controlled fashion following left hemisphere stroke. In Study 1, participants were asked to sort faces according to the emotion they portrayed - with numbers, written labels and picture examples each provided as category anchors across tasks. Semantic aphasia patients made more errors and showed a larger benefit from word anchors that reduced the need to internally constrain categorization than comparison participants. They successfully sorted portrayals that differed in valence (positive vs. negative) but had difficulty categorizing different negative emotions. They were unimpaired on a control task that involved sorting faces by identity. In Study 2, participants matched facial emotion portrayals to written labels following vocal emotion prosody cues, miscues, or no cues. Patients presented with overall poorer performance and benefited from cue trials relative to within-valence miscue trials. This same effect was seen in comparison participants, who also showed deleterious effects of within-valence miscue relative to no cue trials. Overall, we found that patients with deregulated semantic retrieval have deficits in emotional perception but that word anchors and cue conditions can facilitate emotion perception by increasing access to relevant emotion concepts and reducing reliance on semantic control. Semantic control may be of particular importance in emotion perception when it is necessary to interpret ambiguous inputs, or when there is interference between conceptually similar emotional states. These findings extend constructionist accounts of emotion to encompass difficulties in controlled semantic retrieval.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0277,Mismatch negativity (MMN) as an index of asymmetric processing of consonant duration in fake Mandarin geminates,"Unlike languages where consonant duration is used contrastively to distinguish word meanings, long consonants in Mandarin Chinese only occur across morpheme boundaries as a result of concatenation and are referred to as fake geminates. To investigate whether Mandarin speakers employ duration contrast to differentiate fake Mandarin geminates and corresponding singletons as well as the underlying pattern of the processing, two auditory oddball tasks were carried out to measure the component of MMN, an index of the automatic detection of deviant stimulus. Mandarin pseudoword pairs which differ only in the duration of the medial consonant ([an1 an1] similar to [an1 nan1] vs. [an2 an2] similar to [an2 nan2]) were used as stimuli. An asymmetric pattern of brain activation was observed where the singleton deviant in the context of geminate words elicited higher MMNs than in the reversed condition. These findings are in line with earlier research suggesting that the singleton is unspecified for a moraic representation, while the geminate is specified. Mandarin speakers can employ duration contrast to distinguish fake geminates and corresponding singletons; furthermore, the processing of fake concatenated geminates in contrast to singletons is similar to that of real geminates and corresponding singletons.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0278,Phonemic mismatch negativity mediates the association between phoneme awareness and character reading ability in young Chinese children,Poor phonological awareness is associated with greater risk for reading disability. The underlying neural mechanism of such association may lie in the brain processing of phonological information. Lower amplitude of auditory mismatch negativity (MMN) has been associated with poor phonological awareness and with the presence of reading disability. The current study recorded auditory MMN to phoneme and lexical tone contrast with odd-ball paradigm and examined whether auditory MMN mediated the associations between phonological awareness and character reading ability through a three-year longitudinal study in 78 native Mandarin-speaking kindergarten children. Hierarchical linear regression and mediation analyses showed that the effect of phoneme awareness on the character reading ability was mediated by the phonemic MMN in young Chinese children. Findings underscore the key role of phonemic MMN as the underlying neurodevelopmental mechanism linking phoneme awareness and reading ability.,,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0279,Native language advantage in electrical brain responses to speech sound changes in passive and active listening condition,"It is not clear whether the brain can detect changes in native and non-native speech sounds in both unattended and attended conditions, but this information would be important to understand the nature of potential native language advantage in speech perception. We recorded event-related potentials (ERPs) for changes in duration and in Chinese lexical tone in a repeated vowel /a/ in native speakers of Finnish and Chinese in passive and active listening conditions. ERP amplitudes reflecting deviance detection (mismatch negativity; MMN and N2b) and attentional shifts towards changes in speech sounds (P3a and P3b) were investigated. In the passive listening condition, duration changes elicited increased amplitude in the MMN latency window for both standard and deviant sounds in the Finnish speakers compared to the Chinese speakers, but no group differences were observed for P3a. In passive listening to lexical tones, P3a was increased in amplitude for both standard and deviant stimuli in Chinese speakers compared to Finnish speakers, but the groups did not differ in MMN. In active listening, both tone and duration changes elicited N2b and P3b, but the groups differed only in pattern of results for the deviant type. The results thus suggest an overall increased sensitivity to native speech sounds, especially in passive listening, while the mechanisms of change detection and attentional shifting seem to work well for both native and non-native speech sounds in the attentive mode.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0280,EARLY CORTICAL PROCESSING OF LINGUISTIC PITCH PATTERNS AS REVEALED BY THE MISMATCH NEGATIVITY,"Previous brain imaging studies have shown the left hemispheric dominance for processing of lexical tone in native speakers. However, the low temporal resolution related to neuroimaging techniques might not explicitly detect the brain activities that occur at a relatively small or a determined time frame. We used the mismatch negativity (MMN) and a source estimation technique (low-resolution electromagnetic tomography [LORETA]) to probe the brain activities underlying the early pre-attentive processing of Mandarin lexical tone and intonation. A passive oddball paradigm was applied to present tone and intonation contrast in a speech and nonspeech context. The results showed that no difference of the MMN amplitudes existed between speech and nonspeech conditions, although a larger MMN was found for tone than intonation condition. Source localization of the MMNs for all of the conditions showed the right hemispheric dominance, regardless of their linguistic functions (tone vs. intonation) or speech context (speech vs. nonspeech). Interestingly, the MMN generator for normal tone and hummed tone originated from the same cortical area (right parietal lobe, BA 19). These findings suggest that the pre-attentive cortical processing can be modulated not only by speech stimuli, but also by their nonspeech hums. Our data are compatible with the acoustic hypothesis of speech processing. Crown Copyright (C) 2009 Published by Elsevier Ltd on behalf of IBRO. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0281,CATEGORICAL PERCEPTION OF LEXICAL TONES IN CHINESE REVEALED BY MISMATCH NEGATIVITY,"The present study investigated the neurophysiological correlates of categorical perception of Chinese lexical tones in Mandarin Chinese. Relative to standard stimuli, both within- and across-category deviants elicited mismatch negativity (MMN) in bilateral frontal-central recording sites. The MMN elicited in the right sites was marginally larger than in the left sites, which reflects the role of the right hemisphere in acoustic processing. At the same time, relative to within-category deviants, the across-category deviants elicited larger MMN in the left recording sites, reflecting the long-term phonemic traces of lexical tones. These results provide strong neurophysiological evidence in support of categorical perception of lexical tones in Chinese. More important, they demonstrate that acoustic and phonological information is processed in parallel within the MMN time window for the perception of lexical tones. Finally, homologous nonspeech stimuli elicited similar MMN patterns, indicating that lexical tone knowledge influences the perception of nonspeech signals. (C) 2010 IBRO. Published by Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0282,PROCESSING CANTONESE LEXICAL TONES: EVIDENCE FROM ODDBALL PARADIGMS,"Two event-related potential (ERP) experiments were conducted to investigate whether Cantonese lexical tones are processed with general auditory perception mechanisms and/or a special speech module. Two tonal features (f0 direction and f0 height deviation) were manipulated to reflect acoustic processing, and the contrast between syllables and hums was used to reveal the involvement of a speech module. Experiment 1 adopted a passive oddball paradigm to study a relatively early stage of tonal processing. Mismatch negativity (MMN) and novelty P3 (P3a) were modulated by the interaction between tonal feature and stimulus type. Similar interactions were found for N2 and P3 in Experiment 2, where more in-depth tonal processing was examined with an active oddball paradigm. Moreover, detecting tonal deviants of syllables elicited N1 and P2 that were not found in hum detection. Together, these findings suggest that the processing of lexical tone relies on both acoustic and linguistic processes from the early stage. Another noteworthy finding is the absence of brain lateralization in both experiments, which challenges the use of a lateralization pattern as evidence for processing lexical tones through a special speech module. (C) 2015 IBRO. Published by Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0283,Sensory Intelligence for Extraction of an Abstract Auditory Rule: A Cross-Linguistic Study,"In a complex linguistic environment, while speech sounds can greatly vary, some shared features are often invariant. These invariant features constitute so-called abstract auditory rules. Our previous study has shown that with auditory sensory intelligence, the human brain can automatically extract the abstract auditory rules in the speech sound stream, presumably serving as the neural basis for speech comprehension. However, whether the sensory intelligence for extraction of abstract auditory rules in speech is inherent or experience-dependent remains unclear. To address this issue, we constructed a complex speech sound stream using auditory materials in Mandarin Chinese, in which syllables had a flat lexical tone but differed in other acoustic features to form an abstract auditory rule. This rule was occasionally and randomly violated by the syllables with the rising, dipping or falling tone. We found that both Chinese and foreign speakers detected the violations of the abstract auditory rule in the speech sound stream at a pre-attentive stage, as revealed by the whole-head recordings of mismatch negativity (MMN) in a passive paradigm. However, MMNs peaked earlier in Chinese speakers than in foreign speakers. Furthermore, Chinese speakers showed different MMN peak latencies for the three deviant types, which paralleled recognition points. These findings indicate that the sensory intelligence for extraction of abstract auditory rules in speech sounds is innate but shaped by language experience. (C) 2018 IBRO. Published by Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0284,Visual Benefit in Lexical Tone Perception in Mandarin: An Event-related Potential Study,"Congruent visual information enhances auditory speech perception. This visual benefit has been widely observed in perception of consonants and vowels, and linked to reduced amplitudes and latencies of audi -tory N1 and P2 event-related potential (ERP) components when visual information was present. However, it remains unclear whether lexical tone perception in Mandarin also shows this visual benefit. This question is the-oretically important given the low visual saliency of lexical tones. The current study compared the N1/P2 reduc-tion in Mandarin lexical tones and consonants perception with a discrimination task. Result showed amplitude reductions in N1/P2 and a latency reduction in N1 for audiovisual lexical tone perception. These findings suggest that lexical tone perception was also helped by visual information as found in consonants. Furthermore, this visual benefit in N1 for lexical tone perception was delayed relative to consonants. (c) 2021 IBRO. Published by Else -vier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0285,Processing of emotional faces in congenital amusia: An emotional music priming event-related potential study,"Congenital amusia is characterized by lifelong impairments in music perception and processing. It is unclear whether pitch detection deficits impact amusic individuals perception of musical emotion. In the current work, 19 amusics and 21 healthy controls were subjected to electroencephalography (EEG) while being exposed to music excerpts and emotional faces. We assessed each individual's ability to discriminate positive- and negative-valenced emotional faces and analyzed electrophysiological indices, in the form of event-related potentials (ERPs) recorded at 32 sites, following exposure to emotionally positive or negative music excerpts. We observed smaller N2 amplitudes in response to facial expressions in the amusia group than in the control group, suggesting that amusics were less affected by the musical stimuli. The late-positive component (LPC) in amusics was similar to that in controls. Our results suggest that the neurocognitive deficit characteristic of congenital amusia is fundamentally an impairment in musical information processing rather than an impairment in emotional processing. (C) 2017 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0286,Talker normalization in typical Cantonese-speaking listeners and congenital amusics: Evidence from event-related potentials,"Despite the lack of invariance in the mapping between the acoustic signal and phonological representation, typical listeners are capable of using information of a talker's vocal characteristics to recognize phonemes, a process known as ""talker normalization"". The current study investigated the time course of talker normalization in typical listeners and individuals with congenital amusia, a neurodevelopmental disorder of refined pitch processing. We examined the event-related potentials (ERPs) underling lexical tone processing in 24 Cantonese-speaking amusics and 24 typical listeners (controls) in two conditions: blocked-talker and mixed-talker conditions. The results demonstrated that for typical listeners, effects of talker variability can be observed as early as in the N1 time-window (100-150 ms), with the N1 amplitude reduced in the mixed-talker condition. Significant effects were also found in later components: the N2b/c peaked significantly earlier and the P3a and P3b amplitude was enhanced in the blocked-talker condition relative to the mixed-talker condition, especially for the tone pair that is more difficult to discriminate. These results suggest that the blocked-talker mode of stimulus presentation probably facilitates auditory processing and requires less attentional effort with easier speech categorization than the mixed-talker condition, providing neural evidence for the ""active control theory"". On the other hand, amusics exhibited comparable N1 amplitude to controls in both conditions, but deviated from controls in later components. They demonstrated overall later N2b/c peak latency significantly reduced P3a amplitude in the blocked-talker condition and reduced P3b amplitude irrespective of talker conditions. These results suggest that the amusic brain was intact in the auditory processing of talker normalization processes, as reflected by the comparable N1 amplitude, but exhibited reduced automatic attentional switch to tone changes in the blocked-talker condition, as captured by the reduced P3a amplitude, which presumably underlies a previously reported perceptual ""anchoring"" deficit in amusics. Altogether, these findings revealed the time course of talker normalization processes in typical listeners and extended the finding that conscious pitch processing is impaired in the amusic brain.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0287,Functional patterns of neural activation during vocal emotion recognition in youth with and without refractory epilepsy,"Epilepsy has been associated with deficits in the social cognitive ability to decode others' nonverbal cues to infer their emotional intent (emotion recognition). Studies have begun to identify potential neural correlates of these deficits, but have focused primarily on one type of nonverbal cue (facial expressions) to the detriment of other crucial social signals that inform the tenor of social interactions (e.g., tone of voice). Less is known about how individuals with epilepsy process these forms of social stimuli, with a particular gap in knowledge about representation of vocal cues in the developing brain. The current study compared vocal emotion recognition skills and functional patterns of neural activation to emotional voices in youth with and without refractory focal epilepsy. We made novel use of inter-subject pattern analysis to determine brain areas in which activation to emotional voices was predictive of epilepsy status. Results indicated that youth with epilepsy were comparatively less able to infer emotional intent in vocal expressions than their typically developing peers. Activation to vocal emotional expressions in regions of the mentalizing and/or default mode network (e.g., right temporo-parietal junction, right hippocampus, right medial prefrontal cortex, among others) differentiated youth with and without epilepsy. These results are consistent with emerging evidence that pediatric epilepsy is associated with altered function in neural networks subserving social cognitive abilities. Our results contribute to ongoing efforts to understand the neural markers of social cognitive deficits in pediatric epilepsy, in order to better tailor and funnel interventions to this group of youth at risk for poor social outcomes.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0288,"Survey on speech emotion recognition: Features, classification schemes, and databases","Recently, increasing attention has been directed to the study of the emotional content of speech signals, and hence, many systems have been proposed to identify the emotional content of a spoken utterance. This paper is a survey of speech emotion classification addressing three important aspects of the design of a speech emotion recognition system. The first one is the choice of suitable features for speech representation. The second issue is the design of an appropriate classification scheme and the third issue is the proper preparation of an emotional speech database for evaluating system performance. Conclusions about the performance and limitations of current speech emotion recognition systems are discussed in the last section of this survey. This section also suggests possible ways of improving speech emotion recognition systems. (C) 2010 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0289,Specifying the diffusion MRI connectome in Chinese-speaking children with developmental dyslexia and auditory processing deficits,"Background: Lexical tone identification has a unique role in the perceptual processes of Chinese readers. Reduced lexical tone awareness, along with poor word-decoding abilities, is frequently observed in Chinese-speaking children with developmental dyslexia. However, whether this deficit is linked to reduced auditory processing and interrupted structural connectivity in the brain requires further investigation. This study therefore explores the white matter pathways associated with Chinese character recognition and auditory processing of pitch variations, with the objective of to identify the most representative neural correlates for Chinese developmental dyslexia. Methods: Diffusion magnetic resonance imaging and several behavior measures related to reading attainment and phonological awareness were acquired in twenty-four Chinese-speaking children with developmental dyslexia and twenty-two age-matched controls. We used diffusion magnetic resonance imaging connectometry to explore the relationships between behavior performance and specific white matter tracts. Results: The results revealed significant correlations of the left inferior fronto-occipital fasciculus, cerebellar pathways, and thalamopontine tracts with Chinese character recognition (FDR = 0.03235). In addition, the posterior isthmus and anterior splenium of the corpus callosum correlated with auditory processing (FDR = 0.03980). Conclusion: The study provides evidence that the dysconnectivity on white matter pathways correlated with developmental dyslexia in Chinese-speaking children. Furthermore, the impairments of auditory temporal timing processing presented in poor readers with significant phonological deficits are likely to be a result of impoverished myelinization in sub-cortical tracts. Such findings may assist in the clinical identification of Chinese developmental dyslexia. Copyright (C) 2018, Taiwan Pediatric Association. Published by Elsevier Taiwan LLC.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0290,Prosody and idioms in English,"English idioms commonly appear to exhibit relatively fixed prosodic patterns, and departure from the expected prosodic pattern can give rise to humorous and bizarre effects. As idioms are generally supposed to require phrasal entries in the mental lexicon, there is some initial plausibility in the proposal that such entries might include arbitrary prosodic or accentual properties. Various categories of idiom can be distinguished, according to which aspects of the prosodic pattern seem to be fixed, and the relationship the pattern bears to those which would be expected on corresponding literal expressions. Nevertheless it is argued that the prosodic patterns of idioms are in reality neither fixed nor arbitrary. The bizarre effects in interpretation result not from deviation from a lexically specified pattern, but from the attempt to introduce focus distinctions into the non-compositional parts of idioms. Implications for psycholinguistic studies of the processing of ambiguous sentences are discussed. (c) 2006 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0291,"The workings of the Japanese token hee in informing sequences - An analysis of sequential context, turn shape, and prosody","Employing the methodological framework of conversation analysis (cf. Sacks, Schegloff, and Jefferson, 1974), this study provides a detailed analysis of the ways in which the token hee is used in Japanese conversations. Hee has often been identified as a ""news-receipt token,"" and indeed it frequently occurs in response to deliveries of news or other types of informing in the current data. However, close examination of the sequential context, turn shape, and prosody involved in each occurrence of hee demonstrates the varying contributions this token can make in the development of informing sequences. On some occasions, the token displays its producer's ""assessment"" (cf. Goodwin, 1986; Goodwin and Goodwin, 1987, 1992) of the news received, whereas on other occasions it serves as a ""continuer"" (cf. Schegloff, 1982), occurring in the midst of a projected, extended informing, and yet on other occasions it is treated as a ""repair initiator"" (cf. Schegloff, Jefferson, and Sacks, 1997). The results of the current microanalysis reveal potential pitfalls in classifying variants of a particular token into one category by virtue of its prescriptive orthographic representation and perceived core function. The concluding discussion considers implications for methodological issues involved in cross-linguistic, cross-cultural comparisons of interactional styles. (c) 2005 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0292,Tone and intonation in discourse management - How do speakers of Standard Vietnamese initiate a repair?,"This paper investigates the intonation of other-initiation of repair in Standard Vietnamese and provides an analysis of its interaction with lexical tone at the word level. Results from both single-word and multi-word utterances show that speakers make predominant use of a rising pitch contour to mark other-initiation of repair, a contour that has been found in a large number of languages. This rise occurs consistently at the right edge of utterances and may be analysed as a phonetic reflex of a high boundary tone that can overlap, partially or completely, with the lexical tone of an utterance-final word. Further, the study demonstrates the application of talk-in-interaction to the study of the relation between lexical tone and intonation in tone languages. (C) 2016 Elsevier B.V. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0293,Electrophysiological basis of reading related phonological impairment in Chinese speakers with schizophrenia: An ERP study,"It has been reported in alphabetic languages that individuals with schizophrenia showed language-related cognitive impairments including phonological deficits, which were in turn associated with clinical symptoms such as auditory hallucinations and thought disorders. To date, however, the phonological deficits involved in schizophrenia in Chinese and its neural basis have not been well established. In order to establish such a relationship we conducted a behavioral study using lexical tone judgment and digit span tasks as well as an event-related potential (ERP) study with an auditory oddball paradigm, in particular, for P300 effects, the event related brain potential (ERP) index of discrimination. Chinese patients with schizophrenia and Chinese healthy controls in China participated in the current study. Compared to the healthy controls, the patients with schizophrenia showed significant impairments in phonological processing skills, which in turn significantly correlated with smaller P300 effects. Thus these behavioral and electrophysiological findings in Chinese patients with schizophrenia were critically evaluated in terms of their phonological processing abilities.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0294,Intact representation of vocal smile in autism: A reverse correlation approach,"Atypical emotional prosody production and perception have been reported in autism. However, it is unclear whether these particularities are associated with unusual mental representations of vocal emotions. The objective of the current study was to explore the mental representation of vocal smile in autistic adults. Twenty-nine autistic (ASD) and 29 neurotypical (NT) adults performed an auditory reverse correlation task, that affords the opportunity to extract acoustic features of mental representation and their variability. Most ASD participants (17) based their representation of vocal smile on similar acoustic features as NT participants and no difference in the level of internal noise was observed. However, comparisons between groups revealed a more typical representation in NT than in ASD. Subsequent cluster analysis revealed that the difference of typicality was explained by a small subset of ASD participants displaying different representations. A correlation analysis also revealed that the typicality was positively correlated with the empathetic level within both groups. While most autistic adults have a preserved mental representation of vocal smiles, a subset shows less robust and typical representations, which is linked to lower levels of empathy. This study highlights that the perception of vocal smiles in autism is more nuanced than previously reported, with empathy playing a substantial role in shaping these mental representations.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0295,Pitch perception deficits in nonverbal learning disability,"The nonverbal learning disability (NLD) is a neurological dysfunction that affects cognitive functions predominantly related to the right hemisphere such as spatial and abstract reasoning. Previous evidence in healthy adults suggests that acoustic pitch (i.e., the relative difference in frequency between sounds) is, under certain conditions, encoded in specific areas of the right hemisphere that also encode the spatial elevation of external objects (e.g., high vs. low position). Taking this evidence into account, we explored the perception of pitch in preadolescents and adolescents with NLD and in a group of healthy participants matched by age, gender, musical knowledge and handedness. Participants performed four speeded tests: a stimulus detection test and three perceptual categorization tests based on colour, spatial position and pitch. Results revealed that both groups were equally fast at detecting visual targets and categorizing visual stimuli according to their colour. In contrast, the NLD group showed slower responses than the control group when categorizing space (direction of a visual object) and pitch (direction of a change in sound frequency). This pattern of results suggests the presence of a subtle deficit at judging pitch in NLD along with the traditionally-described difficulties in spatial processing. (C) 2016 Published by Elsevier Ltd.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0296,Auditory identification of frequency-modulated sweeps and reading difficulties in Chinese,"Background: In Chinese Mandarin, lexical tones play an important role of providing contrasts in word meaning. They are pitch patterns expressed by frequency-modulated (FM) signals. Yet, few studies have looked at the relationship between low-level auditory processing of frequency signals and Chinese reading skills. Aims: The study aims to identify the role of auditory frequency processing in Chinese lexical tone awareness as well as character recognition in Chinese-speaking children. Methods: Children with (N = 28) and without (N = 27) developmental dyslexia (DD) were recruited. All participants completed two linguistic tasks, Chinese character recognition and lexical tone awareness, and two auditory frequency processing tasks, frequency discrimination and FM sweep direction identification. Results: The results revealed that Chinese-speaking children with DD were significantly poorer at all tasks. Particularly, Chinese character recognition was significantly related to FM sweep identification. Lexical tone awareness was significantly associated with both auditory frequency processing tasks. Regression analyses suggested the influence of FM sweep identification on Chinese character recognition contributed through lexical tone awareness. Conclusions and implication: This study suggests that poor auditory frequency processing may associate with Chinese developmental dyslexia with phonological deficits. In support of the phonological deficit hypothesis, what underlies phonological deficit is likely to be auditory-basis. A potential clinical implication is to reinforce auditory perception and sensitivity through intervention for phonological processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0297,Age-sensitive associations of segmental and suprasegmental perception with sentence-level language skills in Mandarin-speaking children with cochlear implants,"Background and aim: It remains unclear how recognition of segmental and suprasegmental phonemes contributes to sentence-level language processing skills in Mandarin-speaking children with cochlear implants (CIs). Our study examined the influence of implantation age on the recognition of consonants, lexical tones and sentences respectively, and more importantly, the contribution of phonological skills to sentence repetition accuracy in Mandarin-speaking children with CIs. Methods: The participants were three groups of prelingually deaf children who received cochlear implants at various ages and their age-matched controls with normal hearing. Three tasks were administered to assess their consonant perception, lexical tone recognition and language skills in open-set sentence repetition. Results: Children with CIs lagged behind NH peers in all the three tests, and performances on segmental, suprasegmental and sentence-level processing were differentially modulated by implantation age. Furthermore, performances on recognition of consonants and lexical tones were significant predictors of sentence repetition accuracy in the children with CIs. Conclusion: Overall, segmental and suprasegmental perception as well as sentence-level processing is impaired in Mandarin-speaking children with CIs compared with age-matched children with NH. In children with CIs recognition of segmental and suprasegmental phonemes at the lower level predicts sentence repetition accuracy at the higher level. More importantly, implantation age plays an important role in the development of phonological skills and higher-order language skills, suggesting that age-appropriate aural rehabilitation and speech intervention programs need to be developed in order to better help CI users who receive CIs at different ages.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0298,Implementing discourse-gating tasks to study the timing of speech act recognition,"This paper presents the development of two novel discourse-gating tasks to investigate the processing of pragmatic information, namely, the timing of the recognition of genuine (sincere) and ostensible (transparently insincere) refusals in Chinese and provides preliminary validity evidence for the tasks. Gating tasks were introduced to investigate spoken word recognition and have been successfully extended to spoken language processing, most notably sentences. Following Grosjean's (1996) observation that gating tasks could be used to investigate a variety of linguistic features, we extended the gating tasks to spoken discourse using turns as gates. The open-prediction gating task allows participants to make a single prediction about the outcome of each of 12 recorded conversations as soon as they can. The fixed-prediction gating task asks participants to make predictions at regular intervals while listening to a second set of 12 conversations. One hundred and seven participants (60 L1 speakers and 47 third- and fourth-year learners of Chinese) were recruited to test the tasks. The tasks reveal a lag in speech-act identification not found when retrospective speech-act identification tasks are used. The fixedprediction task additionally reveals alternatives that are considered during processing. The paper discusses the benefits of the discourse gating tasks and the merits of each, the quantitative and qualitative evidence for the tasks, and future directions for discourse gating tasks.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0299,Tone nucleus modeling for Chinese lexical tone recognition,"This paper presents a new scheme to deal with variations in fundamental frequency (F0) contours for lexical tone recognition in continuous Chinese speech. We divide F0 contour of a syllable into tone nucleus and adjacent articulatory transitions. We only use acoustic features of the tone nucleus for tone recognition. Tone nucleus of a syllable is assumed to be the target F0 of the associated lexical tone, and usually conforms more likely to the standard tone pattern than the articulatory transitions. A tone nucleus can be detected from a syllable F0 contour by a two-step algorithm. First, the syllable F0 contour is segmented into several linear F0 loci that serve as candidates for the tone-nucleus using segmental K-means segmentation algorithm. Then, tone nucleus is chosen from a set of candidates by a predictor based on linear discriminant analysis. Speaker dependent tone recognition experiments using tonal HMMs showed our new approach achieved an improvement of up to 6% for tone recognition rate compared with a conventional one. This indicates not only that tone-nucleus keeps important discriminant information for the lexical tones, but also that our tone-nucleus based tone recognition algorithm works properly. (C) 2004 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0300,Audiovisual representation of prosody in expressive speech communication,"Prosody in a single speaking style-often read speech-has been studied extensively in acoustic speech. During the past few years we have expanded our interest in two directions: (1) Prosody in expressive speech communication and (2) prosody as an audiovisual expression. Understanding the interactions between visual expressions (primarily in the face) and the acoustics of the corresponding speech presents a substantial challenge. Some of the visual articulation is for obvious reasons tightly connected to the acoustics (e.g. lip and jaw movements), but there are other articulatory movements that do not show up on the outside of the face. Furthermore, many facial gestures used for communicative purposes do not affect the acoustics directly, but might nevertheless be connected on a higher communicative level in which the timing of the gestures could play an important role. In this presentation we will give some examples of recent work, primarily at KTH, addressing these questions. We will report on methods for the acquisition and modeling of visual and acoustic data, and some evaluation experiments in which audiovisual prosody is tested. The context of much of our work in this area is to create an animated talking agent capable of displaying realistic communicative behavior and suitable for use in conversational spoken language systems, e.g. a virtual language teacher. (c) 2005 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0301,Form and function in the representation of speech prosody,"The way in which prosody contributes to meaning is still, today, a poorly understood process corresponding to a mapping between two levels of representation, for neither of which there is any general consensus. It is argued that annotation of prosody generally consists in describing both prosodic function and prosodic form, but that it would be preferable to clearly distinguish the two levels. One elementary annotation system for prosodic function, IF-annotation, is, it has been argued, sufficient to capture at least those aspects of prosodic function which influence syntactic interpretation. The annotation of prosodic form can be carried out automatically by means of an F0 modelling algorithm, MOMEL, and an automatic coding scheme, INTSINT. The resulting annotation is under-determined by the IF-annotation, but defining mapping rules between representations of function and representation of form could provide an interesting means of establishing an enriched functional annotation system through analysis by synthesis. (c) 2005 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0302,Applying data mining techniques to corpus based prosodic modeling,"This article presents MEMOInt, a methodology to automatically extract the intonation patterns which characterize a given corpus, with applications in text-to-specch systems. Easy to understand information about the form of the characteristic patterns found in the corpus can be obtained from MEMOint in a way which allows easy comparison with other proposals. A visual representation of the relationship between the set of prosodic features which could have been selected to label the corpus and the intonation contour patterns is also easy to obtain. The particular function-form correspondence associated to the given corpus is represented by means of a list of dictionaries of classes of parameterized FO patterns, where the access key is given by a sequence of prosodic features. MEMOInt can also be used to obtain valuable information about the relative impact of the use of different parameterization techniques of FO contours or of different types of intonation units and information about the relevance of different prosodic features. The methodology has been specifically designed to provide a successful strategy to solve the data sparseness problem which usually affects corpora as a consequence of the inherent high variability of the intonation phenomenon. (c) 2007 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0303,Extraction and representation of prosodic features for language and speaker recognition,"In this paper, we propose a new approach for extracting and representing prosodic features directly from the speech signal. We hypothesize that prosody is linked to linguistic units such as syllables, and it is manifested in terms of changes in measurable parameters such as fundamental frequency (F-0), duration and energy. In this work, syllable-like unit is chosen as the basic unit for representing the prosodic characteristics. Approximate segmentation of continuous speech into syllable-like units is obtained by locating the vowel onset points (VOP) automatically. The knowledge of the VOPs serve as reference for extracting prosodic features from the speech signal. Quantitative parameters are used to represent F-0 and energy contour in each region between two consecutive VOPs. Prosodic features extracted using this approach may be useful in applications such as recognition of language or speaker, where explicit phoneme/syllable boundaries are not easily available. The effectiveness of the derived prosodic features for language and speaker recognition is evaluated in the case of NIST language recognition evaluation 2003 and the extended data task of NIST speaker recognition evaluation 2003, respectively. (c) 2008 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0304,A Danish phonetically annotated spontaneous speech corpus (DanPASS),"A corpus is described consisting of non-scripted monologues and dialogues, recorded by 27 speakers, comprising a total of 73,227 running words, corresponding to 9 h and 46 min of speech. The monologues were recorded as one-way communication with an unseen partner where the speaker performed three different tasks: (s)he described a network consisting of various geometrical shapes in various colours, (s)he guided the listener through four different routes in a virtual city map, and (s)he instructed the listener how to build a house from its individual pieces. The dialogues are replicas of the HCRC map tasks. Annotation is performed in Praat. The sound files are segmented into prosodic phrases, words, and syllables. The files are supplied, in separate interval tiers, with an orthographical representation, detailed part-of-speech tags, simplified part-of-speech tags, a phonemic notation, a semi-narrow phonetic notation, a symbolic representation of the pitch relation between each stressed and post-tonic syllable, and a symbolic representation of the phrasal intonation. (C) 2008 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0305,Identification of multi-speaker Mandarin tones in noise by native and non-native listeners,"The similarities and contrasts between native and non-native identification of multi-speaker Mandarin tones in quiet and in noise were explored in a perception experiment. Mandarin tone materials produced by three male and three female speakers were presented with five levels of signal-to-noise ratios (quiet, 0, -5, -10, and -15 dB) in two presentation formats (blocked by speaker and mixed across speakers) to listeners with various Mandarin experience (native, first-year, second-year, third-year, and fourth-year students). Stimuli blocked by speaker yielded higher accuracy and shorter reaction time. The additional demand of processing mixed-speaker stimuli, however, did not compromise non-native performance more than native performance. Noise expectedly compromised identification performance, although it did not compromise non-native identification more than native identification. Native listeners expectedly outperformed non-native listeners, although identification performance did not vary systematically as a function of duration of Mandarin experience. It is speculated that sources of variability in speech would affect non-native more than native tone identification only if syllable-internal, canonical F0 information is removed or altered. Published by Elsevier B.V.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0306,"Characterization of atypical vocal source excitation, temporal dynamics and prosody for objective measurement of dysarthric word intelligibility","Objective measurement of dysarthric speech intelligibility can assist clinicians in the diagnosis of speech disorder severity as well as in the evaluation of dysarthria treatments. In this paper, several objective measures are proposed and tested as correlates of subjective intelligibility. More specifically, the kurtosis of the linear prediction residual is proposed as a measure of vocal source excitation oddity. Additionally, temporal perturbations resultant from imprecise articulation and atypical speech rates are characterized by short- and long-term temporal dynamics measures, which in turn, are based on log-energy dynamics and on an auditory-inspired modulation spectral signal representation, respectively. Motivated by recent insights in the communication disorders literature, a composite measure is developed based on linearly combining a salient subset of the proposed measures with conventional prosodic parameters. Experiments with the publicly-available 'Universal Access' database of spastic dysarthric speech (10 patient speakers; 300 words spoken in isolation, per speaker) show that the proposed composite measure can achieve correlation with subjective intelligibility ratings as high as 0.97; thus the measure can serve as an accurate indicator of dysarthric speech intelligibility. (C) 2011 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0307,Toward invariant functional representations of variable surface fundamental frequency contours: Synthesizing speech melody via model-based stochastic learning,"Variability has been one of the major challenges for both theoretical understanding and computer synthesis of speech prosody. In this paper we show that economical representation of variability is the key to effective modeling of prosody. Specifically, we report the development of PENTAtrainer-A trainable yet deterministic prosody synthesizer based on an articulatory functional view of speech. We show with testing results on Thai, Mandarin and English that it is possible to achieve high-accuracy predictive synthesis of fundamental frequency contours with very small sets of parameters obtained through stochastic learning from real speech data. The first key component of this system is syllable-synchronized sequential target approximation implemented as the qTA model, which is designed to simulate, for each tonal unit, a wide range of contextual variability with a single invariant target. The second key component is the automatic learning of function-specific targets through stochastic global optimization, guided by a layered pseudo-hierarchical functional annotation scheme, which requires the manual labeling of only the temporal domains of the functional units. The results in terms of synthesis accuracy demonstrate that effective modeling of the contextual variability is the key also to effective modeling of function-related variability. Additionally, we show that, being both theory-based and trainable (hence data-driven), computational systems like PENTAtrainer can serve as an effective modeling tool in basic research, with which the level of falsifiability in theory testing can be raised, and also a closer link between basic and applied research in speech science can be developed. (C) 2013 Elsevier B.V. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0308,Exploring the relationship between intonation and the lexicon: Evidence for lexicalised storage of intonation,"In Germanic languages like English and German, intonation is usually thought to be 'post-lexical'. That is, it is usually assumed that the choice of intonation contour and the form of the realised contour itself are largely independent of the words used. We present three corpus experiments which show clear evidence of lexical storage of intonation, contrary to these assumptions. Specifically, in each experiment, we show that distributional properties of words affect the prosodic realisation of those words, including accent and boundary placement, and the shape of pitch accents. The first experiment looks at the frequency of occurrence of a given word with a particular pitch accent type and its effect on the shape of accents on that word. We found that the more frequently a word and an accent type appear together, the greater the amplitude of the accent. The second experiment investigates the effect of both the absolute and relative frequency of occurrence of a given word with a particular accent type and their effect on the variability of the shape of these accents. We found that while absolute frequency increases the variability in pitch accent shape, relative frequency reduces it. The final experiment looks at the effect of the relative frequency of a word in its lexical (trigram) context on both variability in its prosodic context and on accent shape variability. We found that both kinds of prosodic variability decrease as the relative frequency of the word in its lexical context increases. We argue that all of these findings are expected within an exemplar approach assuming storage of tonal information with lexical items, and discuss the implications of this for the production and mental representation of intonation. (C) 2014 Elsevier B.V. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0309,Improving HMM speech synthesis of interrogative sentences by pitch track transformations,"Modeling interrogative sentence prosody is a challenging task due to the significant variation of questions. Prosody is produced by intonation, intensity and duration features. Intonation clearly identifies the type of question in most European languages. If only limited training data is available from certain sentence types, synthetic intonation often lacks accuracy, richness and detail due to averaging, inherent in statistical approaches. In this paper, we discuss two rule-based solutions to improve intonation of interrogative sentences. The first approach utilizes a pitch prediction algorithm where a rule-set forms the pitch pattern. The output pattern is combined with an HMM generated F-0 contour and the resulting pattern is used for speech synthesis. Our second solution uses key points to define a scaling function. The position of the key points is described by a rule-set, and the value at intermediate points is calculated in training time in a data-driven way in order to maintain the characteristics of the speakers' voice. The proposed two hybrid rule-based-HMM systems were evaluated by speech experts and by perceptual tests. Our evaluation shows that both approaches could significantly improve the prosodic representation of questions in our framework without deteriorating the perceived naturalness of synthetic speech. The pitch contours generated by the systems were compared and evaluated according to how well they could reproduce the unique characteristics of different Hungarian interrogative sentence types. The analysis shows that both of the methods could successfully model the required patterns. In a separate perception test the interrogative prosody identification rate of the solutions was also measured. The results demonstrate that applying our proposed methods the identification rate of questions could be improved significantly. Although our work focuses on our mother tongue-Hungarian- the methodology can be extended to other languages. (C) 2016 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0310,Multimodal analysis of speech and arm motion for prosody-driven synthesis of beat gestures,"We propose a framework for joint analysis of speech prosody and arm motion towards automatic synthesis and realistic animation of beat gestures from speech prosody and rhythm. In the analysis stage, we first segment motion capture data and speech audio into gesture phrases and prosodic units via temporal clustering, and assign a class label to each resulting gesture phrase and prosodic unit. We then train a discrete hidden semi-Markov model (HSMM) over the segmented data, where gesture labels are hidden states with duration statistics and frame-level prosody labels are observations. The HSMM structure allows us to effectively map sequences of shorter duration prosodic units to longer duration gesture phrases. In the analysis stage, we also construct a gesture pool consisting of gesture phrases segmented from the available dataset, where each gesture phrase is associated with a class label and speech rhythm representation. In the synthesis stage, we use a modified Viterbi algorithm with a duration model, that decodes the optimal gesture label sequence with duration information over the HSMM, given a sequence of prosody labels. In the animation stage, the synthesized gesture label sequence with duration and speech rhythm information is mapped into a motion sequence by using a multiple objective unit selection algorithm. Our framework is tested using two multimodal datasets in speaker-dependent and independent settings. The resulting motion sequence when accompanied with the speech input yields natural-looking and plausible animations. We use objective evaluations to set parameters of the proposed prosody-driven gesture animation system, and subjective evaluations to assess quality of the resulting animations. The conducted subjective evaluations show that the difference between the proposed HSMM based synthesis and the motion capture synthesis is not statistically significant. Furthermore, the proposed HSMM based synthesis is evaluated significantly better than a baseline synthesis which animates random gestures based on only joint angle continuity. (C) 2016 Elsevier B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0311,Contextual effect in second language perception and production of Mandarin tones,"A robust contextual effect is well documented in native tone perception and production but less well studied in non-native speech. The current study examined English-speaking learners' Mandarin tone perception and production with varying preceding and following tones. Fifteen intermediate-level learners performed an Identification and a Reading task with disyllabic stimuli encompassing various tone combinations. The results revealed that the learners' accuracy rates and error patterns varied in the initial and final position as well as in different tonal environments. For example, the learners predominantly misidentified T3 as T4 in the initial position when the following tone was T1, but rarely made such an error in the final position. In the Reading task, on the other hand, the learners frequently misproduced T3 as T2 in the final position, but often misproduced T2 as T3 in the initial position, especially when the following tone was T1. The learners' overall accuracy and error rates in Identification correlated with those in Reading, indicating comparable tone perception and production abilities. However, there were some significant differences between perception and production with regard to T3. These findings suggest that the prosodic position and surrounding tones both have a significant effect on the learners' performance with L2 tones, and that the effect differs in perception and production.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0312,Computer-vision analysis reveals facial movements made during Mandarin tone production align with pitch trajectories,"Using computer-vision and image processing techniques, we aim to identify specific visual cues as induced by facial movements made during Mandarin tone production and examine how they are associated with each of the four Mandarin tones. Audio-video recordings of 20 native Mandarin speakers producing Mandarin words involving the vowel /3/ with each of the four tones were analyzed. Four facial points of interest were detected automatically: medial point of left eyebrow, nose tip (proxy for head movement), and midpoints of the upper and lower lips. The detected points were then automatically tracked in the subsequent video frames. Critical features such as the distance, velocity, and acceleration describing local facial movements with respect to the resting face of each speaker were extracted from the positional profiles of each tracked point. Analysis of variance and feature importance analysis based on random forest were performed to examine the significance of each feature for representing each tone and how well these features can individually and collectively characterize each tone. Results suggest alignments between articulatory movements and pitch trajectories, with downward or upward head and eyebrow movements following the dipping and rising tone trajectories respectively, lip closing movement being associated with the falling tone, and minimal movements for the level tone.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0313,DeepConversion: Voice conversion with limited parallel training data,"A deep neural network approach to voice conversion usually depends on a large amount of parallel training data from source and target speakers. In this paper, we propose a novel conversion pipeline, DeepConversion, that leverages a large amount of non-parallel, multi-speaker data, but requires only a small amount of parallel training data. It is believed that we can represent the shared characteristics of speakers by training a speaker independent general model on a large amount of publicly available, non-parallel, multi-speaker speech data. Such general model can then be used to learn the mapping between source and target speaker more effectively from a limited amount of parallel training data. We also propose a strategy to make full use of the parallel data in all models along the pipeline. In particular, the parallel data is used to adapt the general model towards the source-target speaker pair to achieve a coarse grained conversion, and to develop a compact Error Reduction Network (ERN) for a fine-grained conversion. The parallel data is also used to adapt the WaveNet vocoder towards the source-target pair. The experiments show that DeepConversion that only uses a limited amount of parallel training data, consistently outperforms the traditional approaches that use a large amount of parallel training data, in both objective and subjective evaluations. (C) 2020 The Authors. Published by Elsevier B.V.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0314,Multimodal perception of prominence in spontaneous speech: A methodological proposal using mixed models and AIC,"Research on prominence perception has made use of animated agents and controlled speech in experimental settings, but these methodologies have disregarded some aspects of the acoustic and visual correlates of prom-inence. To overcome these limitations we propose a new methodological approach using spontaneous speech data. For this, we created a small database with extracts from a television talent show and neutralised the prominence-lending properties of the acoustic cues of prominence in the speech signal. In our pilot study twelve naive listeners marked words for binary prominence (prominent vs. non-prominent) in two modalities, i.e. audio only and audiovisual, under three conditions involving neutralisation of (a) fundamental frequency, (b) intensity, and (c) both fundamental frequency and intensity. Additionally, the marks of two trained listeners served as control condition. Different generalised linear mixed models were estimated and compared using the Akaike Information Criterion (AIC). The most parsimonious model was then examined using traditional null-hypothesis testing in order to provisionally establish the effects of our independent variables on prominence marking. We argue that spontaneous speech can be successfully applied to the study of the multimodal perception of prominence.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0315,Seeing lexical tone: Head and face motion in production and perception of Cantonese lexical tones,"Previous studies show that lexical tones can be discriminated visually, but the locus of this information is unknown. Here we investigate the role of visual face and head information in the production and perception of the six Cantonese lexical tones. Experiment 1 (speech production) showed that tone category membership is predicted independently by each of three visual components, Head-only, Face-only, and combined Face&Head motion, that Face&Head combined is a better predictor than the sum of Head-only and Face-only, that Head-only was a better predictor than Face-only, and that up-down head motion may be involved. In Experiment 2 (speech perception) discrimination of Cantonese tones and phones (consonants and vowels) was investigated in AuditoryOnly (AO), Visual-Only (VO), and Auditory-Visual (AV) presentations of Face-only, Head-only or combined Face&Head information. There was successful visual discrimination of both phones and tones, but in different manners. For VO, face movement (Face-only, Face&Head) was sufficient for Phone discrimination, whereas both head and face (Face&Head) were necessary for Tone discrimination; and Phone discrimination was better for Face-only than Head-only, whereas Tone discrimination was the better for Head-only than Face-only. For visual augmentation of auditory information, (AV-AO)/AO), for Phones there was significant facilitation in Head-only and Face-only, but not Face&Head, whereas for Tones there was significant facilitation only in the Face&Head condition. Overall, head motion is integrally involved in the differential production of Cantonese tones and, as for consonants and vowels, there is visual perception of lexical tone, albeit based on entirely different cues: face motion is sufficient for phone perception (head motion is not necessary), whereas head motion is necessary but not sufficient for tone perception (Face&Head required). Head motion is integral both to the differential production of Cantonese tones and to the visual discrimination of those tones.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0316,Modulation spectral features for speech emotion recognition using deep neural networks,"This work explores the use of constant-Q transform based modulation spectral features (CQT-MSF) for speech emotion recognition (SER). The human perception and analysis of sound comprise of two important cognitive parts: early auditory analysis and cortex-based processing. The early auditory analysis considers spectrogram-based representation whereas cortex-based analysis includes extraction of temporal modulations from the spectrogram. This temporal modulation representation of spectrogram is called modulation spectral feature (MSF). As the constant-Q transform (CQT) provides higher resolution at emotion salient low -frequency regions of speech, we find that CQT-based spectrogram, together with its temporal modulations, provides a representation enriched with emotion-specific information. We argue that CQT-MSF when used with a 2-dimensional convolutional network can provide a time-shift invariant and deformation insensitive representation for SER. Our results show that CQT-MSF outperforms standard mel-scale based spectrogram and its modulation features on two popular SER databases, Berlin EmoDB and RAVDESS. We also show that our proposed feature outperforms the shift and deformation invariant scattering transform coefficients, hence, showing the importance of joint hand-crafted and self-learned feature extraction instead of reliance on complete hand-crafted features. Finally, we perform Grad-CAM analysis to visually inspect the contribution of constant-Q modulation features over SER.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0317,The role of prosody and hand gestures in the perception of boundaries in speech,"This paper investigates the use of prosodic, gestural, and syntactic information in the perception of boundaries in extracts of spontaneous speech in British English. Experiment 1 aimed at investigating the effect of prosody on naive participants' perception of boundary strength. 13 naive listeners had to rate boundary strength for 64 extracts on a 5-point scale. The stimuli all contained three tone-units, the second being a syntactic subordinate construction, which was established as a variable. The prosodic cues at the boundary between the tone-units were also established as variables, and were subject to manipulation (addition of a single cue associated with the perception of a prosodic boundary). Experiment 2 aimed at assessing the effect of gesture on naive participants' perception of boundary strength. In Experiment 2, 24 naive listeners had to measure boundary strength for 24 extracts on a 5-point scale. The stimuli all contained three tone-units, the second being a syntactic subordinate construction, which was established as a variable. The hand gestures produced in co-occurrence with the tone-units were established as variables, and were subject to manipulation. Results show that prosody modulates perceived boundary strength, but not gesture, based on the variables we included. Silent pauses have the strongest effect on perceived boundary strength, but final syllabic lengthening and pitch reset also have separate effects as single predictors. Our data also shows a trend concerning the production of two identical hand gestures in terms of configuration and trajectory.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0318,The combined effects of bilingualism and musicianship on listeners' perception of non-native lexical tones,"Non-native lexical tone perception can be affected by listeners' musical or linguistic experience, but it remains unclear of whether there will be combined effects and how these impacts will be modulated by different types of non-native tones. This study adopted an orthogonal design with four participant groups, namely, Mandarin-L1 monolinguals and Mandarin-L1 and Cantonese-L2 bilinguals with or without musical training, to investigate effects of bilingualism and musicianship on perception of non-native lexical tones. The closely matched four groups, each encompassing an equal number of 20 participants, attended a modified ABX discrimination task of lexical tones of Teochew, which was unknown to all participants and consists of multiple tone types of level tones, contour tones, and checked tones. The tone perceptual sensitivity index of d' values and response times were calculated and compared using linear mixed-effects models. Results on tone sensitivity and response time revealed that all groups were more sensitive to contour tones than level tones, indicating the effect of native phonology of Mandarin tones on non-native tone perception. Besides, as compared to monolinguals, bilinguals obtained a higher d' value when discriminating non-native tones, and musically trained bilinguals responded faster than their non-musician peers. It indicates that bilinguals enjoy a perceptual advantage in non-native tone perception, with musicianship further enhancing this advantage. This extends prior studies by showing that an L2 with a more intricate tone inventory than L1 could facilitate listeners' non-native tone perception. The pedagogical implications were discussed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0319,Sequential perception of tone and focus in parallel-A computational simulation,"Speech is produced continuously over time. So, the information it conveys, including intonational functions, also unfolds over time. But many intonational functions are encoded across whole utterances rather than only within certain words. How can perception process speech signals continuously over time, even for communicative functions that are globally encoded? In this study we used computational simulation to test the idea that even for intonational functions with large temporal scopes, it is possible to process f0 contours syllable-by-syllable, and recognize the functions by continuous estimation of progressive probabilistic inference. We trained SVM and GRU models to simulate the perception of Mandarin tone and sentence focus with either syllable-sized or sentence-sized f0 contours as input. The sentence-wide f0 contours are gated at different syllable locations to test the incrementality of the recognition of tone and intonation. We also tested human listeners' perception of tone and focus with full and fragmented f0 contours from the same dataset to evaluate the validity of the simulated perception. The results showed that the simulated syllable-by-syllable processing of tone and focus generated the closest recognition patterns to human perception. The simulations also show that there is little difference whether tone and focus are recognized separately or as tone-focus combinations, which suggests that despite sharing the same acoustic dimension, the two functions are sufficiently separated from each other in their f0 coding.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0320,"Context, not grammar, is key to structural priming","Structural priming - a change in processing after repeated exposure to a syntac-tic structure - has been put forward as evidence for the psychological reality of constituent structures derived from grammar. However, converging evidence from memory research, large language models (LLMs), and structural priming it-self challenges the validity of mapping structural representations onto grammat-ical constituents and demonstrates structural priming in the absence of such structure. Instead of autonomous representations specified by grammar, we pro-pose that contextual representations emerging from multiple constraints (e.g., words, prosody, gesture) underlie structural priming. This perspective ac-counts for existing anomalous findings, is supported by the strong dependence on lexical cues observed in structural priming, and suggests that future research should prioritize studying linguistic representations in more naturalistic contexts.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0321,Assessment of music experience after cochlear implantation: A review of current tools and their utilization,"Objective: To provide an overview of the current available music assessment tools after cochlear implantation (CI); to report on the utilization of music assessments in the literature; to propose potential future directions in music assessment after CI. Methods: A thorough search was performed in PubMed, Embase, and The Cochrane Library through October 31, 2020. MeSH search terms, keywords, and phrases included ""cochlear implant,"" ""cochlear prosthesis,"" ""auditory prosthesis,"" ""music,"" ""music assessment,"" ""music questionnaire,"" ""music perception,"" ""music enjoyment, and ""music experience."" Potentially relevant studies were reviewed for inclusion, with particular focus on assessments developed specifically for the cochlear implant population and intended for widespread use. Results/conclusions: Six hundred and forty-three studies were screened for relevance to assessment of music experience among cochlear implantees. Eighty-one studies ultimately met criteria for inclusion. There are multiple validated tools for assessment of music experience after cochlear implantation, each of which provide slightly differing insights into the patients' subjective and/or objective post-activation experience. However, no single assessment tool has been adopted into widespread use and thus, much of the literature pertaining to this topic evaluates outcomes non-uniformly, including single-use assessments designed specifically for the study at hand. The lack of a widely accepted universal tool for assessment of music limits our collective understanding the contributory and mitigating factors applicable to current music experience of cochlear implantees, and limits our ability to uniformly evaluate the success of new implant technologies or music training paradigms. Copyright (c) 2021 Chinese Medical Association. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0322,Segmental and prosodic effects on coda glottalization,"This paper examines effects of segmental context and prosodic phrasing on the occurrence of coda glottalization in American English. On the basis of acoustic data from six female speakers, it is argued that the main effect of following segmental context on the occurrence of coda glottalization is due to anticipatory coarticulation, with anticipation of following sonorant consonant articulations favoring the aerodynamic conditions for glottalized voicing. The cross-dialect propensity to have high coda glottalization rates before sonorant consonants can then be understood to arise as a phonetically natural consequence of normal coarticulation processes. This coarticulatory account is supported by the fact that when an intonation phrase (IP) boundary intervenes between the coda consonant and a following sonorant, the contextual effect on glottalization rates is weakened. Our data also suggest that, all else being equal, presence of an IP boundary favors the occurrence of glottalization. These patterns are compatible with an account in which a glottal constriction goal on the coda consonant is optional in the linguistic representation. The optionality of this goal is somewhat problematic for many models of phonetic knowledge, but the coarticulatory explanation of the sonorancy effect helps to insightfully simplify any account of the distribution of glottalization. (c) 2005 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0323,Identification of acoustically modified Mandarin tones by native listeners,"This study investigated Mandarin tone identification by 40 native listeners when only partial acoustic information was available. Twelve minimal tone pairs including all six Mandarin tonal contrasts were digitally processed to generate four syllable modifications: intact, silent-center, center-only, and onset-only. The syllables were recorded in two carrier phrases such that the offset of the carrier tone and the onset of the test tone were either a match or mismatch in fundamental frequency (f(0)). In three experiments, the test syllables were presented in the original carrier phrases, excised from the carrier phrases, or excised and cross-spliced with another carrier phrase. Response accuracy and reaction time (RT) were measured. Listeners identified the tones at better than 86% correct with or without the carriers except when they heard the onset-only syllables in isolation, when their identification accuracy fell to 73% but still beyond chance. The modifications impacted the four tones differently, with Tones 1 and 2 being compromised more than Tones 3 and 4. Confusion matrix analyses showed that Tone 2 was predominantly confused with Tone 3. and Tone 1 was primarily confused with Tone 4. There were no main effects of splicing or match/mismatch with the carrier tone; however, in the cross-spliced context, syllables originally produced with a matching carrier tone were identified faster and/or more accurately. The implications of these findings for lexical tone processing are discussed. Published by Elsevier Ltd.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0324,Speaker variability and context in the identification of fragmented Mandarin tones by native and non-native listeners,"This study investigated the individual and joint contribution of speaker variability, context, and type of acoustic input to the identification of Mandarin tones. Mandarin syllables produced by single vs. multiple speakers were digitally processed to generate intact. silent-center, center-only, and onset-only syllables to be presented in isolation or with a precursor carrier phrase. Forty native listeners and 55 non-native listeners were put under time pressure to identify the tones of the syllables. The results showed higher identification accuracy for single-speaker tones and tones presented in context. Tone identification accuracy also decreased as acoustic input was reduced. The speaker variability effect showed comparable magnitude for the native and non-native listeners. In contrast, non-native tone identification, compared to the native performance, was facilitated less by context and compromised more when acoustic input was minimal. Tone confusion analyses showed a Tone 2-Tone 3 confusion and a bias towards Tone 4 responses for both groups of listeners, but the patterns of confusion and bias are more variable in the non-native responses. The non-native listeners also showed inconsistent evidence of using F0 height for tone identification when acoustic input was minimal. Published by Elsevier Ltd.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0325,How does phonology guide phonetics in segment-f0 interaction?,"The phonetics and phonology of consonant-f0 interaction in Shanghai Chinese were examined to further refine our understanding of consonant-f0 interaction in general. Bi-syllabic nouns, which form tone sandhi domains, were elicited within template sentences. These nouns vary in (1) lexical tone of the sandhi domain-initial syllable; (2) laryngeal contrast in the stop onset of the second syllable; and (3) discourse context (i.e., with focus vs. without focus). Results suggest that the interaction of stop onset and f0 in the sandhi domain non-initial syllables in Shanghai Chinese cannot be just attributed to the phonetic implementation of the phonological feature contrast in the language (as suggested by the tone-consonant co-occurrence pattern in tone sandhi domain-initial position), nor can it be due to purely automatic results of consonant production. Rather, the observed f0 perturbation pattern suggests the interaction of both effects. Speakers do voluntarily control their articulatory settings, at least to some extent, so as to enhance the stop contrast. Such voluntary control of articulation is dependent on both tonal and discourse context. Furthermore, complex laryngeal settings, which may not condition categorical co-occurrence patterns between tone and consonant, can nevertheless determine in part patterns of consonant-f0 interaction in the language. Our results also have implications for consonant feature specifications and their phonetic implication in general. (C) 2011 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0326,Question or tone 2? How language experience and linguistic function guide pitch processing,"How does language experience shape pitch processing? Do speakers of tone languages, which use pitch to signal lexical contrasts (e.g., Mandarin Chinese) attend to pitch movements more closely than speakers of intonation languages (e.g., Dutch)? Contradictory findings have been reported in the literature. In the current study, we hypothesize that listeners should be particularly attentive to any pitch information that signals meaningful information in the native language. This includes pitch movements signaling lexical contrasts (present in tone languages only) as well as postlexical contrasts (present in all languages). Both Mandarin and Dutch listeners performed speeded ABX match to sample tasks on the same sets of nonsense words. As predicted, the same pitch movements were attended to differentially by the two language populations depending on the role that information played in the native language. Mandarin listeners were more attentive than Dutch listeners to pitch movements as these signaled potential lexical contrasts in Mandarin (but not Dutch). Importantly, Dutch listeners were more attentive to pitch movements signaling postlexical information than to pitch movements signaling no meaningful linguistic information. These findings underscore the importance of postlexical information in online speech processing and explain apparent contradictions in the literature. (C) 2011 Elsevier Ltd. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0327,Acoustic characteristics of three-year-olds' correct and incorrect monosyllabic Mandarin lexical tone productions,"This study aimed to provide insights into children's development of lexical tone production by combining both perceptual and acoustic analyses. Duration and fundamental frequency analyses were performed on the monosyllabic Mandarin lexical tones produced by the 13 three-year-old children and four female adults reported in Wong, Schwartz, and Jenkins (2005). Seven acoustic parameters that strongly associated with the tonal judgments of 10 Mandarin-speaking judges were identified. Qualitative differences of the seven parameters in adult correct, child correct, and child incorrect tone productions were compared and interpreted with reference to the perception data.. The results confirmed that three-year-old children do not produce adult-like tones in isolated monosyllabic words. Even children's tones that are correctly categorized by adult listeners are phonetically different than adults' tones. The four tones from the most to the least adult-like are Tone 4 (Falling), Tone 1 (High Level), Tone 2 (Rising), and Tone 3 (Falling-Rising), perhaps corresponding to the complexity of speech motor control for producing these tones. Children demonstrate more difficulties producing low fundamental frequencies than high fundamental frequencies. The findings support the position that tone acquisition is a protracted process, which may be affected by production complexities. (C) 2011 Elsevier' Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0328,Cross-linguistic perception of Itunyoso Trique tone,"Recent findings have argued in favor of the categorical perception of tonal contrasts in Taiwanese Mandarin and Standard Mandarin (Halle, Chang, & Best, 2004; Xu, Gandour, & Francis, 2006), and most recently in Mandarin and Cantonese (Peng et al., 2010). Findings in favor of the categorical perception of tone emerge most clearly from cross-linguistic Work on speech perception. The current study continues this line of research by investigating the categorical perception of Itunyoso Trique tone among Trique and French listeners. Tonal stimuli were presented to listeners in an AXB discrimination task (2AFC) and an AXB identification task (2AFC), closely following methods used in Halle et al. (2004). Evidence for a listener sensitivity to tonal categories was found for Trique listeners in their discrimination performance, but this pattern did not correspond to the identification performance. Overall, French speakers performed better overall at tone discrimination than Trique listeners, who largely ignored within-category phonetic differences. Both Trique and French listeners were found to be sensitive to psychoacoustic differences between stimuli, though French speakers relied more heavily on such differences. The findings here argue for the importance of both phonetic and auditory memory for the perception of Trique lexical tone. (c) 2012 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0329,Prosodic patterning in distal speech context: Effects of list intonation and f0 downtrend on perception of proximal prosodic structure,"Prosodic structure is often perceived as exhibiting regularities in the patterning of tone sequences or stressed syllables. Recently, prosodic regularities in the distal (non-local) context have been shown to influence the perceived prosodic constituency of syllables. Three experiments tested the nature of distal prosodic patterns influencing perceptions of prosodic structure, using eight-syllable items ending in ambiguous lexical structures (e.g., tie murder bee, timer derby). For distinct combinations of distal fundamental frequency (f0) and/or timing cues, two patterns were resynthesized on the initial five syllables of experimental items; these were predicted to favor prosodic grouping of final syllables such that listeners would hear a final disyllabic or monosyllabic word, respectively. Results showed distal prosodic patterning affected perceived prosodic constituency when (1) patterns consisted of regularity in timing cues, f0 cues, or both (Experiments 1-2); (2) items ended with either a low high (Experiment 1) or a high low (Experiment 2) tonal pattern; and (3) tonal patterns consisted of altemating low and high-pitched syllables with progressive f0 decrease, i.e., a 'downtrend' (Experiment 3). The results reveal that a variety of prosodic patterns in the distal context can influence perceived prosodic constituency and thus lexical processing, and provide a perceptually-motivated explanation for the organization of acoustic speech input into prosodic constituents. (C) 2014 Elsevier Ltd. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0330,The effects of language learning and vocal training on sensorimotor control of lexical tone,"To perceive and produce Mandarin, adult second language (L2) learners need to learn to discriminate lexical pitch variations and develop the new sensorimotor skills needed to produce the lexical tones. In this paper, we investigated whether auditory discrimination and sensorimotor integration differ with Mandarin (tonal) language experience in the context of tonal language syllables and simple sustained vowels. We tested four distinct groups: native Mandarin speakers, Mandarin L2 adults, trained vocalists, and naive adults (those with no tonal language exposure). Auditory discrimination was measured using two perceptual tasks, musical tone discrimination and Mandarin tone discrimination, the results of which were compared across the four groups. Group differences in sensorimotor integration related to lexical tone production were examined with a pitch-shift paradigm that assessed rapid motor responses to unexpected pitch perturbations. Mandarin speakers performed significantly better on Mandarin tone discrimination compared to the other three groups. Mandarin speakers also showed more attenuation of pitch-shift response amplitude (better vocal pitch control) during production of both the sustained vowel and Mandarin tones, especially compared to naive speakers. These findings suggest that Mandarin speakers have more robust pitch control over self-produced vocalizations and are thus less affected by auditory feedback perturbations. This effect was particularly evident in response to the Mandarin high level lexical tone, for which the pitch-shift compensation patterns (with apparent attenuation in Mandarin speakers only) differed qualitatively from those of sustained vowels (with clear compensation in all groups), the rising tone (with apparent attenuation in all groups but naive speakers), and the falling tone (with apparent attenuation in all groups). Trained vocalists also appear to rely more than naive speakers on internal models when regulating voice FO in the nonlinguistic domain (sustained vowel), but not in the linguistic domain (Mandarin tone). Native Mandarin speakers demonstrate robust internal models for lexical tone in both perception and production; this underscores the importance of developmental language experience but also provides evidence for the declination of the high level lexical tone which requires a mastery of tonal languages. (C) 2014 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0331,Relative contributions of vowels and consonants in recognizing isolated Mandarin words,"This study investigated the relative contributions of vowels and consonants in recognizing isolated Mandarin words. Normal-hearing native-Mandarin listeners were instructed to recognize isolated Mandarin words and identify consonants, vowels and tones with stimuli synthesized to contain different proportions of consonant or vowel segments, including five conditions of consonant-only, vowel-only, consonant or vowel plus consonant-vowel (C-V) transition, and C-V transition. The recognition score of the vowel-only Mandarin words was significantly higher than that of the consonant-only words; and word recognition scores had a higher correlation with vowel identification scores than consonant identification scores. Moreover, adding a small portion of C-V transition significantly improved the recognition score of the consonant-only Mandarin words. In the conditions of C-V transition and consonant plus C-V transition, the duration of preserved portion predicted modestly well the scores of isolated word recognition, and vowel, consonant and lexical tone identification in Mandarin. These findings suggest that there is a greater contribution of vowels than consonants to isolated word recognition in Mandarin, which is different from previous outcomes in English. (C) 2015 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0332,Rise and fall: Effects of tone and intonation on spoken word recognition in early childhood,"A crucial component of word learning is the ability to recognize words in spite of the varying forms they assume. This may be particularly challenging in tone languages as learners have to develop tone representations in the face of intonational variation in order to accurately recognize words. The effects of intonational variation on word recognition of tone-marked words in Mandarin Chinese were investigated in toddlers and preschoolers using a cross-sectional design. Participants were presented with known words where intonation (question/statement) and tone (rising/falling) were independently manipulated. Results demonstrated that word recognition in toddlers was heavily influenced by changes in the pitch contour of a tone due to intonational variation. In contrast, preschool were able to recognize tone-marked words regardless of simultaneous intonational variation, demonstrating a comparatively robust representation of lexical tone. Results chart an evolution in integrating pitch cues to tone and intonation over the first few years of life. (C) 2016 Elsevier Ltd. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0333,Impoverished acoustic input triggers probability-based tone processing in mono-dialectal Mandarin listeners,"Previous research on Mandarin spoken word recognition suggests that when processing lexical tone native listeners tune to various acoustic properties of the incoming signal such as f0 height, contour and change. These studies overlook the uneven distribution of tone across the Mandarin lexicon; given a particular string of segments, native listeners may be more likely to anticipate a certain tone due to prior experience with the language. The present study used the gating paradigm to investigate how much of the acoustic signal is needed for listeners to trigger such probability-based tone processing. Duration-blocked gates were generated from twelve low frequency and twelve high frequency mono-syllabic morphemes, with each syllable carrying either the most or least probable tone based on spoken corpora. Results from 22 mono-dialectal Mandarin speakers indicate that listeners immediately begin making use of tonal probability information after hearing only the onset and 40 ms of the vowel, primarily when hearing infrequent syllables. These findings demonstrate that the processing of suprasegmental information can be initiated with previously learned distributional knowledge until sufficient acoustic cues inform the listener of an incoming word. (C) 2016 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0334,The phonetics of information structure in Yoloxochitl Mixtec,"Research on speech prosody has shown that higher-level phonological constituents can be examined directly via their influence on low level phonetic processes (Beckman & Edwards, 1990; Fougeron & Keating, 1997). Despite the strong tradition of research in this area, the existing work has focused mainly on languages which lack lexical tone. This contributes to the view that prosodic structures show little influence on tone, i.e. a language may either have lexical tone or lexical/phrasal stress, the latter of which fits into the prosodic hierarchy. The current paper examines prosodic focus in Yoloxochitl Mixtec, an endangered Otomanguean language spoken in Mexico. Using experimental data from ten speakers in the field, we investigated how sentence position, stress, and focus type influenced the realization of F-o and duration in different tonal melodies. The findings show that the tonal F-o space was expanded and raised on words produced with contrastive focus, less on words produced with narrow focus, and least on words produced under broad, sentential focus. Focus-related lengthening asymmetrically affected stressed syllables in the language more than unstressed syllables. In stressed syllables, this resulted in an increase in tonal hyperarticulation. (C) 2018 Elsevier Ltd. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0335,Development of tonal discrimination in young heritage speakers of Cantonese,"This study uses the Perceptual Assimilation Model for Suprasegmentals (PAM-S) (So & Best, 2008, 2010), supported by the assumptions of the L2 Intonation Learning theory (LILt, Mennen, 2015), to investigate how young heritage speakers of Cantonese living in the United States acquired Cantonese tones. Sixty-seven heritage speakers, aged 5-11, were tested on their perception of Cantonese tonal contrasts using an ABX discrimination task. They were compared to 64 peers aged 5-12 in Hong Kong, where Cantonese is spoken as the majority language but English is also acquired from a young age. Two pairs of tones were tested: Tones 2 (mid rising) and 5 (low rising), which have similar pitch heights and contours, and Tones 1 (high level) and 4 (low falling), which have a larger phonetic contrast. As predicted, the heritage speakers were more accurate in discriminating between the more distinct pair of tones than between the more similar pair. They also scored lower than their peers from Hong Kong in both contrast conditions. Age of testing predicted accuracy for both groups, and Chinese literacy also had a significant effect for the heritage speakers. The potential lack of the Tone 2-5 contrast in the heritage speakers' input is discussed as an explanation for these findings. This study illustrates the divergence in heritage speakers' phonological development compared to majority language speakers, and shows the relevance of the PAM-S and LILt to the heritage language context. (C) 2018 Elsevier Ltd. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0336,Alignment of f0 peak in different pitch accent types affects perception of metrical stress,"In intonation languages, pitch accents are associated with stressed syllables, therefore accentuation is a sufficient cue to the position of metrical stress in perception. This paper investigates how stress perception in German is affected by different pitch accent types (with different f0 alignments). Experiment 1 showed more errors in stress identification when f0 peaks and stressed syllables were not aligned - despite phonological association of pitch accent and stressed syllable. Erroneous responses revealed a response bias towards the syllable with the f0 peak. In a visual-world eye-tracking study (Experiment 2), listeners fixated a stress competitor with initial stress more when the spoken target, which had penultimate stress, was realized with an early-peak accent (f0 peak preceding stressed syllable), compared to a condition with the f0 peak on the stressed syllable. Hence, high-pitched unstressed syllables are temporarily interpreted as stressed - a process directly affecting lexical activation. To investigate whether this stress competitor activation is guided by the frequent co-occurrence of high f0 and lexical stress, Experiment 3 increased the frequency of low-pitched stressed syllables in the immediate input. The effect of intonation on competitor fixations disappeared. Our findings are discussed with respect to a frequency-based mechanism and their implications for the nature of f0 processing. (C) 2019 The Authors. Published by Elsevier Ltd.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0337,Phonetic effects of grammatical category: How category-specific prosodic phrasing and lexical frequency impact the duration of nouns and verbs,"This paper is concerned with phonetic correlates of grammatical category, specifically the finding that nouns are pronounced with greater duration than verbs in discourse. Most previous research has attributed this difference to the sentence positions that the two grammatical categories occupy and concomitant prosodic effects. Based on previous findings, we test two further effects, namely a category-specific effect on prosodic phrasing, which leads to stronger prosodic boundaries after nouns than verbs even in maximally similar syntactic contexts, and a reductive effect of lexical frequency leading to shorter durations of the more frequent word. These effects are tested in a production study investigating durational differences of twelve noun verb homophone pairs in English in two clause-medial contexts. We find evidence for both effects: prosodic boundaries are stronger after nouns than verbs across all conditions, resulting in greater durations of nouns due to pre-boundary lengthening. Furthermore, differences in frequency result in a reduced duration of the homophone of the pair which has the greater frequency. We propose an explanation in which phonetic effects of grammatical category are caused by the interplay of sentence prosody, category-specific prosodic phrasing and lexical frequency. (C) 2019 Elsevier Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0338,"On (and off) ramps in intonational phonology: Rises, falls, and the Tonal Center of Gravity","Two conflicting views have been advanced of what defines 'default' high pitch accents in various West Germanic languages, including English: One equates these accents fundamentally with a rise to a high turning point, while the other focuses on the fall from it. Both views arise from the assumption within Autosegmental-Metrical theory that the phonological representations of intonational categories can be discerned more-or-less directly from the string of intentional-seeming changes of direction in the F0 curve, identified as production 'targets'. Two perceptual experiments reveal that, at least in American English, this view critically oversimplifies how pitch accents containing High tones are defined and distinguished: instead, both the shape of the rise and the shape of the fall are seen to contribute to the alignment of the overall bulk of the high region, defined by the rise-fall shape, with the segmental string, and thus to its categorization by listeners as an early, mid or late rise-fall (H + !H*, L + H*, or L* + H). These findings are consistent with the view that the Tonal Center of Gravity (TCoG) of the rise-fall shape as a whole, rather than an F0 turning point per se, is what speakers align with segmental content to distinguish different pitch accent categories. Questioning the primacy of the turning points as the phonetic targets for these pitch accents, in turn, seriously problematizes standard assumptions about the nature of phonological representations of intonation and their relation to the signal. CO 2021 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http:// creativecommons.org/licenses/by/4.0/).",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0339,The effect of linguistic experience on perceived vowel duration: Evidence from Taiwan Mandarin speakers,"Perceived vowel duration is known to be influenced by many factors, including f0 height/movement and ones' native phonological system. Using multiple experimental paradigms, this study examined whether native tonal representations and phonetic knowledge of duration associated with different lexical tones may further shape the ways in which vowel duration is perceived. In a perception experiment, Taiwan Mandarin and Korean listeners rated the duration of duration-controlled CV syllables carrying one of the four lexical tones in Mandarin or a reduced T3half (X-21). The results showed that perceived vowel duration by Korean listeners, the control group, reflected general perceptual biases: contour tones were rated as longer than level tones, and high-f0 tones were rated as longer than low-f0 tones. Taiwan Mandarin listeners, on the other hand, overestimated the duration of vowels carrying T3 (X-214) and T3half, despite their short phonetic duration in Taiwan Mandarin, indicating the significance of the canonical representation of the complex T3 contour. A spontaneous imitation experiment further supported the canonicity effect: T3half was again hyperarticulated, produced as longer and with similar f0 trajectories as T3full, based on its phonological association to T3. Taken together, the findings of the present study suggest that the perception of vowel duration is guided by higher-order phonological knowledge from speakers' linguistic experience as well as by general perceptual biases. (C) 2021 The Author(s). Published by Elsevier Ltd.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0340,The impact of phonotactic features on novel tone discrimination,"Many studies have examined novel tone perception, but few have investigated the interaction between novel tone perception ability and phonotactic structure. We examined the discrimination of Thai low and mid tones by native Mandarin and native English participants across four syllable types (CCVV, CVV, VV, hums) to test the interaction between first language suprasegmental experience and phonotactic complexity on novel tone discrimination. We also tested the impact of unfamiliar consonant clusters and unfamiliar segments in the onset. Across syllable types, native Mandarin participants discriminated tones better than native English speakers. Further, discrimination ability was not impacted by phonotactic complexity. However, unfamiliar syllable structure impacted discrimination ability for native Mandarin participants in an unintuitive way. They discriminated tones significantly better in CCVV syllables. Unfamiliar segments in the onset, however, had a negative impact on tone discrimination. The presence of /eta/ onsets, which are illegal in English and allophonically permitted in Mandarin, significantly reduced tone discrimination accuracy. For native English participants, /eta/ onsets resulted in no discrimination between tones. These results suggest that the phonotactic structure of carrier words for tones interacts with L1 phonotactic experience in modulating novel tone perception ability. (c) 2022 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0341,The influence of preceding speech and nonspeech contexts on Mandarin tone identification,"This study examines the effect of preceding tones on tone perception within Mandarin disyllabic utterances and the underlying mechanism that causes such an effect. Listeners were presented with a series of tone targets varying perceptually from Mandarin Tone 3 to Tone 4 following Tone 1, Tone 2, or Tone 4. The results showed that the targets were more likely to be categorized as Tone 3 following the context tones with high offset f0 (Tone 1 and Tone 2) than following those with low offset f0 (Tone 4). The effect of preceding tones compensated for the acoustic consequence of coarticulation because the context tone with high offset f0 produced Tone 4-like Tone 3 variant in production, but tended to elicit Tone 3 identification of this variant in perception. Moreover, we also observed an effect of nonspeech contexts that preserved only the f0 contours of speech contexts. However, the effect of nonspeech contexts was significantly smaller than that of speech contexts, and the difference was not caused by focal attention. Our findings lend evidence to a general auditory mechanism and point to future work clarifying the factors that modulate the magnitude of perceptual context effect. (c) 2022 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0342,The prosodic marking of rhetorical questions in Standard Chinese,"The present study investigates the prosody of information-seeking (ISQs) and rhetorical questions (RQs) in Standard Chinese, in polar and wh-questions. Like in other languages, ISQs and RQs in Standard Chinese can have the same surface structure, allowing for a direct prosodic comparison between illocution types (ISQ vs RQ). Since Standard Chinese has lexical tone, the use of f0 as a cue to illocution type may be restricted. We inves-tigate the prosodic differences between ISQs and RQs as well as the interplay of prosodic cues to RQs. In terms of f0, results showed that RQs were lower in f0, with the f0 range on the first word being expanded followed by f0 compression. RQs were further longer in duration and more often realized with non-modal voice quality (glottalized voice) as compared to ISQs. These prosodic cues were largely manipulated in tandem (illocutionary pairs with lar-ger durational differences also showed larger differences in mean f0; voice quality, in turn, seemed to be an addi-tional cue). We suggest three possible explanations (assertive force, focus, speaker attitude) that unite the present findings on RQs in Standard Chinese with the findings on RQs in other, non-tonal languages.CO 2022 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0343,Phonological and phonetic contributions to perception of non-native lexical tones by tone language listeners: Effects of memory load and stimulus variability,"The present study examined native language phonological and phonetic factors in non-native lexical tone percep-tion by tone language listeners, manipulating memory load and stimulus variability to bias listeners towards a more phonological or more phonetic mode of perception. Mandarin and Vietnamese listeners categorised the five Thai lexical tones to their native tones, and discriminated five selected Thai tone contrasts that were predicted by the Perceptual Assimilation Model (PAM, Best, 1995) to be discriminated differently. Categorisation responses showed more phonologically-based patterns under high than low memory load but were unaffected by talker and vowel variability, whereas discrimination accuracy was reduced by talker and vowel variability but not by mem-ory load. Phonological factors indicated by type of categorisation and category overlap generally predicted the dis-crimination of non-native tone contrasts in line with PAM principles. Phonetic factors reflected in category overlap scores and fit index difference scores predicted variations in discriminating contrasts of the same contrast cate-gorisation type. These findings uphold the extension of PAM principles to non-native tone perception by native lis-teners of other tone languages. Native phonological and phonetic contributions to non-native speech perception differ between categorisation and discrimination tasks, as reflected in differential modulation by memory load and stimulus variability.CO 2022 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0344,The relation between musical abilities and speech prosody perception: A meta-analysis,"Previous research has suggested a relationship between musical abilities and the perception of speech prosody. However, effect sizes and significance differ across studies. In a meta-analysis, we assessed the overall size of this relation across 109 studies and investigated which factors moderated the effect. We found a significant, medium-sized positive correlation between musical abilities and speech prosody perception. This correlation was larger for studies on non-native compared to native prosody perception. We attribute this difference to ceiling performance in native perception, while non-native perception may be more difficult and can thus be facilitated by musical abilities. In addition, prosody perception was more strongly correlated with music perception than with music training, possibly because training metrics disregard untrained individuals with naturally strong musical abilities. Further analyses showed a stronger correlation for prosodic pitch compared to prosodic timing perception, and a stronger correlation for behavioural accuracy measures compared to reaction times. We did not find differences in effects between linguistic and emotional prosody, between L1 tone language users or non-tone language users, or between adults and children. This meta-analysis generally supports theories proposing a connection between music and speech prosody. Furthermore, this study highlights the potential importance of individuals' musical abilities for the acquisition of second language prosody.(c) 2023 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY license (http:// creativecommons.org/licenses/by/4.0/).",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0345,Contrast enhancement and the distribution of vowel duration in Japanese,"This paper offers a case study of the role of information in patterns of segmental duration, focusing on the short vs. long vowel contrast in Japanese. We confirmed that the position of vowels in linguistic units (intonation phrase/IP and word) and the length of the units affect vowel duration, but the patterns differ depending on the type of units. The results suggest that at the word level, vowel duration patterns in such a way that cues for vowel contrast are enhanced in salient (earlier) positions and units with less predictability or more lexical competitors (shorter words), while cues are relatively reduced in non-salient (later) positions and units with more predictability or less lexical competitors (longer words). In this way, the variation in vowel duration balances the successful transmission of lexical information and the cost for phonetic implementation, which supports previous findings in communication-based approaches of language.<br /> (c) 2024 The Author(s). Published by Elsevier Ltd.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0346,Imitation of F0 tone contours by Mandarin and English speakers is both categorical and continuous,"Native speakers imitate F0 contours that vary between two lexical tones non-linearly-they do not precisely reproduce the presented F0 features but instead cluster them toward tonal categories, the so-called contrast mediation effect. However, less is known whether non-native speakers who lack the lexical tone phonology will show linear imitation of F0 contours. Addressing this question will deepen our understanding of whether F0 imitation is solely influenced by lexical tone contrasts or also shaped by other sources of non-linearity beyond phonological contrasts. To investigate this, the current study examined the categorization and imitation of a Mandarin flat-falling tonal continuum by both Mandarin speakers and English speakers who were na & iuml;ve to tonal languages. Imitation distributions were analyzed by comparing two models: a linear regression model, which assumes participants linearly track phonetic cues, and a mixture regression model, which assumes imitation reflects underlying categories. The mixture regression model fit the data better for the Mandarin speakers while the reverse was true for the English speakers, suggesting that Mandarin speakers imitated the F0 contours more categorically than English speakers. However, for both groups, the data was best fit using a weighted combination of both models. For the Mandarin group this result along with additional analyses of duration, F1 and intensity suggest that tone categories involve both phonological and phonetic information and imitation taps both, possibly via hyper- and hypo-articulation. For English participants, the evidence for categorical mediation suggests that imitation is mediated by factors other than lexically contrastive linguistic categories, although the exact nature of the factors is unclear.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0347,Cross-linguistic similarity in L2 suprasegmental learning: evidence from Chinese learners' perception of Japanese pitch accents,"The acquisition of suprasegmental features in a second language (L2), like lexical tone and pitch accent, can be challenging, yet the impact of cross-linguistic similarity on learning these suprasegmental features has been underexplored. This study explored the role of cross-linguistic similarity in Chinese learners' perception of Japanese pitch accents, aiming to verify the Perceptual Assimilation Model for Suprasegmentals (PAM-S). In experiment 1, 25 Chinese learners of Japanese with lower proficiency level and 24 learners with higher proficiency level completed a perceptual assimilation task (PAT) that examined the cross-linguistic perceptual similarity between Mandarin tones and Japanese pitch accents. In experiment 2, the same Chinese groups and 35 native Japanese listeners completed a perceptual discrimination test (PDT) of Japanese pitch accent contrasts. Results of PAT showed that Chinese learners successfully categorized Japanese pitch accents into their native Mandarin tone categories: they perceived Japanese H*L as Mandarin Tone 4 (falling tone), LH* as Tone 2 (rising tone), and LH as Tone 1 (level tone). Moreover, results of PDT showed that Chinese learners were able to discriminate H*L-LH* and H*L-LH but had difficulty in the discrimination of LH*-LH. The results also show that Chinese learners' ability to discriminate Japanese pitch accent contrasts did not improve consistently with increased Japanese experience. This study argues that the LH*-LH contrast is hard for L2 learners regardless of their L2 experience, because of these two accents' acoustic similarity. The results extended the PAM-S, suggesting that L2 speech perception could be influenced by both the L1-L2 assimilation patterns and acoustic similarity. (c) 2025 Elsevier Ltd. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0348,Gelastic epilepsy and dysprosodia in a case of late-onset right frontal seizures,"Gelastic epilepsy (GE) is an uncommon type of seizure disorder characterized by stereotyped, unprovoked, inappropriate ictal laughter. GE is most frequently associated with hypothalamic hamartoma, with onset almost invariably occurring during childhood. GE also occurs occasionally with temporal and frontal cortical seizure foci. We describe an unusual case of senescent-onset GE with a right frontal seizure focus. In addition to laughter, dysprosodia was a clinical feature. Clinical and electroencephalographic evidence of seizure activity ceased on levetiracetam, and the patient showed concurrent improvement in cognitive function. We review the evidence for the cerebral representation of laughter and prosody, and discuss issues bearing on the differential diagnosis and management of GE. Published by Elsevier Inc.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0349,Acquiring the phonology of lexical tone in infancy,"Phonology is bimodular: prosody (relational aspects) and melody ('phonetic' aspects) are to some extent autonomous in steady-state (adult) language. Therefore the acquisition of individual melodic and prosodic modules and their subsequent orientation with respect to one another must constitute three different developmental tasks. The acquisition of melodic primes may take place independently of any other process. If infant perception has this kind of phonological import, then disparate phonetic reflexes which are predicted as phonologically identical might show parallels in acquisition. In Government Phonology, general theory argues that the same abstract melodic objects underlie both laryngeal contrasts in stops and lexical tonal contrasts. Earlier studies show that language-specific attunement to stop contrasts has taken place by the age of six to eight months. Tests on lexical tone perception are now reported, using both adults and infants. The results show that at around six months of age, babies acquiring Yoruba, a language which has a three-way contrast for tone, attend more closely to pitch changes within the minimal domain word than do English controls. Further, they only attend to those pitch changes that possess phonological import within that domain in the steady-state language. In this their perception exactly parallels that displayed by the adult speakers. Finally, both adults and infants display a particular asymmetry in perception in that they discriminate the high/mid tone distinction but fail to perceive the mid/low contrast where no linguistic context is present. This was not predicted at the onset of testing, but it is argued that it may well have a phonological explanation, which could align this finding with the typological asymmetries to be found in the distribution of lexical tone in Yoruba and other west African languages. (C) 2000 Elsevier Science B.V. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0350,"The neural response to emotional prosody, as revealed by functional magnetic resonance imaging","Prosody is an important feature of language, comprising intonation, loudness, and tempo. Emotional prosodic processing forms an integral part of our social interactions. The main aim of this study was to use bold contrast fMRI to clarify the normal functional neuroanatomy of emotional prosody, in passive and active contexts. Subjects performed six separate scanning studies, within which two different conditions were contrasted: (1) ""pure"" emotional prosody versus rest; (2) congruent emotional prosody versus 'neutral' sentences; (3) congruent emotional prosody versus rest; (4) incongruent emotional prosody versus rest; (5) congruent versus incongruent emotional prosody; and (6) an active experiment in which subjects were instructed to either attend to the emotion conveyed by semantic content or that conveyed by tone of voice. Data resulting from these contrasts were analysed using SPM99. Passive listening to emotional prosody consistently activated the lateral temporal lobe (superior and/or middle temporal gyri). This temporal lobe response was relatively right-lateralised with or without semantic information. Both the separate and direct comparisons of congruent and incongruent emotional prosody revealed that subjects used fewer brain regions to process incongruent emotional prosody than congruent. The neural response to attention to semantics, was left lateralised, and recruited an extensive network not activated by attention to emotional prosody. Attention to emotional prosody modulated the response to speech, and induced right-lateralised activity, including the middle temporal gyrus. In confirming the results of lesion and neuropsychological studies, the current study emphasises the importance of the right hemisphere in the processing of emotional prosody, specifically the lateral temporal lobes. (C) 2003 Elsevier Science Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0351,Temporal integration of speech prosody is shaped by language experience: An fMRI study,"Differences in hemispheric functions underlying speech perception may be related to the size of temporal integration windows over which prosodic features (e.g., pitch) span in the speech signal. Chinese tone and intonation, both signaled by variations in pitch contours, span over shorter (local) and longer (global) temporal domains, respectively. This cross-linguistic (Chinese and English) study uses functional magnetic resonance imaging to show that pitch contours associated with tones are processed in the left hemisphere by Chinese listeners only, whereas pitch contours associated with intonation are processed predominantly in the right hemisphere. These findings argue against the view that all aspects of speech prosody are lateralized to the right hemisphere, and promote the idea that varying-sized temporal integration windows reflect a neurobiological adaptation to meet the 'prosodic needs' of a particular language. (C) 2003 Elsevier Science (USA). All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0352,Sensitivity to prosodic structure in left-and right-hemisphere-damaged individuals,"An experiment was conducted in order to determine whether left- (LHD) and right-hemisphere-damaged (RHD) patients exhibit sensitivity to prosodic information that is used in syntactic disambiguation. Following the work of Marslen-Wilson, Tyler, Warren, Gremer, and Lee (1992), a cross-modal lexical decision task was performed by LHD and RHD subjects, as well as by adults without brain pathology (NC). Subjects listened to sentences with attachment ambiguities with either congruent or incongruent prosody, while performing a visual lexical decision task. Results showed that each of the unilaterally damaged populations differed from each other, as well as from the NCs in terms of sensitivity regarding prosodic cues. Specifically, the RHD group was insensitive to sentence prosody as a whole. This was in contrast to the LHD patients, who responded to the prosodic manipulation, but in the unexpected direction. Results are discussed in terms of current hypotheses regarding the hemispheric lateralization of prosodic cues. (C) 2003 Elsevier Science (USA). All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0353,Identification and discrimination of Mandarin Chinese tones by Mandarin Chinese vs. French listeners,"Previous work has not yielded clear conclusions about the categorical nature of perception of tone contrasts by native listeners of tone languages. We reopen this issue in a cross-linguistic study comparing Taiwan Mandarin and French listeners. We tested these listeners on three tone continua derived from natural Mandarin utterances within carrier sentences, created via a state-of-the-art pitch-scaling technique in which within-continuum interpolation was applied to both f(0) and intensity contours. Classic assessments of categorization and discrimination of each tone continuum were conducted with both groups of listeners. In Experiment 1. Taiwanese listeners identified the tone of target syllables within carrier sentence context and discriminated tones of single syllables. In Experiment 2, both French and Taiwanese listeners completed an AXB identification task on single syllables. Finally, French listeners were run on an AXB discrimination task in Experiment 3. Results indicated that Taiwanese listeners' perception of tones is quasi-categorical whereas French listeners' is psych ophysically based. French listeners nevertheless show substantial sensitivity to tone contour differences, though to a lesser extent than Taiwanese listeners. Thus, the findings suggest that despite the lack of lexical tone contrasts in the French language, French listeners are not absolutely ""deaf"" to tonal variations. They simply fail to perceive tones along the lines of a well-defined and finite set of linguistic categories. (C) 2003 Elsevier Ltd. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0354,SPEAKER NORMALIZATION IN PERCEPTION OF LEXICAL TONE,,,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0355,The format of representation of recognized words in infants' early receptive lexicon,"Eleven-month-olds can recognize a few auditorily presented familiar words in experimental situations where no hints are given by the intonation, the situation, or the presence of possible visual referents. That is, infants of this age (and possibly somewhat younger) can recognize words based on sound patterns alone. The issue addressed in this article is what is the type of mental representations infants use to code words they recognize. The results of a series of experiments with French-learning infants indicate that word representations in 11-month-olds are segmentally underspecified and suggest that they are all the more underspecified when infants engage in recognizing words rather than merely attending to meaningless speech sounds. But underspecification has limits, which were explored here with respect to word-initial consonants. The last two experiments show the way to investigating further these limits for word-initial consonants as well as for segments in other word positions. In French, infants' word representations are flexible enough to allow for structural changes in the voicing or even in the manner of articulation of word-initial consonants. Word-initial consonants must be present, however, for words to be recognized. In conclusion, a parallel is proposed between the emerging capacities to ignore variations that are irrelevant for word recognition in a ''lexical mode'' and to ignore variations that are phonemically irrelevant in a ''neutral mode'' of listening to native speech.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0356,The analysis of speech in different temporal integration windows: cerebral lateralization as 'asymmetric sampling in time',"The 'asymmetric sampling in time' (AST) hypothesis developed here provides a framework for understanding a range of psychophysical and neuropsychological data on speech perception in the context of a revised cortical functional anatomic model. The AST model is motivated by observations from psychophysics and cognitive neuroscience that speak to the fractionation of auditory processing, in general, and speech perception, in particular. Building on the observations (1) that the speech signal contains more than one time scale relevant to auditory cognition (e.g. time scales commensurate with processing formant transitions versus scales commensurate with syllabicity and intonation contours), and (2) that speech perception is mediated by both left and right auditory cortices, AST suggests a time-based perspective that maintains anatomic symmetry while permitting functional asymmetry. AST proposes that the input speech signal has a neural representation that is bilaterally symmetric at an early representational level. Beyond the initial representation, however, the signal is elaborated asymmetrically in the time domain: left auditory areas preferentially extract information from short (similar to20-40 ms) temporal integration windows. The right hemisphere homologues preferentially extract information from long (similar to150-250 ms) integration windows. It is suggested that temporal integration is reflected as oscillatory neuronal activity in different frequency bands (gamma, theta). (C) 2002 Elsevier Science B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0357,Flow of information in the spoken word recognition system,"Spoken word recognition consists of two major component processes. First, at the prelexical stage, an abstract description of the utterance is generated from the information in the speech signal. Second, at the lexical stage, this description is used to activate all the words stored in the mental lexicon which match the input. These multiple candidate words then compete with each other. We review evidence which suggests that positive (match) and negative (mismatch) information of both a segmental and a suprasegmental nature is used to constrain this activation and competition process. We then ask whether, in addition to the necessary influence of the prelexical stage on the lexical stage, there is also feedback from the lexicon to the prelexical level. In two phonetic categorization experiments, Dutch listeners were asked to label both syllable-initial and syllable-final ambiguous fricatives (e.g., sounds ranging from [f] to [s]) in the word-nonword series maf-mas, and the nonword-word series jaf-jas. They tended to label the sounds in a lexically consistent manner (i.e., consistent with the word endpoints of the series). These lexical effects became smaller in listeners' slower responses, even when the listeners were put under pressure to respond as fast as possible. Our results challenge models of spoken word recognition in which feedback modulates the prelexical analysis of the component sounds of a word whenever that word is heard. (C) 2002 Elsevier Science B.V. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0358,Quantitative measurement of prosodic strength in Mandarin,"We describe models of Mandarin prosody that allow us to make quantitative measurements of prosodic strengths. These models use Stem-ML, which is a phenomenological model of the muscle dynamics and planning process that controls the tension of the vocal folds, and therefore the pitch of speech. Because Stem-ML describes the interactions between nearby tones, we were able to capture surface tonal variations using a highly constrained model with only one template for each lexical tone category, and a single prosodic strength per word. The model accurately reproduces the intonation of the speaker, capturing 87% of the variance of f(0) with these strength parameters. The result reveals alternating metrical patterns in words, and shows that the speaker marks a hierarchy of boundaries by controlling the prosodic strength of words. The strengths we obtain are also correlated with syllable duration, mutual information and part-of-speech. (C) 2003 Elsevier B.V. All rights reserved.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0359,"Deficient auditory processing in children with Asperger Syndrome, as indexed by event-related potentials","Asperger Syndrome (AS) is characterized by normal language development but deficient understanding and use of the intonation and prosody of speech. While individuals with AS report difficulties in auditory perception, there are no studies addressing auditory processing at the sensory level. In this study, event-related potentials (ERP) were recorded for syllables and tones in children with AS and in their control counterparts. Children with AS displayed abnormalities in transient sound-feature encoding, as indexed by the obligatory ERPs, and in sound discrimination, as indexed by the mismatch negativity. These deficits were more severe for the tone stimuli than for the syllables. These results indicate that auditory sensory processing is deficient in children with AS, and that these deficits might be implicated in the perceptual problems encountered by children with AS. (C) 2002 Elsevier Science Ireland Ltd. All rights reserved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0360,Human frequency-following response: representation of pitch contours in Chinese tones,"Auditory nerve single-unit population studies have demonstrated that phase-locking plays a dominant role in the neural encoding of both the spectrum and voice pitch of speech sounds. Phase-locked neural activity underlying the scalp-recorded human frequency-following response (FFR) has also been shown to encode certain spectral features of steady-state and time-variant speech sounds as well as pitch of several complex sounds that produce time-invariant pitch percepts. By extension, it was hypothesized that the human FFR may preserve pitch-relevant information for speech sounds that elicit time-variant as well as steady-state pitch percepts. FFRs were elicited in response to the four lexical tones of Mandarin Chinese as well as to a complex auditory stimulus which was spectrally different but equivalent in fundamental frequency (f(0)) contour to one of the Chinese tones. Autocorrelation-based pitch extraction measures revealed that the FFR does indeed preserve pitch-relevant information for all stimuli. Phase-locked interpeak intervals closely followed f(0). Spectrally different stimuli that were equivalent in F-0 similarly showed robust interpeak intervals that followed f(0). These FFR findings support the viability of early, population-based 'predominant interval' representations of pitch in the auditory brainstem that are based on temporal patterns of phase-locked neural activity. (C) 2004 Elsevier B.V. All rights reserved.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0361,Converging evidence for the relevance of alternative sets: data from NPs with focus sensitive particles in German,"Recent psycholinguistic studies on the reality of alternative sets in processing focus NPs have shown that focus particles like 'only' play a special role in activating the mental representation of alternatives to focused nouns. In this paper we present a new corpus study which provides converging evidence to support psycholinguistic findings and suggests that alternatives preceded by a focus particle are not only more activated in experimental contexts, but are also more likely to be discussed in the subsequent context. To this end we develop and evaluate inter-annotator agreement on two novel annotation tasks in naturally occurring German corpus data: recognition of nominal alternatives in general without any context, and recognition of alternatives in the context of sentence pairs. We show that while annotators agree poorly on the first, they agree strongly on the second. We also develop a concept of 'alternative density', the number of alternatives realized in a sentence following a target NP, and present a mixed-eff ects model showing a very significant rise in density after the presence of German nur 'only' independently of other factors.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0362,Categorical perception of lexical tone contrasts and gradient perception of the statement-question intonation contrast in Zhumadian Mandarin,"We intended to establish if two lexical tone contrasts in Zhumadian Mandarin, one between early and late aligned falls and another between early and late aligned rises, are perceived categorically, while the difference between declarative and interrogative pronunciations of these four tones is perceived gradiently. Presenting stimuli from 7-point acoustic continua between tones and between intonations, we used an identification task and a discrimination task with an experimental group of native listeners and a control group of Indonesian listeners, whose language employs none of the differences within either the falling or the rising pitch contours in its phonology. Only the lexical condition as perceived by the experimental group yielded sigmoid identification functions and a heightened discriminatory sensitivity around the midpoint of continua. The intonational condition in the native group and both conditions in the control group yielded gradient identification functions and smaller, reverse effects of the continuum midpoints in the discrimination task. The results are interpreted to mean that sentence modality contrasts can be expressed gradiently, but that lexical tone differences are represented phonologically, and hence are perceived categorically, despite low phonetic salience of the contrast. This conclusion challenges assumptions about the relation between linguistic functions and linguistic structures.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0363,Musical perception skills predict speech imitation skills: differences between speakers of tone and intonation languages,"The ability to imitate speech is linked to individual cognitive abilities such as working memory and the auditory processing of music. However, little research has focused on the role of specific components of musical perception aptitude in relation to an individual's native language from a crosslinguistic perspective. This study explores the predictive role of four components of musical perception skills and working memory on phonetic language abilities for speakers of two typologically different languages, Catalan (an intonation language) and Chinese (a tone language). Sixty-one Catalan and 144 Chinese participants completed four subtests (accent, melody, pitch and rhythm) of the Profile of Music Perception Skills, a forward digit span task and a speech imitation task. The results showed that for both groups of participants, musical perception skills predicted speech imitation accuracy but working memory did not. Importantly, among the components of musical perception skills, accent was the only predictive factor for Chinese speakers, whereas melody was the only predictive factor for Catalan speakers. These findings suggest that speech imitation ability is predicted by musical perception skills rather than working memory and that the predictive role of specific musical components may depend on the phonological properties of the native language.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0364,Aging and distributional tone learning: the role of pitch memory in older adults' discrimination of mandarin lexical tones,"Distributional learning enables listeners to form phonetic categories by extracting statistical regularities from speech input. Younger Cantonese speakers can acquire the Mandarin level-falling (T1-T4) contrast through distributional learning, with bimodal exposure facilitating category formation and unimodal exposure suppressing it, and with fine-grained pitch sensitivity predicting success. However, aging is associated with declines in pitch sensitivity and phonetic boundary formation, which may disrupt this process. This study examined whether Cantonese-speaking older adults exhibit distributional learning of Mandarin T1-T4 and whether individual cognitive factors predict learning success. Sixty-four participants completed a pretest-training-posttest procedure with bimodal or unimodal exposure. While older adults improved in tone discrimination, no group differences emerged. Further analysis showed that those with lower pitch-related auditory memory failed to learn from unimodal input. On the other hand, fine-grained pitch perception abilities did not predict learning outcomes. These results suggest that older adults may rely on alternative learning mechanisms, such as memory-based strategies, when exposed to ambiguous input distributions. The findings indicate a shift from perceptual encoding to memory-driven processing in aging and highlight the limits of passive statistical learning in older adulthood.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0365,Focal accent and emphatic prominence in the description of French intonation,"Many questions about the expressive or distinctive character of focal accent and emphatic prominence remain unanswered in studies concerning natural language prosody. Recent works show that fundamental frequency variation underlies a categorial perception that permits a differenciation in the representation of these two types of prominence. Using data from French, the contribution of each type of prominence to the intonation contour is analyzed. This study reconciles the difference in expressive character and phonological description associated with each prominence type through the use of association rules linking tones and text in different prosodic domains. An instrumental study of 280 utterances documents the phonetic particularities in the production of both prominences, and in neutral utterances.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0366,The phonology of focus in Sign Language of the Netherlands,"Signed languages are similar to spoken languages in the overall organisation of their grammars, displaying a prosodic level of organisation that is not isomorphic to the syntactic organisation. Their rich inventory of manual and non-manual features allows for a prolific range of functions if used prosodically. New data from Sign Language of the Netherlands (NGT, Nederlandse Gebarentaal) are discussed to demonstrate that focused constituents are not marked by a single prosodic feature, but rather by multiple properties that can also have other functions in the prosodic phonology of the language. These findings are integrated in an overall model of sign language prosody that emphasises the distinction between phonetic appearance and phonological representation and that allows for the interaction of linguistic and paralinguistic cues in visual communication.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0367,"An inverse relation between expressiveness and grammatical integration: On the morphosyntactic typology of ideophones, with special reference to Japanese","Words and phrases may differ in the extent to which they are susceptible to prosodic foregrounding and expressive morphology: their expressiveness. They may also differ in the degree to which they are integrated in the morphosyntactic structure of the utterance: their grammatical integration. We describe an inverse relation that holds across widely varied languages, such that more expressiveness goes together with less grammatical integration, and vice versa. We review typological evidence for this inverse relation in ten spoken languages, then quantify and explain it using Japanese corpus data. We do this by tracking ideophones - vivid sensory words also known as mimetics or expressives - across different morphosyntactic contexts and measuring their expressiveness in terms of intonation, phonation and expressive morphology. We find that as expressiveness increases, grammatical integration decreases. Using gesture as a measure independent of the speech signal, we find that the most expressive ideophones are most likely to come together with iconic gestures. We argue that the ultimate cause is the encounter of two distinct and partly incommensurable modes of representation: the gradient, iconic, depictive system represented by ideophones and iconic gestures, and the discrete, arbitrary, descriptive system represented by ordinary words. The study shows how people combine modes of representation in speech and demonstrates the value of integrating description and depiction into the scientific vision of language.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0368,The phonology of tone and intonation in the Dutch dialect of Venlo,"The Dutch dialect of Venlo has a lexical tone opposition comparable to the distinction between Accent I and Accent II in Scandinavian. The two word tone patterns are realised in a variety of different ways, depending on the intonation contour, on whether the word has a focus tone, and on whether it occurs finally or nonfinally in the intonational phrase (IP). Twelve such contexts are identified, and an autosegmental-metrical analysis is presented of the contours for the word tones in each of these. The analysis is instructive because of its clear illustration of the distinction between the phonological underlying representation and the phonological surface representation, as well as of the distinction between the latter representation and the phonetic realisation. In addition, because of the complexity of its tonal phonology, the dialect is of considerable typological interest for the study of word prosody and intonation.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0369,THREE TABLES FOR BOB,"To distinguish differences of harmony, to compose music so that players and listeners may sense these variations naturally, in real time, the representation of harmonic space in musical notation must be precise yet simple to read. Shadings of intonation are most clearly conceived in relation to consonant, untempered intervals. Intervals derived from a common frame of reference may be represented by microtonal signs, which suggest potentially tuneable intervallic relationships.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0370,The role of linguistic experience in the hemispheric processing of lexical tone,"This study investigated hemispheric lateralization of Mandarin tone. Four groups of listeners were examined: native Mandarin listeners, English-Mandarin bilinguals, Norwegian listeners with experience with Norwegian tone, and American listeners with no tone experience. Tone pairs were dichotically presented and listeners identified which tone they heard in each ear. For the Mandarin listeners, 57% of the total errors occurred in the left ear, indicating a right-ear (left-hemisphere) advantage. The English-Mandarin bilinguals exhibited nativelike patterns, with 56% left-car errors. However, no ear advantage was found for the Norwegian or American listeners (48 and 47% left-ear errors, respectively). Results indicate left-hemisphere dominance of Mandarin tone by native and proficient bilingual listeners, whereas normative listeners show no evidence of lateralization, regardless of their familiarity with lexical tone.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0371,Is second language lexical access prosodically constrained? Processing of word stress by French Canadian second language learners of English,"The objectives of this study are (a) to determine if native speakers of Canadian French at different English proficiencies can use primary stress for recognizing English words and (b) to specify how the second language (L2) learners' (surface-level) knowledge of L2 stress placement influences their use of primary stress in L2 word recognition. Two experiments were conducted: a cross-modal word-identification task investigating (a) and a vocabulary production task investigating (b). The results show that several L2 learners can use primary stress for recognizing English words, but only the L2 learners with targetlike knowledge of stress placement can do so. The results also indicate that knowing where primary stress falls in English words is not sufficient for L2 learners to be able to use stress for L2 lexical access. This suggests that the problem that L2 word stress poses for many native speakers of (Canadian) French is at the level of lexical processing.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0372,"Orthographic and phonological parafoveal processing of consonants, vowels, and tones when reading Thai","Four eye movement experiments investigated whether readers use parafoveal input to gain information about the phonological or orthographic forms of consonants, vowels, and tones in word recognition when reading Thai silently. Target words were presented in sentences preceded by parafoveal previews in which consonant, vowel, or tone information was manipulated. Previews of homophonous consonants (Experiment I) and concordant vowels (Experiment 2) did not substantially facilitate processing of the target word, whereas the identical previews did. Hence, orthography appears to be playing the prominent role in early word recognition for consonants and vowels. Incorrect tone marker previews (Experiment 3) substantially retarded the subsequent processing of the target word, indicating that lexical tone plays an important role in early word recognition. Vowels in VOP (Experiment 4) did not facilitate processing, which points to vowel position being a significant factor. Primarily, orthographic codes of consonants and vowels (HOP) in conjunction with tone information are assembled from parafoveal input and used for early lexical access.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0373,Native and nonnative processing of Japanese pitch accent,"The theoretical framework of this study is based on the prevalent debate of whether prosodic processing is influenced by higher level linguistic-specific circuits or reflects lower level encoding of physical properties. Using the dichotic listening technique, the study investigates the hemispheric processing of Japanese pitch accent by native Japanese listeners and two groups of nonnative listeners with no prior pitch accent experience but differing in their native language experience with linguistic pitch: native listeners of Mandarin (a tone language with higher linguistic functional use of pitch) and native listeners of English (a stress language with lower functional use of pitch). The overall results reveal that, for both native and nonnative listeners, the processing of Japanese pitch accent is less lateralized (compared to lexical tone processing, which has been found to be a left hemisphere property). However, detailed analysis with individual pitch accents across groups shows a right hemisphere preference for processing the high-accent-low (H*L) pattern, a left hemisphere preference for LH*, and no hemisphere dominance for LH, indicating a significant reliance on the acoustic cues. These patterns are particularly prominent with the English listeners who are least experienced with linguistic pitch. Together, the findings suggest an interplay of linguistic and acoustic aspects in the processing of Japanese pitch accent by native and nonnative listeners.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0374,Universality and language-specific experience in the perception of lexical tone and pitch,"Two experiments focus on Thai tone perception by native speakers of tone languages (Thai, Cantonese, and Mandarin), a pitch-accent (Swedish), and a nontonal (English) language. In Experiment 1, there was better auditory-only and auditory-visual discrimination by tone and pitch-accent language speakers than by nontone language speakers. Conversely and counterintuitively, there was better visual-only discrimination by nontone language speakers than tone and pitch-accent language speakers. Nevertheless, visual augmentation of auditory tone perception in noise was evident for all five language groups. In Experiment 2, involving discrimination in three fundamental frequency equivalent auditory contexts, tone and pitch-accent language participants showed equivalent discrimination for normal Thai speech, filtered speech, and violin sounds. In contrast, nontone language listeners had significantly better discrimination for violin sounds than filtered speech and in turn speech. Together the results show that tone perception is determined by both auditory and visual information, by acoustic and linguistic contexts, and by universal and experiential factors.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0375,Auditory cues that drive language development are language specific: Evidence from Cantonese,"The mechanisms that allow for both language-specific and universal constraints in language development are not fully understood. According to the rhythm detection hypothesis, sensitivity to rhythm is the underlying mechanism that is fundamental to language development. Support from a number of Western languages, as well as Mandarin, has led to the proposal that rhythm detection may provide a language-universal account of language development. However, claims of universality may be premature because most research has addressed reading (rather than language) development, only a small number of languages have been investigated, and pitch is a better predictor of reading than rhythm in Mandarin children. Therefore, we examined language development using a narrative story-retelling task in children who speak Cantonese (a more complex tone inventory than Mandarin) and also assessed temporal and pitch-based auditory abilities to consider whether temporal processing drives development in a tone language. Both temporal and pitch abilities correlated with language development, but only pitch explained unique variance in language after age. The findings support the role of basic auditory processing mechanisms in language development, but they extend beyond the rhythm detection hypothesis by demonstrating that the fundamental cues for development are dependent on the specific processing demands of each language, rather than being universal.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0376,The use of tonal coarticulation in segmentation of artificial language speech: A study with Mandarin listeners,"Tonal carryover assimilation, whereby a tone is assimilated to the preceding one, is conditioned by prosodic boundaries in a way suggesting that its presence may signal continuity or lack of a boundary. Its possibility as a speech segmentation cue was investigated in two artificial language (AL) learning experiments. Mandarin-speaking listeners identified the ""words"" of a three-tone AL (e.g., [pe.ti.ku]) after listening to six long speech streams in which the words were repeated continuously without pauses. The first experiment revealed that segmentation was disrupted in an ""incongruent-cues"" condition where tonal carryover assimilation occurred across AL word boundaries and conflicted with statistical regularities in the speech streams. Segmentation was neither facilitated nor inhibited in a ""congruent-cues"" condition where tonal carryover assimilation occurred only within the AL words in 27% of the repetitions and never across word boundaries. A null effect was again found for the congruent-cues condition of the second experiment, where all AL word repetitions carried tonal carryover assimilation. These findings show that tonal carryover assimilation is exploited to resolve segmentation problems when cues conflict. Its null effect in the congruent-cues conditions might be linked to cue redundancy and suggest that it is weighted low in the segmentation cue hierarchy.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0377,"Phonology, homophony, and eyes-closed rest in Mandarin novel word learning: An eye-tracking study in adult native and non-native speakers","This study used the visual world paradigm to investigate novel word learning in adults from different language backgrounds and the effects of phonology, homophony, and rest on the outcome. We created Mandarin novel words varied by types of phonological contrasts and homophone status. During the experiment, native (n = 34) and non-native speakers (English; n = 30) learned pairs of novel words and were tested twice with a 15-minute break in between, which was spent either resting or gaming. In the post-break test of novel word recognition, an interaction appeared between language backgrounds, phonology, and homophony: non-native speakers performed less accurately than native speakers only on non-homophones learned in pairs with tone contrasts. Eye movement data indicated that non-native speakers' processing of tones may be more effortful than their processing of segments while learning homophones, as demonstrated by the time course. Interestingly, no significant effects of rest were observed across language groups; yet after gaming, native speakers achieved higher accuracy than non-native speakers. Overall, this study suggests that Mandarin novel word learning can be affected by participants' language backgrounds and phonological and homophonous features of words. However, the role of short periods of rest in novel word learning requires further investigation.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0378,The contribution of prosody to spoken word recognition,"The experiment reported here employed a word-onset gating technique to investigate the role of prosody in word recognition. Subjects were asked to identify words based on word onsets alone, word onsets followed by information about the word duration, or word onsets followed by information about full word prosody (i.e., both duration and stress). Results showed that words were correctly recognized with significantly less segmental onset information when word prosody was available to the subjects. Consistent with this finding, prerecognition error responses reflected correct length and prosody with less onset phonology when prosody information was provided in the stimulus than when only length information was provided. The findings of this experiment confirm the importance of word prosody for spoken word recognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0379,FROM THE TIBER TO THE THAMES: THOMAS WATSON'S ITALIAN MADRIGALLS ENGLISHED AND THE NATURALISATION OF MARENZIO'S PASTORAL MADRIGAL,"One of the five Elizabethan anthologies of 'Englished' Italian songs, Thomas Watson's 1590 Italian Madrigalls Englished (IME) presents itself as a selection of madrigals - almost all by Marenzio - with texts that do not strictly translate the original lyrics yet remain equally suitable to the music they underlay. Contrary to earlier studies of the IME, this article argues that Watson's contrafacta, while indeed far from faithful translations, in fact remain deeply invested in the appropriation and subversion of the madrigals' original verse. Most crucially, the IME carefully naturalises Marenzio's pastoral landscapes - originally meant to evoke the Roman milieu of the composer's patron - by repopulating this Arcadia with prominent Elizabethans and recognisable characters drawn from Watson's own poetry. His contrafacta equally engage with the madrigals' representation of characteristic formal elements of Italian verse, to prove not only the English language's capacity to assimilate foreign prosody but also the Italian madrigal's capacity to accommodate native English rhythms. Ultimately, the IME seeks to prove that English verse is equally suitable to being sung to the period's most prestigious secular compositions, that the madrigal is equally capable of evoking a musical Arcadia in Elizabethan England.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0380,Neurostimulation and Pupillometry: New Directions for Learning and Research in Applied Linguistics,"This paper begins by discussing new trends in the use of neurostimulation techniques in cognitive science and learning research, as well as the nascent research on their application in second language learning. To illustrate this, an experiment designed to investigate the impact of transcutaneous vagus nerve stimulation (tVNS), which is delivered via earbuds, on how learners process and learn Mandarin tones is reported. Pupillometry, which is an index of cognitive effort, is explained and illustrated as one way to assess the impact of tVNS. Participants in the study were native English speakers, naive to tone languages, pseudorandomly assigned to active or control conditions, while balancing for nonlinguistic pitch ability and musical experience. Their performance after tVNS was assessed using a range of more traditional language outcome measures, including accuracy and reaction times from lexical recognition and recall tasks and was triangulated with pupillometry during word-learning to help understand the mechanism through which tVNS operates. Findings are discussed in light of the literatures on lexical tone learning, cognitive effort, and neurostimulation, including specific benefits for learners of tone languages. Recommendations are made for future work on the increasingly popular area of neurostimulation for the field of applied linguistics in the 40th anniversary issue of ARAL.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0381,PHONETIC INFLUENCES ON ENGLISH AND FRENCH LISTENERS' ASSIMILATION OF MANDARIN TONES TO NATIVE PROSODIC CATEGORIES,"This study examined how native speakers of Australian English and French, nontone languages with different lexical stress properties, perceived Mandarin tones in a sentence environment according to their native sentence intonation categories (i-Categories) in connected speech. Results showed that both English and French speakers categorized Mandarin tones primarily on the phonetic similarities of the pitch contours between the Mandarin tones and their native i-Categories. Moreover, French but not English speakers were able to detect the fine-detailed phonetic differences between Tone 3 (T3) and Tone 4 (T4; i.e., low or low-falling tone vs. high-falling tone), which suggests that the stress differences between these languages may affect nonnative tone perception: English uses lexical stress, whereas French does not. In the discrimination task, the French listeners' performance was better than that of the English listeners. For each group, discrimination of the Tone 1 (T1)-T4 and Tone 2 (T2)-T3 pairs was consistently and significantly lower than that of the other tone pairs, and the difference between T1-T4 and T2-T3 was significant. Discrimination of the Mandarin tone pairs was not fully predicted by pairwise categorizations to native i-Categories, however. Some discrimination differences were observed among tone pairs showing the same assimilation patterns. Phonetic overlaps in native i-Category choices for the Mandarin tones, strength of categorization (So, 2012), and tonal coarticulation effects (Xu, 1994, 1997) may offer possible accounts of these discrepancies between categorization and discrimination performance. These findings support the perceptual assimilation model for suprasegmentals (So & Best, 2008, 2010a, 2010b, 2011, 2013), extended to categorization of nonnative tone words within sentence contexts to native i-Categories.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0382,BEYOND SEGMENTS Prosody in SLA,,,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0383,FACTORS INFLUENCING SENSITIVITY TO LEXICAL TONE IN AN ARTIFICIAL LANGUAGE Implications for Second Language Learning,"This study examined whether musical training, ethnicity, and experience with a natural tone language influenced sensitivity to tone while listening to an artificial tone language. The language was designed with three tones, modeled after level-tone African languages. Participants listened to a 15-min random concatenation of six 3-syllable words. Sensitivity to tone was assessed using minimal pairs differing only in one syllable (nonword task: e.g., to-ka-su compared to ca-fi-to) or only in tone (tone task: e.g., to-ka-su compared to to-ka-su). Proficiency in an East Asian heritage language was the strongest predictor of success on the tone task. Asians without tone language experience were no better than other ethnic groups. We conclude by considering implications for research on second language learning, especially as approached through artificial language learning.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0384,ADVANCED SECOND LANGUAGE LEARNERS' PERCEPTION OF LEXICAL TONE CONTRASTS,"It is commonly believed that second language (L2) acquisition of lexical tones presents a major challenge for learners from nontonal language backgrounds. This belief is somewhat at odds with research that consistently shows beginning learners making quick gains through focused tone training, as well as research showing advanced learners achieving near-native performance in tone identification tasks. However, other long-term difficulties related to L2 tone perception may persist, given the additional demands of word recognition and the effects of context. In the current study, we used behavioral and event-related potential (ERP) experiments to test whether perception of Mandarin tones is difficult for advanced L2 learners in isolated syllables, disyllabic words in isolation, and disyllabic words in sentences. Stimuli were more naturalistic and challenging than in previous research. While L2 learners excelled at tone identification in isolated syllables, they performed with very low accuracy in rejecting disyllabic tonal nonwords in isolation and in sentences. We also report ERP data from critical mismatching words in sentences; while L2 listeners showed no significant differences in responses in any condition, trends were not inconsistent with the overall pattern in behavioral results of less sensitivity to tone mismatches than to semantic or segmental mismatches. We interpret these results as evidence that Mandarin tones are in fact difficult for advanced L2 learners. However, the difficulty is not due primarily to an inability to perceive tones phonetically, but instead is driven by the need to process tones lexically, especially in multisyllable words.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0385,OBSERVING AND PRODUCING PITCH GESTURES FACILITATES THE LEARNING OF MANDARIN CHINESE TONES AND WORDS,"This study investigates the perception and production of a specific type of metaphoric gesture that mimics melody in speech, also called pitch gesture, in the learning of L2 suprasegmental features. In a between-subjects design, a total of 106 participants with no previous knowledge of Chinese were asked to observe (Experiment 1) and produce (Experiment 2) pitch gestures during a short multimodal training session on Chinese tones and words. In both experiments they were tested on (a) tone identification and (b) word learning. Results showed the positive effect of a training session with pitch gesture observation compared to a training session without it (Experiment 1) and the benefits of producing gestures compared to only observing them and repeating the words aloud (Experiment 2). A comparison of the results of the two experiments revealed that there was no significant difference between the simple observation of pitch gestures and the production of speech accompanied by pitch gestures in facilitating lexical tone identification and word learning. Thus, both perception and production tasks with pitch gestures can be regarded as beneficial learning strategies for the initial stages of tones acquisition in the Chinese as a Second Language classroom.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0386,EFFECTS OF MULTITALKER INPUT AND INSTRUCTIONAL METHOD ON THE DIMENSION-BASED STATISTICAL LEARNING OF SYLLABLE-TONE COMBINATIONS AN EYE-TRACKING STUDY,"To test the effects of talker variability and explicit instruction on the statistical learning of lexical tone, 80 monolingual English listeners were taught an artificial language that mimicked Mandarin's asymmetric distribution of syllable-tone co-occurrences. Training stimuli consisted of either speech from one talker or speech from four talkers. Participants were either never instructed or explicitly taught associations between phonemes (CVs), tones, and nonce symbols across four consecutive days. Learning was assessed by the accuracy of mouse clicks and eye movements to visual nonce symbols. Critical trials induced competition between the target symbol, which matched the acoustic input, and a competitor symbol that had a statistically more probable tone (but mismatched the acoustic input). Eye fixations indicated that participants were sensitive to syllable-tone co-occurrence probabilities even without explicit instruction of tone. The degree to which statistical knowledge was used to recognize words appeared to increase when participants processed more variable speech.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0387,A LONGITUDINAL INVESTIGATION OF EXPLICIT AND IMPLICIT AUDITORY PROCESSING IN L2 SEGMENTAL AND SUPRASEGMENTAL ACQUISITION,"Precise auditory perception at a subcortical level (neural representation and encoding of sound) has been suggested as a form of implicit L2 aptitude in naturalistic settings. Emerging evidence suggests that such implicit aptitude explains some variance in L2 speech perception and production among adult learners with different first language backgrounds and immersion experience. By examining 46 Chinese learners of English, the current study longitudinally investigated the extent to which explicit and implicit auditory processing ability could predict L2 segmental and prosody acquisition over a 5-month early immersion. According to the results, participants' L2 gains were associated with more explicit and integrative auditory processing ability (remembering and reproducing music sequences), while the role of implicit, preconscious perception appeared to be negligible at the initial stage of postpubertal L2 speech learning.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0388,DO L1-L2 DIFFERENCES IN DISCOURSE PROCESSING REFLECT PROCESSING DEMANDS OR DIFFICULTY OF FORM-FUNCTION MAPPING? EVIDENCE FROM SELF-PACED LISTENING OF CONTRASTIVE PROSODY,"We examined what causes L1-L2 differences in sensitivity to prominence cues in discourse processing. Participants listened to recorded stories in segment-by-segment fashion at their own pace. Each story established a pair of contrasting items, and one item from the pair was rementioned and manipulated to carry either a contrastive or presentational pitch accent. By directly comparing the current self-paced listening data to previously obtained experimenter-paced listening data, we tested whether reducing online-processing demands allows L2 learners to show a nativelike behavior, such that contrastive pitch accents facilitate later ruling out the salient alternative. However, reduced time pressure failed to lead even higher proficiency L1-Korean learners of English to reach a nativelike level, suggesting that L2 learners' nonnativelike processing and representation of the prominence cue in spoken discourse processing can be due to the inherent difficulty of fully learning a complex form-function mapping rather than to online-processing demands.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0389,Age-related changes in acoustic modifications of Mandarin maternal speech to preverbal infants and five-year-old children: a longitudinal study,"Acoustic-phonetic exaggeration of infant-directed speech (IDS) is well documented, but few studies address whether these features are modified with a child's age. Mandarin-speaking mothers were recorded while addressing an adult and their child at two ages (0;7-1;0 and 5;0) to examine the acoustic-phonetic differences between IDS and child-directed speech (CDS). CDS exhibits an exaggeration pattern resembling that of IDS-expanded vowel space, longer vowels, higher pitch and greater lexical tone differences - when compared to ADS. Longitudinal analysis demonstrated that the extent Of acoustic exaggeration Is significantly smaller in CDS than in IDS. Age-related changes in maternal speech provide Some Support for the hypothesis that mothers adjust their speech directed toward children as a function of the child's language ability.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0390,The processing of lexical tones by young Chinese children,"The current study examined five-and seven-year-old Mandarin-speaking children's processing of lexical tones in relation to speech segments by varying onset and rime in an oddity task (onset +/- rime +/-). Results showed that children experienced more difficulty in lexical tone oddity judgment when rimes differed across monosyllables (e. g. onset+rime-) than when onsets differed (e. g. onset-rime+). This finding suggests that vowels interfere more than consonants in lexical tone processing. Seven-year-olds consistently outperformed five-year-olds, suggesting that the growth of metalinguistic awareness and literacy exposure may play a joint role in the development of lexical tone processing skills.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0391,"The interaction of gesture, intonation, and eye-gaze in proto-imperatives","In the present study we investigate the production of gesture, intonation, and eye-gaze within the proto-imperative behaviour of one English child aged 1; 0 to 1; 7. The study is based on the qualitative and quantitative analysis of the three behaviours. The results indicate a shift from reaching gestures to points, and from rising terminal pitch contours to non-rising contours. The analysis also highlights changes in eye-gaze to the co-participant over time. In addition we identify a significant relationship between pitch contour and gesture type within the sample, with points being more closely associated with non-rise intonation than reaching gestures. We suggest that the changes in proto-imperative behaviour signal a shift in the underlying representation of the function from a request for help to a demand for a particular object, and that this development paves the way for the subsequent conventional linguistic expression of the imperative function.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0392,Production and perception of listener-oriented clear speech in child language,"In this paper, we ask whether children are sensitive to the needs of their interlocutor, and, if so, whether they - like adults - modify acoustic characteristics of their speech as part of a communicative goal. In a production task, preschoolers participated in a word learning task that favored the use of clear speech. Children produced vowels that were longer, more intense, more dispersed in the vowel space, and had a more expanded F. range than normal speech. Two perception studies with adults showed that these acoustic differences were perceptible and were used to distinguish normal and clear speech styles. We conclude that preschoolers are sensitive to aspects of the speaker-hearer relationship calling upon them to modify their speech in ways that benefit their listener.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0393,Lexical tone variation and spoken word recognition in preschool children: effects of perceptual salience,"Children undergo gradual progression in their ability to differentiate correct and incorrect pronunciations of words, a process that is crucial to establishing a native vocabulary. For the most part, the development of mature phonological representations has been researched by investigating children's sensitivity to consonant and vowel variation, with a much lesser focus on lexical tones. The current study investigates sensitivity to lexical tones in word recognition with specific attention to role of perceptual salience. Chinese-speaking preschoolers were presented with familiar words that were correctly pronounced, substituted for a subtle tone variant (Tones 2 and 3), or substituted for a salient tone variant (Tones 1 and 4). Results demonstrated that subtle tone variants were mistakenly perceived as correct pronunciations and only salient tone variants were recognized as mispronunciations. Findings suggest that tone integration follows a more complex developmental course that previously concluded.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0394,No perceptual reorganization for Limburgian tones? A cross-linguistic investigation with 6-to 12-month-old infants,"Despite the fact that many of the world's languages use lexical tone, the majority of language acquisition studies has focused on non-tone languages. Research on tone languages has typically investigated well-known tone languages such as Mandarin and Cantonese Chinese. The current study looked at a Limburgian dialect of Dutch that uses lexical pitch differences, albeit in a rather restricted way. Using a visual habituation paradigm, 6- to 12-month-old Limburgian and Dutch infants were tested for their ability to discriminate Limburgian tones. The results showed that both Limburgian and Dutch infants discriminate the Limburgian tones throughout their first year of life. The role of linguistic experience, acoustic salience, and the degree of similarity to the native prosodic system are discussed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0395,Korean-speaking children's perceptual development in multidimensional acoustic space,"This study investigated how Korean toddlers' perception of stop categories develops in the acoustic dimensions of VOT and F0. To examine the developmental trajectory of VOT and F0 in toddlers' perceptual space, a perceptual identification test with natural and synthesized sound stimuli was conducted with 58 Korean monolingual children (aged 2-4 years). The results revealed that toddlers' perceptual mapping functions on VOT mainly in the high-pitch environment, resulting in more successful perceptual accuracy in fortis or aspirated stops than in lenis stops. F0 development is correlated with the perceptual distinction of lenis from aspirated stops, but no consistent categorical perception for F0 was found before four years of age. The findings suggest that multi-parametric control in perceptual development guides an acquisition ordering of Korean stop phonemes and that tonal development is significantly related to the acquisition of Korean phonemic contrasts.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0396,Lexical and Prosodic Pitch Modifications in Cantonese Infant-directed Speech,"The functions of acoustic-phonetic modifications in infant-directed speech (IDS) remain a question: do they specifically serve to facilitate language learning via enhanced phonemic contrasts (the hyperarticulation hypothesis) or primarily to improve communication via prosodic exaggeration (the prosodic hypothesis)? The study of lexical tones provides a unique opportunity to shed light on this, as lexical tones are phonemically contrastive, yet their primary cue, pitch, is also a prosodic cue. This study investigated Cantonese IDS and found increased intra-talker variation of lexical tones, which more likely posed a challenge to rather than facilitated phonetic learning. Although tonal space was expanded which could facilitate phonetic learning, its expansion was a function of overall intonational modifications. Similar findings were observed in speech to pets who should not benefit from larger phonemic distinction. We conclude that lexical-tone adjustments in IDS mainly serve to broadly enhance communication rather than specifically increase phonemic contrast for learners.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0397,Neural correlates of lexical-tone and vowel-quality processing in 6-and 9-month-old German-learning infants and adults,"We examined the neurophysiological underpinnings of lexical-tone and vowel-quality perception in learners of a non-tonal language. We tested 25 6- and 25 9-month-old German-learning infants, as well as 24 German adults and expected developmental differences for the two linguistic properties, as they are both carried by vowels, but have a different status in German. In adults, both lexical-tone and vowel-quality contrasts elicited mismatch negativities, with a stronger response to the vowel-quality contrast. Six-month-olds showed positive mismatch responses for lexical-tone and vowel-quality contrasts, with an emerging negative mismatch response for vowel-quality only. The negative mismatch responses became more pronounced for the vowel-quality contrast at 9 months, while the lexical-tone contrast elicited mainly positive mismatch responses. Our data reveal differential developmental changes in the processing of vowel properties that differ in their lexical relevance in the ambient language.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0398,Cross-talker lexical tone discrimination in infancy,"This study investigated how infants deal with cross-talker variability in the perception of native lexical tones, paying specific attention to developmental changes and the role of task demands. Using the habituation-based visual fixation procedures, we tested Cantonese-learning infants of different age groups on their ability to discriminate Cantonese Tone 1 (high level) and Tone 3 (mid level) produced by either multiple talkers or a single talker. Results demonstrated that the 12-month-old and 24-month-old groups showed reliable discrimination across talkers, whereas the 18-month-old group did not (Experiment 1), despite their ability to discriminate the same contrast when the talker was held constant (Experiment 2). In a task that included a novel object as a referent to the sound, the 18-month-olds discriminated the contrast across talkers from Tone 1 to Tone 3 (Experiment 3). These results revealed a U-shaped developmental path and perceptual asymmetry in native lexical tone discrimination across talkers. (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic).(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(T1)(sic)(sic)(sic)(sic)(T3)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic).(sic)(sic)(sic)(sic), 12 (sic)(sic)(sic) 24 (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic) 18 (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)((sic)(sic) 1),(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)((sic)(sic) 2).(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), 18 (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic) T1 (sic) T3 (sic)(sic)(sic)(sic)(sic)((sic)(sic) 3).(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic) U (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)..",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0399,Durational correlates of English sublexical constituent structure,"This study investigates whether differences (a) in word-internal morphological structure and (b) in lexical stress patterns are reflected in prosodic Constituent structure, by examining duration measurements in Scottish English. In Experiments 1 and 2, at a slow speech rate, stem-final rhymes followed by Level II suffixes were on average 4-6% longer than corresponding strings in monomorphemic words, and 7-8% longer than stem-final rhymes followed by Level I suffixes. These results are consistent with the view that stems preceding Level II suffixes are mapped onto prosodic words in the prosodic representation. Experiment 3 obtained no reliable durational differences, even at a slow, speech rate, between the initial syllable rhymes of SS words and SW words, which does not provide evidence for the hypothesis that these different stress patterns are represented as differences in foot structure.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0400,Testing the role of phonetic knowledge in Mandarin tone sandhi,"Phonological patterns often have phonetic bases. But whether phonetic substance should be encoded in synchronic phonological grammar is controversial. We aim to test the synchronic relevance of phonetics by investigating native Mandarin speakers' applications of two exceptionless tone sandhi processes to novel words: the contour reduction 213 -> 21/_T (T not equal 213), which has a clear phonetic motivation, and the perceptually neutralising 213 -> 35/_213, whose phonetic motivation is less clear. In two experiments, Mandarin subjects were asked to produce two individual monosyllables together as two different types of novel disyllabic words. Results show that speakers apply the 213 -> 21 sandhi with greater accuracy than the 213 -> 35 sandhi in novel words, indicating a synchronic bias against the phonetically less motivated pattern. We also show that lexical frequency is relevant to the application of the sandhis to novel words, but cannot account alone for the low sandhi accuracy of 213 -> 35.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0401,Asymmetries in the intonation system of the tonal dialect of Maastricht Limburgish,"The lexical tone and intonation contrasts in the Limburgish dialect of Maastricht are remarkable in a number of ways. While a falling pitch contour on an IP-medial syllable signals a non-declarative intonation, on an IP-final syllable it signals a declarative intonation. In addition, although there is a binary tone contrast (Accent 1 vs. Accent 2) and four nuclear intonation contours, only three intonation contours exist for nuclear syllables with Accent 2, while in IP-final position only two intonation contours exist for nuclear syllables with Accent 1, so that the full set of four intonation contours is only observable in IP-medial nuclear syllables with Accent 1. The context-dependent function of the pitch fall and the asymmetries are explained by a grammar in which the OCP is enforced absolutely, and the number of tones per syllable is restricted to two, unless the three tones each represent a different morpheme: OCP, REALISEMORPH >>#TTT.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0402,Contrastive foot structure in Franconian tone-accent dialects,"Franconian has a contrast between two tone accents, commonly referred to as Accent 1 and Accent 2. Traditional autosegmental analyses of the phenomenon suggest that this opposition derives from the presence of lexical tone. In contrast to this 'tonal approach', I argue that the Franconian accent contrast is based on contrastive foot structure - there is no tone in the lexicon. This 'metrical approach' not only accounts for the tonal differences between the accents, but also captures a variety of facts that are hard to incorporate into a synchronic tonal analysis, involving morphological alternations between Accent 1 and Accent 2, as well as the effects of vowel duration, vowel quality and consonant quality on accent-class membership. The metrical analysis of these patterns is in line with similar approaches to tone-accent contrasts in North Germanic and Scottish Gaelic.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0403,Exceptional prosodification effects revisited in Gradient Harmonic Grammar,"This paper presents an analysis of exceptional prosodification effects, in which exceptional lexical items appear to follow a regular pattern that is found in a different prosodic context. These patterns have been analysed as cases of prosodic prespecification, where morphemes select a non-default prosodic representation. I argue that prespecification approaches should be reconsidered, and that such patterns are predicted without morpheme-specific prosody in Gradient Harmonic Grammar, a weighted constraint system with gradiently active symbols. Exceptional prosodification effects result from the interaction of two influences on constraint penalties: (i) scaling of constraint violations by prosodic context and (ii) contrastive activity values in underlying forms. This interaction is illustrated with the distribution of French nasal vowels and linking [n]. This approach reduces the amount of structure posited for URs, and provides new arguments for a more uniform syntax-prosody mapping.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0404,The phonological determinants of tone in English loanwords in Mandarin,"This paper presents the results of a corpus study and an online loanword adaptation experiment examining the tonal adaptation of English loanwords in Mandarin. Using maximum entropy models, I control for the substantial influences of lexical tone distributions and standardisation, and uncover phonological determinants of tone beyond these lexical and conventional factors. The most important phonological determinant of tone in the corpus was English voicing, while in the experiment it was English stress-aligned pitch contours. I argue that these distinct tonal adaptation patterns constitute two different perceptual mappings, one from F0 perturbations to tone and the other from English intonation to tone, both arising due to particular borrowing contexts. I suggest that increasingly close contact between English and Mandarin may lead to more intonation-driven tonal adaptation in the latest wave of borrowing. The maximum entropy approach holds promise for the analysis of complex cases of tonal adaptation in other languages.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0405,Processing of contrastiveness by heritage Russian bilinguals,"Two eye-tracking experiments in the Visual World paradigm compared how monolingual Russian (Experiment 1) and heritage Russian-English bilingual (Experiment 2) listeners process contrastiveness online in Russian. Materials were color adjective-noun phrases embedded into the split-constituent construction Krasnuju polozite zvezdocku ... "" Red put star ... "" whose inherent contrastiveness results from integration of multiple sources of information, i.e., word order, prosody and visual context. The results showed that while monolinguals rapidly used word order and visual context (but not contrastive prosody) to compute the contrast set even before the noun appeared in speech, heritage Russian bilinguals were very slow and took notice of multiple sources of information only when the lexical identity of the noun made the task superfluous. These results are similar to slowed processing reported in the literature for L2 learners. It is hypothesized that this slowdown in HL processing is due to cascading effects of covert competition between the two languages that starts at the level of spoken word recognition and culminates at the interfaces and, with time, it may become a major contributing force to heritage language attrition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0406,"Long-term experience with a tonal language shapes the perception of intonation in English words: How Chinese-English bilinguals perceive ""Rose?"" vs. ""Rose""","Long-term experience with a tonal language shapes pitch perception in specific ways, and consequently Chinese speakers may not process pitch in English words -e.g., ""Rose?"" spoken as a question versus ""Rose"" spoken as a statement-in the same way as native speakers of non-tonal languages do. If so, what are those pitch processing differences and how do they affect Chinese recognition of English words? We investigated these questions by administering a primed lexical-decision task in English to proficient Chinese-English bilinguals and two control groups, namely, Spanish-English and native English speakers. Prime-target pairs differed in one sound and/or in pitch. Results showed specific cross-language differences in pitch processing between the Chinese speakers and the control groups, confirming that experience with a tonal language shaped the perception of English words' intonation. Moreover, such experience helps to incorporate pitch into models of word-recognition for bilinguals of tonal and non-tonal languages.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0407,Perception of tones by bilingual infants learning non-tone languages,"This paper examines the ability of bilingual infants who were learning Dutch and another non-tone language to discriminate tonal contrasts. All infants from 5 to 18 months of age succeeded in discriminating a tonal contrast of Mandarin Chinese (Tone 1 versus Tone 4) and showed a U-shaped pattern when facing a less acoustically salient manipulated version (contracted) of the aforementioned contrast. Specifically, infants showed initial sensitivity to the contracted contrast during their early months, followed by a loss of sensitivity at the stage where tonal perceptual reorganization typically occurs, and a sensitivity rebound by the end of the first year after birth. Compared to a previous studying of ours testing monolingual Dutch infants (Liu & Kager, 2014), the discrimination patterns of bilingual infants revealed both similarities and differences. On one hand, as with monolinguals, non-tone-learning bilingual infants' tonal perception presented plasticity influenced by contrast acoustic salience along the trajectory of perceptual reorganization; as well as a general U-shaped perceptual pattern when discriminating non-native tones. On the other hand, bilingual infants appeared to regain sensitivity to the contracted tonal contrast at an earlier age (11-12 months) in comparison with monolinguals infants (17-18 months). We provide several explanations, stemming from the simultaneous exposure to two languages, to account for the 6-month bilingual perceptual plasticity from linguistic and cognitive perspectives. The overall outcomes of the study offer insights into the infant perceptual reorganization and language development trajectory, expand on the differences between monolingual and bilingual language development, and broaden our understanding of the influence of bilingual exposure to the perception of non-native contrasts in infancy from linguistic and cognitive perspectives.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0408,Effects of L1 tone on perception of L2 tone - a study of Mandarin tone learning by native Cantonese children,"In the present study, the Perceptual Assimilation Model (PAM) was tested on its applicability in child L2 lexical tone acquisition. The possible effect of L1 (Cantonese) lexical tones on L2 (Mandarin) lexical tone learning was explored. Accuracy rate and error patterns were examined with an AX discrimination task and a forced-choice identification task. Forty-nine native Cantonese-speaking students aged 8 years participated in the study. Results revealed that these children exhibited nearly perfect performance in the discrimination of Mandarin tones. However, significant tone differences were detected in the identification task. Tone 4 (T4) was identified with the lowest accuracy, and T1 with the highest. Error analysis revealed that Mandarin T2-T3 was the most confusing pair, followed by the T1-T4 pair. The inherent phonetic similarity between lexical tones in a language and the tone similarities across languages may also have contributed to perception difficulties, which could help to refine and supplement the PAM in the tonal/suprasegmental domain.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0409,Interlingual two-to-one mapping of tonal categories,"Both Standard Chinese (SC) high- and low-rising tones sound like the rising tone in Jinan Mandarin (JM) Chinese. Acoustically (Experiment 1), the JM rising tone overlaps with both SC rising tones, but more with the high-rising tone than with the low-rising tone. Perceptually (Experiment 2), the JM rising tone was more likely identified as the SC high-rising tone by SC monolinguals. Experiment 3 examined the role of this two-to-one interlingual tonal mapping in bilingual lexical access. Final high-rising SC pseudo-words were more frequently and more quickly accepted as JM real words than final low-rising SC pseudo-words were. However, both high- and low-rising SC pseudo-words triggered equivalent facilitatory semantic priming on JM real-word targets. The results suggest that different tones are represented in the bilinguals' mental lexicon in terms of fine-grained and sometimes overlapping acoustic specifications. Lexical activation and semantic activation are partially independent.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0410,Effects of contrastive accents in memory for L2 discourse,"Contrastive pitch accents benefit native English speakers' memory for discourse by enhancing a representation of a specific relevant contrast item (Fraundorf et al., 2010). This study examines whether and how second language (L2) listeners differ in how contrastive accents affect their encoding and representation of a discourse, as compared to native speakers. Using the same materials as Fraundorf et al. (2010), we found that low and mid proficiency L2 learners showed no memory benefit from contrastive accents. High proficiency L2 learners revealed some sensitivity to contrastive accents, but failed to fully integrate information conveyed by contrastive accents into their discourse representation. The results suggest that L2 listeners' non-native performance in processing contrastive accents, observed in this and other prior studies, may be attributed at least in part to a difference in the depth of processing of the information conveyed by contrastive accents.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0411,Electrophysiological correlates of categorical perception of lexical tones by English learners of Mandarin Chinese: an ERP study,"This study examines brain responses to boundary effects with respect to Mandarin lexical tone continua for three groups of adult listeners: (1) native English speakers who look advanced Mandarin courses; (2) naive English speakers; and (3) native Mandarin speakers. A cross-boundary tone pair and a within-category tone pair derived from tonal contrasts (Mandarin Tone 1/Tone 4; Tone 2/Tone 3) with equal physical/acoustical distance were used in an auditory oddball paradigm. Fir native Mandarin speakers, the cross-category deviant elicited a larger MMN over Jell hemisphere sensors and larger P300 responses over both hemispheres relative to within-category deviants, suggesting categorical perception of tones at both pre-attentive and attentional stages of processing. In contrast, native English speakers and Mandarin learners did not demonstrate categorical effects. However, learners of Mandarin showed larger P300 responses than the other two groups, suggesting heightened sensitivity to tones and possibly greater attentional resource allocation to tone identification.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0412,Lexical tone as a cue in statistical word learning from bilingual input,"Learners can track word-referent co-occurrences across individually-ambiguous naming events to form correct word-referent mappings, termed statistical word learning (SWL). Prior research largely focuses on learning from a single language input, where a referent co-occurs with a single word (1:1 mapping). Here, we tested adults' SWL from a simulated bilingual environment, where one referent co-occurred with two words (2:1 mapping) and the two words were either differentiated by a linguistic cue (Mandarin lexical tones, Cued condition) or not (Uncued condition). Results showed that in the Cued condition, Chinese-English bilinguals (N = 38) outperformed Spanish-English bilinguals (N = 56) and English monolinguals (N = 55), while Spanish-English bilinguals and English monolinguals performed similarly. The three groups did not differ in the Uncued condition. Self-reported learning confidence and strategies showed limited conscious awareness of learning. Results demonstrate that familiarity with a linguistic cue boosts overall statistical word learning from bilingual input.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0413,Non-native tone categorization and word learning across a spectrum of L1 tonal statuses,"Adults differ in the ease with which they acquire lexical tones in a non-native language. Individual differences have been attributed to several factors, such as the role that pitch plays in a learner's L1 to signal lexical meaning (L1 tonal status), the shape of the tones to be acquired (tone types), as well as extralinguistic factors (such as musical experience and working memory). Here, we ask whether learners from a spectrum of L1 tonal statuses (Dutch, Swedish and Japanese, and Thai) differ in their tone word learning facility, whilst we simultaneously investigate the effects of tone type, and musical experience and working memory. Our findings suggest that above and beyond L1 tonal status, the strongest predictor of tone word learning was pre-lexical tone processing (measured by a tone categorization task), although the strength of the link between pre-lexical and lexical processing may be modulated by L1 tonal status.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0414,The role of phonology in non-native word learning: Evidence from cross-situational statistical learning,"Adults often encounter difficulty perceiving and processing sounds of a second language (L2). In order to acquire word-meaning mappings, learners need to determine what the language-relevant phonological contrasts are in the language. In this study, we examined the influence of phonology on non-native word learning, determining whether the language-relevant phonological contrasts could be acquired by abstracting over multiple experiences, and whether awareness of these contrasts could be related to learning. We trained English- and Mandarin-native speakers with pseudowords via a cross-situational statistical learning task (CSL). Learners were able to acquire the phonological contrasts across multiple situations, but similar-sounding words (i.e., minimal pairs) were harder to acquire, and words that contrast in a non-native suprasegmental feature (i.e., Mandarin lexical tone) were even harder for English-speakers, even with extended exposure. Furthermore, awareness of the non-native phonology was not found to relate to learning.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0415,Segmentation and selection of appropriate Chinese characters in writing place names in Japanese,"This paper explores the relation between an unknown place name written in hiragana (a Japanese syllabary) and its corresponding written representation in kanji (Chinese characters). We propose three principles as those operating in the selection of the appropriate Chinese characters in writing unknown place names. The three principles are concerned with the combination of on and kun readings (zyuubako-yomi), the number of segmentations. and the bimoraicity characteristics of kanji chosen. We performed two experiments to test the principles: the results supported our hypotheses. These results have some implications for the structure of the Japanese mental lexicon, for the processing load in the use of Chinese characters, and for Japanese prosody and morphology.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0416,Prosodic structure and wh-questions,"This study examines the influence of wh-gaps on the prosodic contour of spoken utterances. A previous study (Nagel, Shapiro, & Nawy, 1994) claimed that the phonological representation of a sentence containing a filler-gap dependency explicitly encodes the location of the syntactic gap. In support of this hypothesis, Nagel et at. presented evidence that the word immediately preceding a gap is lengthened and that there is a reliable increase in pitch excursion across the gap location. Our study challenges Nagel et al.'s claim. We argue that their materials confounded the presence/absence of a gap with other factors that are known to affect intonational phrasing independently. We show that, when these factors are separated, the evidence that syntactic gaps are explicitly encoded in the phonological representation of a sentence disappears.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0417,"Who do you love, your mother or your horse? An event-related brain potential analysis of tone processing in Mandarin Chinese","In Mandarin Chinese, word meaning is partially determined by lexical tone (Wang, 1973). Previous studies suggest that lexical tone is processed as linguistic information and not as pure tonal information (Gandour, 1998; Van Lanker & Fromkin, 1973). The current study explored the online processing of lexical tones. Event-related potentials were obtained from 25 Mandarin speakers while they listened to normal and anomalous sentences containing one of three types of semantic anomalies created by manipulating the tone, the syllable, or both tone and syllable (double-anomaly) of sentence-final words. We hypothesized N400 effects elicited by all three types of anomalies and the largest by the double-anomaly. As expected, all three elicited N400 effects starting approximately 150 ms poststimulus and continuing until 1000 ms in some areas. Surprisingly, onset of the double-anomaly effect was approximately 50 ms later than the rest. Delayed detection of errors in this condition may be responsible for the apparent delay. Slight differences between syllable and tone conditions may be due to the relative timing of these acoustic cues.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0418,Are Prosodic Variants Stored in the French Mental Lexicon?,"A long-term priming experiment examined the way stress information is processed and represented in French speakers' mind. Repeated prime and target words either matched (/ba'do/ - /ba'do/ ""headband"") or mismatched their stress pattern (/bado/ - /ba'do/). In comparison to a control condition (/makc/ - /ba'do/), the results showed that matching and mismatching primes were equally effective in facilitating the processing of the target words. Thus, despite the fact that French speakers routinely produce and hear words in their stressed and unstressed versions, this study suggests that stress in French is not integrated into lexical representations.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0419,"Infants' response to the audible and visible properties of the human face .1. Role of lexical syntactic content, temporal synchrony, gender, and manner of speech","Four-, 6-, and 8-month-old infants' perception of the multimodal features of the human face was investigated. First, infants were habituated to a visible and audible face of a person reciting a prepared script. Then they were tested by changing various features of just the audible, just the visible, or both components of the face. When features were changed, such as the lexical-syntactic content, the speaker's gender, or the synchrony relation between the audible and visible components, the infants discriminated their multimodal and visible representation but not the audible one. When the manner of speech was changed from adult- to infant-directed, the 2 older groups discriminated all 3 types of changes but the 4-month-old infants only discriminated its visible and multimodal representation. Results show that speech-related exaggerated prosody cues facilitate detection of the audible features of multimodally represented faces but not until 6 months of age.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0420,Acoustic analysis of lexical tone in Mandarin infant-directed speech,"Using Mandarin Chinese, a ""tone language"" in which the pitch contours of syllables differentiate words, the authors examined the acoustic modifications of infant-directed speech (IDS) at the syllable level to test 2 hypotheses: (a) the overall increase in pitch and intonation contour that occurs in IDS at the phrase level would not distort lexical pitch at the syllable level and (b) IDS provides exaggerates cues to lexical tones. Sixteen Mandarin-speaking mothers were recorded while addressing their infants and addressing an adult. The results indicate that IDS does not distort the acoustic cues that are essential to word meaning at the syllable level; evidence of exaggeration of the acoustic differences in IDS was observed, extending previous findings of phonetic exaggeration to the lexical level.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0421,CREATION OF PROSODY DURING SENTENCE PRODUCTION,"Phrase-final words tend to be lengthened and followed by a pause. The dominant view of prosodic production is that word lengthening and pausing reflect the syntax of a sentence. The author demonstrates that, instead, lengthening and pausing reflect a distinctly prosodic representation, in which phonological constituents are arranged in a hierarchical, nonrecursive structure. Prosodic structure is created without knowledge of words' phonemic content. As a result, within a single sentential position, greater word lengthening necessitates shorter pauses, but across positions, word and pause durations show a positive correlation. The author presents a model of prosodic production that describes the process of prosodic encoding and provides a quantitative specification of the relation between word lengthening and pausing. This model has implications for studies of language production, comprehension, and development.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0422,Stress versus coarticulation: Toward an integrated approach to explicit speech segmentation,"Although word stress has been hailed as a powerful speech-segmentation cue, the results of 5 cross-modal fragment priming experiments revealed limitations to stress-based segmentation. Specifically, the stress pattern of auditory primes failed to have any effect on the lexical decision latencies to related visual targets. A determining factor was whether the onset of the prime was coarticulated with the preceding speech fragment. Uncoarticulated (i.e., concatenated) primes facilitated priming. Coarticulated ones did not. However, when the primes were presented in a background of noise, the pattern of results reversed, and a strong stress effect emerged: Stress-initial primes caused more priming than non-initial-stress primes, regardless of the coarticulatory cues. The results underscore the role of coarticulation in the segmentation of clear speech and that of stress in impoverished listening conditions. More generally, they call for an integrated and signal-contingent approach to speech segmentation.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0423,PERCEPTUAL SPECIFICITY OF AUDITORY PRIMING - IMPLICIT MEMORY FOR VOICE INTONATION AND FUNDAMENTAL-FREQUENCY,"Five experiments explored the effect of acoustic changes between study and test on implicit and explicit memory for spoken words. Study-test changes in the speaker's voice, intonation, and fundamental frequency produced significant impairments of auditory priming on implicit tests of auditory identification and stem completion but had little or no effect on explicit recall and recognition tests (Experiments 1-4). However, study-test changes in overall decibel level had no effect on priming on an auditory stem-completion test or on cued-recall performance (Experiment 5). The results are consistent with the idea that fundamental frequency information is represented in a perceptual representation system that plays an important role in auditory priming.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0424,Prosody-assisted head-driven access to spoken German compounds,"Auditory processing of German 2-noun compound words was investigated with 328 participants in 4 experiments by monitoring semantic priming effects of the left constituents of the compound words. The authors demonstrated that there is no primacy of the left constituents in accessing auditorily presented German compound words in the mental lexicon. A clear priming effect of left constituents occurred only for compound words with a transparent right constituent that is the head of compound words in Germanic languages. The data suggest that the access to German compounds in the auditory domain involves 2 temporally overlapping routes: direct and decompositional. The prosodic structure (i.e., the duration) of the first morphemes of compound words appears to be a determining factor for activation of the decompositional route.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0425,If you say thee uh you are describing something hard:: The on-line attribution of disfluency during reference comprehension,"Eye-tracking and gating experiments examined reference comprehension With fluent (Click on the red...) and disfluent (Click on [pause] thee uh red...) instructions while listeners viewed displays with 2 familiar (e.g., ice cream cones) and 2 unfamiliar objects (e.g., squiggly shapes). Disfluent instructions made unfamiliar objects more expected, which influenced listeners' on-line hypotheses from the onset of the color word. The unfamiliarity bias was sharply reduced by instructions that the speaker had object agnosia, and thus difficulty naming familiar objects (Experiment 2), but was not affected by intermittent sources of speaker distraction (beeps and construction noises; Experiments 3). The authors conclude that listeners can make situation-specific inferences about likely sources of disfluency, but there are some limitations to these attributions.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0426,Does Speech Rhythm Sensitivity Predict Children's Reading Ability 1 Year Later?,"There is a growing literature demonstrating that speech rhythm sensitivity is related to children's reading development, independent of phonological awareness. However, the precise nature of this relationship is less well understood, and further research is warranted to investigate whether speech rhythm sensitivity predicts the different components of reading over time. In this 1-year longitudinal study, 69 five- to 8-year-old English-speaking children completed a speech rhythm assessment at Time I along with other cognitive assessments and then completed a variety of reading assessments at Time 2 (1 year later). A series of hierarchical regression analyses revealed that after controlling for individual differences in age, vocabulary, and phonological awareness, speech rhythm sensitivity was able to predict unique variance in word reading and the phrasing component of the reading fluency measure I year later. The findings emphasize the contribution of speech rhythm sensitivity in children's reading development, and the authors argue that speech rhythm sensitivity should now be included in current models of children's reading development.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0427,SENSITIZATION TO EMOTIONS AND REPRESENTATION FORMATION THROUGH SOCIAL BIOFEEDBACK: Is Markedness a Necessary Mechanism?,"This study reviews Gergely and Watson's (1996) social biofeedback theory of parental affect-mirroring, furthered by Fonagy, Gergely, Jurist, and Target (2002), especially the ""markedness"" hypothesis. These authors hypothesize that a salient facial expression and the singsong prosody of markedness called ""motherese"" are necessary for successful emotion sensitization and symbolization. The revision investigates whether this is accomplished through ""internalization"" mechanisms requiring markedness of the mirroring, or solely through social biofeedback processes. The article argues that the infant contingency-detection mechanism (similar to that of adult biofeedback training) mediates the functions of sensitization, representation, and symbolization of emotions through its processes of covariance-invariance detection, maximization, and contingent control of the parental mirroring, whether the mirroring is marked or not. The review argues that a caregiver producing a covariant-invariant mirroring can help bring the infant's emotional somatosensations to consciousness along with its implicit dispositional content without using motherese. It considers the clinical implications of the new model and speculates about parental difficulties centered on sharing troubling emotions. Finally, it discusses how the model may be a mediating mechanism in the change process in the therapy of adults through the promotion of sensitive emotions, their awareness, and symbolization.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0428,Metrical Expectations From Preceding Prosody Influence Perception of Lexical Stress,"Two visual-world experiments tested the hypothesis that expectations based on preceding prosody influence the perception of suprasegmental cues to lexical stress. The results demonstrate that listeners' consideration of competing alternatives with different stress patterns (e.g., 'jury/gi'raffe) can be influenced by the fundamental frequency and syllable timing patterns across material preceding a target word. When preceding stressed syllables distal to the target word shared pitch and timing characteristics with the first syllable of the target word, pictures of alternatives with primary lexical stress on the first syllable (e.g., jury) initially attracted more looks than alternatives with unstressed initial syllables (e.g., giraffe). This effect was modulated when preceding unstressed syllables had pitch and timing characteristics similar to the initial syllable of the target word, with more looks to alternatives with unstressed initial syllables (e.g., giraffe) than to those with stressed initial syllables (e.g., jury). These findings suggest that expectations about the acoustic realization of upcoming speech include information about metrical organization and lexical stress and that these expectations constrain the initial interpretation of suprasegmental stress cues. These distal prosody effects implicate online probabilistic inferences about the sources of acoustic-phonetic variation during spoken-word recognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0429,Effects of Age and Bilingualism on Sensitivity to Native and Nonnative Tone Variation: Evidence From Spoken Word Recognition in Mandarin Chinese Learners,"Most children learn a language such as Mandarin Chinese that uses lexical tone to communicate meaning. This study aimed to examine the phonological specificity of tone representations in monolingual and bilingual learners of Mandarin. Two age-groups were tested: toddlers (2.5 to 3.5 years) and preschoolers (4 to 5 years; N = 80). Using a preferential looking paradigm, children were presented with correct and mispronounced (Cantonese or Mandarin tone substitutions) forms of familiar words. In the aggregate, participants demonstrated preferential fixation to visual targets (naming effects) for correctly pronounced words, words mispronounced with native (Mandarin) tone substitutions, and for words mispronounced with nonnative (Cantonese) tone substitutions. However, naming effects were greater for correct pronunciations and for Cantonese tone mispronunciations relative to Mandarin tone mispronunciations. There were no interactions of responses to each trial type with age or with language exposure. Results are discussed in terms of possible determinants of tone sensitivity in early childhood.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0430,Beyond Perceptual Narrowing: Monolingual and Bilingual Infants Discriminate Hindi Contrasts When Learning Words in the Second Year of Life,"A significant body of literature has demonstrated that infants demonstrate a decline in sensitivity to nonnative sound contrasts by their first birthday, a transition often thought to be adaptive for later word learning. The present study investigated infants' sensitivity to these contrasts in a habituation-based discrimination and word learning task (total N = 168, 78 males and 90 females). All infants were native to Singapore and were of Chinese origin. Family socioeconomic status (SES) was measured by parental education. The mean number of years of parental education was 4.02 years after high school. Using a habituation-based discrimination paradigm, monolingual, and bilingual infants' sensitivity to the Hindi dental/retroflex voiceless stop was investigated at 14 months (Experiment 1). Neither group discriminated the contrast. Using the Switch paradigm, we assessed sensitivity to the same contrast in a word learning task. Monolingual and bilingual infants were tested at 14- and 19 months (Experiment 2a) and subsequently, an older group of bilingual infants was tested at 24 months (Experiment 2b). Results demonstrated an overall sensitivity to the Hindi contrast in Experiment 2a. Bilingual infants tested in Experiment 2b were not sensitive to the Hindi contrast. Findings are discussed with reference to discontinuities in the growth of a phonological lexicon as well as possible mechanisms elicit nonnative sensitivity in word learning.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0431,The Ability to Use Contextual Cues to Achieve Phonological Constancy Emerges by 14 Months,"The ability to map similar sounding words to different meanings alone is far from enough for successful speech processing. To overcome variability in the speech signal, young learners must also recognize words across surface variations. Previous studies have shown that infants at 14 months are able to use variations in word-internal cues (i.e., acoustic cues within the target word) to form phonological categories and to learn words. The present study takes into consideration the fact that talker variability can easily lead to acoustic overlap between phonological categories, in which case reliance on word-external cues (i.e., acoustic cues in the context preceding and/or following the target word, also referred to as contextual cues) as a frame of reference is obligatory for successful talker adaptation. In a series of experiments, the present study examines when infants are able to use word-external cues to tune to different talkers for the benefit of word learning. Cantonese-learning 14-month-old, 18-month-old, and 24-month-old infants (N = 258) were tested on the associative learning of Cantonese Tone 1-Tone 3 contrast. Results showed that talker variability that yielded acoustic overlap between the two tonal categories compromised infants' ability to map the contrast onto word meanings. However, when given speaker-matched contextual cues, infants as young as 14 months of age demonstrated a certain degree of talker adaptation which may have subserved their use of phonetic details in novel word learning.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0432,The Acquisition of Contrastive Focus During Online Sentence-Comprehension by Children Learning Mandarin Chinese,"Contrastive focus, conveyed by prosodic cues, marks important information. Studies have shown that 6-year-olds learning English and Japanese can use contrastive focus during online sentence comprehension: focus used in a contrastive context facilitates the identification of a target referent (speeding up processing), whereas focus used inappropriately in a noncontrastive context misleads listeners to predict an incorrect referent, hindering the identification process (Ito et al., 2012, 2014). In Mandarin Chinese, the mapping between prosodic form and contrastive focus is less transparent, potentially delaying the acquisition of contrastive focus. This study assessed the online processing of contrastive focus by 196 Mandarin-speaking 4-10-year-olds and 34 adults in China, using the visual world paradigm. Stimuli contained a target NP in a mini discourse, with focus being used in contrastive (Experiment 1) versus Noncontrastive contexts (Experiment 2). Experiment 1 showed that the appropriate use of prosodic form for contrastive focus facilitated the identification of a target referent for 7-10-year-olds and adults, though not younger children. Experiment 2 showed that the inappropriate use of prosodic form for contrastive focus slowed the identification process only for 10-year-olds and adults. Thus, whereas 7-10-year-olds are sensitive to prosodic form for contrastive focus, only 10-year-olds use it as a primary cue to predict an upcoming referent like adults. The acquisition of contrastive focus in Mandarin is therefore a gradual process, with children showing sensitivity to contrastive focus during the early school years, and developing adult-like form-function mapping between prosody and focus until the end of primary school.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0433,Similar Representations of Emotions Across Faces and Voices,"Emotions are a vital component of social communication, carried across a range of modalities and via different perceptual signals such as specific muscle contractions in the face and in the upper respiratory system. Previous studies have found that emotion recognition impairments after brain damage depend on the modality of presentation: recognition from faces may be impaired whereas recognition from voices remains preserved, and vice versa. On the other hand, there is also evidence for shared neural activation during emotion processing in both modalities. In a behavioral study, we investigated whether there are shared representations in the recognition of emotions from faces and voices. We used a within-subjects design in which participants rated the intensity of facial expressions and nonverbal vocalizations for each of the 6 basic emotion labels. For each participant and each modality, we then computed a representation matrix with the intensity ratings of each emotion. These matrices allowed us to examine the patterns of confusions between emotions and to characterize the representations of emotions within each modality. We then compared the representations across modalities by computing the correlations of the representation matrices across faces and voices. We found highly correlated matrices across modalities, which suggest similar representations of emotions across faces and voices. We also showed that these results could not be explained by commonalities between low-level visual and acoustic properties of the stimuli. We thus propose that there are similar or shared coding mechanisms for emotions which may act independently of modality, despite their distinct perceptual inputs.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0434,"Coleridge, Mary Robinson, and the prosody of dreams","Samuel Taylor Coleridge and Mary Robinson are two poets of the Romantic period whose poetry is ostensibly concerned with the experience of dreaming and the representation of dreams in verse. Both poets knew and admired the other, perhaps because of their shared experiences with opium and dreams. They both find the poem in his or her dreams but then fit the dream in poetic form. The metrical experiments of both poets in their dream poems are conscious acts of representing in verse the experience of dreaming. This is accomplished not merely through figurative poetic language but also through what I will call the prosody of dreams-the theory and principles of versification as they pertain to dream poetry. The prosody of dreams, then refers specifically to the way these two Romantic poets use metrical effects to represent a dream world, suggesting that, for them at least, dreams can be understood through the self-conscious approximation of the dream experience in verse. Robinson and Coleridge use poetic form and metrical experimentation to explore in verse the unfathomed depths of the unconscious mind and the creative potentialities of dreaming. Their poetry suggests that each had an intimate familiarity with the work of the other, but their strikingly similar approaches to the prosody of dreams remains a compelling intersection that has yet to be discussed. A closer look at the poetic forms of dreams created by these two pioneering dream poets, in addition to illuminating some pertinent poems by Mary Robinson, will substantially inform our understanding of Coleridge and the way he understood his dreams and of the poetic practice of representing dreams in verse.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0435,Age Effects on Prosodic Boundary Perception,"The redundancy hypothesis proposes that older listeners need a larger array of acoustic cues than younger listeners for effective speech perception. This research investigated this hypothesis by examining the aging effects on the use of prosodic cues in speech segmentation in Mandarin Chinese. We examined how younger and older listeners perceived prosodic boundaries using three main prosodic cues (pause, final lengthening, and pitch change) across eight conditions involving different cue combinations. The stimuli consisted of syntactically ambiguous phrase pairs, each containing two or three objects. Participants (22 younger listeners and 22 older listeners) performed a speech recognition task to judge the number of objects they heard. Both groups primarily relied on the pause cue for identifying prosodic boundaries, using final lengthening and pitch change as secondary cues. However, older listeners showed reduced sensitivity to these cues, compensating by integrating the primary cue pause with the secondary cue pitch change for more precise segmentation. The present study reveals older listeners' integration strategy in using prosodic cues for speech segmentation, supporting the redundancy hypothesis.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0436,Toward an Integrative Model of Talker Normalization,"Successful speech perception requires accurate mapping of speech signals to linguistic categories despite talker variation in signals. Although factors like intrinsic and context cues have been identified, a full understanding of talker normalization remains to be achieved. In particular, it is important to examine the cocontribution of intrinsic, extrinsic and other cues in an integrative way. In Experiment 1, we examined the effect of intrinsic cues and typicality of a talker's F0 range relative to population F0 range on word identification in isolation. In Experiment 2, we compared the effects of 4 contexts to identify those that consistently facilitate talker normalization. We found that without contexts, word identification accuracy was low and variable depending on talker typicality. Contexts improved performance across all talkers regardless of typicality. But only meaningless and meaningful speech contexts with cues to a talker's acoustic-phonological space showed consistent effects. We proposed a new model, integrating talker typicality, talker familiarity, and context. Whereas speech signals from familiar or typical talkers may be accurately identified standing alone, a context with cues to a talker's acoustic-phonological space is necessary in the case of unfamiliar and atypical talkers. It is thus the first model that integrates memory and context effects.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0437,Asymmetries in the Perception of Mandarin Tones: Evidence From Mismatch Negativity,"Most investigations of the representation and processing of speech sounds focus on their segmental representations, and considerably less is known about the representation of suprasegmental phenomena (e.g., Mandarin tones). Here we examine the mismatch negativity (MMN) response to the contrast between Mandarin Tone 3 (T3) and other tones using a passive oddball paradigm. Because the MMN response has been shown to be sensitive to the featural contents of speech sounds in a way that is compatible with underspecification theories of phonological representations, here, we test the predictions of such theories regarding suprasegmental phenomena. Assuming T3 to be underspecified in Mandarin (because it has variable surface representations and low pitch), we predicted that an asymmetric MMN response would be elicited when T3 is contrasted with another tone. In 2 of our 3 experiments, this was observed, but in non-Mandarin-speaking participants as well as native speakers, suggesting that the locus of the effect was perceptual (acoustic or phonetic) rather than phonological. In a third experiment, the predicted asymmetry was limited to native speakers. These results highlight the importance of distinguishing phonological and perceptual contributions to MMN asymmetries, but also demonstrate a role of abstract phonological representations in which certain information is underspecified in long-term memory.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0438,Priming Stress Patterns in Word Recognition,"This study addresses the lexical representation of stress in a series of 5 intramodal and cross-modal priming experiments in the Greek language using lexical decision tasks with auditory and visual targets. Three-syllable primes and targets were matched in first syllable segments, length, and other variables, and differed segmentally in the second and third syllable. Primes matched or mismatched targets in stress, which was placed on the penultimate or antepenultimate syllable. There was no evidence for stress priming in either accuracy or latency of responses to either words or pseudowords in any of these experiments, either intramodally or cross-modally. In contrast, a control fragment priming experiment using only the first 2 syllables of the primes produced a significant effect of stress congruence for words but not for pseudowords. The results are interpreted in the context of previous findings in the literature as arising from lexical activation rather than from matching stress patterns. Overall, findings are consistent with lexical representations including stress information that is inseparable from segmental specification, rather than with abstract representations of metrical templates.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0439,"Lexical Tone is Perceived Relative to Locally Surrounding Context, Vowel Quality to Preceding Context","Important speech cues such as lexical tone and vowel quality are perceptually contrasted to the distribution of those same cues in surrounding contexts. However, it is unclear whether preceding and following contexts have similar influences, and to what extent those influences are modulated by the auditory history of previous trials. To investigate this, Cantonese participants labeled sounds from (a) a tone continuum (mid-to high-level), presented with a context that had raised or lowered fundamental frequency (F0) values and (b) a vowel quality continuum (/u/ to /o/), where the context had raised or lowered first formant (F1) values. Contexts with high or low F0/F1 were presented in separate blocks or intermixed in 1 block. Contexts were presented following (Experiment 1) or preceding the target continuum (Experiment 2). Contrastive effects were found for both tone and vowel quality (e.g., decreased F0 values in contexts lead to more high tone target judgments and vice versa). Importantly, however, lexical tone was only influenced by F0 in immediately preceding and following contexts. Vowel quality was only influenced by the F1 in preceding contexts, but this extended to contexts from preceding trials. Contextual influences on tone and vowel quality are qualitatively different, which has important implications for understanding the mechanism of context effects in speech perception. Public Significance Statement Speech perception is highly context dependent. This study compares the strength of contextual influences in the perception of lexical tone and vowel quality in a number of ways. Perception of lexical tone was found to be influenced by locally preceding and following contexts, while vowel quality was only influenced by the preceding context, and that influence extended further back in time. These patterns demonstrate that the temporal scope of contextual influences are cue specific.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0440,A Note by Any Other Name: Intonation Context Rapidly Changes Absolute Note Judgments,"Absolute pitch (AP) judgments, by definition, do not require a reference note, and thus might be viewed as context independent. Here, we specifically test whether short-term exposure to particular intonation contexts influences AP categorization on a rapid time scale and whether such context effects can change from moment to moment. In Experiment 1, participants heard duets in which a ""lead"" instrument always began before a ""secondary"" instrument. Both instruments independently varied on intonation (flat, in-tune, or sharp). Despite participants being instructed to judge only the intonation of the secondary instrument, we found that participants treated the lead instrument's intonation as ""in-tune"" and intonation judgments of the secondary instrument were relativized against this standard. In Experiment 2, participants heard a short antecedent context melody (flat, in-tune, or sharp) followed by an isolated target note (flat, in-tune, or sharp). Target note intonation judgments were once again relativized against the context melody's intonation, though only for notes that were experienced in the context or implied by the context key signature. Moreover, maximally contrastive intonation combinations of context and target engendered systematic note misclassifications. For example, a flat melody resulted in a greater likelihood of misclassifying a ""sharp F-sharp"" as a ""G."" These results highlight that both intonation and note category judgments among AP possessors are rapidly modified by the listening environment on the order of seconds, arguing against an invariant mental representation of the absolute pitches of notes. Implications for general auditory theories of perception are discussed.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0441,Pitch Perception in Music: Do Scoops Matter?,"Studies of musical pitch perception typically treat pitches as if they are stable within a tone. Although pitches are represented this way in notation, performed tones are rarely stable, particularly in singing, which is arguably the most common form of melody production. This paper examines how brief dynamic changes at the beginnings and endings of sung pitches, a.k.a. ""scoops,"" influence intonation perception. Across three experiments, 110 participants evaluated the intonation of four-tone melodies in which the third tone's tuning could vary within the central steady-state (the asymptote), or by virtue of scoops at the beginning and/or end of the tone. As expected, listeners were sensitive to mistuning. Importantly, our results also point to unique contributions of scoops. As in the language domain, dynamic changes in a small time window are perceptually significant in music. More specifically, this study revealed the coexistence of two distinct mechanisms: sensitivity to the average pitch across the duration of the tone (assimilating the scoop), and processing the relationship of the scoop to the surrounding context. In addition to clarifying intonation perception in music, the identification of these mechanisms paves the way to cross-domain comparisons and, more generally, to the better understanding of auditory sequences processing.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0442,Tracking Talker-Specific Cues to Lexical Stress: Evidence from Perceptual Learning,"When recognizing spoken words, listeners are confronted by variability in the speech signal caused by talker differences. Previous research has focused on segmental talker variability; less is known about how suprasegmental variability is handled. Here we investigated the use of perceptual learning to deal with between-talker differences in lexical stress. Two groups of participants heard Dutch minimal stress pairs (e.g., VOORnaam vs. voorNAAM, ""first name"" vs. ""respectable"") spoken by two male talkers. Group 1 heard Talker 1 use only F0 to signal stress (intensity and duration values were ambiguous), while Talker 2 used only intensity (F0 and duration were ambiguous). Group 2 heard the reverse talker-cue mappings. After training, participants were tested on words from both talkers containing conflicting stress cues (""mixed items""; e.g., one spoken by Talker 1 with F0 signaling initial stress and intensity signaling final stress). We found that listeners used previously learned information about which talker used which cue to interpret the mixed items. For example, the mixed item described above tended to be interpreted as having initial stress by Group 1 but as having final stress by Group 2. This demonstrates that listeners learn how individual talkers signal stress and use that knowledge in spoken-word recognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0443,The Role of Semantic Transparency in the Processing of Spoken Compound Words,"The question of whether lexical decomposition is driven by semantic transparency in the lexical processing of morphologically complex words, such as compounds, remains controversial. Prior research on compound processing has predominantly examined visual processing. Focusing instead on spoken word word recognition, the present study examined the processing of auditorily presented English compounds that were semantically transparent (e.g., farmyard) or partially opaque with an opaque head (e.g., airline) or opaque modifier (e.g., pothole). Three auditory primed lexical decision experiments were run to examine to what extent constituent priming effects are affected by the semantic transparency of a compound and whether semantic transparency affects the processing of heads and modifiers equally. The results showed priming effects for both modifiers and heads regardless of their semantic transparency, indicating that individual constituents are accessed in transparent as well as opaque compounds. In addition, the results showed smaller priming effects for semantically opaque heads compared with matched transparent compounds with the same head. These findings suggest that semantically opaque heads induce an increased processing cost, which may result from the need to suppress the meaning of the head in favor of the meaning of the opaque compound.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0444,Cognate Translation Priming With Chinese-Japanese Bilinguals: No Effect of Interlingual Phonological Similarity,"Previous masked translation priming studies, especially those with different-script bilinguals, have shown that cognates provide more priming than noncognates, a difference attributed to cognates' phonological similarity. In our experiments employing a word naming task, we examined this issue for Chinese-Japanese bilinguals in a slightly different way, using same-script cognates as primes and targets. In Experiment 1, significant cognate priming effects were observed. The sizes of the priming effects were, however, statistically not different for phonologically similar (e.g.,../xin4lai4/-../shiNrai/) and dissimilar cognate pairs (e.g.,.. /bao3zheng4/-.. /hoshoR/), suggesting no impact of phonological similarity. In Experiment 2, using exclusively Chinese stimuli, we demonstrated a significant homophone priming effect using two-character logographic primes and targets, indicating that phonological priming is possible for two-character Chinese targets. However, priming only emerged for pairs that had the same tone pattern (e.g.,../shou3wei4/-../shou3wei4/), suggesting that a match in lexical tone is crucial for observing phonologically based priming in that situation. Therefore, Experiment 3 involved phonologically similar Chinese-Japanese cognate pairs in which the similarity of their suprasegmental phonological features (i.e., lexical tone and pitch-accent information) was varied. Priming effects were statistically not different for tone/accent similar pairs (e.g.,../ guan1xin1/-../kaNsiN/) and dissimilar pairs (e.g.,../man3zu2/-.. /maNzoku/). Our results indicate that phonological facilitation is not involved in producing cognate priming effects for Chinese-Japanese bilinguals. Possible explanations, based on underlying representations of logographic cognates, are discussed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0445,Musical Abilities Influence the Use of Durational Prosodic Cues in Spoken Word Recognition,"Prosody plays a fundamental role in both speech and music. In spoken language, word-level local prosodic cues, such as segment duration, contribute to word recognition. This study investigated whether individual differences in musical abilities are associated with the utilization of prosodic cues during spoken word recognition, both in speech-in-quiet and speech-on-speech conditions (i.e., in the presence of competing talkers). Using the visual world paradigm, we measured listeners' gaze fixations and pupil dilations toward images depicting a referent (e.g., hamster) and a competitor word (e.g., ham), while they simultaneously listened to utterances containing the referent word, whose segment duration either matched or mismatched the referent, with the mismatched duration signaling the competitor word. Participants with varying musical backgrounds completed tasks assessing rhythmic and melodic abilities, and a questionnaire evaluating overall musical sophistication. Our results revealed that listeners with higher scores across the three measures exhibited greater sensitivity to durational cues, as indicated by increased fixations to the competitor and greater pupil dilation when the durational cue mismatched the referent word, both in speech-in-quiet and speech-on-speech. These findings highlight that individual differences in musical abilities are associated with the use of prosodic cues during spoken word recognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0446,Transfer of Sensorimotor Adaptation Reveals Independent Prosodic Representation After Segment-Prosody Coordination in Speech Production,"In speech production, there is a long-standing debate regarding whether word-level prosodic structure has an independent representation separable from segments. Using a novel assay of sensorimotor adaptation, we observe straightforward evidence supporting an independent prosodic structure even after its coordination with the segments. Participants exposed to opposing formant perturbations applied to the two syllables of a single trained word (e.g., ""bedhead"" -> ""bidhad"") adapted separately to counteract the syllable-specific perturbations (i.e., producing ""badhid"") and, critically, transferred the learned adaptation to untrained words based purely on shared prosodic structure (words with the same prosody but novel syllables, e.g., producing ""breastfed"" like ""brastfid""). This evidence for an independent word-level prosodic representation, which has been hard to detect in previous studies, highlights the usefulness of sensorimotor adaptation as a tool for language production research.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0447,Human cortical encoding of pitch in tonal and non-tonal languages,"Languages can use a common repertoire of vocal sounds to signify distinct meanings. In tonal languages, such as Mandarin Chinese, pitch contours of syllables distinguish one word from another, whereas in non-tonal languages, such as English, pitch is used to convey intonation. The neural computations underlying language specialization in speech perception are unknown. Here, we use a cross-linguistic approach to address this. Native Mandarin- and English- speaking participants each listened to both Mandarin and English speech, while neural activity was directly recorded from the non-primary auditory cortex. Both groups show language-general coding of speaker-invariant pitch at the single electrode level. At the electrode population level, we find language-specific distribution of cortical tuning parameters in Mandarin speakers only, with enhanced sensitivity to Mandarin tone categories. Our results show that speech perception relies upon a shared cortical auditory feature processing mechanism, which may be tuned to the statistics of a given language. Different languages rely on different vocal sounds to convey meaning. Here the authors show that language-general coding of pitch occurs in the non-primary auditory cortex for both tonal (Mandarin Chinese) and non-tonal (English) languages, with some language specificity on the population level.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0448,Bilateral human laryngeal motor cortex in perceptual decision of lexical tone and voicing of consonant,"Speech perception is believed to recruit the left motor cortex. However, the exact role of the laryngeal subregion and its right counterpart in speech perception, as well as their temporal patterns of involvement remain unclear. To address these questions, we conducted a hypothesis-driven study, utilizing transcranial magnetic stimulation on the left or right dorsal laryngeal motor cortex (dLMC) when participants performed perceptual decision on Mandarin lexical tone or consonant (voicing contrast) presented with or without noise. We used psychometric function and hierarchical drift-diffusion model to disentangle perceptual sensitivity and dynamic decision-making parameters. Results showed that bilateral dLMCs were engaged with effector specificity, and this engagement was left-lateralized with right upregulation in noise. Furthermore, the dLMC contributed to various decision stages depending on the hemisphere and task difficulty. These findings substantially advance our understanding of the hemispherical lateralization and temporal dynamics of bilateral dLMC in sensorimotor integration during speech perceptual decision-making. The role of the laryngeal motor cortex (LMC) in speech perception is poorly understood. Here, using transcranial magnetic stimulation, the authors found a causal contribution of bilateral LMC to consonant and lexical tone perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0449,Cortical processing of discrete prosodic patterns in continuous speech,"Prosody has a vital function in speech, structuring a speaker's intended message for the listener. The superior temporal gyrus (STG) is considered a critical hub for prosody, but the role of earlier auditory regions like Heschl's gyrus (HG), associated with pitch processing, remains unclear. Using intracerebral recordings in humans and non-human primate models, we investigated prosody processing in narrative speech, focusing on pitch accents-abstract phonological units that signal word prominence and communicative intent. In humans, HG encoded pitch accents as abstract representations beyond spectrotemporal features, distinct from segmental speech processing, and outperforms STG in disambiguating pitch accents. Multivariate models confirm HG's unique representation of pitch accent categories. In the non-human primate, pitch accents were not abstractly encoded, despite robust spectrotemporal processing, highlighting the role of experience in shaping abstract representations. These findings emphasize a key role for the HG in early prosodic abstraction and advance our understanding of human speech processing.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0450,Normal pre-attentive and impaired attentive processing of lexical tones in Cantonese-speaking congenital amusics,"The neural underpinnings of congenital amusia, an innate neurogenetic disorder of musical pitch processing, are not well understood. Previous studies suggest that amusia primarily impairs attentive processing (P300) of small pitch deviations in music, leaving pre-attentive pitch processing (mismatch negativity or MMN) more or less intact. However, it remains unknown whether the same neuro-dynamic mechanism of deficiency underlies pitch processing in speech, where amusics also often show impairment behaviorally. The current study examined how lexical tones are processed in pre-attentive (MMN) and attentive (P300) conditions in 24 Cantonese-speaking amusics and 24 matched controls. At the pre-attentive level, Cantonese-speaking amusics exhibited normal MMN responses to lexical tone changes, even for tone pairs with small pitch differences (mid level vs. low level tone; high rising vs. low rising tone). However, at the attentive level, amusics exhibited reduced P3a amplitude for all tone pairs, and further reduced P3b amplitude for tone pairs with small pitch differences. These results suggest that the amusic brain detects tone changes normally pre-attentively, but shows impairment in consciously detecting the same tone differences. Consistent with previous findings in nonspeech pitch processing, this finding provides support for a domain-general neuro-dynamic mechanism of deficient attentive pitch processing in amusia.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0451,Phonological network fluency identifies phonological restructuring through mental search,"We investigated network principles underlying mental search through a novel phonological verbal fluency task. Post exclusion, 95 native-language Mandarin speakers produced as many items that differed by a single lexical tone as possible within one minute. Their verbal productions were assessed according to several novel graded fluency measures, and network science measures that accounted for the structure, cohesion and interconnectedness of lexical items. A multivariate regression analysis of our participants' language backgrounds included their mono- or multi-lingual status, English proficiency, and fluency in other Chinese languages/dialects. Higher English proficiency predicted lower error rates and greater interconnectedness, while higher fluency in other Chinese languages/dialects revealed lower successive similarity and lower network coherence. This inverse relationship between English and other Chinese languages/dialects provides evidence of the restructuring of the phonological mental lexicon.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0452,Enhanced Pitch Discrimination for Cochlear Implant Users with a New Haptic Neuroprosthetic,"The cochlear implant (CI) is the most widely used neuroprosthesis, recovering hearing for more than half a million severely-to-profoundly hearing-impaired people. However, CIs still have significant limitations, with users having severely impaired pitch perception. Pitch is critical to speech understanding (particularly in noise), to separating different sounds in complex acoustic environments, and to music enjoyment. In recent decades, researchers have attempted to overcome shortcomings in CIs by improving implant technology and surgical techniques, but with limited success. In the current study, we take a new approach of providing missing pitch information through haptic stimulation on the forearm, using our new mosaicOne_B device. The mosaicOne_B extracts pitch information in real-time and presents it via 12 motors that are arranged in ascending pitch along the forearm, with each motor representing a different pitch. In normal-hearing subjects listening to CI simulated audio, we showed that participants were able to discriminate pitch differences at a similar performance level to that achieved by normal-hearing listeners. Furthermore, the device was shown to be highly robust to background noise. This enhanced pitch discrimination has the potential to significantly improve music perception, speech recognition, and speech prosody perception in CI users.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0453,Sequences of Intonation Units form a  1 Hz rhythm,"Studies of speech processing investigate the relationship between temporal structure in speech stimuli and neural activity. Despite clear evidence that the brain tracks speech at low frequencies (similar to 1 Hz), it is not well understood what linguistic information gives rise to this rhythm. In this study, we harness linguistic theory to draw attention to Intonation Units (IUs), a fundamental prosodic unit of human language, and characterize their temporal structure as captured in the speech envelope, an acoustic representation relevant to the neural processing of speech. IUs are defined by a specific pattern of syllable delivery, together with resets in pitch and articulatory force. Linguistic studies of spontaneous speech indicate that this prosodic segmentation paces new information in language use across diverse languages. Therefore, IUs provide a universal structural cue for the cognitive dynamics of speech production and comprehension. We study the relation between IUs and periodicities in the speech envelope, applying methods from investigations of neural synchronization. Our sample includes recordings from every-day speech contexts of over 100 speakers and six languages. We find that sequences of IUs form a consistent low-frequency rhythm and constitute a significant periodic cue within the speech envelope. Our findings allow to predict that IUs are utilized by the neural system when tracking speech. The methods we introduce here facilitate testing this prediction in the future (i.e., with physiological data).",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0454,Combination of absolute pitch and tone language experience enhances lexical tone perception,"Absolute pitch (AP), a unique ability to name or produce pitch without any reference, is known to be influenced by genetic and cultural factors. AP and tone language experience are both known to promote lexical tone perception. However, the effects of the combination of AP and tone language experience on lexical tone perception are currently not known. In the current study, using behavioral (Categorical Perception) and electrophysiological (Frequency Following Response) measures, we investigated the effect of the combination of AP and tone language experience on lexical tone perception. We found that the Cantonese speakers with AP outperformed the Cantonese speakers without AP on Categorical Perception and Frequency Following Responses of lexical tones, suggesting an additive effect due to the combination of AP and tone language experience. These findings suggest a role of basic sensory pre-attentive auditory processes towards pitch encoding in AP. Further, these findings imply a common mechanism underlying pitch encoding in AP and tone language perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0455,Emotional prosody recognition enhances and progressively complexifies from childhood to adolescence,"Emotional prosody results from the dynamic variation of language's acoustic non-verbal aspects that allow people to convey and recognize emotions. The goal of this paper is to understand how this recognition develops from childhood to adolescence. We also aim to investigate how the ability to perceive multiple emotions in the voice matures over time. We tested 133 children and adolescents, aged between 6 and 17 years old, exposed to 4 kinds of linguistically meaningless emotional (anger, fear, happiness, and sadness) and neutral stimuli. Participants were asked to judge the type and intensity of perceived emotion on continuous scales, without a forced choice task. As predicted, a general linear mixed model analysis revealed a significant interaction effect between age and emotion. The ability to recognize emotions significantly increased with age for both emotional and neutral vocalizations. Girls recognized anger better than boys, who instead confused fear with neutral prosody more than girls. Across all ages, only marginally significant differences were found between anger, happiness, and neutral compared to sadness, which was more difficult to recognize. Finally, as age increased, participants were significantly more likely to attribute multiple emotions to emotional prosody, showing that the representation of emotional content becomes increasingly complex. The ability to identify basic emotions in prosody from linguistically meaningless stimuli develops from childhood to adolescence. Interestingly, this maturation was not only evidenced in the accuracy of emotion detection, but also in a complexification of emotion attribution in prosody.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0456,A simple psychophysical procedure separates representational and noise components in impairments of speech prosody perception after right-hemisphere stroke,"After a right hemisphere stroke, more than half of the patients are impaired in their capacity to produce or comprehend speech prosody. Yet, and despite its social-cognitive consequences for patients, aprosodia following stroke has received scant attention. In this report, we introduce a novel, simple psychophysical procedure which, by combining systematic digital manipulations of speech stimuli and reverse-correlation analysis, allows estimating the internal sensory representations that subtend how individual patients perceive speech prosody, and the level of internal noise that govern behavioral variability in how patients apply these representations. Tested on a sample of N = 22 right-hemisphere stroke survivors and N = 21 age-matched controls, the representation + noise model provides a promising alternative to the clinical gold standard for evaluating aprosodia (MEC): both parameters strongly associate with receptive, and not expressive, aprosodia measured by MEC within the patient group; they have better sensitivity than MEC for separating high-functioning patients from controls; and have good specificity with respect to non-prosody-related impairments of auditory attention and processing. Taken together, individual differences in either internal representation, internal noise, or both, paint a potent portrait of the variety of sensory/cognitive mechanisms that can explain impairments of prosody processing after stroke.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0457,Pupillometry reveals effects of pitch manipulation within and across words on listening effort and short-term memory,"For individuals with hearing loss, even successful speech communication comes at a cost. Cochlear implants transmit degraded information, specifically for voice pitch, which demands extra and sustained listening effort. The current study hypothesized that abnormal pitch patterns contribute to the additional listening effort, even in non-tonal language native speaking normally hearing listeners. We manipulated the fundamental frequency (F0) within and across words, while participants listen and repeat (simple intelligibility task), or listen, repeat, and later recall (concurrent encoding task) the words. In both experiments, the F0 manipulations resulted in small changes in intelligibility but no difference in free recall or subjective effort ratings. Pupillary metrics were yet sensitive to these manipulations: pupil dilations were larger when words were monotonized (flat contour) or inverted (the natural contour flipped upside-down), and larger when successive words were organized into a melodic pattern. The most likely interpretation is that the natural or expected F0 contour of a word contributes to its identity and facilitate its matching and retrieval from the phonological representation stored in long-term memory. Consequently, degrading words' F0 contour can result in extra listening effort. Our results call for solutions to improve pitch saliency and naturalness in future development of cochlear implants' signal processing strategies, even for non-tonal languages.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0458,Preliminary study on the neural mechanisms of four tone recognition in deaf children using fMRI,"Vocal intonation, a fundamental element of speech, is pivotal for comprehending and communicating effectively. Nevertheless, children suffering from hearing impairment encounter difficulties in recognizing vocal intonation patterns, primarily stemming from their auditory deficits. In 2020, a study conducted at Tianjin Medical University General Hospital in Tianjin, China, recruited five deaf children and two children with normal hearing (male; mean age = 10.21 +/- 0.4 years) to compare the differences between deaf and normal children in four Chinese tone recognition tasks. The results revealed that (1) Due to hearing loss, some of the auditory cortices responsible for processing vocal intonation in deaf children do not function optimally, (2) When decoding vocal intonation information, deaf children might utilize alternative neural pathways or networks, (3) Deaf children exhibit hemispheric specialization in their processing of vocal intonation cues.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0459,Speech prosody enhances the neural processing of syntax,"Human language relies on the correct processing of syntactic information, as it is essential for successful communication between speakers. As an abstract level of language, syntax has often been studied separately from the physical form of the speech signal, thus often masking the interactions that can promote better syntactic processing in the human brain. However, behavioral and neural evidence from adults suggests the idea that prosody and syntax interact, and studies in infants support the notion that prosody assists language learning. Here we analyze a MEG dataset to investigate how acoustic cues, specifically prosody, interact with syntactic representations in the brains of native English speakers. More specifically, to examine whether prosody enhances the cortical encoding of syntactic representations, we decode syntactic phrase boundaries directly from brain activity, and evaluate possible modulations of this decoding by the prosodic boundaries. Our findings demonstrate that the presence of prosodic boundaries improves the neural representation of phrase boundaries, indicating the facilitative role of prosodic cues in processing abstract linguistic features. This work has implications for interactive models of how the brain processes different linguistic features. Future research is needed to establish the neural underpinnings of prosody-syntax interactions in languages with different typological characteristics. An MEG study in native English speakers on naturalistic speech comprehension suggests that prosodic boundaries enhance the neural encoding of syntactic phrase boundaries.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0460,Spatio-temporal dynamics of automatic processing of phonological information in visual words,"Sensory-specific cortices appear to be sensitive to information from another modality. Here we investigate whether the human brain automatically extracts the phonological information in visual words in early visual processing. We continuously presented native Chinese speakers peripherally with Chinese homophone characters in an oddball paradigm, while they performed a visual detection task presented in the centre of the visual field. We found the lexical tone phonology embedded in the characters is processed automatically by the brain of native speakers, as revealed by whole-head electrical recordings of the mismatch negativity (MMN). Source solution further revealed the MMN involved the neural activations from the visual cortex to the auditory cortex (130-460 ms). The spatial-temporal dynamics indicate a visual-auditory interaction in the early, automatic processing of phonological information in visual words.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0461,Abnormal topological organization of the white matter network in Mandarin speakers with congenital amusia,"Congenital amusia is a neurogenetic disorder that mainly affects the processing of musical pitch. Brain imaging evidence indicates that it is associated with abnormal structural and functional connections in the fronto-temporal region. However, a holistic understanding of the anatomical topology underlying amusia is still lacking. Here, we used probabilistic diffusion tensor imaging tractography and graph theory to examine whole brain white matter structural connectivity in 31 Mandarin-speaking amusics and 24 age-and IQ-matched controls. Amusics showed significantly reduced global connectivity, as indicated by the abnormally decreased clustering coefficient (C-p) and increased normalized shortest path length (lambda) compared to the controls. Moreover, amusics exhibited enhanced nodal strength in the right inferior parietal lobule relative to controls. The co-existence of the lexical tone deficits was associated with even more deteriorated global network efficiency in amusics, as suggested by the significant correlation between the increments in normalized shortest path length (lambda) and the insensitivity in lexical tone perception. Our study is the first to reveal reduced global connectivity efficiency in amusics as well as an increase in the global connectivity cost due to the co-existed lexical tone deficits. Taken together these results provide a holistic perspective on the anatomical substrates underlying congenital amusia.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0462,Speech-specific categorical perception deficit in autism: An Event-Related Potential study of lexical tone processing in Mandarin-speaking children,"Recent studies reveal that tonal language speakers with autism have enhanced neural sensitivity to pitch changes in nonspeech stimuli but not to lexical tone contrasts in their native language. The present ERP study investigated whether the distinct pitch processing pattern for speech and nonspeech stimuli in autism was due to a speech-specific deficit in categorical perception of lexical tones. A passive oddball paradigm was adopted to examine two groups (16 in the autism group and 15 in the control group) of Chinese children's Mismatch Responses (MMRs) to equivalent pitch deviations representing within-category and between-category differences in speech and nonspeech contexts. To further examine group-level differences in the MMRs to categorical perception of speech/nonspeech stimuli or lack thereof, neural oscillatory activities at the single trial level were further calculated with the inter-trial phase coherence (ITPC) measure for the theta and beta frequency bands. The MMR and ITPC data from the children with autism showed evidence for lack of categorical perception in the lexical tone condition. In view of the important role of lexical tones in acquiring a tonal language, the results point to the necessity of early intervention for the individuals with autism who show such a speech-specific categorical perception deficit.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0463,Children's recognition of cartoon voices,"We examined developmental changes in talker recognition skills by assessing 3-, The University of Texas at Dallas 4-, and 5-year-old children's recognition of 20 cartoon characters' voices. For each participant, the character set was subdivided into more and less familiar talkers based on the participant's ability to name each character. Four- and 5-year-old children recognized more of the voices (81% and 86%, respectively) than did 3-year-olds (61%), although performance of all age groups was well above chance. All groups of children were more accurate at recognizing more familiar than less familiar characters. These results suggest that indexical information about a talker becomes an integral part of the perceptual record in memory and can be used by children at a very young age. These results are important because children's ability to learn vocal sources may be an important aid to the development of spoken word recognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0464,Perceptual normalization for inter- and intratalker variation in Cantonese level tones,"Inter- and intratalker variation in the production of lexical tones may contribute to acoustic overlap among tone categories. The present study investigated whether such category overlap gives rise to perceptual ambiguity and, if so, whether listeners are able to reduce this ambiguity using contextual information. In the first experiment, native Cantonese-speaking listeners were asked to identify isolated Cantonese level tones produced by 7 talkers. Identification accuracy was significantly higher when the presentation of items was blocked by talker rather mixed across talkers. In the second experiment, listeners identified the final (target) tone of 6-syllable semantically neutral sentences with f(0) patterns of the context (i.e., the first 5 syllables) altered. The same target tone was identified differently depending on the context. In the third experiment, the context portions of stimulus sentences from the second experiment were divided into 2 halves, and their f(0) patterns were altered independently. In identifying the target tone, listeners relied more heavily on the f(0) pattern of the second (last) half of the context. These results are discussed in relation to characteristic inter- and intratalker variations of lexical tones.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0465,"Perception and production of lexical tones by 3-year-old, Mandarin-speaking children","The present study investigated 3-year-old children's perception and production of Mandarin lexical tones in monosyllabic words. Thirteen 3-year-old, Mandarin-speaking children participated in the study. Tone perception was examined by a picture-pointing task, and tone production was investigated by picture naming. To compare children's productions with the adult forms, 4 mothers of the children were asked to say the same set of words to their children in a picture-reading activity. The children's and mothers' productions were low-pass filtered at 500 Hz and 400 Hz, respectively, to eliminate segmental information. Ten Mandarin-speaking judges identified the productions of tones from the filtered speech. Adult productions were more accurately identified than productions of the children. The children perceived the level, rising, and falling tones with relatively high accuracy. The dipping tone posed the greatest difficulty for the children in both perception and production.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0466,The Perception of Lexical Tone Contrasts in Cantonese Children With and Without Specific Language Impairment (SLI),"Purpose: This study examined the perception of fundamental frequency (f0) patterns by Cantonese children with and without specific language impairment (SLI). Method: Participants were 14 five-year-old children with SLI, and 14 age-matched (AM) and 13 four-year-old vocabulary-matched (VM) controls. The children identified a word from familiar word pairs that illustrated the 8 minimally contrastive pairs of the 6 lexical tones. They discriminated the f0 patterns within contrastive tonal pairs in speech and nonspeech stimuli. Results: In tone identification, the SLI group performed worse than the AM group but not the VM group. In tone discrimination, the SLI group did worse than the AM group on 2 contrasts and showed a nonsignificant trend of poorer performance on all contrasts combined. The VM group generally did worse than the AM group. There were no group differences in discrimination performance between speech and nonspeech stimuli. No correlation was found between identification and discrimination performance. Only the normal controls showed a moderate correlation between vocabulary scores and performance in the 2 perception tasks. Conclusion: The SLI group's poor tone identification cannot be accounted for by vocabulary knowledge alone. The group's tone discrimination performance suggests that some children with SLI have a deficit in f0 processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0467,Intonation Contrast in Cantonese Speakers With Hypokinetic Dysarthria Associated With Parkinson's Disease,"Purpose: Speech produced by individuals with hypokinetic dysarthria associated with Parkinson's disease (PD) is characterized by a number of features including impaired speech prosody. The purpose of this study was to investigate intonation contrasts produced by this group of speakers. Method: Speech materials with a question-statement contrast were collected from 14 Cantonese speakers with PD. Twenty listeners then classified the productions as either questions or statements. Acoustic analyses of F0, duration, and intensity were conducted to determine which acoustic cues distinguished the production of questions from statements, and which cues appeared to be exploited by listeners in identifying intonational contrasts. Results: The results show that listeners identified statements with a high degree of accuracy, but the accuracy of question identification ranged from 0.56% to 96% across the 14 speakers. The speakers with PD used similar acoustic cues as nondysarthric Cantonese speakers to mark the question-statement contrast, although the contrasts were not observed in all speakers. Listeners mainly used F0 cues at the final syllable for intonation identification. Conclusion: These data contribute to the researchers' understanding of intonation marking in speakers with PD, with specific application to the production and perception of intonation in a lexical tone language.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0468,Effects of Lexical Tone Contour on Mandarin Sentence Intelligibility,"Purpose: This study examined the effects of lexical tone contour on the intelligibility of Mandarin sentences in quiet and in noise. Method: A text-to-speech synthesis engine was used to synthesize Mandarin sentences with each word carrying the original lexical tone, flat tone, or a tone randomly selected from the 4 Mandarin lexical tones. The synthesized speech signals were presented to 11 normal-hearing listeners for recognition in quiet and in speech-shaped noise at 0 dB signal-to-noise ratio. Results: Normal-hearing listeners nearly perfectly recognized the Mandarin sentences produced with modified tone contours in quiet; however, performance declined substantially in noise. Conclusions: Consistent with previous findings to some extent, the present findings suggest that lexical tones are relatively redundant cues for Mandarin sentence intelligibility in quiet and that other cues could compensate for the distorted lexical tone contour. However, in noise, the results provide direct evidence that lexical tone contour is important for the recognition of Mandarin sentences.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0469,Development of Spatial Release From Masking in Mandarin-Speaking Children With Normal Hearing,"Purpose: This study investigated the development of spatial release from masking in children using closed-set Mandarin disyllabic words and monosyllabic words carrying lexical tones as test stimuli and speech spectrum-weighted noise as a masker. Method: Twenty-six children ages 4-9 years and 12 adults, all with normal hearing, participated in speech recognition tests under 2 conditions: (a) speech and noise spatially mixed and presented from the front (NF), and (b) speech presented from the front with noise spatially separated and presented from the side (NS) with different signal-to-noise ratios (SNRs). Performance-SNR psychometric functions were obtained that generated the SNR for a 50% correct score (SNR-50%) as the outcome measure. Results: In the child participants, SNR-50% improved with age in NS but not NF. The difference in SNR-50% between NS and NF-the spatial release from masking (SRM)-increased with age with an average improvement of 0.1-0.15 dB per month. Conclusions: SRM has a long developmental time, at least up to 9 years of age, which is significantly longer than some previous developmental studies have suggested. The child participants had not yet reached the adult SRM performance level. SRM is a potential clinical measure to reflect the maturation of spatial auditory processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0470,Cues for Lexical Tone Perception in Children: Acoustic Correlates and Phonetic Context Effects,"Purpose: The authors investigated the effects of acoustic cues (i.e., pitch height, pitch contour, and pitch onset and offset) and phonetic context cues (i.e., syllable onsets and rimes) on lexical tone perception in Cantonese-speaking children. Method: Eight minimum pairs of tonal contrasts were presented in either an identical phonetic context or in different phonetic contexts (different syllable onsets and rimes). Children were instructed to engage in tone identification and tone discrimination. Results: Cantonese children attended to pitch onset in perceiving similarly contoured tones and attended to pitch contour in perceiving different-contoured tones. There was a decreasing level of tone discrimination accuracy, with tone perception being easiest for same rime-different syllable onset, more difficult for different rime-same syllable onset, and most difficult for different rime-different syllable onset phonetic contexts. This pattern was observed in tonal contrasts in which the member tones had the same contour but not in ones in which the member tones had different contours. Conclusion: These findings suggest that in addition to pitch contour, the pitch onset is another important acoustic cue for tone perception. The relative importance of acoustic cues for tone perception is phonetically context dependent. These findings are discussed with reference to a newly modified TRACE model for tone languages (TTRACE).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0471,On Older Listeners' Ability to Perceive Dynamic Pitch,"Purpose: Natural speech comes with variation in pitch, which serves as an important cue for speech recognition. The present study investigated older listeners' dynamic pitch perception with a focus on interindividual variability. In particular, we asked whether some of the older listeners' inability to perceive dynamic pitch stems from the higher susceptibility to the interference from formant changes. Method: A total of 22 older listeners and 21 younger controls with at least near-typical hearing were tested on dynamic pitch identification and discrimination tasks using synthetic monophthong and diphthong vowels. Results: The older listeners' ability to detect changes in pitch varied substantially, even when musical and linguistic experiences were controlled. The influence of formant patterns on dynamic pitch perception was evident in both groups of listeners. Overall, strong pitch contours (i.e., more dynamic) were perceived better than weak pitch contours (i.e., more monotonic), particularly with rising pitch patterns. Conclusions: The findings are in accordance with the literature demonstrating some older individuals' difficulty perceiving dynamic pitch cues in speech. Moreover, they suggest that this problem may be prominent when the dynamic pitch is carried by natural speech and when the pitch contour is not strong.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0472,Mandarin Lexical Tone Acquisition in Cochlear Implant Users With Prelingual Deafness: A Review,"Purpose: The purpose of this review article is to synthesize evidence from the fields of developmental linguistics and cochlear implant technology relevant to the production and perception of Mandarin lexical tone in cochlear implant users with prelingual deafness. The aim of this review was to identify potential factors that determine outcomes for tonal-language speaking cochlear implant users and possible directions for further research. Method: A computerized database search of MEDLINE, CINAHL, Academic Search Premier, Web of Science, and Google Scholar was undertaken in June and July 2014. Search terms used were lexical tone AND tonal language, speech development AND/OR speech production AND/OR speech perception AND cochlear implants, and pitch perception AND cochlear implants, anywhere in the title or abstract. Conclusion: Despite the demonstrated limitations of pitch perception in cochlear implant users, there is some evidence that typical production and perception of lexical tone is possible by cochlear implant users with prelingual deafness. Further studies are required to determine the factors that contribute to better outcomes to inform rehabilitation processes for cochlear implant users in tonal-language environments.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0473,Prosody and Spoken Word Recognition in Early and Late Spanish-English Bilingual Individuals,"Purpose: This study was conducted to compare the influence of word properties on gated single-word recognition in monolingual and bilingual individuals under conditions of native and nonnative accent and to determine whether wordform prosody facilitates recognition in bilingual individuals. Method: Word recognition was assessed in monolingual and bilingual participants when English words were presented with English and Spanish accents in 3 gating conditions: onset only, onset plus prosody/word length only, and onset plus prosody. Word properties were quantified to assess their influence on word recognition in the onset-only condition. Results: Word recognition speed was proportional to language experience. In the onset-only condition, only word frequency facilitated word recognition across groups. Addition of duration information or prosodic word form did not facilitate word recognition in bilingual individuals the way it did in monolingual individuals. For the bilingual groups, Spanish accent significantly facilitated recognition in the presence of prosodic information. Word attributes were far more consequential in the English accent than in the Spanish accent condition. Conclusions: Word rhyme information, word properties, and accent affect gated word recognition differently in monolingual and bilingual individuals. Top-down strategies emanating from word properties that may facilitate single-word recognition are experience and context dependent and become less available in the presence of a nonnative accent.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0474,Tone Attrition in Mandarin Speakers of Varying English Proficiency,"Purpose: The purpose of this study was to determine whether the degree of dominance of Mandarin-English bilinguals' languages affects phonetic processing of tone content in their native language, Mandarin. Method: We tested 72 Mandarin-English bilingual college students with a range of language-dominance profiles in the 2 languages and ages of acquisition of English. Participants viewed 2 photographs at a time while hearing a familiar Mandarin word referring to 1 photograph. The names of the 2 photographs diverged in tone, vowels, or both. Word recognition was evaluated using clicking accuracy, reaction times, and an online recognition measure (gaze) and was compared in the 3 conditions. Results: Relative proficiency in English was correlated with reduced word recognition success in tone-disambiguated trials, but not in vowel-disambiguated trials, across all 3 dependent measures. This selective attrition for tone content emerged even though all bilinguals had learned Mandarin from birth. Lengthy experience with English thus weakened tone use. Conclusions: This finding has implications for the question of the extent to which bilinguals' 2 phonetic systems interact. It suggests that bilinguals may not process pitch information language-specifically and that processing strategies from the dominant language may affect phonetic processing in the nondominant language-even when the latter was learned natively.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0475,Processing of Acoustic Cues in Lexical-Tone Identification by Pediatric Cochlear-Implant Recipients,"Purpose: The objective was to investigate acoustic cue processing in lexical-tone recognition by pediatric cochlear-implant (CI) recipients who are native Mandarin speakers. Method: Lexical-tone recognition was assessed in pediatric CI recipients and listeners with normal hearing (NH) in 2 tasks. In Task 1, participants identified naturally uttered words that were contrastive in lexical tones. For Task 2, a disyllabic word (yanjing) was manipulated orthogonally, varying in fundamental-frequency (F0) contours and duration patterns. Participants identified each token with the second syllable jing pronounced with Tone 1 (a high level tone) as eyes or with Tone 4 (a high falling tone) as eyeglasses. Results: CI participants' recognition accuracy was significantly lower than NH listeners' in Task 1. In Task 2, CI participants' reliance on F0 contours was significantly less than that of NH listeners; their reliance on duration patterns, however, was significantly higher than that of NH listeners. Both CI and NH listeners' performance in Task 1 was significantly correlated with their reliance on F0 contours in Task 2. Conclusion: For pediatric CI recipients, lexical-tone recognition using naturally uttered words is primarily related to their reliance on F0 contours, although duration patterns may be used as an additional cue.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0476,Effect of Linguistic and Musical Experience on Distributional Learning of Nonnative Lexical Tones,"Purpose: Evidence suggests that extensive experience with lexical tones or musical training provides an advantage in perceiving nonnative lexical tones. This investigation concerns whether such an advantage is evident in learning nonnative lexical tones based on the distributional structure of the input. Method: Using an established protocol, distributional learning of lexical tones was investigated with tone language (Mandarin) listeners with no musical training (Experiment 1) and nontone language (Australian English) listeners with musical training (Experiment 2). Within each experiment, participants were trained on a bimodal (2-peak) or a unimodal (single peak) distribution along a continuum spanning a Thai lexical tone minimal pair. Discrimination performance on the target minimal pair was assessed before and after training. Results: Mandarin nonmusicians exhibited clear distributional learning (listeners in the bimodal, but not those in the unimodal condition, improved significantly as a function of training), whereas Australian English musicians did not (listeners in both the bimodal and unimodal conditions improved as a function of training). Conclusions: Our findings suggest that veridical perception of lexical tones is not sufficient for distributional learning of nonnative lexical tones to occur. Rather, distributional learning appears to be modulated by domain-specific pitch experience and is constrained possibly by top-down interference.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0477,Assessing the Importance of Lexical Tone Contour to Sentence Perception in Mandarin-Speaking Children With Normal Hearing,"Purpose: The aim of the present study was to evaluate the influence of lexical tone contour and age on sentence perception in quiet and in noise conditions in Mandarin-speaking children ages 7 to 11 years with normal hearing. Method: Test materials were synthesized Mandarin sentences, each word with a manipulated lexical contour, that is, normal contour, flat contour, or a tone contour randomly selected from the four Mandarin lexical tone contours. A convenience sample of 75 Mandarin-speaking participants with normal hearing, ages 7, 9, and 11 years (25 participants in each age group), was selected. Participants were asked to repeat the synthesized speech in quiet and in speech spectrum-shaped noise at 0 dB signal-to-noise ratio. Results: In quiet, sentence recognition by the 11-year-old children was similar to that of adults, and misrepresented lexical tone contours did not have a detrimental effect. However, the performance of children ages 9 and 7 years was significantly poorer. The performance of all three age groups, especially the younger children, declined significantly in noise. Conclusions: The present research suggests that lexical tone contour plays an important role in Mandarin sentence recognition, and misrepresented tone contours result in greater difficulty in sentence recognition in younger children. These results imply that maturation and/or language use experience play a role in the processing of tone contours for Mandarin speech understanding, particularly in noise.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0478,"Mandarin-Speaking, Kindergarten-Aged Children With Cochlear Implants Benefit From Natural F0 Patterns in the Use of Semantic Context During Speech Recognition","Purpose: The purpose of this study was to investigate the extent to which semantic context and F-0 contours affect speech recognition by Mandarin-speaking, kindergarten-aged children with cochlear implants (CIs). Method: The experimental design manipulated two factors, that is, semantic context, by comparing the intelligibility of normal sentence versus word list, and F-0 contours, by comparing the intelligibility of utterances with natural versus flat F-0 patterns. Twenty-two children with CIs completed a speech recognition test. Results: Children with CIs could use both semantic context and F-0 contours to assist speech recognition. Furthermore, natural F-0 patterns provided extra benefit when semantic context was present than when it was absent. Conclusion: Dynamic F-0 contours play an important role in speech recognition by Mandarin-speaking children with CIs despite the well-known limitation of CI devices in extracting F-0 information.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0479,The Effects of Acoustic Variation on the Perception of Lexical Tone in Cantonese-Speaking Congenital Amusics,"Purpose: Congenital amusia is an inborn neurogenetic disorder of fine-grained pitch processing. This study attempted to pinpoint the impairment mechanism of speech processing in tonal language speakers with amusia. We designed a series of perception tasks aiming at selectively probing low-level pitch processing and relatively high-level phonological processing of lexical tones, with an aim to illuminate the deficiency mechanism underlying tone perception in amusia. Method: Sixteen Cantonese speaking amusics and 16 matched controls were tested on the effects of acoustic (talker/syllable) variations on the identification and discrimination of Cantonese tones in two conditions. In the low-variation condition, tones were always associated with the same talker or syllable; in the high-variation condition, tones were associated with either different talkers (with the syllable controlled) or different syllables (with the talker controlled). Results: Largely similar results were obtained in talker and syllable variation conditions. Amusics exhibited overall poorer performance than controls in tone identification. Although amusics also demonstrated poorer performance in tone discrimination, the group difference was more obvious in low-variation conditions, where more acoustic constancy was provided. Besides, controls exhibited a greater increase in discrimination sensitivity from high- to low-variation conditions, implying a stronger benefit of acoustic constancy. Conclusions: The findings suggested that amusics' lexical tone perception abilities, in terms of both low-level pitch processing and high-level phonological processing, as measured in low- and high-variation conditions, are impaired. Importantly, amusics were more impaired in taking advantage of low acoustic variation contexts and thus less efficiently sharpened their perception of tones when perceptual anchors in talker/syllable were provided, suggesting a possible ""anchoring deficit"" in congenital amusia.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0480,Suprasegmental Features Are Not Acquired Early: Perception and Production of Monosyllabic Cantonese Lexical Tones in 4-to 6-Year-Old Preschool Children,"Purpose: Previous studies reported that children acquire Cantonese tones before 3 years of age, supporting the assumption in models of phonological development that suprasegmental features are acquired rapidly and early in children. Yet, recent research found a large disparity in the age of Cantonese tone acquisition. This study investigated Cantonese tone development in 4- to 6-year-old children. Method: Forty-eight 4- to 6-year-old Cantonese-speaking children and 28 mothers of the children labeled 30 pictures representing familiar words in the 6 tones in a picture-naming task and identified pictures representing words in different Cantonese tones in a picture-pointing task. To control for lexical biases in tone assessment, tone productions were low-pass filtered to eliminate lexical information. Five judges categorized the tones in filtered stimuli. Tone production accuracy, tone perception accuracy, and correlation between tone production and perception accuracy were examined. Results: Children did not start to produce adultlike tones until 5 and 6 years of age. Four-year-olds produced none of the tones with adultlike accuracy. Five-and 6-year-olds attained adultlike productions in 2 (T5 and T6) to 3 (T4, T5, and T6) tones, respectively. Children made better progress in tone perception and achieved higher accuracy in perception than in production. However, children in all age groups perceived none of the tones as accurately as adults, except that T1 was perceived with adultlike accuracy by 6-year-olds. Only weak association was found between children's tone perception and production accuracy. Conclusions: Contradicting to the long-held assumption that children acquire lexical tone rapidly and early before the mastery of segmentals, this study found that 4- to 6-yearold children have not mastered the perception or production of the full set of Cantonese tones in familiar monosyllabic words. Larger development was found in children's tone perception than tone production. The higher tone perception accuracy but weak correlation between tone perception and production abilities in children suggested that tone perception accuracy is not sufficient for children's tone production accuracy. The findings have clinical and theoretical implications.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0481,Testing the Hyperarticulation and Prosodic Hypotheses of Child-Directed Speech: Insights From the Perceptual and Acoustic Characteristics of Child-Directed Cantonese Tones,"Purpose: The function of child-directed speech has been debated for decades. This study examined the perceptual and acoustic characteristics of child- and adult-directed Cantonese tones to test the hyperarticulation and prosodic hypotheses that have been proposed to account for the acoustic modifications in child-directed speech. Method: Sixty-two mother-child dyads participated in the study. The mothers verbally labeled 30 pictures in monosyllabic isolated words and in the final position of a carrier sentence to the experimenter and their 1- to 5-year-old children. The 8,634 adult-and child-directed productions were low-pass filtered to eliminate lexical information and presented to 5 judges for tone identification. Acoustic analysis was performed on the productions. Results: Acoustically, child-directed tones were produced with an elevated pitch, and the pitch level decreased as the child's age increased. Acoustic contrasts between phonetically similar and more confusing tones were not enhanced in child-directed speech, and unexpectedly, child-directed tones were identified with a lower accuracy than adult-directed tones. The perceptual errors of child-directed tones mirrored the errors found in identifying tones excised from sentence-final position, which had a pitch-lowering effect on the tones. The lower perceptual accuracy, the lack of enhanced acoustic contrasts in confusing tone pairs, and the similarities in the error patterns in identifying tones in child-directed speech and tones in utterance-final position suggest that the acoustic modifications in child-directed tones are prosodic effects serving pragmatic purposes. Conclusion: The findings reject the hyperarticulation hypothesis and support the prosodic hypothesis of child-directed speech.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0482,The Acquisition of Mandarin Tonal Processes by Children With Cochlear Implants,"Purpose: Children with cochlear implants (CIs) face challenges in acquiring tonal languages, as CIs do not efficiently code pitch information. Mandarin is a tonal language with lexical tones and tonal processes such as neutral tone and tone sandhi, exhibiting contextually conditioned tonal realizations. Previous studies suggest that early implantation and long CI experience facilitate the acquisition of lexical tones by children with CIs. However, there is lack of acoustic evidence on children's tonal productions demonstrating that this is the case, and it is unclear whether and how children with CIs are able to acquire contextual tones. This study therefore examined the acoustic realization of both lexical tones and contextual tones as produced by children fitted with CIs, exploring the potential effects of age at implantation and length of CI experience on their acquisition of the Mandarin tonal system. Method: Seventy-two Mandarin-learning preschoolers with CIs, varying in age at implantation (13-42 months) and length of CI experience (2-49 months), and 44 normal hearing 3-year-old controls were recruited. Tonal productions were elicited from both groups using picture-naming tasks and acoustically compared. Results: Only the early implanted group (i.e., implanted before the age of 2 years) produced normal-like lexical tones and generally had contextual tones approximating those of the normal-hearing children. The other children, including those with longer CI experience, did not have typical tonal productions; their pitch patterns for lexical tones tended to be flatter, and contextual tone productions were unchanged across tonal contexts. Conclusion: Children with CIs face challenges in acquiring Mandarin tones, but early implantation may help them to develop normal-like lexical tone categories, which further facilitates their implementation of contextual tones.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0483,On the Relationship Between General Auditory Sensitivity and Speech Perception: An Examination of Pitch and Lexical Tone Perception in 4-to 6-Year-Old Children,"Purpose: Theoretical models and substantial research have proposed that general auditory sensitivity is a developmental foundation for speech perception and language acquisition. Nonetheless, controversies exist about the effectiveness of general auditory training in improving speech and language skills. This research investigated the relationships among general auditory sensitivity, phonemic speech perception, and word-level speech perception via the examination of pitch and lexical tone perception in children. Method: Forty-eight typically developing 4- to 6-yearold Cantonese-speaking children were tested on the discrimination of the pitch patterns of lexical tones in synthetic stimuli, discrimination of naturally produced lexical tones, and identification of lexical tone in familiar words. Results: The findings revealed that accurate lexical tone discrimination and identification did not necessarily entail the accurate discrimination of nonlinguistic stimuli that followed the pitch levels and pitch shapes of lexical tones. Although pitch discrimination and tone discrimination abilities were strongly correlated, accuracy in pitch discrimination was lower than that in tone discrimination, and nonspeech pitch discrimination ability did not precede linguistic tone discrimination in the developmental trajectory. Conclusions: Contradicting the theoretical models, the findings of this study suggest that general auditory sensitivity and speech perception may not be causally or hierarchically related. The finding that accuracy in pitch discrimination is lower than that in tone discrimination suggests that comparable nonlinguistic auditory perceptual ability may not be necessary for accurate speech perception and language learning. The results cast doubt on the use of nonlinguistic auditory perceptual training to improve children's speech, language, and literacy abilities.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0484,From Cantonese Lexical Tone Awareness to Second Language English Vocabulary: Cross-Language Mediation by Segmental Phonological Awareness,"Purpose: Cantonese lexical tone awareness is closely associated with 1st language Cantonese vocabulary knowledge, but its role in 2nd language English vocabulary knowledge was unclear. We addressed this issue by investigating whether and, if so, how Cantonese lexical tone awareness contributes to English expressive vocabulary knowledge in Hong Kong Cantonese-English bilingual children. Method: A sample of 112 Hong Kong Cantonese-English bilingual 2nd graders were tested on Cantonese lexical tone awareness, English lexical stress sensitivity, Cantonese-English segmental phonological awareness, and both Cantonese and English expressive vocabulary knowledge. Results: Structural equation modeling analyses revealed that Cantonese lexical tone awareness contributed indirectly to English expressive vocabulary knowledge through English lexical stress sensitivity and Cantonese-English segmental phonological awareness. Conclusion: These results demonstrate the role of Cantonese lexical tone awareness in Cantonese-English bilingual children's English vocabulary knowledge. This also underscores the importance of 1st language suprasegmental phonological awareness in 2nd language expressive vocabulary knowledge.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0485,Vowel and Tone Identification for Mandarin Congenital Amusics: Effects of Vowel Type and Semantic Content,"Purpose: This study aimed to explore the effects of Mandarin congenital amusia with or without lexical tone deficit (i.e., tone agnosia and pure amusia) on Mandarin vowel and tone identification in different types of vowels (e.g., monophthong, diphthongs, and triphthongs) embedded in consonant-vowel contexts with and without semantic content. Method: Thirteen pure amusics (i.e., amusics with normal lexical processing), 5 tone agnosics (i.e., with lexical tone deficit), and 12 controls were screened with Montreal Battery of Evaluation of Amusia and lexical tone tests (Nan et al., 2010; Peretz et al., 2003). Vowel-plus-tone identification tasks with the factors of vowel type and syllables with and without semantic content (e.g., real and nonsense words) were examined among the 3 groups, and identification scores were calculated in 3 formats: vowel-plus-tone identification, vowel identification, and tone identification. Results: Tone agnosics showed significantly poorer performances on identifications of vowel, tone, and vowel plus tone across monophthongs, diphthongs, and triphthongs in both real and nonsense words compared to pure amusics and controls. Their deficits were similar across the 3 types of vowels, while the deficit on vowel-plus-tone identification was more severe in nonsense words than in real words. On the other hand, pure amusics performed similarly with controls across all these conditions. Conclusions: Tone agnosia might affect both musical pitch and phonological processing, resulting in deficits in lexical tone and vowel perception. On the contrary, pure amusics's effect is primarily on musical pitch perception but not on lexical tone or phonemic deficit. Vowel type did not affect speech deficits for tone agnosics, while they relied more on semantic content as a compensation.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0486,Effects of Fundamental Frequency Contours on Sentence Recognition in Mandarin-Speaking Children With Cochlear Implants,"Purpose: Fundamental frequency (F0) is the primary acoustic cue for lexical tone perception in tonal languages but is processed in a limited way in cochlear implant (CI) systems. The aim of this study was to evaluate the importance of F0 contours in sentence recognition in Mandarin-speaking children with CIs and find out whether it is similar to/different from that in age-matched normal-hearing (NH) peers. Method: Age-appropriate sentences, with F0 contours manipulated to be either natural or flattened, were randomly presented to preschool children with CIs and their age-matched peers with NH under three test conditions: in quiet, in white noise, and with competing sentences at 0 dB signal-to-noise ratio. Results: The neutralization of F0 contours resulted in a significant reduction in sentence recognition. While this was seen only in noise conditions among NH children, it was observed throughout all test conditions among children with CIs. Moreover, the F0 contour-induced accuracy reduction ratios (i.e., the reduction in sentence recognition resulting from the neutralization of F0 contours compared to the normal F0 condition) were significantly greater in children with CIs than in NH children in all test conditions. Conclusions: F0 contours play a major role in sentence recognition in both quiet and noise among pediatric implantees, and the contribution of the F0 contour is even more salient than that in age-matched NH children. These results also suggest that there may be differences between children with CIs and NH children in how F0 contours are processed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0487,Bimodal Benefits Revealed by Categorical Perception of Lexical Tones in Mandarin-Speaking Kindergarteners With a Cochlear Implant and a Contralateral Hearing Aid,"Purpose: Pitch reception poses challenges for individuals with cochlear implants (CIs), and adding a hearing aid (HA) in the nonimplanted ear is potentially beneficial. The current study used fine-scale synthetic speech stimuli to investigate the bimodal benefit for lexical tone categorization in Mandarin-speaking kindergarteners using a CI and an HA in opposite ears. Method: The data were collected from 16 participants who were required to complete two classical tasks for speech categorical perception (CP) with CI + HA device condition and CI alone condition. Linear mixed-effects models were constructed to evaluate the identification and discrimination scores across different device conditions. Results: The bimodal kindergarteners showed CP for the continuum varying from Mandarin Tone 1 and Tone 2. Moreover, the additional acoustic information from the contralateral HA contributes to improved lexical tone categorization, with a steeper slope, a higher discrimination score of between-category stimuli pair, and an improved peakedness score (i.e., an increased benefit magnitude for discriminations of between-category over within-category pairs) for the CI + HA condition than the CI alone condition. The bimodal kindergarteners with better residual hearing thresholds at 250 Hz level in the nonimplanted ear could perceive lexical tones more categorically. Conclusion: The enhanced CP results with bimodal listening provide clear evidence for the clinical practice to fit a contralateral HA in the nonimplanted ear in kindergarteners with unilateral CIs with direct benefits from the low-frequency acoustic hearing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0488,Aging Effect on Mandarin Chinese Vowel and Tone Identification in Six-Talker Babble,"Purpose: The purpose of this study was to measure Mandarin Chinese vowel-plus-tone identification in quiet and noise for younger and older listeners. Method: Two types of noise served as the masker, namely, six-talker babble and babble-modulated noise, at two signal-to-noise ratios (SNRs) of -4 and -8 dB. Fourteen listeners from both age groups were recruited, and three sets of data analyses were conducted: the identification of vowel plus tone, the identification of vowel, and the identification of tone. Results: Younger listeners outperformed older listeners in all listening conditions, whereas the younger-older listener difference became greater in noise than in quiet, indicating a more detrimental effect of noise for older listeners than for younger listeners. In addition, vowel identification showed slightly better scores than tone identification in noise, suggesting that noise appeared to affect tone perception more negatively than vowel perception in Mandarin Chinese. At -4 dB SNR, there was a significantly greater amount of informational masking (IM) and a greater amount of energetic masking (EM) for older listeners than for younger listeners. At -8 dB SNR, there was a greater amount of EM for older listeners than for younger listeners but with no group difference in the amount of IM. Conclusion: These results suggest that older listeners received a more negative impact of noise for Mandarin Chinese phonemic and tone recognition and had a larger amount of IM or EM from competing speech noise than younger listeners, depending on the SNR.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0489,Pitch Variation in Children With Childhood Apraxia of Speech: Preliminary Findings,"Introduction: Pitch variation, which refers to one's ability to vary fundamental frequency (F0) within or between syllables when speaking, has not been investigated in children with childhood apraxia of speech (CAS). However, pitch variation plays an important role in tone languages, as varying F0 patterns communicate different lexical meanings. This study investigated pitch variation abilities in individuals with CAS via the tone-sequencing tasks (TSTs), focusing on task performance and the effects of syllable structure, lexical status, and tones. Method: Three Cantonese-speaking children with CAS (aged 3;7-5;8 [years;months]) and six children without CAS participated in the study. Children without CAS were divided into two control groups, comprising those with speech and/or language impairment or typical development. TSTs consisted of 56 sets of five repetitions of stimuli. The stimuli varied in syllable structure, lexical status, and tones. Percentage of tones correct (PTC), consistency scores, F0 values, and acoustic repetition duration were measured. Results: The CAS group performed more poorly than the control groups on the TST with respect to tone accuracy, consistency, and repetition duration. No interaction effects between group and syllable structure or group and lexical status were found. No significant difference was found on F0 values across time between Tone 1 and Tone 2 syllables in the CAS group. However, interaction effects between group and time points of F0 values on Tone 2 syllables were found. Discussion: The results suggest that children with CAS have difficulty with pitch variation, which was revealed on the TST with respect to tone accuracy, consistency, and repetition duration. Moreover, children with CAS have difficulty in varying F0 values to produce high-rising tones and tend to use high-level tones to substitute. Clinically, the TST may be useful to assist in the diagnosis of CAS. Isolated vowel stimuli may be useful to test young children or children with severe impairment. Future investigations and development of a normed tool for children with CAS are suggested.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0490,Deficits in Processing of Lexical Tones in Mandarin-Speaking Children With Developmental Language Disorder: Electrophysiological Evidence,"Purpose: This study explored the neural marker indexing deficits in discriminating lexical tone changes in Mandarin speaking children with developmental language disorders (DLDs) using mismatch negativity, an event-related potential component for auditory change detection. Mandarin has four lexical tones characterized by a high-level tone (T1), high rising tone (T2), low-dipping tone (T3), and high-falling tone (T4), in which the T2/T3 contrast is acoustically less discriminable in developmental groups. Therefore, this study further examined how deficits in children with DLD would vary with tonal contrasts? acoustic saliency. Method: Event-related potentials were measured using the multideviant oddball paradigm described by Lee et al. (2012), who used Mandarin syllables [i] in T3 as the standard sound (80%), T1 as the large deviant (10%), and T2 as the small deviant (10%). Twelve children with DLD aged between 4 and 6 years participated in this study, and 12 age-matched children with typical development were selected from the data set of Lee et al. (2012) as the controls. Results: The T1/T3 change elicited adultlike mismatch negativity in both the DLD and control groups, while no group difference was revealed. The T2/T3 change elicited a robust positive mismatch response (P-MMR) in children with DLD, while the P-MMR was less significant in the control group. The group comparisons revealed a larger P-MMR in children with DLD than in the control group. Furthermore, children with lower scores in language assessments tend to reveal larger P-MMRs. Conclusions: This study demonstrated that deficits in children with DLD in discriminating subtle lexical tone changes reflect greater positivity of P-MMR to T2/T3 change. This implies that MMR to T2/T3 may serve as a neural marker for evaluating language delay in preschoolers.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0491,Categorical Perception of Chinese Lexical Tones by Late Second Language Learners With High Proficiency: Behavioral and Electrophysiological Measures,"Purpose: Although acquisition of Chinese lexical tones by second language (L2) learners has been intensively investigated, very few studies focused on categorical perception (CP) of lexical tones by highly proficient L2 learners. This study was designed to address this issue with behavioral and electrophysiological measures. Method: Behavioral identification and auditory event-related potential (ERP) components for speech discrimination, including mismatch negativity (MMN), N2b, and P3b, were measured in 23 native Korean speakers who were highly proficient late L2 learners of Chinese. For the ERP measures, both passive and active listening tasks were administered to examine the automatic and attentioncontrolled discriminative responses to within- and across-category differences for carefully chosen stimuli from a lexical tone continuum. Results: The behavioral task revealed native-like identification function of the tonal continuum. Correspondingly, the active oddball task demonstrated larger P3b amplitudes for the across-category than within-category deviants in the left recording site, indicating clear CP of lexical tones in the attentive condition. By contrast, similar MMN responses in the right recording site were elicited by both the across- and within-category deviants, indicating the absence of CP effect with automatic phonological processing of lexical tones at the pre-attentive stage even in L2 learners with high Chinese proficiency. Conclusion: Although behavioral data showed clear evidence of categorical perception of lexical tones in proficient L2 learners, ERP measures from passive and active listening tasks demonstrated fine-grained sensitivity in terms of response polarity, latency, and laterality in revealing different aspects of auditory versus linguistic processing associated with speech decoding by means of largely implicit native language acquisition versus effortful explicit L2 learning.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0492,Characteristics of Effective Auditory Training: Implications From Two Training Programs That Successfully Trained Nonnative Cantonese Tone Identification in Monolingual Mandarin and Bilingual Mandarin-Taiwanese Tone Speakers,"Purpose: Auditory training is important in pedagogical and clinical settings. In search of a more effective perceptual program for training new suprasegmental categories, this study examined the effect of two auditory programs that incorporated five elements that have previously been identified to be effective for training nonnative segmental and suprasegmental speech sounds on the identification of a complex foreign lexical tone system (Cantonese) that contrasts both pitch shapes and pitch heights. To investigate the training outcomes in learners with different tonal systems, monolingual Mandarin-speaking learners who have a smaller native tonal system that contrasts pitch shapes only and bilingual Mandarin-Taiwanese-speaking learners who have a larger native tonal system that contrasts both pitch shapes and pitch heights were recruited for training. Method: Thirty Mandarin-speaking monolinguals and 33 Mandarin-Taiwanese-speaking bilinguals in Taiwan were randomly assigned to two training programs, one with different tones and the other with the same tone preceding the target words in the same training block, and received six 90-min training sessions within 2 weeks. They took a Cantonese Tone Identification Test before training and after each training session. Twenty Cantonese native speakers in Hong Kong served as the reference group and took the same Cantonese Tone Identification Test. Results: The two training programs were equally effective. Before training, the monolinguals performed poorer than the bilinguals. After training, the monolinguals and bilinguals in both training programs identified the six Cantonese tones in new words, new utterances, and novel speakers with comparable results, and their overall accuracy did not differ from that of the Cantonese native speakers. Conclusions: Though learners with a larger and more complex native tonal system have initial advantage in learning nonnative tones, the intensive high- variability full-set training programs that provide explicit phonetic instruction and contrastive feedback of nonnative tones effectively promote nonnative tone acquisition in learners of different tone languages. The findings revealed factors affecting nonnative tone acquisition in tone speakers. The design of the two programs can be adopted in future programs for effective auditory training of segmental and suprasegmental speech sounds.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0493,High-Variability Phonetic Training Benefits Lexical Tone Perception: An Investigation on Mandarin-Speaking Pediatric Cochlear Implant Users,"Purpose: Lexical tone perception is known to be persistently difficult for individuals with cochlear implants (CIs). The purpose of this study was to evaluate the efficacy of high variability phonetic training (HVPT) in improving Mandarin tone perception for native-speaking children with CIs. Method: A total of 28 Mandarin-speaking pediatric CI recipients participated in the study. Half of the children with CIs received a five-session HVPT within a period of 3 weeks. Identification and discrimination of lexical tones produced by familiar talkers (used during training) and novel talkers (not used during training) were measured before, immediately after, and 10 weeks after training termination. The other half untrained children served as control for the identical pre-and posttests. Results: Lexical tone perception significantly improved in both trained identification task and untrained discrimination task for the trainees. There was also a significant effect in transfer of learning to perceiving tones produced by novel talkers. Moreover, training-induced gains were retained for up to 10 weeks after training. By comparison, no significant pre-post changes were observed in the control group. Conclusion: The results provide the first systematical assessment for the efficacy of the HVPT protocol for Mandarin-speaking pediatric CI users with congenital hearing loss, which supports the clinical utility of intensive short-term HVPT in these children's rehabilitative regimens.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0494,Auditory-Motor Mapping Training Facilitates Speech and Word Learning in Tone Language-Speaking Children With Autism: An Early Efficacy Study,"Purpose: It has been reported that tone language-speaking children with autism demonstrate speech-specific lexical tone processing difficulty, although they have intact or even better-than-normal processing of nonspeech/melodic pitch analogues. In this early efficacy study, we evaluated the therapeutic potential of Auditory-Motor Mapping Training (AMMT) in facilitating speech and word output for Mandarin-speaking nonverbal and low-verbal children with autism, in comparison with a matched non-AMMT-based control treatment. Method: Fifteen Mandarin-speaking nonverbal and low-verbal children with autism spectrum disorder participated and completed all the AMMT-based treatment sessions by intoning (singing) and tapping the target words delivered via an app, whereas another 15 participants received control treatment. Generalized linear mixed-effects models were created to evaluate speech production accuracy and word production intelligibility across different groups and conditions. Results: Results showed that the AMMT-based treatment provided a more effective training approach in accelerating the rate of speech (especially lexical tone) and word learning in the trained items. More importantly, the enhanced training efficacy on lexical tone acquisition remained at 2 weeks after therapy and generalized to untrained tones that were not practiced. Furthermore, the low-verbal participants showed higher improvement compared to the nonverbal participants. Conclusions: These data provide the first empirical evidence for adopting the AMMT-based training to facilitate speech and word learning in Mandarin-speaking nonverbal and low-verbal children with autism. This early efficacy study holds promise for improving lexical tone production in Mandarin-speaking children with autism but should be further replicated in larger scale randomized studies. Supplemental Material: https://doi.org/10.23641/asha.16834627",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0495,The Perception of Lexical Tone and Intonation in Whispered Speech by Mandarin-Speaking Congenital Amusics,"Purpose: A fundamental feature of human speech is variation, including the manner of phonation, as exemplified in the case of whispered speech. In this study, we employed whispered speech to examine an unresolved issue about congenital amusia, a neurodevelopmental disorder of musical pitch processing, which also affects speech pitch processing such as lexical tone and intonation perception. The controversy concerns whether amusia is a pitch-processing disorder or can affect speech processing beyond pitch. Method: We examined lexical tone and intonation recognition in 19 Mandarin-speaking amusics and 19 matched controls in phonated and whispered speech, where fundamental frequency (f(o)) information is either present or absent. Results: The results revealed that the performance of congenital amusics was inferior to that of controls in lexical tone identification in both phonated and whispered speech. These impairments were also detected in identifying intonation (statements/questions) in phonated and whispered modes. Across the experiments, regression models revealed that f(o) and non-f(o) (duration, intensity, and formant frequency) acoustic cues predicted tone and intonation recognition in phonated speech, whereas non-f(o) cues predicted tone and intonation recognition in whispered speech. There were significant differences between amusics and controls in the use of both f(o) and non-f(o) cues. Conclusion: The results provided the first evidence that the impairments of amusics in lexical tone and intonation identification prevail into whispered speech and support the hypothesis that the deficits of amusia extend beyond pitch processing.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0496,Research Article Impaired Categorical Perception of Speech Sounds Under the Backward Masking Condition in Adults Who Stutter,"Purpose: Evidence increasingly indicates that people with developmental stuttering have auditory perception deficits. Our previous research has indicated similar but slower performance in categorical perception of the speech sounds under the quiet condition in children who stutter and adults who stutter (AWS) compared with their typically fluent counterparts. We hypothesized that the quiet condition may not be sufficiently sensitive to reveal subtle perceptual deficiencies in people who stutter. This study examined this hypothesis by testing the categorical perception of speech and nonspeech sounds under backward masking condition (i.e., a noise was presented immediately after the target stimuli). Method: Fifteen Cantonese-speaking AWS and 15 adults who do not stutter (AWNS) were tested on the categorical perception of four stimulus continua, namely, consonant varying in voice onset time (VOT), vowel, lexical tone, and nonspeech, under the backward masking condition using identification and discrimination tasks. Results: AWS demonstrated a broader boundary width than AWNS in the identification task. AWS also exhibited a worse performance than AWNS in the discrimination of between-category stimuli but a comparable performance in the discrimination of within-category stimuli, indicating reduced sensitivity to sounds that belonged to different phonemic categories among AWS. Moreover, AWS showed similar patterns of impaired categorical perception across the four stimulus types, although the boundary location on the VOT continuum occurred at an earlier point in AWS than in AWNS. Conclusions: The findings provide robust evidence that AWS exhibit impaired categorical perception of speech and nonspeech sounds under the backward masking condition. Temporal processing (i.e., VOT manipulation), frequency/ spectral/formant processing (i.e., lexical tone or vowel manipulations), and nonlinguistic pitch processing were all found to be impaired in AWS. Altogether, the findings support the hypothesis that AWS might be less efficient in accessing the phonemic representations when exposed to a demanding listening condition.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0497,Categorical Perception of Lexical Tones in Mandarin-Speaking Seniors,"Purpose: This study aims to investigate the different degeneration processes of categorical perception (CP) of Mandarin lexical tones in the normal aging popu-lation and the pathological aging population with mild cognitive impairment (MCI).Method: In Experiment I, we compared the identification and discrimination of Tone 1 and Tone 2 across young adults, seniors aged 60-65 years, and older seniors aged 75-80 years with normal cognitive abilities. In Experiment II, we compared lexical tone identification and discrimination across young adults, healthy seniors, and age-matched seniors with MCI.Results: In Experiment I, tone perception was intact in seniors aged below 65 years. Those aged above 75 years could also maintain normal tone identifi-cation, whereas they showed poorer tone discrimination correlated with age -related poorer hearing level. In Experiment II, healthy seniors showed normal CP of Mandarin tones. Tone identification was also normal in those with MCI, whereas their tone discrimination had significantly degenerated.Conclusions: In the normal aging population, age-related hearing loss decreased signal audibility, accounting for poorer discrimination of Mandarin lexical tones in seniors above 75 years. In the pathological aging population with MCI, the poorer discrimination of lexical tones may be attributed to the additive effect of age, hearing loss, and cognitive impairment (e.g., impaired working memory and long-term phonological memory). This study uncovered the roles of low-level sensory processing and high-level cognitive processing in lexical tone perception in the Chinese aging population.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0498,Visual-Auditory Integration and High-Variability Speech Can Facilitate Mandarin Chinese Tone Identification,"Purpose: Previous studies have demonstrated that tone identification can be facilitated when auditory tones are integrated with visual information that depicts the pitch contours of the auditory tones (hereafter, visual effect). This study investigates this visual effect in combined visual-auditory integration with high- and low-variability speech and examines whether one's prior tonal-language learning experience shapes the strength of this visual effect. Method: Thirty Mandarin-naive listeners, 25 Mandarin second language learners, and 30 native Mandarin listeners participated in a tone identification task in which participants judged whether an auditory tone was rising or falling in pitch. Moving arrows depicted the pitch contours of the auditory tones. A priming paradigm was used with the target auditory tones primed by four multi-modal conditions: no stimuli (A-V-), visual-only stimuli (A-V+), auditory-only stimuli (A+V-), and both auditory and visual stimuli (A+V+). Results: For Mandarin naive listeners, the visual effect in accuracy produced under the cross-modal integration (A+V+ vs. A+V-) was superior to a unimodal approach (A-V+ vs. A-V-), as evidenced by a higher d prime of A+V+ as opposed to A+V-. However, this was not the case in response time. Additionally, the visual effect in accuracy and response time under the unimodal approach only occurred for high-variability speech, not for low-variability speech. Across the three groups of listeners, we found that the less tonal-language learning experience one had, the stronger the visual effect. Conclusion: Our study revealed the visual-auditory advantage and disadvantage of the visual effect and the joint contribution of visual-auditory integration and high-variability speech on facilitating tone perception via the process of speech symbolization and categorization.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0499,Auditory Discrimination Elicited by Nonspeech and Speech Stimuli in Children With Congenital Hearing Loss,"Purpose: Congenital deafness not only delays auditory development but also hampers the ability to perceive nonspeech and speech signals. This study aimed to use auditory event-related potentials to explore the mismatch negativity (MMN), P3a, negative wave (Nc), and late discriminative negativity (LDN) components in children with and without hearing loss. Method: Nineteen children with normal hearing (CNH) and 17 children with hearing loss (CHL) participated in this study. Two sets of pure tones (1 kHz vs. 1.1 kHz) and lexical tones (/ba2/ vs. /ba4/) were used to examine the auditory discrimination process. Results: MMN could be elicited by the pure tone and the lexical tone in both groups. The MMN latency elicited by nonspeech and speech was later in CHL than in CNH. Additionally, the MMN latency induced by speech occurred later in the left than in the right hemisphere in CNH, and the MMN amplitude elicited by speech in CHL produced a discriminative deficiency compared with that in CNH. Although the P3a latency and amplitude elicited by nonspeech in CHL and CNH were not significantly different, the Nc amplitude elicited by speech performed much lower in CHL than in CNH. Furthermore, the LDN latency elicited by nonspeech was later in CHL than in CNH, and the LDN amplitude induced by speech showed higher dominance in the right hemisphere in both CNH and CHL. Conclusion: By incorporating nonspeech and speech auditory conditions, we propose using MMN, Nc, and LDN as potential indices to investigate auditory perception, memory, and discrimination.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0500,Audiovisual Mandarin Lexical Tone Perception in Quiet and Noisy Contexts: The Influence of Visual Cues and Speech Rate,"Purpose: Armed with the theory of embodied cognition proposing tight interactions between perception, motor, and cognition, this study aimed to test the hypothesis that speech rate-altered Mandarin lexical tone perception in quiet and noisy environments could be affected by the bodily dynamic cross-modal information. Method: Fifty-three adult listeners completed a Mandarin tone perception task with 720 tone stimuli in auditory-only (AO), auditory-facial (AF), and auditory-facial-plus-gestural (AFG) modalities, at fast, normal, and slow speech rates under quiet and noisy conditions. In AF and AFG modalities, both congruent and incongruent audiovisual information were designed and presented. Generalized linear mixed-effects models were constructed to analyze the accuracy of tone perception across different conditions. Results: In Mandarin tone perception, the magnitude of enhancement of AF and AFG cues across three speech rates was significantly higher than that of the AO cue in the adverse context of noise, yet additional metaphoric gestures did not show significant differences from the facial information. Furthermore, the performance of auditory tone perception at the fast speech rate was significantly better than that at the normal speech rate when the inputs were incongruent between auditory and visual channels in quiet. Conclusions: This study provided compelling evidence showing that integrated audiovisual information plays a vital role not only in improving lexical tone perception in noise but also in modulating the effects of speech rate on Mandarin tone perception in quiet for native listeners. Our findings, supporting the theory of embodied cognition, are implicational for speech and hearing rehabilitation among both young and old clinical populations.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0501,Native Language Perceptual Sensitivity Predicts Nonnative Speech Perception Differently in Younger and Older Singaporean Bilinguals,"Purpose: We investigate in this study how individual variability in native language speech perception (termed Perceptual Sensitivity) influences nonnative speech perception in Singaporean Tamil-English bilinguals. Further, we assess if and how contextual and demographic factors influence Perceptual Sensitivity in the acquired languages and if the influence of Perceptual Sensitivity on nonnative speech perception is different across younger and older bilinguals. Method: Perceptual Sensitivity in the native languages was examined by implementing Tamil and English gating tasks in 87 Singaporean Tamil-English bilinguals from two age groups (younger: 19-33 years; older: 55-70 years). Mandarin lexical tone discrimination was implemented as a measure of nonnative speech perception. Results: There was a wide range of variability in Perceptual Sensitivity scores in both languages across both age groups. Perceptual Sensitivity in the first native language (L1 Tamil) was a robust predictor of nonnative speech perception across both age groups, especially for the older bilinguals. However, general intelligence emerged as a stronger predictor than Tamil Perceptual Sensitivity in younger bilinguals. The influence of Tamil Perceptual Sensitivity on lexical tone perception was not tone-specific, supporting a general enhancement of lexical tone perception with better Tamil Perceptual Sensitivity. There was an influence of demographic factors on English Perceptual Sensitivity in older bilinguals, but not for Tamil and not in younger bilinguals. Conclusions: Our findings corroborate with previous studies in showing that native language Perceptual Sensitivity is positively associated with and predicts nonnative speech perception in younger and older adulthood regardless of language similarity but to varying degrees. Specifically, the influence of Perceptual Sensitivity on nonnative speech perception is stronger in older adulthood, suggesting a possible shift in reliance on crystallized language knowledge with age. Proficiency and use, among other demographic and language variables, do not appear to influence L1 Perceptual Sensitivity in a lesser used language (Tamil) as significantly as previously assumed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0502,Development of Achieving Constancy in Lexical Tone Identification With Contextual Cues,"Purpose: The aim of this study was to explore when and how Mandarinspeaking children use contextual cues to normalize speech variability in perceiving lexical tones. Two different cognitive mechanisms underlying speech normalization (lower level acoustic normalization and higher level acoustic- phonemic normalization) were investigated through the lexical tone identification task in nonspeech contexts and speech contexts, respectively. Besides, another aim of this study was to reveal how domain-general cognitive abilities contribute to the development of the speech normalization process.Method: In this study, 94 five- to eight-year-old Mandarin-speaking children (50 boys, 44 girls) and 24 young adults (14 men, 10 women) were asked to identify ambiguous Mandarin high-level and mid-rising tones in either speech or nonspeech contexts. Furthermore, in this study, we tested participants' pitch sensitivity through a nonlinguistic pitch discrimination task and their working memory using the digit span task.Results: Higher level acoustic-phonemic normalization of lexical tones emerged at the age of 6 years and was relatively stable thereafter. However, lower level acoustic normalization was less stable across different ages. Neither pitch sensitivity nor working memory affected children's lexical tone normalization.Conclusions: Mandarin-speaking children above 6 years of age successfully achieved constancy in lexical tone normalization based on speech contextual cues. The perceptual normalization of lexical tones was not affected by pitch sensitivity and working memory capacity.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0503,Research Article Speech Prosody and Reading Comprehension in Chinese-English Bilingual Children: The Mediating Role of Syntactic Awareness,"Purpose: Although children's prosodic sensitivity links with their reading comprehension, the factors affecting this link remain unclear. By simultaneously measuring first language (L1) Chinese and second language (L2) English prosodic sensitivity and reading comprehension, this study examined the mediating role of syntactic awareness on prosody-reading comprehension among Hong Kong Chinese-English bilingual children. Method: A group of 227 Hong Kong Chinese-English bilingual fourth graders completed L1 and L2 prosodic sensitivity (Cantonese lexical tone awareness and English prosodic sensitivity), syntactic awareness, and reading comprehension and control measures of cognitive (nonverbal IQ, short-term memory, and working memory), metalinguistic (phonological awareness and morphological awareness), linguistic (vocabulary knowledge), and word reading skills. Results: The within-language analyses showed a partial mediation effect of Chinese syntactic awareness on the relation between Cantonese lexical tone awareness and Chinese reading comprehension, but a full mediation effect of English syntactic awareness on the relation between English prosodic sensitivity and English reading comprehension. The cross-language analyses revealed a significant direct effect of Cantonese lexical tone awareness on English reading comprehension and a significant indirect effect of English prosodic sensitivity on Chinese reading comprehension via Chinese syntactic awareness. Conclusion: These findings suggest that, despite the language-independent mediating role of syntactic awareness in bridging prosody and reading comprehension, the degree of this mediation is shaped by language-specific prosody and its relations with other linguistic structures, including semantics.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0504,Influence of Lexical Tone Similarity on Spoken Word Recognition in Mandarin Chinese: Evidence From Eye Tracking,"Purpose: Using the visual world paradigm with the eye-tracking technique, this study examined the extent to which lexical tone similarity influences spoken word recognition. Method: In two experiments, participants were audibly presented with a target word and visually presented with the same target word, a tonal competitor, and two distractors, and they were required to identify the target word. In Experi-ment 1, the two tonal competitors shared either acoustically highly similar tones (e.g., target word: ""(sic)"" /yang2tai2/, ""balcony"" vs. competitor: ""offspring"" /yang3zi3/, ""adopted son"") or acoustically lowly similar tones (e.g., target word: ""(sic)"" /yang2tai2/, ""balcony"" vs. competitor: """" /yang4ben3/, ""sample""). In Experiment 2, the acoustic similarity of the target words and the tonal competi-tors shared either acoustically highly similar tones or acoustically lowly similar tones or identical tones (e.g., target word: ""(sic)"" /yang2tai2/, ""balcony"" vs. competitor: ""(sic)"" /yang2mao2/, ""wool""). Results: The results of the two experiments consistently demonstrated a graded tonal competitor effect, in which acoustically highly similar tonal com-petitors attracted more visual attention than acoustically lowly similar tonal competitors. Conclusion: Tonal similarity plays a graded constraining role in spoken word recognition in Mandarin Chinese.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0505,Development of Mandarin Lexical Tone Identification in Noise and Its Relation With Working Memory,"Purpose: This study aimed to examine the developmental trajectory of Mandarin tone identification in quiet and two noisy conditions: speech-shaped noise (SSN) and multitalker babble noise. In addition, we evaluated the relationship between tonal identification development and working memory capacity.Method: Ninety-three typically developing children aged 5-8 years and 23 young adults completed categorical identification of two tonal continua (Tone 1-4 and Tone 2-3) in quiet, SSN, and babble noise. Their working memory was additionally measured using auditory digit span tests. Correlation analyses between digit span scores and boundary widths were performed.Results: Six-year-old children have achieved the adultlike ability of categorical identification of Tone 1-4 continuum under both types of noise. Moreover, 6 year-old children could identify Tone 2-3 continuum as well as adults in SSN. Nonetheless, the child participants, even 8-year-olds, performed worse when tokens from Tone 2-3 continuum were masked by babble noise. Greater working memory capacity was associated with better tone identification in noise for preschoolers aged 5-6 years; however, for school-age children aged 7-8 years, such correlation only existed in Tone 2-3 continuum in SSN.Conclusions: Lexical tone perception might take a prolonged time to achieve adultlike competence in babble noise relative to SSN. Moreover, a significant interaction between masking type and stimulus difficulty was found, as indicated by Tone 2-3 being more susceptible to interference from babble noise than Tone 1-4. Furthermore, correlations between working memory capacity and tone perception in noise varied with developmental stage, stimulus difficulty, and masking type.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0506,Research Article Tone Deafness in Music Does Not Preclude Distributional Learning of Nonnative Tonal Languages in Individuals With Congenital Amusia,"Purpose: Previous studies have shown that individuals with congenital amusia exhibit deficient pitch processing across music and language domains. This study investigated whether adult Chinese-speaking listeners with amusia were still able to learn Thai lexical tones based on stimulus frequency of statistical distribution via distributional learning, despite their degraded lexical tone perception. Method: Following a pretest-training-posttest design, 21 amusics and 23 typical, musically intact listeners were assigned into bimodal and unimodal distribution conditions. Listeners were asked to discriminate minimal pairs of Thai midlevel tone and falling tone superimposed on variable base syllables and uttered by different speakers. The perceptual accuracy for each test session and improvement from pretest to posttest were collected and analyzed between the two groups using generalized mixed-effects models. Results: When discriminating Thai lexical tones, amusics were less accurate than typical listeners. Nonetheless, similarly to control listeners, perceptual gains from pretest to posttest were observed in bimodally rather than unimodally trained amusics, as evidenced by both trained and nontrained test words. Conclusions: Amusics are able to learn lexical tones in a second or foreign context of speech. This extends previous research by showing that amusics' distributional learning of linguistic pitch remains largely preserved despite their degraded pitch processing. It is thus likely that manifestations of amusia in speech could not result from their abnormal statistical learning mechanism. This study meanwhile provides a heuristic approach for future studies to apply this paradigm into amusics' treatment to mitigate their pitch-processing disorder.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0507,No Musician Advantage in the Perception of Degraded-Fundamental Frequency Speech in Noisy Environments,"Purpose: Pitch variations of the fundamental frequency (fo) contour contribute to speech perception in noisy environments, but whether musicians confer an advantage in speech in noise (SIN) with altered fo information remains unclear. This study investigated the effects of different levels of degraded fo contour (i.e., conveying lexical tone or intonation information) on musician advantage in speech-in-noise perception.Method: A cohort of native Mandarin Chinese speakers, comprising 30 trained musicians and 30 nonmusicians, were tested on the intelligibility of Mandarin Chinese sentences with natural, flattened-tone, flattened-intonation, and flattened-all fo contours embedded in background noise masked under three signal-to-noise ratios (0, -5, and -9 dB). Pitch difference thresholds and innate musical skills associated with speech-in-noise benefits were also assessed.Results: Speech intelligibility score improved with increasing signal-to-noise level for both musicians and nonmusicians. However, no musician advantage was observed for identifying any type of flattened-fo contour SIN. Musicians exhibited smaller fo pitch discrimination limens than nonmusicians, which correlated with benefits for perceiving speech with intact tone-level fo information. Regardless of musician status, performance on the pitch and accent musical skill subtests correlated with speech intelligibility score. Conclusions: Collectively, these results provide no evidence for a musician advantage for perceiving speech with distorted fo information in noisy environments. Results further show that perceptual musical skills on pitch and accent processing may benefit the perception of SIN, independent of formal musical training. Our findings suggest that the potential application of music training in speech perception in noisy backgrounds is not contingent on the ability to process fo pitch contours, at least for Mandarin Chinese speakers.Supplemental Material: https://doi.org/10.23641/asha.23706354",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0508,Investigating Perception to Production Transfer in Children With Cochlear Implants: A High Variability Phonetic Training Study,"Purpose: This study builds upon an established effective training method to investigate the advantages of high variability phonetic identification training for enhancing lexical tone perception and production in Mandarin -speaking pediatric cochlear implant (CI) recipients, who typically face ongoing challenges in these areas. Method: Thirty-two Mandarin -speaking children with CIs were quasirandomly assigned into the training group (TG) and the control group (CG). The 16 TG participants received five sessions of high variability phonetic training (HVPT) within a period of 3 weeks. The CG participants did not receive the training. Perception and production of Mandarin tones were administered before (pretest) and immediately after (posttest) the completion of HVPT via lexical tone recognition task and picture naming task. Both groups participated in the identical pretest and posttest with the same time frame between the two test sessions. Results: TG showed significant improvement from pretest to posttest in identifying Mandarin tones for both trained and untrained speech stimuli. Moreover, perceptual learning of HVPT significantly facilitated trainees' production of T1 and T2 as rated by a cohort of 10 Mandarin -speaking adults with normal hearing, which was corroborated by acoustic analyses revealing improved fundamental frequency (F0) median for T1 and T2 production and enlarged F0 movement for T2 production. In contrast, TG children's production of T3 and T4 showed nonsignificant changes across two test sessions. Meanwhile, CG did not exhibit significant changes in either perception or production. Conclusions: The results suggest a limited and inconsistent transfer of perceptual learning to lexical tone production in children with CIs, which challenges the notion of a robust transfer and highlights the complexity of the interaction between perceptual training and production outcomes. Further research on individual differences with a longitudinal design is needed to optimize the training protocol or tailor interventions to better meet the diverse needs of learners.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0509,Mandarin-Speaking Amusics' Online Recognition of Tone and Intonation,"Purpose: Congenital amusia is a neurogenetic disorder of musical pitch processing. Its linguistic consequences have been examined separately for speech intonations and lexical tones. However, in a tonal language such as Chinese, the processing of intonations and lexical tones interacts with each other during online speech perception. Whether and how the musical pitch disorder might affect linguistic pitch processing during online speech perception remains unknown. Method: We investigated this question with intonation (question vs. statement) and lexical tone (rising Tone 2 vs. falling Tone 4) identification tasks using the same set of sentences, comparing behavioral and event -related potential measurements between Mandarin -speaking amusics and matched controls. We specifically focused on the amusics without behavioral lexical tone deficits (the majority, i.e., pure amusics). Results: Results showed that, despite relative to normal performance when tested in word lexical tone test, pure amusics demonstrated inferior recognition than controls during sentence tone and intonation identification. Compared to controls, pure amusics had larger N400 amplitudes in question stimuli during tone task and smaller P600 amplitudes in intonation task. Conclusion: These data indicate that musical pitch disorder affects both tone and intonation processing during sentence processing even for pure amusics, whose lexical tone processing was intact when tested with words.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0510,Speechreading Ability Affects Mandarin Tone Perception in Young Adults With Prelingual Hearing Impairment in China,"Purpose: This cross-sectional study explored how the speechreading ability of adults with hearing impairment (HI) in China would affect their perception of the four Mandarin Chinese lexical tones: high (Tone 1), rising (Tone 2), falling-rising (Tone 3), and falling (Tone 4). We predicted that higher speechreading ability would result in better tone performance and that accuracy would vary among individual tones. Method: A total of 136 young adults with HI (ages 18-25 years) in China participated in the study and completed Chinese speechreading and tone awareness tests. The participants were divided into three groups on their basis of their speechreading performance (HIGH, MIDDLE, and LOW speechreading ability), and their ability to recognize the four Mandarin tones was compared. Results: HI adults with high speechreading ability identified tones more accurately than HI adults with low speechreading ability. The overall performance for Tone 2 was the lowest across all the groups. We found a significant interaction between speechreading ability groups and tone levels; the high speechreading ability group performed significantly better than the low ability group when identifying Tones 1 and 4, and performance on Tone 3 also differed by speechreading ability. Conclusions: These results suggest that speechreading ability affects Mandarin tone perception in adults with HI in China. Higher speechreading ability was associated with better overall tone perception. Tone 2 was the most difficult tone to identify, while identification of the other three lexical tones depended on speechreading ability. In visual language processing, adults with HI must reconstitute phonological units from visual and auditory fragments. To determine the generalizability of these results, they should be examined in languages beyond Mandarin Chinese. Supplemental Material: https://doi.org/10.23641/asha.28207784",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0511,Effect of Age and Gender on Categorical Perception of Vocal Emotion Under Tonal Language Background,"Purpose: Categorical perception (CP) manifests in various aspects of human cognition. While there is mounting evidence for CP in facial emotions, CP in vocal emotions remains understudied. The current study attempted to test whether individuals with a tonal language background perceive vocal emotions categorically and to examine how factors such as gender and age influence the plasticity of these perceptual categories. Method: This study examined the identification and discrimination performance of 24 Mandarin-speaking children (14 boys and 10 girls) and 32 adults (16 males and 16 females) when they were presented with three vocal emotion continua. Speech stimuli in each continuum consisted of 11 resynthesized Mandarin disyllabic words. Results: CP phenomena were detected when Mandarin participants perceived vocal emotions. We further found the modulating effect of age and gender in vocal emotion categorization. Conclusions: Our results demonstrate for the first time that a categorical strategy is used by Mandarin speakers when perceiving vocal emotions. Furthermore, our findings reveal that the categorization ability of vocal emotions follows a prolonged course of development and the maturation patterns differ across genders. This study opens a promising line of research for investigating how sensory features are mapped to higher order perception and provides implications for our understanding of clinical populations characterized by altered emotional processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0512,The Primacy of Prosody in Bilingual Reading Comprehension: Uncovering Morpholexical Pathways in First-Language Chinese and Phonolexical Pathways in Second-Language English,"Purpose: Prosodic sensitivity, or the ability to perceive suprasegmental features of speech such as lexical tone in Cantonese and lexical stress in English, has been recognized as an essential component of reading acquisition. However, the specific pathways by which it influences reading comprehension remain unclear. This study evaluates how prosodic sensitivity contributes to reading comprehension in both a first language (L1), Chinese, and a second language (L2), English, through segmental phonological awareness and morphological awareness among bilingual children in Hong Kong. Method: A group of 227 Hong Kong fourth graders was assessed on prosodic sensitivity, segmental phonological awareness, morphological awareness, vocabulary, word reading, and reading comprehension in both L1 and L2, as well as nonverbal intelligence and working memory. Results: Within-language structural equation modeling analyses revealed that Cantonese lexical tone awareness significantly contributed to Chinese word reading via morphological awareness, constituting a morpholexical pathway. Conversely, English prosodic sensitivity significantly contributed to English word reading through segmental phonological awareness, establishing a phonolexical pathway. Word reading within each language significantly predicted reading comprehension. The cross-language model further indicated that Cantonese lexical tone awareness contributed directly to English reading comprehension and indirectly through English phonological awareness and English word reading. In contrast, English prosodic sensitivity significantly contributed to Chinese reading comprehension through Chinese morphological awareness and Chinese word reading. Conclusions: The results underscore the primacy of suprasegmental elements in reading acquisition across languages, highlighting the pivotal roles of morphological awareness in Chinese and segmental phonological awareness in English. These findings suggest that intervention aimed at improving reading skills in bilinguals may consider the contributions of prosodic sensitivity across different linguistic contexts. Supplemental Material: https://doi.org/10.23641/asha.29792549",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0513,Amateur Choir Experience Modulates the Perception of Mandarin Sounds in Older Adults,"Purpose: Existing literature suggests that musical experience can enhance the auditory perception of acoustic features. However, it remains unclear whether such improvements in acoustic processing affect speech perception in older adults. The present study aimed to explore whether older adults with amateur choir experience can exhibit finer processing of acoustic features in perceiving Mandarin sounds than those without such experience, as indicated by hemispheric lateralization patterns in processing acoustic attributes of Mandarin sounds and the performance in lexical tone identification based on different acoustic cues. Method: Two experiments were conducted in this study. Experiment 1 compared the performance of older adults with and without amateur choir experience in dichotic listening tasks based on consonant-vowel (CV) syllables varying in consonants, vowels, and lexical tones. Experiment 2 examined the performance of two groups of older adults in identifying Mandarin tones under the fundamental frequency (F0)-only (with equal durations) and F0 + duration (with natural durations) conditions. Results: Experiment 1 revealed that older adults with amateur choir experience demonstrated an analytical processing mode for time and frequency features of speech sounds, as evidenced by more pronounced left-hemispheric lateralization in dichotic listening tasks based on CV syllables varying in consonants and in lexical tones. Furthermore, Experiment 2 showed that older adults with choir experience exhibited better performance than those without such experience in lexical tone identification under both the F0-only and F0 + duration conditions. Conclusions: The amateur choir experience may lead to an analytical processing mode of time and frequency features and boost the identification accuracy of Mandarin sounds based on these cues in the aging population. These findings substantiate the overlap, precision, emotion, repetition, and attention hypothesis and highlight the beneficial impact of amateur choir experience on speech perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0514,"Strings Versus Keys: A Comparison of Violinists, Pianists, and Nonmusicians in Lexical Tone Perception and Tone Word Learning","Purpose: Correlational research has found that musicians outperform nonmusicians in lexical tone perception and tone word learning. In these studies, participants were dichotomized as either musicians or nonmusicians. However, musicianship is nonbinary, as different musical instruments have different pitch processing demands. Examining different types of musicianship would enable a more comprehensive understanding of music-to-language transfer. The present study investigated whether violinists would outperform pianists and nonmusicians in discriminating and identifying Thai contour tones and, if so, whether the violinists' advantage in Thai contour tones would also apply to Thai tone word learning. Method: Eighty-one Cantonese violinists, pianists, and nonmusicians completed a Thai tone discrimination task, a Thai tone identification task, and seven sessions of tone word identification training. Participants also completed a working memory task, a nonverbal intelligence task, and an inhibitory control task. Results: In the Thai tone discrimination task, the violinists, pianists, and nonmusicians performed similarly. In the Thai tone identification task, the violinists and pianists performed similarly, but both groups outperformed the nonmusicians. In the Thai tone word learning task, both the violinists and the pianists outperformed the nonmusicians, with violinists demonstrating the largest musical advantage. Unexpectedly, the violinists' advantage applied to some but not all contour and level tones. Conclusions: The results indicated that music-to-language transfer in tone word learning hinges on the type of musical instrument. Theoretically, there is a need to introduce elements of higher level linguistic processing and selectivity into the Overlap, Precision, Emotion, Repetition, and Attention hypothesis. Practically, the study suggests the potential of violin training in facilitating nonnative tone word learning.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0515,PERCEPTION OF EMOTIONAL INTONATION BY BRAIN-DAMAGED ADULTS - THE INFLUENCE OF TASK PROCESSING LEVELS,"This research examined perception of moods from the tone-of-voice of semantically neutral phrases following unilateral cerebrovascular accident. It was hypothesized that right hemisphere damage (RHD) would impair even low-level discrimination and recognition of affective prosody, while left hemisphere damage (LHD) would affect performance only as associational-congnitive task demands increased. Thirty-three male subjects, 11 each in RHD, LHD, and normal groups, were given three tasks that varied in presumed amounts of processing undertaken for successful completion. Discrimination of prosodic patterns was expected to require the fewest cognitive operations. An intermediate task involved selecting from two possibilities the label that described moods conveyed prosodically. In the third task, prosodic mood selection was made from four choices, increasing the number of comparisons necessary for accurate judgement. As hypothesized, RHD subjects were inferior to normal subjects in all tasks. LHD subjects were equivalent to normal subjects for the first two tasks, but fell to the level of the RHD group for the third task. These results indicated that the right hemisphere in men was primarily involved in the reception and recognition of emotional prosodic stimuli. Increasing cognitive demands, however, brought about a shift in emphasis from the right hemisphere to both hemispheres. An implication of these findings concerns the need to examine performance levels that invoke changes from expected pattens of hemispheric specialization to advance our knowledge of functional asymmetries.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0516,COMMUNICATION OF LEXICAL TONES IN CANTONESE ALARYNGEAL SPEECH,"Cantonese is a tone language with six lexical tones. Each word has a distinctive tone, signaled by fundamental frequency variations at the syllable level. We investigated the relative efficiency of alaryngeal Cantonese speakers in conveying tonal variations in words in citation form. isolated tone tokens were produced by three esophageal speakers, two tracheoesophageal speakers, two pneumatic artificial laryngeal speakers, and two electrolaryngeal speakers for perceptual tests. The correct responses from 22 listeners were highest for the pneumatic artificial laryngeal speakers, and could be graded in order of proficiency as esophageal, tracheoesophageal, and electrolaryngeal speakers. These results provide a linguistic perspective tor guiding voice rehabilitation and the choice of voice in alaryngeal patients who speak a tone language.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0517,Effects of age and hearing sensitivity on the use of prosodic information in spoken word recognition,"It is well known that spoken words can often be recognized From lust their onsets and that older adults require a greater word onset duration For recognition than young adults. In this study, young and older adults heard either just word onsets, word onsets followed by white noise indicating the full duration of the target word, or word onsets Followed by a low-pass-Filtered signal that indicated the number of syllables and syllabic stress (word prosody) in the absence of segmental information. Older adults required longer stimulus durations for word recognition under all conditions, with age differences in hearing sensitivity contributing significantly to this age difference. Within this difference, however, word recognition was facilitated by knowledge of word prosody to the same degree for young and older adults. These findings suggest, first, that listeners can detect and utilize word stress in making perceptual judgments and, second, that this ability remains spared in normal aging.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0518,Effects of stimulation rates on Cantonese lexical tone perception by cochlear implant users in Hong Kong,"High, moderate and low stimulation rates of 1800, 800 and 400 pulse per second (pps)/channel, respectively, were used to test the effects of stimulation rates on the discrimination and identification of Cantonese lexical tones in 11 Chinese post-lingually deafened adults with cochlear implants (CIs). The subjects were implanted with the MED-EL Combi 40+ CI system. They were randomly assigned to each of the stimulation rate conditions according to an ABC design. In both the Cantonese lexical tone perception tests, the subjects reached the highest scores in the high-stimulation-rate condition, and the lowest scores in the low-stimulation-rate condition (P < 0.01). Post hoc comparisons between different stimulation rates did not yield consistent results. This study demonstrated that the maximum stimulation rate of 1800 pps/channel could be an 'optimal' stimulation rate and an informed choice of parameter for the benefit of Cantonese-speaking CI users in lexical tone perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0519,Improvement of the synthesised speech intonation with stylisation and neural network learning,"A new F0 contour generation model with stylisation and neural network learning is proposed. Our F0 contour consists of (i) global tune, (ii) word pitch bias, (iii) lexical tone, and (iv) syllabic pitch pattern. A preconstructed melodic table takes care of (i), (ii) and (iv) while a neural network deals with (iii). A 5-level subject test shows 0.5 point intonation quality improvement compared to our previous model.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0520,Very low bit rate speech coding using a diphone-based recognition and synthesis approach,High compression rates of speech signals may be achieved by coding schemes based on relevant linguistic segments. A system is described that relies on a diphone recogniser as the coder and on a speech synthesiser reproducing speech starting from a diphone codebook as the decoder. The spoken message is encoded in textual (phoneme labels) plus prosody representation. This speech coding technique may be used for voice mail or phone communication over low bit rate channels.,,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0521,Superiority of bilateral cochlear implantation over unilateral cochlear implantation in tone discrimination in Chinese patients,"Purpose: The benefits of bilateral cochlear implants (Cis) versus unilateral Cis were evaluated by comparing the Cantonese lexical tone discrimination scores conducted in a quiet environment and against a background noise. Materials and Methods: Four postlingually deafened Cantonese-speaking adults (2 men and 2 women) with bilateral Cis were included in this study. The subjects were their own control in the monaural hearing condition. Both the Cantonese lexical tone stimuli and the speech-weighted background noise were presented at 0degrees azimuth and at a distance of 1 m from the subject. The speech stimuli, which were maintained at 65 dB sound pressure level, were presented, in both a quiet environment and against a background noise at signal to noise ratios (SNRs) of +15, +10, +5, 0, -5, -10, and -15. Results: Against a background noise, the bilateral Cis required +5 dB SNR only to obtain significant scores in discriminating Cantonese lexical tones and to achieve discrimination scores that were comparable to the optimal discrimination scores obtained in quiet. No significant difference in the discrimination scores was observed between binaural and monaural hearing conditions when the tests were conducted in quiet. Conclusions: Our study showed that in the presence of background noise, bilateral Cis were better than unilateral Cis in discriminating Cantonese lexical tones.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0522,The impact of multifaceted factors on auditory mapping between acoustic cues and Spanish intonation categories in a cross-linguistic context,"Recent research has revealed cross-linguistic and individual variations in the processing of acoustic cues for phonetic categorization. This study extends this line of inquiry by examining the auditory perception of native Spanish listeners and Chinese learners of Spanish, focusing on their ability to map acoustic signals onto intonation categories. Through two identification tasks employing synthesized stimuli with systematically varied acoustic and stress patterns, we investigated how listeners navigate multiple cues in recognizing Spanish sentence types. Results indicated that changes in fundamental frequency (F0), duration, and intensity significantly influenced native Spanish listeners' intonation judgments, while Chinese learners predominantly relied on F0 modulations to differentiate statements from yes/no questions. Compared to native Spanish listeners, Chinese learners demonstrated lower sensitivity to changes across the three cues and less proficiency in reconciling cue trade-offs. Furthermore, our study revealed that both Spanish and Chinese listeners' perceptual performance was modulated by stress patterns and their chronological age. Overall, our research elucidates the multifaceted nature of intonation perception, underscoring the critical role of linguistic background, individual characteristics, and lower-level prosodic context in the transformation of acoustic details into intonation categories.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0523,Opposite patterns of hemisphere dominance for early auditory processing of lexical tones and consonants,"in tonal languages such as Mandarin Chinese, a lexical tone carries semantic information and is preferentially processed in the left brain hemisphere of native speakers as revealed by the functional MRI or positron emission tomography studies, which likely measure the temporally aggregated neural events including those at an attentive stage of auditory processing. Here, we demonstrate that early auditory processing of a lexical tone at a preattentive stage is actually lateralized to the right hemisphere. We frequently presented to native Mandarin Chinese speakers a meaningful auditory word with a consonant-vowel structure and infrequently varied either its lexical tone or initial consonant using an odd-ball paradigm to create a contrast resulting in a change in word meaning. The lexical tone contrast evoked a stronger preattentive response, as revealed by whole-head electric recordings of the mismatch negativity, in the right hemisphere than in the left hemisphere, whereas the consonant contrast produced an opposite pattern. Given the distinct acoustic features between a lexical tone and a consonant, this opposite lateralization pattern suggests the dependence of hemisphere dominance mainly on acoustic cues before speech input is mapped into a semantic representation in the processing stream.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0524,Reduced sensitivity to emotional prosody in congenital amusia rekindles the musical protolanguage hypothesis,"A number of evolutionary theories assume that music and language have a common origin as an emotional protolanguage that remains evident in overlapping functions and shared neural circuitry. The most basic prediction of this hypothesis is that sensitivity to emotion in speech prosody derives from the capacity to process music. We examined sensitivity to emotion in speech prosody in a sample of individuals with congenital amusia, a neurodevelopmental disorder characterized by deficits in processing acoustic and structural attributes of music. Twelve individuals with congenital amusia and 12 matched control participants judged the emotional expressions of 96 spoken phrases. Phrases were semantically neutral but prosodic cues (tone of voice) communicated each of six emotional states: happy, tender, afraid, irritated, sad, and no emotion. Congenitally amusic individuals were significantly worse than matched controls at decoding emotional prosody, with decoding rates for some emotions up to 20% lower than that of matched controls. They also reported difficulty understanding emotional prosody in their daily lives, suggesting some awareness of this deficit. The findings support speculations that music and language share mechanisms that trigger emotional responses to acoustic attributes, as predicted by theories that propose a common evolutionary link between these domains.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0525,Mapping the unconscious maintenance of a lost first language,"Optimal periods during early development facilitate the formation of perceptual representations, laying the framework for future learning. A crucial question is whether such early representations are maintained in the brain over time without continued input. Using functional MRI, we show that internationally adopted (IA) children from China, exposed exclusively to French since adoption (mean age of adoption, 12.8 mo), maintained neural representations of their birth language despite functionally losing that language and having no conscious recollection of it. Their neural patterns during a Chinese lexical tone discrimination task matched those observed in Chinese/French bilinguals who have had continual exposure to Chinese since birth and differed from monolingual French speakers who had never been exposed to Chinese. They processed lexical tone as linguistically relevant, despite having no Chinese exposure for 12.6 y, on average, and no conscious recollection of that language. More specifically, IA participants recruited left superior temporal gyrus/planum temporale, matching the pattern observed in Chinese/French bilinguals. In contrast, French speakers who had never been exposed to Chinese did not recruit this region and instead activated right superior temporal gyrus. We show that neural representations are not overwritten and suggest a special status for language input obtained during the first year of development.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0526,Cooperative cortical network for categorical processing of Chinese lexical tone,"In tonal languages such as Chinese, lexical tone with varying pitch contours serves as a key feature to provide contrast in word meaning. Similar to phoneme processing, behavioral studies have suggested that Chinese tone is categorically perceived. However, its underlying neural mechanism remains poorly understood. By conducting cortical surface recordings in surgical patients, we revealed a cooperative cortical network along with its dynamics responsible for this categorical perception. Based on an oddball paradigm, we found amplified neural dissimilarity between cross-category tone pairs, rather than between within-category tone pairs, over cortical sites covering both the ventral and dorsal streams of speech processing. The bilateral superior temporal gyrus (STG) and the middle temporal gyrus (MTG) exhibited increased response latencies and enlarged neural dissimilarity, suggesting a ventral hierarchy that gradually differentiates the acoustic features of lexical tones. In addition, the bilateral motor cortices were also found to be involved in categorical processing, interacting with both the STG and the MTG and exhibiting a response latency in between. Moreover, the motor cortex received enhanced Granger causal influence from the semantic hub, the anterior temporal lobe, in the right hemisphere. These unique data suggest that there exists a distributed cooperative cortical network supporting the categorical processing of lexical tone in tonal language speakers, not only encompassing a bilateral temporal hierarchy that is shared by categorical processing of phonemes but also involving intensive speech-motor interactions over the right hemisphere, which might be the unique machinery responsible for the reliable discrimination of tone identities.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0527,Piano training enhances the neural processing of pitch and improves speech perception in Mandarin-speaking children,"Musical training confers advantages in speech-sound processing, which could play an important role in early childhood education. To understand the mechanisms of this effect, we used event-related potential and behavioral measures in a longitudinal design. Seventy-four Mandarin-speaking children aged 4-5 y old were pseudorandomly assigned to piano training, reading training, or a no-contact control group. Six months of piano training improved behavioral auditory word discrimination in general as well as word discrimination based on vowels compared with the controls. The reading group yielded similar trends. However, the piano group demonstrated unique advantages over the reading and control groups in consonant-based word discrimination and in enhanced positive mismatch responses (pMMRs) to lexical tone and musical pitch changes. The improved word discrimination based on consonants correlated with the enhancements in musical pitch pMMRs among the children in the piano group. In contrast, all three groups improved equally on general cognitive measures, including tests of IQ, working memory, and attention. The results suggest strengthened common sound processing across domains as an important mechanism underlying the benefits of musical training on language processing. In addition, although we failed to find far-transfer effects of musical training to general cognition, the near-transfer effects to speech perception establish the potential for musical training to help children improve their language skills. Piano training was not inferior to reading training on direct tests of language function, and it even seemed superior to reading training in enhancing consonant discrimination.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0528,Exploring pitch and duration reflexesin Taiwanese Southern Min tones and variations across generations,"Extensive research has explored tonal contrasts, dialectal differences, and sandhi patterns of Taiwanese Southern Min tones. However, the duration reflexes of these tones, which hold theoretical significance, and their potential variations between the older generation, who use Taiwanese Southern Min as a first language, and the younger generation, who use it as a ""heritage"" or second language, have received comparatively less attention. In a sizable corpus study, we demonstrated that Taiwanese Southern Min syllables are best described as bimoraic, akin to other Chinese dialects, where syllables with fewer segments have comparable durations to those with more segments, and vowel durations in simpler syllable structures are longer than those in more complex structures. Furthermore, lexical tone durations partially follow patterns observed cross-linguistically, with rising tones produced longer than falling tones, and tones with higher Fo produced shorter than those with lower Fo. Finally, we observed a much narrower tonal space in young speakers compared to old speakers, with less separation in their F trajectories in both level and contour tone productions. Our comprehensive study delves into underexplored Taiwanese Southern Min tonal duration reflexes, shedding light on potential generational variations and contributing to a better understanding of sound change trajectories.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0529,The modification of compound nouns by three adjectives,"This paper is the final instalment in a series of studies investigating the modification patterns in complex noun phrases (NPs) in English. It particularly focuses on the modification of two-noun compounds by three attributive adjectives. An analysis of all such NPs from the BNC reveals a strong preference for head modification over modifier modification, similar rates of convergent and divergent modification and the non-occurrence of crossed modification. The single most important factor influencing the modification patterns is functional status. The larger the number of adjectives modifying the head of the compound, the higher the frequency of the modification type (modulo a proximity effect). The absence of crossed modification is expected under the no-crossing constraint, which is understood here not as a formal but as a functional principle ensuring successful communication. The various factors can be tied together under the rubric of accessibility. The probability of selecting a particular modification target is argued to be a function of the accessibility of the nouns in an NP.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0530,Intensifier actions in Israeli Sign Language (ISL) discourse,"The study describes certain structural modifications employed on the citation forms of ISL during signing for intensification purposes. In Signed Languages, citation forms are considered relatively immune to modifications. Nine signers signed several scenarios describing some intense quality. The signers used conventional adverbs existing in ISL for intensification purposes. Yet, they also employed idiosyncratic modifications on the formational components of adjectives simultaneously to form realization. These optional modifications enriched the messages conveyed merely by the conventional forms. They show that signers can incorporate gradient modes of expressions directly into the production of the lexical items to communicate more diverse and explicit messages in context. Using a comparative semiotic approach allowed us to describe the synergetic cooperation manifested at the stage of utterance construction between formational elements which were more suited to convey gradient and analog meanings in context and those that were less suited and thus not modified.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0531,Towards a novel conceptualization of prosody that accounts for spoken and visual signals,"Prosody influences speech organization by signaling phrasal prominence, grouping patterns, and speakers' pragmatic intentions. While traditionally viewed as restricted to speech, research shows prosody is also conveyed visually. This article reviews research showing strong parallels between spoken prosody and co-speech gestures in prominence marking, grouping phrasal structures, and signaling pragmatic intent. We extend this discussion and propose a modality-neutral prosodic framework hypothesis comprising three propositions: (a) prosody should be viewed as a modality-neutral grammar component that operates as an abstract level of representation while adapting to different sensory channels and language modalities; (b) in spoken languages, prosody is implemented flexibly through two distinct channels, spoken and gestural, which enable to mark prominence, grouping and meaning in a multimodal way; (c) parallel implementations are found in the way prosody is manifested in spoken and sign languages. A modality-neutral view of prosody will enrich current formal and developmental theories of language.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0532,Enhancing Mandarin tone acquisition,"This study investigates the effectiveness of a Computer-Assisted Pronunciation Training (CAPT) system designed to enhance Mandarin lexical tone acquisition among second language (L2) learners in a self-directed learning context. Drawing on automated speech recognition (ASR) technology, the CAPT system provides real-time, individualized feedback on tone production accuracy. Elementary-level Mandarin learners at an international university in China were randomly assigned to either an experimental group engaged with the CAPT system or a control group using traditional listen-and-repeat practice. Over a two-week period, participants completed pre- and post-tests evaluating their production of four Mandarin tones at the sentence level. Statistical analyses revealed that the experimental group achieved significant gains in tonal accuracy across all four tones, while the control group showed no notable improvement. Survey data further supported the CAPT system's usability and pedagogical value, with learners demonstrating increased awareness of tone patterns and pronunciation strategies. These findings highlight the efficacy of ASR-based CAPT for supporting tone acquisition in tonal languages like Mandarin, especially in supplementing classroom instruction for pronunciation development. The study addresses the underrepresented research in CAPT for L2 Mandarin learning and introduces an effective tool for L2 Mandarin learners and instructors.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0533,Intonation in Palenquero,"The least understood aspect of Palenquero phonology is its intonational system. This is a serious gap, as it is precisely in the realm of prosody that the most striking phonological differences between Palenquero and (Caribbean) Spanish are apparent. Although several authors have speculated that African influence may be at the source of Palenquero's peculiar intonation, to date published research offers no detailed information about the intonation of the creole. The goal of this study is to remedy this situation. Here we identify several specific intonational features where conservative (or older-generation) Palenquero differs from (Caribbean) Spanish. One of these features is a strong tendency to use invariant word-level contours, with a H tone on the stressed syllable and L tones on unstressed syllables, in all sentential contexts, including prenuclear positions. A second feature that we have identified is the use of a sustained phrase-final high or mid level contour in declaratives accented on the final syllable, and a long fall in declaratives accented on the penult. The final section addresses the issue of the possible origin of these intonational features. We point out similarities with Equatorial Guinea Spanish and conclude that, at some point in the history of Palenquero, the Spanish prosodic system was interpreted as involving lexical tone, in conformity with claims in the literature regarding several Atlantic creoles.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0534,Suprasegmental information cues morphological anticipation during L1/L2 lexical access,"We use visual-world eye-tracking and gating methods to investigate whether Spanish monolinguals and English late learners of Spanish use prosodic cues (lexical stress) to anticipate morphological information (suffixes) during spoken word recognition, and if they do, whether L2 proficiency and working memory (WM) mediate their anticipatory abilities. Our findings show that the monolinguals used prosodic information to predict word endings in both tasks, regardless of first-syllable stress (stressed, unstressed) and structure (CV, CVC). In contrast, the beginning learners did not use prosodic information to anticipate word suffixes in any task or condition. Importantly, the advanced learners mirrored the monolinguals, except in words with first-syllable CV structure, but were slower than the monolinguals. Finally, WM was not associated with anticipatory eye movements, though results were inconclusive for offline processing. Taken together, the present study shows that suprasegmental information facilitates morphological anticipation during spoken word recognition, and that adult learners can gain anticipatory processing patterns qualitatively, but not quantitatively, similar to monolinguals.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0535,Prosodic disambiguation of questions in Korean Theory and processing,"This study aims to identify the acoustic and perceptual properties that contribute to identifying the meaning of Korean sentences that are ambiguous between wh-question and yes-no question readings. While in most cases the Accentual Phrase (AP) tonal pattern (Jun 1993) differs between the two question readings, there are cases where the two readings are predicted to have the same AP tonal pattern. However, our experimental results indicate that even in those cases a typical AP tonal contrast between the two question interpretations, i.e. the presence vs. absence of the tone in the syllable that immediately follows the wh-word, is observed in production and plays a meaningful role in perception. The results suggest that there is a production and processing strategy to utilize a consistent contrast in accentual phrasing between the two types of questions for disambiguation.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0536,Simultaneous bilinguals who do not speak a tone language show enhancement in pitch sensitivity but not in executive function,"Previous studies have reported perceptual advantages, such as when discriminating non-native linguistic or musical pitch differences, among first-year infants growing up in bilingual over monolingual environments. It is unclear whether such effects should be attributed to bilinguals' enhanced perceptual sensitivity and/or cognitive abilities, and whether such effects would extend to adulthood. Twenty-four Dutch, 24 Dutch simultaneous bilingual (DSB), and 24 Chinese Mandarin speakers were examined by three sets of tasks assessing their linguistic pitch and music perception, executive function, as well as interactions across these modalities. Results showed degrees of advantages for DSB and Chinese participants' over their Dutch peers in lexical tone discrimination and pitch-related music tasks. In tasks related to executive function, no difference was observed between DSB and Dutch participants, while Chinese participants' performances were modulated by cognitive interference of language processing. Findings suggest that listeners' enhanced sensitivity to linguistic and musical pitch may stem from acoustic (DSB) and experience (Chinese) rather than cognitive factors. Moreover, Dutch participants showed robust correlations between their linguistic and musical pitch perception, followed by limited correlations in DSB, and virtually no correlation among Chinese participants, illustrating how distinct language experiences can lead to specific pitch perception patterns between language and music.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0537,"Native language, L2 experience, and pitch processing in music","The current study investigated how the role of pitch in one's native language and L2 experience influenced musical melodic processing by testing Turkish and Mandarin Chinese advanced and beginning learners of English as an L2. Pitch has a lower functional load and shows a simpler pattern in Turkish than in Chinese as the former only contrasts between presence and the absence of pitch elevation, while the latter makes use of four different pitch contours lexically. Using the Musical Ear Test as the tool, we found that the Chinese listeners outperformed the Turkish listeners, and the advanced L2 learners outperformed the beginning learners. The Turkish listeners were further tested on their discrimination of bisyllabic Chinese lexical tones, and again an L2 advantage was observed. No significant difference was found for working memory between the beginning and advanced L2 learners. These results suggest that richness of tonal inventory of the native language is essential for triggering a music processing advantage, and on top of the tone language advantage, the L2 experience yields a further enhancement. Yet, unlike the tone language advantage that seems to relate to pitch expertise, learning an L2 seems to improve sound discrimination in general, and such improvement exhibits in non-native lexical tone discrimination.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0538,The role of lexical stress in spoken English word recognition by listeners of English and Taiwan Mandarin,"Two perceptual experiments investigated how the suprasegmental information of monosyllables is perceived and exploited in spoken English word recognition by listeners of English and Taiwan Mandarin (TM). Using an auditory lexical decision task in which correctly stressed English words and mis-stressed nonwords (e.g. camPAIGN vs. *CAMpaign) were presented for lexical decisions, Experiment I demonstrated that TM listeners could perceive the differences between stressed and unstressed syllables with native-like accuracy and rapidity. To examine how the perceived suprasegmental contrast would constrain English lexical access, Experiment II was conducted. It used a cross-modal fragment priming task in which a lexical decision had to be made for a visually presented English word or nonword following an auditory prime, which was a spoken word-initial syllable. The results showed that English and TM listeners recognized the displayed word (e.g. campus) faster both after a stress-matching (e.g. CAM-) prime and a stress-mismatching (e.g. cam-) prime than after a control prime (e.g. MOUN , with mismatching segments). This indicates that suprasegmental information does not inhibit a segmentally matching but suprasegmentally mismatching word candidate for both the two groups, although TM is a language where lexical prosody is expressed syllabically and its listeners tend to interpret lexical stress tonally. Yet, the two groups' responses were slower after the stressed primes than after the unstressed ones, presumably because the former generally had more possible continuations than the latter do. It is therefore concluded that when recognizing spoken English words, both the native and non-native (TM-speaking) listeners can exploit the suprasegmental cues of monosyllables, which, however, are not so effective that they will outweigh the segmental cues.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0539,Perception of tones in Mandarin and Dutch adult listeners,"This paper examines the nature of categorical perception (CP) effects in Mandarin and Dutch adult listeners through identification and discrimination tasks using lexical tonal contrasts and through the CP index analysis. In identification tasks, Mandarin listeners identify tones in accordance with their native tonal categories whereas Dutch listeners do so based on acoustic properties. In discrimination tasks, Dutch listeners outperform Mandarin listeners especially in tonal steps on the continuum falling within the Mandarin tonal category boundary, whereas Mandarin listeners display high sensitivity in discrimination of stimuli falling across the native boundary. The CP index analysis shows a higher degree of CP in Mandarin (categorical perception) than in Dutch (psycho-acoustic perception) listeners.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0540,"Why we need to investigate casual speech to truly understand language production, processing and the mental lexicon","The majority of studies addressing psycholinguistic questions focus on speech produced and processed in a careful, laboratory speech style. This 'careful' speech is very different from the speech that listeners encounter in casual conversations. This article argues that research on casual speech is necessary to show the validity of conclusions based on careful speech. Moreover, research on casual speech produces new insights and questions on the processes underlying communication and on the mental lexicon that cannot be revealed by research using careful speech. This article first places research on casual speech in its historic perspective. It then provides many examples of how casual speech differs from careful speech and shows that these differences may have important implications for psycholinguistic theories. Subsequently, the article discusses the challenges that research on casual speech faces, which stem from the high variability of this speech style, its necessary casual context, and that casual speech is connected speech. We also present opportunities for research on casual speech, mostly in the form of new experimental methods that facilitate research on connected speech. However, real progress can only be made if these new methods are combined with advanced (still to be developed) statistical techniques.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0541,Testing the storage of prosody-induced phonetic detail via auditory lexical decision A case study of noun/verb homophones,"This article reports the results of an auditory lexical decision task, testing the processing of phonetic detail of English noun/verb conversion pairs. The article builds on recent findings showing that the frequent occurrence in certain prosodic environments may lead to the storage of prosodyinduced phonetic detail as part of the lexical representation. To investigate this question with noun/verb conversion pairs, ambicategorical stimuli were used that exhibit systematic occurrence differences with regard to prosodic environment, as indicated by either a strong verb-bias, e.g., talk (N/V) or a strong noun-bias, e.g., voice (N/V). The auditory lexical decision task tests whether acoustic properties reflecting either the typical or the atypical prosodic environment impact the processing of recordings of the stimuli. In doing so assumptions about the storage of prosody-induced phonetic detail are tested that distinguish competing model architectures. The results are most straightforwardly accounted for within an abstractionist architecture, in which the acoustic signal is mapped onto a representation that is based on the canonical pronunciation of the word.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0542,Rich pitch The humorous effects of deaccent and L plus H* pitch accent,"This paper argues that intonation contributes to the humorous meaning of a certain class of jokes. Examples of both canned and spontaneous jokes show that two intonation patterns, the intonation of contrast, or ""L+H* pitch accent"", and the intonation of given information, or ""deaccent"", can contribute to a humorous effect. Both of these patterns act as cohesive devises in discourse: they trigger a mental search in the mind of a hearer for a cohesive tie that may not be obvious from the lexicogrammatical structure alone. A punch line effect is created if this search yields an unexpected incongruity between the hearer's initial mental model of the joke discourse and a humorous alternative. The hearer must shift his ""script"" (Raskin 1984) of the discourse in an unexpected way. To the extent that intonation facilitates processing by directing attention to particular elements in the information structure of the discourse (Chafe 1994), the processing of jokes depends in part on their intonation. The implications of this premise for the processing of humorous texts will be discussed for the two intonation patterns in question. It is argued that intonation analysis can lead to a broader understanding of cognitive processes and structures.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0543,"Periodicity, pattern formation, and metric structure","This article describes an approach to metrical structure focussing on its role as an active listening strategy. The theory postulates that metrical structure is a self-organized, dynamic structure composed of self-sustaining oscillations. The emergence of this structural representation is modeled as a pattern formation process whose the neural correlate is the formation of a spatiotemporal pattern of neural activity. The primary function of the dynamic structure is attentional. It enables anticipation of future events thus, targeting of perception, and coordination of action with exogenous events. Stability and flexibility properties arise through nonlinearities in the underlying pattern-forming dynamics. Furthermore, this dynamic representation functions in musical communication. Transient stimulus fluctuations observed in musical performance (e.g., rate changes, intonation) are not noise, but rather communicate structural information, intention, and affect. These communicative gestures are recognized as deviations from temporal expectations embodied in the metrical structure. Experiments are reviewed that investigate stimuli of varying complexity, from simple isochronous tone sequences to performed music, and the model's success at capturing these data is assessed.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0544,Schoenberg-Hartleben's Pierrot Lunaire: Speech - Poem - Melody - Vocal performance,"This work presents detailed examination of the various aspects of the vocal part of Pierrot Lunaire: literary, linguistic, melodic, acoustic, and vocal performance, and their interrelations. An examination of Hartleben's German translation reveals the decisive role of characteristic linguistic features in the German language in shaping rhythms in Schoenberg's ""Sprechmelodies"". Vocal analysis of speech intonation contours in the spoken texts, as read aloud by two persons of German native language, brings experimental evidence and elucidates the origin of Schoenberg's ""Sprechmelodies"" in intonation in German speech. Intonation patterns in excerpts of Schoenberg's own speech, recorded in 1931 and 1936, also subjected to such analysis, reveal typical intonation patterns in German speech, of relevance to his ""Sprechmelodies"" in Pierrot Lunaire. Singing-reciting of four poems by five artists: Erika Stiedry Wagner (recorded 1941), Jan de Gaetani (rec. similar to1969), Yvonne Minton (1976), Marianne Pousseur (1992), and Christine Schaefer (1997), in Schoenberg's Pierrot Lunaire is analyzed and compared, by analysis of their Fast Fourier Transform (FFT) spectrograms. These performances, spanning almost sixty years, reflect a variety of styles and approaches to this extraordinary work. A large variety of types and (temporal) structures of vocal tones were encountered-reflecting the vocal means the artists designed (mentally) in order to capture and express the unique atmosphere of the texts, and the music, in Schoenberg's unique style of Sprechgesang. These vocal tones are decomposed into elementary units and a special notation was devised for their description and classification. This forms the basis of a detailed analysis on the microlevel of a single tone and a single syllable. Analysis of the melodic phrases of Schoenberg's ""Sprechmelodien"" (speech melodies) in terms of melodic segment contours by means of this notation is also described. Thus, this special notation enables a symbolic representation, or transcription, of speech intonation, singing, and ""Sprechgesang"" melodic contours.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0545,Continuous improvement in Mandarin lexical tone perception as the number of channels increased: a simulation study of cochlear implant,"Conclusion. With reference to English phoneme recognition, where performance usually does not improve after six or eight channels in cochlear implants (CIs), increasing total channel numbers continuously improved perception of Mandarin tones. Objective. To test our hypothesis that current CI strategies might be modified to improve Mandarin lexical tonal perception. Materials and methods. Lexical tonal perception tests using 48 monosyllables in Mandarin Chinese were conducted in 32 native Mandarin speakers with normal hearing. The performance of tonal perception was compared among the controlled factors, which were total channel number, number of channels allocated to the F0 spectrum, and whether there were spectral shifts in the electrode configuration. The experimental condition that preserves fine structure was used as a comparison. Results. The signal processing strategy using 16 channels - which is technically possible with current CI devices - produced better tonal perception than those using 12 or 8 channels. Increasing the number of fundamental channels did not improve tonal perception, and spectral shifts did not change tonal perception. An experimental condition (FiC12) that preserves the fine structure produced significantly better overall scores for tone perception than other experimental conditions with envelope strategies.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0546,Recognition of lexical tone production of children with an artificial neural network,"Conclusion. This study demonstrated that the artificial neural network can successfully classify Mandarin Chinese tone patterns produced by multiple children. The neural network can be used as an objective way of evaluating tone production of children. Objectives. Traditionally, tone production is evaluated subjectively using human listeners. The aim of the present study was to investigate the efficacy of using an artificial neural network in evaluating tone production of Mandarin-speaking children. Subjects and methods. Speech materials were recorded from 61 normal-hearing children. The fundamental frequency (F0) of each monosyllabic word was extracted and then used as inputs to a feed-forward backpropagation artificial neural network. The number of inputs was set at 12, whereas the number of hidden neurons was set at 16 in the neural network. The output layer consisted of four neurons representing the four Mandarin tone patterns. The tone recognition performance of the neural network was further compared with that of native Mandarin-speaking adult listeners. Results. The neural network successfully classified the tone patterns of the 61 child speakers with an accuracy of about 85% correct. This high accuracy exceeded the tone recognition performance by the adult listeners. Individual child speakers showed varied tone production accuracy as recognized by the adult listeners or by the neural network.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0547,Effects of frequency allocation on lexical tone identification by Mandarin-speaking children with a cochlear implant,"Conclusion. Frequency allocation with extended frequency ranges yielded significantly higher accuracy in pediatric CI recipients' lexical tone identification. These findings suggest that frequency allocation with extended frequency ranges may be useful in improving lexical tone recognition for at least some pediatric CI recipients. Objectives. To assess the effects of frequency allocation on lexical tone identification by Mandarin-speaking children with a cochlear implant (CI). Subjects and methods. In a prospective study, 15 prelingually deafened children between 7.17 and 16.17 years of age served as participants. Using Med-el CI devices, each participant's accuracy in lexical tone identification was compared in two conditions: first, the experimental condition, i.e. use of the extended frequency range from 233 to 8501 Hz; second, the control condition, i.e. use of the participant's clinically assigned frequency range from 300 to 8404 Hz. Results. The group mean of pediatric CI users' accuracy in lexical tone identification was 88.02% (SD = 6.31%) in the experimental condition and 83.82% (SD = 9.84%) in the control condition. The group mean was 4.20% (SD = 5.48%) higher in the experimental condition than that in the control condition; this difference was statistically significant (t(14) = 2.97, p=0.010).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0548,Lexical tone identification and consonant recognition in acoustic simulations of cochlear implants,"Conclusion: Cochlear implant (CI) recipients' performance of lexical tone identification and consonant recognition can be enhanced by providing greater spectral details. Objective: To evaluate the effects of increasing the number of total spectral channels on the lexical tone identification and consonant recognition by normally hearing listeners who are native speakers of Mandarin Chinese. Subjects and methods: Lexical tone identification and consonant recognition were measured in 15 Mandarin-speaking, normal-hearing (NH) listeners with varied numbers of total spectral channels (i.e. 4, 6, 8, 10, 12, 16, 20, and 24), using acoustic simulations of CIs. Results: The group of NH listeners' performance of lexical tone identification ranged from 44.53% to 66.60% with 4-24 spectral channels. The performance of tone identification between channels 4 and 16 remained similar; between channels 16 and 20 performance improved significantly. As regards consonant recognition, the NH listeners' overall accuracy ranged from 73.17% to 95.33% with 4-24 channels. Steady improvement in consonant recognition accuracy was observed as a function of increasing the spectral channels. With about 12-16 spectral channels, the NH listeners' overall accuracy in consonant recognition began to be comparable to their accuracy with the unprocessed stimuli.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0549,Categorical perception of lexical tones in native Mandarin-speaking listeners with sensorineural hearing loss,"Background: Categorical perception (CP) of lexical tones was examined in normal hearing (NH) people, but it was unclear whether lexical tones can be perceived categorically in sensorineural hearing loss (SNHL) people. Objectives: To explore the characteristic of lexical tone perception in native Mandarin speakers with SNHL. Materials and methods: Three types of continuum (Tone1/Tone2, Tone1/Tone4 and Tone2/Tone3) were constructed and each of them includes 15 stimuli which were resynthesized by applying the pitch-synchronous overlap and add (PSOLA) method implemented in Praat to the same Mandarin syllable, /a/, with a high-level tone produced by a female speaker. Forty native Mandarin NH speakers and 23 native Mandarin speakers with mild to moderate SNHL were recruited. A two alternative-forced-choice identification task was used to acquire the tonal perceptual data. Results: All tone perception curves owns the characteristic of CP in SNHL subjects. All tone perception curves were S-shape in SNHL subjects same as those in NH subjects. No significant difference of each continuum was observed between SNHL and NH. Conclusions: CP of lexical tone perception could be observed in native Mandarin speakers with mild to moderate SNHL. The slight damage in the peripheral auditory system did not change characteristic of lexical tone perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0550,Bimodal benefits in Mandarin-speaking cochlear implant users for music perception and tone recognition,"Background: Cochlear implant (CI) users have difficulty appreciating music and perceiving lexical tones in Mandarin Chinese. Wearing a hearing aid (HA) in the contralateral ear for bimodal hearing may provide additional benefits. Objectives: To measure the bimodal benefits of music perception and tone recognition and to investigate the relationship between the two in Mandarin-speaking bimodal CI subjects. Materials and methods: Sixteen Mandarin-speaking bimodal CI subjects (aged between 16 and 49 years) participated in the study. Music perception (pitch discrimination, melody discrimination and instrument identification) and lexical tone recognition were tested with electric stimulation (CI alone) or bimodal stimulation (CI + HA). Results: Subjects showed a significant bimodal benefit in tone recognition in quiet and noise, and in all music perception tests. The bimodal benefit for tone recognition in noise was significantly correlated with that of pitch discrimination thresholds and instrument identification scores. Conclusion: Mandarin-speaking bimodal CI users achieved better music perception and tone recognition ability with CI + HA than with CI alone. The bimodal benefit of tone recognition was significantly correlated with that of music perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0551,The contribution of sensitivity to speech rhythm and non-speech rhythm to early reading development,"Both sensitivity to speech rhythm and non-speech rhythm have been associated with successful phonological awareness and reading development in separate studies. However, the extent to which speech rhythm, non-speech rhythm and literacy skills are interrelated has not been examined. As a result, five- to seven-year-old English-speaking children were assessed on measures of speech rhythm sensitivity, non-speech rhythm sensitivity (both receptive and productive), reading attainment and phonological awareness. Hierarchical regression analyses revealed that productive non-speech rhythm was unable to predict variance in reading attainment independently of phonological awareness and speech rhythm sensitivity. Receptive sensitivity to speech rhythm and non-speech rhythm were both able to predict a significant amount of unique variance in reading attainment after controlling for age, vocabulary, phonological awareness, short-term memory and each other. The findings suggest that receptive sensitivity to speech rhythm and non-speech rhythm, while related to each other, also make contributions to reading attainment that are independent of each other. These findings provide only partial consistency with the general auditory processing deficit theory of reading difficulties, but are in line with the emerging theoretical claim that sensitivity to speech prosody may be implicated in successful literacy development.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0552,The role of Mandarin lexical tones in lexical access under different contextual conditions,"The current study pursues Ye & Connine's (1999) suggestion that tonal information is Much more important when words are presented in context, than in isolation. Disyllabic Mandarin words were either presented normally, or with changes in their segmental and/or tonal structure. Critically, these items were presented in isolation, in sentence context, and in idioms; previous studies have not examined these issues in sentential context. In Experiment 1, native Mandarin speakers made lexical decisions about these items. In Experiment 2, the critical stimuli were presented in white noise, and the listeners' task was to detect the vowels and the tones of the stimuli. The results supported a more important role for tonal Cites when the stimulus is presented in context than when it is in isolation; this pattern depended on the task conditions, as suggested by Soto-Faraco et al. (2001), and Mattys et al. (2005).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0553,The impact of tone systems on the categorical perception of lexical tones: An event-related potentials study,"This study investigates the categorical perception (CP) of pitch contours (level and rising) by native listeners of two tone languages, Mandarin and Cantonese, for both speech and nonspeech. Language background was found to modulate participants' behavioural and electrophysiological responses to stimuli presented in an active oddball paradigm, comprising a standard and two equally spaced deviants (within- and across-category). The stimuli were divided into two sets according to the results of a two-alternative forced-choice identification test: a rising set, using a standard that listeners identified as high rising tone, and a level set, using a standard that listeners identified as high level tone. For the rising set, both groups of listeners exhibited CP in terms of their behavioural response. However, only Cantonese listeners exhibited a significant CP effect in terms of P300 amplitude. For the level set, the behavioural data revealed a shift in category boundary due, in part, to the range-frequency effect. According to the d' scores, the CP effect elicited from Mandarin listeners was greater for nonspeech stimuli than for speech, suggesting the presence of a psychophysical boundary. There was no such behavioural contrast for Cantonese listeners. However, Cantonese listeners exhibited a significant CP effect in P300 amplitude that was influenced by the range-frequency effect, as well as a possible secondary phonological boundary. P300 amplitude is believed to index the ease of discrimination of speech stimuli by phonological information. We conclude that Cantonese listeners engaged phonological processing in order to discriminate speech stimuli more efficiently than Mandarin listeners. These findings may be due to the different tonal inventories of Mandarin and Cantonese, with Cantonese listeners required to make finer distinctions in perception of pitch height and slope than Mandarin listeners in order to discriminate the denser tone system of Cantonese.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0554,A theory of second occurrence focus,"This paper proposes to analyse second occurrence foci as foci whose domain is properly contained in the background of another focus domain, and linearly follows the last focus of that domain. It is shown that general assumptions about the representation and prosodic realisation of focus predict that foci with these properties will be realised by stress, but not pitch accent, i.e. as second occurrence foci. Furthermore, whether a focus domain is subordinated in this sense follows from general principles of focus assignment and interpretation. No assumptions specific to second occurrence foci are required to explain the phenomenon. The analysis relies on, and thus indirectly supports, the assumptions that focus/background, rather than new/given are the relevant concepts in stress and accent assignment, and that focus realisation is mediated by prosodic, particularly metrical, structure.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0555,Synthesising meaning and processing approaches to prosody: performance matters,"Words vary in acoustic prominence; for example, repeated words tend to be reduced, while focused elements tend to be acoustically prominent. We discuss two approaches to this phenomenon. On the message-based view, acoustic choices signal the speaker's meaning or pragmatics, or are guided by syntactic structure. On the facilitation-based view, reduced forms reflect facilitation of production-processing mechanisms. We argue that message-based constraints correlate systematically with production facilitation. Moreover, we argue that discourse effects on acoustic reduction may be at least partially mediated by processing facilitation. Thus, research needs to simultaneously consider both competence (message) and performance (processing) constraints on prosody, specifically in terms of the psychological mechanisms underlying acoustic reduction. To facilitate this goal, we present preliminary processing models of message- and facilitation-based approaches and outline directions for future research.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0556,Processing spoken Chinese: The role of tone information,"Vowel and tone monitoring tasks were used to investigate the role of tone information in processing Mandarin. In Experiment 1, participants monitored for a tone-plus-vowel combination (tone 2-/a/). Both existent (known) and non-existent syllables contained targets and monitoring reaction times showed faster monitoring times for existent syllable carriers. Non-target bearing existent syllables mismatched on either the vowel or the tone and showed faster responses for vowel mismatch stimuli. This finding supports a perceptual advantage for vowel information. In Experiment 2, separate vowel and tone monitoring tasks were used for target syllables that occurred phrase-finally in idiomatic (highly constraining) phrases or neutral semantic context. In the neutral context, vowel monitoring was faster than tone monitoring. In contrast, highly predictive contexts showed a tone advantage. Experiment 3 used idiomatic contexts that did or did not contain a mismatching tone in the penultimate syllable. Mismatching tones were very similar to or highly distinct from the intended tone and both tone and vowel monitoring reaction times showed graded effects based on similarity of the mismatch. A modified version of the TRACE model for tone languages that includes a separate level or representation for tones that permit graded activation and highly interconnected syllable representations is discussed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0557,Development of a Setswana tonal minimal pair word list as research tool,"This study reports on the development of a Setswana tonal minimal pair word list, which could be implemented as research tool in the field of Bantu language linguistics and in speech pathology in South Africa. The development of the list was conducted in four phases. These are described as four separate studies. All involved Setswana L1 participants living in the urban areas of Gauteng. In Study 1, a 45-pair preliminary list was compiled from dictionaries. During Study 2, eleven L1 speakers' familiarity with each word was determined. Based on these results the list was narrowed down to 20 pairs. Study 3 entailed the validation of pictorial stimuli, which illustrate the target words. Ten different participants took part. Four pairs were not consistently familiar and were removed, resulting in the experimental list of 16 pairs. This list was validated in Study 4 and involved nine typical L1 speakers and five listeners. Word-specific analyses revealed that some words had a negative impact on the results. Six pairs were removed. A final list of 10 pairs rendered results more aligned to the expectation of typical speakers and listeners. Validation should continue to determine applicability in populations from exclusively rural or urban areas.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0558,Impaired perception of harmonic complexity in congenital amusia: A case study,"This study investigates whether congenital amusia (an inability to perceive music from birth) also impairs the perception of musical qualities that do not rely on fine-grained pitch discrimination. We established that G. G. (64-year-old male, age-typical hearing) met the criteria of congenital amusia and demonstrated music-specific deficits (e.g., language processing, intonation, prosody, fine-grained pitch processing, pitch discrimination, identification of discrepant tones and direction of pitch for tones in a series, pitch discrimination within scale segments, predictability of tone sequences, recognition versus knowing memory for melodies, and short-term memory for melodies). Next, we conducted tests of tonal fusion, harmonic complexity, and affect perception: recognizing timbre, assessing consonance and dissonance, and recognizing musical affect from harmony. G. G. displayed relatively unimpaired perception and production of environmental sounds, prosody, and emotion conveyed by speech compared with impaired fine-grained pitch perception, tonal sequence discrimination, and melody recognition. Importantly, G. G. could not perform tests of tonal fusion that do not rely on pitch discrimination: He could not distinguish concurrent notes, timbre, consonance/dissonance, simultaneous notes, and musical affect. Results indicate at least three distinct problems-one with pitch discrimination, one with harmonic simultaneity, and one with musical affect-and each has distinct consequences for music perception.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0559,Prosody in parsing morphologically complex words: Neurophysiological evidence,"Little is known about the neurophysiological correlates of lexical prosody in the comprehension of compound words, i.e., morphologically complex words. Here, it is investigated whether lexical prosody influences the decomposition of spoken compound words. In order to explore the neurophysiological correlates (event-related potentials, ERP) of a compound prosody, German native speakers had to judge the number agreement between numerals and nouns which did or did not agree in 50% of the cases. Importantly, the nouns carried either a compound or non-compound (single noun) prosody. The compound prosody led to increased reaction times (RTs) and reduced judgement accuracy. Critically, number violations for words with a compound prosody elicited an increased ERP negativity that was delayed by about 600ms relative to a left-anterior negativity elicited by number violations for a single noun prosody. The ERP effect for the compound prosody preceded the according behavioural response by about 200ms and the ERP peak latency effect correlated with the RT effect. These findings suggest that the ERP effect for the compound prosody could be functionally related to the accurate judgement performance for the compound prosody. The results suggest, more generally, that prosody plays a critical role in auditory compound comprehension and morphological processing.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0560,Going on with optimised feet: Evidence for the interaction between segmental and metrical structure in phonological encoding from a case of primary progressive aphasia,"Background: Our knowledge about the interaction between segmental and metrical levels of representation in word production is still largely underspecified. In particular, there is only sparse evidence of how syllables are hierarchically organised into higher-level prosodic structures such as prosodic feet and words. Furthermore, the question whether stress assignment in German is sensitive to syllable weight is unresolved so far. While quantity-insensitive accounts state that stress is predominantly assigned to a default position (i.e., to the penultimate syllable) and other stress patterns are exceptional, quantity-sensitive accounts assume that stress assignment is determined by the weight of the final two syllables. Aims: Impaired lexical retrieval may lead to regularisations of stress assignment. Such an error pattern will be examined to gain insights into the interrelation between different tiers of prosodic representations (e.g., syllable, foot, prosodic word). Methods & Procedures: A reading and a repetition task were conducted with German-speaking patient HT, suffering from primary progressive aphasia, which especially affected her retrieval of lexical information. The material consisted of polysyllabic words with varying stress patterns and syllable structures. Outcomes & Results: In reading, HT produced hardly any segmental errors, but a substantial amount of stress errors. Importantly, the patient not only over-generalised the default penultimate stress as would have been predicted by quantity-insensitive approaches. Instead, she over-applied different stress patterns depending on the weight of the last two syllables. In repetition, HT's output can be characterised as phonological jargon. Crucially, however, she hardly produced any stress errors. Rather, thorough analyses revealed that segmental deviations in her output led to optimised prosodic structures. For instance, insertions of rhyme segments could be observed mainly in strong syllables, i.e., syllables bearing main or secondary stress, whereas deletions occurred predominantly in weak, unstressed syllables. Conclusions: The present data provide evidence for specific forms of interaction between segmental and metrical knowledge: On the one hand, segmental information influenced the patient's stress assignment errors in reading. On the other hand, prosodic information modified segmental errors even in severe jargon observed in repetition. With respect to the prosodic system of German, the observed error patterns show that the structure of the final syllable determines how syllables of a word are parsed into prosodic feet and, accordingly, which syllable has to be prominent. Thus, our results support quantity-sensitive approaches of stress assignment.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0561,Perceptual discrimination of Shona lexical tones and low-pass filtered speech by left and right hemisphere damaged patients,"Background: While the role of the right hemisphere (RH) in prosodic processing is prominent, research on the perception of lexical tones has shown that left hemisphere damaged (LHD) patients are more impaired than right hemisphere damaged (RHD) patients. Dichotic listening and imaging studies with healthy speakers of tone languages demonstrate that at least at the phonemic and lexical level, prosody is processed in the left hemisphere (LH) when the variations in pitch are phonemically distinctive. There is no report available yet on the perceptual discrimination of a Bantu language in patients after unilateral brain damage. Aims: We addressed the question of how well Shona aphasic patients and right hemisphere damaged patients perceive pitch contrasts in Shona lexical words and also in their homologous low-pass filtered counterparts. We also sought to discover the validity of the current hypotheses on hemispheric lateralisation particularly the hypothesis on hemispheric lateralisation based on language function to account for the Shona data. Methods Procedures: A total of 7 LHD and 7 RHD patients and 14 healthy controls participated in two discrimination tasks that examined perception of lexical tone in (a) bisyllabic Shona words and (b) low-pass filtered stimuli. In both tasks the participants were tasked with judging the pitch as the same or different in 120 bisyllabic words and 120 low-pass filtered stimuli. Outcomes Results: The results demonstrated that the tonal discrimination of the LHD group was more reduced in comparison to the RHD group and control participants. However, the performance of the RHD patients was not error free relative to the control participants, although significantly better than the LHD patients in both tasks. Conclusions: At least for the phonemic and lexical levels, brain damage to the dominant hemisphere results in lexical tone impairment for LHD patients, and cognitive load processing results in a subdued but good performance for RHD patients. The LH is therefore dominant for processing tone when it is lexically distinctive.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0562,Lexical tone disruption in Shona after brain damage,"Background: The issue of production and perception of lexical tone in patients with brain lesions has been investigated mainly through East Asian languages and Norwegian. The present study investigated the lateralisation of lexical tone in Shona, a Bantu language. Van Lancker (1980) proposed a continuum scale of the levels of functional pitch in the speech signal. According to the functional lateralisation account (FLH), the left hemisphere (LH) is associated with highly structured pitch contrasts, such as phonological tone, whereas the right hemisphere (RH) is specialised for the least structured pitch functions cueing emotional and personal information. The extant data show that the ability to produce and identify lexical tone is frequently more impaired as a result of lesions to the LH than RH lesions. Aims: The present investigation focused on the lateralisation of lexical tone in Shona speakers. The study sought to validate if the scale of hemispheric lateralisation as proposed by Van Lancker (1980) is also valid for Shona, a Bantu language. Methods & Procedures: We examined five LH damaged (LHD) patients and five RH (RHD) damaged patients using a confrontational picture-naming task and a lexical tone identification task of Shona lexical tone. The first experiment investigated the ability of LHD patients and RHD patients to identify Shona lexical tone in 60 disyllabic minimal pairs. The second experiment examined the ability of Shona brain-damaged patients to produce lexical tone using a confrontational picture-naming task with 120 lexical items. Outcomes & Results: We observed a dissociation in the performance of both the LHD and RHD patients in the two tasks. Both groups were impaired in the tone identification task relative to the non-brain-damaged controls. However, RHD patients performed significantly better than the LHD patients in the tone identification task. On the other hand, both LHD and RHD groups were equally impaired in the tone production task in comparison to the controls. Conclusions: The discrepancy in the production and perception of Shona lexical tone for this group of brain-damaged patients shows that, although the two modes are related, they do not always get disrupted at the same level after brain damage. The results from the tone identification task suggest to a certain extent that the FLH is also valid for Shona. In order to account for all the data there is need to carefully consider alternative accounts like the acoustic cue hypothesis (Van Lancker & Sidtis, 1992).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0563,CROSSED APHASIA - NEW PERSPECTIVES,"Crossed aphasia reflects an unusual lateralization pattern characterized by a dissociation between manual and language hemispheric dominance in a right-handed individual. In order to circumscribe the syndrome of crossed aphasia, authors have developed stringent exclusion criteria to define that syndrome. We believe that such an approach artificially created a homogeneous syndrome. As an alternative to the rigorous exclusion criteria approach, we propose to broaden the definition of crossed aphasia to accommodate the various demonstrated aphasic symptoms. The interest in crossed aphasia can then shift from the search for a universal symptom complex to different subgroups of patients. Indeed, there now appears to be overwhelming evidence (including Wada experiments) to indicate that crossed aphasia can either be the mirror image of uncrossed aphasia or indicate a bilateral language representation pattern. Two patients are presented and their symptoms arc discussed in the light of the literature. One of our patients is the only one to our knowledge to evidence affective prosody impairments. We argue that this latter symptom be added to the list of associated signs to be assessed as part of a crossed aphasia battery.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0564,Tone perception deficits in Chinese-speaking Broca's aphasics,Recent studies have demonstrated some type of tone production problems following left hemisphere lesions in severely aphasic tone-language speakers. We studied the ability of Toisanese Chinese-speaking Broca's aphasics to comprehend lexical tone. Sets of monosyllabic words differing only by tone permitted comprehension testing via a word-picture matching task. The pattern of tone confusion in the aphasics was found to be an exaggeration of that experienced by the normal controls.,,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0565,Effect of intensive voice treatment on tone-language speakers with Parkinson's disease,"The aim of this study was to investigate the effect of intensive voice therapy on Cantonese speakers with Parkinson's disease. The effect of the treatment on lexical tone was of particular interest. Four Cantonese speakers with idiopathic Parkinson's disease received treatment based on the principles of Lee Silverman Voice Treatment (LSVT). Outcome measures included perceptual measures and acoustic correlates of loudness, pitch, intonation, and tone (only intonation and tone are detailed in this study). All four participants demonstrated an increase in loudness, an increase in pitch and pitch range, and improved intonation during connected speech, as measured perceptually and acoustically. However, there were no obvious changes in the accuracy of lexical tone production, based on either phonetic transcription or acoustic analysis. The lack of improvement in lexical tones may support previous claims of a dissociation in fundamental frequency control for intonation and lexical tone production.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0566,How do autistic adults use syntactic and prosodic cues to manage spoken discourse?,"Discourse studies investigating differences in the socio-communicative profiles of autistic (ASD) and neurotypical (NT) individuals have mostly relied on orthographic transcriptions, without taking prosodic information into account. However, atypical prosody is ubiquitous in ASD and a more accurate representation of their discourse abilities should also include prosodic cues. This exploratory study addresses this gap by segmenting the spoken discourse of 12 ASD and NT adults using the framework of Basic Discourse Units (BDUs). BDUs result from the mapping of syntactic boundaries on prosodic units, which can coincide in different ways and are associated with different discourse strategies. We hypothesized that the discourse of ASD adults would display more atypical strategies than NT adults, reflecting a 'pedantic' style and more difficulties in managing ongoing discourse. While ASD adults did not produce more discourse units associated with didactic or pedantic strategies than NT adults, they did produce less units associated with strategies of interactional regulation. This study provides initial evidence that multidimensional linguistic units, such as BDUs can help differentiate speech delivery strategies of ASD adults from those of their NT peers, even based on simple prosodic cues like silent pauses.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0567,Lexical tone perception and production in Cantonese-speaking children with childhood apraxia of speech: a pilot study,"Childhood apraxia of speech (CAS) is a paediatric motor speech disorder. We investigated the lexical tone perception and production abilities of children with CAS and the relationships between the two. Three children with CAS, aged between 3;7 and 5;8, were given the Cantonese Tone Identification Test (CANTIT) and the Hong Kong Cantonese Articulation Test (HKCAT) for assessment of tone perception and production, respectively. Accuracy and error patterns were investigated based on their performance on the two tests. Correlation analysis was performed on children's perception and production scores. Two children scored at the lowest rank on the CANTIT, while one child obtained a Z score of 0. All children scored three standard deviations below the mean on the HKCAT. No statistical differences were found among the six tones with respect to perception accuracy, H(5) = 3.731, p = 0.589. Error analysis showed that children with CAS demonstrated more confusion on perceiving tones compared with TD peers. There were no main effects for task (F(1,2) = 0.040, p = 0.859) or tone (F(5,10 = 0.997, p = 0.467); nor were there task or tone interaction effects on perception versus production accuracy (F(5,10) = 1.772, p = 0.206). Tone perception and production accuracy were not significantly correlated (r(2) = 0.181, p = 0.078). Tone perception deficits were evident in two out of three children with CAS, while all children had lexical tone production difficulties. In this small sample, tone production was more universally affected than tone perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0568,Production of Tone 2 in disyllabic words in Mandarin Chinese speaking children aged 3-5 with a cochlear implant and a contralateral hearing aid,"To investigate Mandarin Tone 2 production of disyllabic words of prelingually deafened children with a cochlear implant (CI) and a contralateral hearing aid (HA) and to evaluate the relationship between their demographic variables and tone-production ability. Thirty prelingually Mandarin-speaking preschoolers with CI+HA and 30 age-matched normal-hearing (NH) children participated in the study. Fourteen disyllabic words were recorded from each child. A total of 840 tokens (14 x 60) were then used in tone-perception tests in which four speech therapists participated. The production of T2-related disyllabic words of the bimodal group was significantly worse than that of the NH group, as reflected in the overall accuracy (88.57% +/- 16.31% vs 99.29% +/- 21.79%, p < 0.05), the accuracy of T1+T2 (93.33% vs 100%), the accuracy of T2+T1 (66.67 +/- 37.91% vs 98.33 +/- 9.13%), and the accuracy of T2+T4 (78.33 +/- 33.95% vs 100%). In addition, the bimodal group showed significantly inferior production accuracy of T2+T1 than T2+T2 and T3+T2, p < 0.05. Both bimodal age and implantation age were significantly negatively correlated with the overall production accuracy, p < 0.05. For the error patterns, bimodal participants experienced more errors when T2 was in the first position of the tone combination, and T2 was most likely to be mispronounced as T1 and T3. Bimodal patients aged 3-5 have T2-related disyllabic lexical tone production defects, and their performances are related to tone combination, implantation age, and bimodal age.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0569,Perception of grammatical tone in Akan patients with left and right hemisphere brain damage,"It remains a matter of debate what roles the left and right hemispheres play in processing speech prosody. Brain lesion studies have demonstrated that lexical tone perception among native speakers of tonal languages is more disrupted in left hemisphere damaged (LHD) individuals than right hemisphere damaged (RHD) individuals. This has been taken to suggest that linguistically-relevant prosodic cues are predominantly left-lateralised, whereas non-linguistic stimuli are predominantly right-lateralised. However, this phenomenon has only been examined in lexical tone, leaving grammatical tone perception unexplored. The aim of this study was twofold: Firstly, to examine how individuals with LHD and RHD perceive grammatical tone, and secondly to compare grammatical tone to non-linguistic tone perception. Therefore, native Akan speakers with LHD, RHD and no-brain damage (NBD) controls were tested in two discrimination tasks that examined linguistic and non-linguistic tone perception. The results showed that while both the individuals with LHD and RHD show impairment in grammatical tone perception, there was a trend of a better performance for the RHD group. Nonetheless, for non-linguistic tone perception, individuals with LHD outperformed the RHD individuals, although both had reduced performance compared to the NBD individuals. A further analysis revealed that the reduced perceptual abilities of both the LHD and RHD groups in grammatical tone perception can be attributed to grammatical problems rather than tone per se. We conclude that there is potentially a bilateral involvement of the two hemispheres in grammatical tone processing, with the left being the dominant hemisphere.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0570,There's more to emotion than meets the eye: A processing bias for neutral content in the domain of emotional prosody,"Research on emotion processing in the visual modality suggests a processing advantage for emotionally salient stimuli, even at early sensory stages; however, results concerning the auditory correlates are inconsistent. We present two experiments that employed a gating paradigm to investigate emotional prosody. In Experiment 1, participants heard successively building segments of Jabberwocky osentenceso spoken with happy, angry, or neutral intonation. After each segment, participants indicated the emotion conveyed and rated their confidence in their decision. Participants in Experiment 2 also heard Jabberwocky osentenceso in successive increments, with half discriminating happy from neutral prosody, and half discriminating angry from neutral prosody. Participants in both experiments identified neutral prosody more rapidly and accurately than happy or angry prosody. Confidence ratings were greater for neutral sentences, and error patterns also indicated a bias for recognising neutral prosody. Taken together, results suggest that enhanced processing of emotional content may be constrained by stimulus modality.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0571,The Role of the Establishment of Causal Connections and the Modality of Presentation of Discourse in the Generation of Emotion Inferences by Argentine College Students,"The purpose of this study was to examine the role of the causal connectivity of the statements (their total number of causal connections) and the modality of presentation of discourse (oral-written) in the generation of emotion inferences by Spanish-speaking students. With this aim, we asked a group of Argentine college students to either listen to or read an excerpt of a radio interview (on the topic 'Argentine radio presenters' relationship with the audience'), and to perform an emotions recall task. Results indicated that statements that promoted the generation of emotion inferences and had a high number of causal connections were more often included in the recall protocols than those that promoted the generation of these inferences and had a low number of them. They also indicated that participants who were presented with the spoken version of the materials included a higher number of statements in this task than those that were presented with the written version. These findings suggest that the establishment of meaningful discourse connections has a role in the generation of emotion inferences, and that there appear to be differences in the representation that listeners and readers construct of speakers' emotions.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0572,Exploring the role of lexical stress in lexical recognition,"Three cross-modal priming experiments examined the role of suprasegmental information in the processing of spoken words. All primes consisted of truncated spoken Dutch words. Recognition of visually presented word targets was facilitated by prior auditory presentation of the first two syllables of the same words as primes, but only if they were appropriately stressed (e.g., OKTOBER preceded by ok TO-); inappropriate stress, compatible with another word (e.g., OKTOBER preceded by OCto-, the beginning of octopus), produced inhibition. Monosyllabic fragments (e.g., OC-) also produced facilitation when appropriately stressed; if inappropriately stressed, they produced neither facilitation nor inhibition. The bisyllabic fragments that were compatible with only one word produced facilitation to semantically associated words, but inappropriate stress caused no inhibition of associates. The results arc explained within a model of spoken-word recognition involving competition between simultaneously activated phonological representations followed by activation of separate conceptual representations for strongly supported lexical candidates; at the level of the phonological representations, activation is modulated by both segmental and suprasegmental information.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0573,AGE DIFFERENCES IN LANGUAGE SEGMENTATION,"Reading bears the evolutionary footprint of spoken communication. Prosodic contour in speech helps listeners parse sentences and establish semantic focus. Readers' regulation of input mirrors the segmentation patterns of prosody, such that reading times are longer for words at the ends of syntactic constituents. As reflected in these micropauses, older readers are often found to segment text into smaller chunks. The mechanisms underlying these micropauses are unclear, with some arguing that they derive from the mental simulation of prosodic contour and others arguing they reflect higher-level language comprehension mechanisms (e.g., conceptual integration, consolidation with existing knowledge, ambiguity resolution) that are common across modality and support the consolidation of the memory representation. The authors review evidence based on reading time and comprehension performance to suggest that (a) age differences in segmentation derive both from age-related declines in working memory, as well as from crystallized ability and knowledge, which have the potential to grow in adulthood, and that (b) shifts in segmentation patterns may be a pathway through which language comprehension is preserved in late life.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0574,Formal distinctiveness of high- and low-imageability nouns: Analyses and theoretical implications,"Words associated with perceptually salient, highly imageable concepts are learned earlier in life, more accurately recalled, and more rapidly named than abstract words (R. W. Brown, 1976; Walker & Hulme, 1999). Theories accounting for this concreteness effect have focused exclusively on semantic properties of word referents. A novel possibility is that word structure may also contribute to the effect. We report a corpus-based analysis of the phonological and morphological structures of a large set of nouns with imageability ratings (N = 2,023). High- and low-imageability nouns differed by length, etymology, prosody, affixation, phonological neighborhood density, and rates of consonant clustering. On average, nouns denoting abstract concepts were longer, more derivationally complex, and emerged in English from a different distribution of languages than did concrete nouns. We address implications for interactivity of word form and meaning as pertain to theories of word concreteness, lexical acquisition, and word processing.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0575,Repetition and variation in the Catalan translation of Virginia Woolf's The years: a corpus-based approach,"The present study is a corpus-based contrastive analysis of the use of repetitive patterns in Virginia Woolf's novel The Years (1937) and its Catalan translation (1988) by Maria-Antonia Oliver. For our study, we build a parallel corpus in which both the source text and the translation are aligned, and then focus on three kinds of repetition: (a) The question Where am I?; (b) the phrase hum in time to + N, and (c) the construction 'verb indicating stillness (sit, stay horizontal ellipsis ) + for a moment + verb of perception.' All these patterns are stylistic devices that Woolf uses for the representation of time in its diverse and overlapping manifestations: as an external measurable dimension and as an internal continuous stream. The use of a parallel corpus allows us to study how all occurrences of these repetitions have been translated into Catalan and to identify the semantic prosody surrounding the three expressions in both English and Catalan. Finally, our corpus-based approach provides evidence for a description of Oliver's translation method, which we define as intentionally literal, though mindful of the numerous stylistic and lyrical effects that can characterize Woolf's prose.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0576,Intonation Analysis of Ragas in Carnatic Music,"Intonation is a fundamental music concept that has a special relevance in Indian art music. It is characteristic of a raga and key to the musical expression of the artist. Describing intonation is of importance to several music information retrieval tasks such as developing similarity measures based on ragas and artists. In this paper, we first assess raga intonation qualitatively by analysing varnarns, a particular form of Carnatic music compositions. We then approach the task of automatically obtaining a compact representation of the intonation of a recording from its pitch track. We propose two approaches based on the parametrization of pitch-value distributions: performance pitch histograms, and context-based svara distributions obtained by categorizing pitch contours based on the melodic context. We evaluate both approaches on a large Carnatic music collection and discuss their merits and limitations. We finally go through different kinds of contextual information that can be obtained to further improve the two approaches.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0577,A Robust Parser-Interpreter for Jazz Chord Sequences,"Hierarchical structure similar to that associated with prosody and syntax in language can be identified in the rhythmic and harmonic progressions that underlie Western tonal music. Analysing such musical structure resembles natural language parsing: it requires the derivation of an underlying interpretation from an unstructured sequence of highly ambiguous elements-in the case of music, the notes. The task here is not merely to decide whether the sequence is grammatical, but rather to decide which among a large number of analyses it has. An analysis of this sort is a part of the cognitive processing performed by listeners familiar with amusical idiom, whether musically trained or not. Our focus is on the analysis of the structure of expectations and resolutions created by harmonic progressions. Building on previous work, we define a theory of tonal harmonic progression, which plays a role analogous to semantics in language. Our parser uses a formal grammar of jazz chord sequences, of a kind widely used for natural language processing (NLP), to map music, in the form of chord sequences used by performers, onto a representation of the structured relationships between chords. It uses statistical modelling techniques used for wide-coverage parsing in NLP to make practical parsing feasible in the face of considerable ambiguity in the grammar. Using machine learning over a small corpus of jazz chord sequences annotated with harmonic analyses, we show that grammar-based musical interpretation using simple statistical parsing models ismore accurate than a baseline HMM. The experiment demonstrates that statistical techniques adapted from NLP can be profitably applied to the analysis of harmonic structure.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0578,From melody to lexical tone: Musical ability enhances specific aspects of foreign language perception,"Previous research shows that music ability provides positive effects on language processing. This study aims at better clarifying the involvement of different linguistic subdomains in this cross-domain link, assessing whether or not musicality and music expertise enhance phonological and lexical tone processing of Mandarin Chinese. In two experiments different groups of adults and children with no previous experience in tonal languages, were invited to perform a same-different task trying to detect phonological and tonal variations in pairs of sequences of monosyllabic Mandarin Chinese words. Main results show that all subjects perform significantly better in detecting phonological variations rather than tonal ones. They also show that both melodic proficiency and music expertise are good predictors for a better tonal, but not phonological identification. Data lead to a model of music-to-language transfer effect in which musicality selectively affects linguistic intonation while leaving phonological processing substantially unaffected.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0579,"Multiple views of the response of an ensemble of spectro-temporal features support concurrent classification of utterance, prosody, sex and speaker identity","Models of auditory processing, particularly of speech, face many difficulties. These difficulties include variability among speakers, variability in speech rate and robustness to moderate distortions such as time compression. In contrast to the 'invariance of percept' (across different speakers, of different sexes, using different intonation, and so on) is the observation that we are sensitive to the identity, sex and intonation of the speaker. In previous work we have reported that a model based on ensembles of spectro-temporal feature detectors, derived from onset sensitive pre-processing of a limited class of stimuli, preserves significant information about the stimulus class. We have also shown that this is robust with respect to the exact choice of feature set, moderate time compression in the stimulus and speaker variation. Here we extend these results to show a) that by using a classifier based on a network of spiking neurons with spike-driven plasticity, the output of the ensemble constitutes an effective rate coding representation of complex sounds; and b) that the same set of spectro-temp oral features concurrently preserve information about a range of qualitatively different classes into which the stimulus might fall. We show that it is possible for multiple views of the same pattern of responses to generate different percepts. This is consistent with suggestions that multiple parallel processes exist within the auditory 'what' pathway with attentional modulation enhancing the task-relevant classification type. We also show that the responses of the ensemble are sparse in the sense that a small number of features respond for each stimulus type. This has implications for the ensembles' ability to generalise, and to respond differentially to a wide variety of stimulus classes.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0580,A systematic review of cross-linguistic and multilingual speech and language outcomes for children with hearing loss,"The purpose of this study was to systematically review the factors affecting the language, speech intelligibility, speech production, and lexical tone development of children with hearing loss who use spoken languages other than English. Relevant studies of children with hearing loss published between 2000 and 2011 were reviewed with reference to (1) methodologies used, (2) children's outcomes, (3) factors affecting children's outcomes, and (4) publication quality. The review included 117 studies describing 20 languages. Monolingual children were described in 109, and multilingual children were described in 8. Better performance outcomes were frequently associated with earlier age of hearing loss diagnosis, intervention, amplification, and less severe hearing loss - a finding similar to studies of English-speaking children. Studies frequently did not report or include information about participant characteristics, blinding of researchers, and reliability. Cross-linguistic comparison of children's outcomes across studies was not possible due to differences in the outcomes assessed, assessment and analysis methods, and participant characteristics. There is a need for cross-linguistic comparisons of the speech and language outcomes of children with hearing loss, but there is little scope for this using existing published research. Few studies described the outcomes of multilingual children with hearing loss.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0581,TIMING IN MOTOR PROGRAMMING OF TYPING,"An abstract, structural representation in the motor programming of skilled performance may specify both a sequence of responses and expressive features of the sequence, such as timing, stress and intonation, which are mapped separately into the response output. In the case of typing, which does not require expressive features, it was previously assumed that response timing is governed more simply by a timekeeper providing a stochastically regular beat. Two sets of specially constructed texts were given to a fast typist and the results support the idea that skilled typing is paced by a regular beat. There were systematic rhythmic departures from the beat, which arose from contingencies of keyboard movement but were not the simple consequences of these; rather they may be regarded as structured anticipations of the contingencies, and as such are analogous to the expressive features of speech and playing music.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0582,Effect of auditory status on visual emotion recognition in adolescents,"Adolescents with severe to profound hearing loss who wear cochlear implants (CIs) experience significantly more peer problems compared to peers with typical hearing (TH). Differences in peer social dynamics may relate to perception not only of message content, but also message intent based on a speaker's emotion from visual (e.g. facial expressions) and auditory (e.g. prosody) cues. Pediatric CI users may experience greater difficulty with auditory emotion recognition due to an impoverished signal representation provided by the device, but the effect of auditory status on visual emotion recognition yields conflicting results. Objectives: The current study examined accuracy and speed of visual emotion recognition in adolescents with CIs and peers with TH. Methods: Participants included 58 adolescents (10-18 years) stratified by auditory status: 34 CI users and 24 TH peers. Participants identified the intended emotion (i.e. happiness, sadness, anger, fear, disgust, and surprise) of static images of faces displayed on a computer screen. Results: No significant differences by auditory status emerged for response accuracy, response time to all trials, or response time to correct trials. Type of emotion significantly affected both accuracy and response time. Conclusion: Adolescents with CIs show similar accuracy and response time in recognizing static facial expressions compared to TH peers. Future studies should explore the association between visual emotion recognition and social well-being to determine the relationship between emotion recognition and overall quality of life in adolescents with CIs.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0583,"Forms of Hesitation: Tadhabdhub and Metrical Hybridity in Syrian taf'ila poetry, 1962-1975","This essay claims that shi'r al-taf'ila, modern poetry which adheres to one or more of the traditional prosodic feet, witnessed a second ideological turn in the moment leading up to the 1967 defeat. Around 1967, the mixing of meters asserts itself in attempt to grapple with the epistemic rupture in Arabist ideology as keyed to the taf'ila form. Hybrids emerge in Syria of the mid- to late-1960s, where modernistic nathr was cordoned off from poetic practice at the same time as social and political developments dictated a complex representation of interior struggles, paradoxes, and agonistic uncertainties. The readiness to experiment with metrical hybrids retrospectively highlights the silenced presence of metrical hybridity in Shi'r magazine (1956-1964), the carrier of the Arabic modernist project. The poets of Shi'r programmatically elided metrical thinking out of ideological considerations, and this paper wishes to rehabilitate prosody in the Arabic modernist legacy.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0584,Use of a single channel dedicated to conveying enhanced temporal periodicity cues in cochlear implants: Effects on prosodic perception and vowel identification,"The continuous interleaved sampling (CIS) strategy for cochlear implants has well-established limitations for the perception of pitch changes in speech. This study investigated a modification of CIS in which one channel was dedicated to the transmission of a temporal encoding of fundamental frequency (F0). Normal hearing subjects listening to noise-excited vocoders, and implantees were tested on labelling the pitch movement of diphthongal glides, on using intonation information to identify sentences as question or statement, and on vowel recognition. There were no significant differences between modified processing and CIS in vowel recognition. However, while there was limited evidence of improved pitch perception relative to CIS with simplified F0 modulation applied to the most basal channel, in general it appears that for most implant users, restricting F0-related modulation to one channel does not provide significantly enhanced pitch information.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0585,Results using the OPAL strategy in Mandarin speaking cochlear implant recipients,"Objective: To evaluate the effectiveness of an experimental pitch-coding strategy for improving recognition of Mandarin lexical tone in cochlear implant (CI) recipients. Design: Adult CI recipients were tested on recognition of Mandarin tones in quiet and speech-shaped noise at a signal-to-noise ratio of + 10 dB; Mandarin sentence speech-reception threshold (SRT) in speech-shaped noise; and pitch discrimination of synthetic complex-harmonic tones in quiet. Two versions of the experimental strategy were examined: (OPAL) linear (1: 1) mapping of fundamental frequency (F0) to the coded modulation rate; and (OPAL+) transposed mapping of high F0s to a lower coded rate. Outcomes were compared to results using the clinical ACE (TM) strategy. Study sample: Five Mandarin speaking users of Nucleus (R) cochlear implants. Results: A small but significant benefit in recognition of lexical tones was observed using OPAL compared to ACE in noise, but not in quiet, and not for OPAL+ compared to ACE or OPAL in quiet or noise. Sentence SRTs were significantly better using OPAL+ and comparable using OPAL to those using ACE. No differences in pitch discrimination thresholds were observed across strategies. Conclusions: OPAL can provide benefits to Mandarin lexical tone recognition in moderately noisy conditions and preserve perception of Mandarin sentences in challenging noise conditions.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0586,Lexical tone recognition in noise in normal-hearing children and prelingually deafened children with cochlear implants,"Objective: The purpose of the present study was to investigate Mandarin tone recognition in background noise in children with cochlear implants (CIs), and to examine the potential factors contributing to their performance. Design: Tone recognition was tested using a two-alternative forced-choice paradigm in various signal-to-noise ratio (SNR) conditions (i.e. quiet, +12, +6, 0, and -6 dB). Linear correlation analysis was performed to examine possible relationships between the tone-recognition performance of the CI children and the demographic factors. Study sample: Sixty-six prelingually deafened children with CIs and 52 normal-hearing (NH) children as controls participated in the study. Results: Children with CIs showed an overall poorer tone-recognition performance and were more susceptible to noise than their NH peers. Tone confusions between Mandarin tone 2 and tone 3 were most prominent in both CI and NH children except for in the poorest SNR conditions. Age at implantation was significantly correlated with tone-recognition performance of the CI children in noise. Conclusions: There is a marked deficit in tone recognition in prelingually deafened children with CIs, particularly in noise listening conditions. While factors that contribute to the large individual differences are still elusive, early implantation could be beneficial to tone development in pediatric CI users.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0587,Applying Rasch model analysis in the development of the cantonese tone identification test (CANTIT),"Objective: Applying Rasch analysis to evaluate the internal structure of a lexical tone perception test known as the Cantonese Tone Identification Test (CANTIT). Design: A 75-item pool (CANTIT-75) with pictures and sound tracks was developed. Respondents were required to make a four-alternative forced choice on each item. A short version of 30 items (CANTIT-30) was developed based on fit statistics, difficulty estimates, and content evaluation. Internal structure was evaluated by fit statistics and Rasch Factor Analysis (RFA). Study Sample: 200 children with normal hearing and 141 children with hearing impairment were recruited. Results: For CANTIT-75, all infit and 97% of outfit values were <2.0. RFA revealed 40.1% of total variance was explained by the Rasch measure. The first residual component explained 2.5% of total variance in an eigenvalue of 3.1. For CANTIT-30, all infit and outfit values were <2.0. The Rasch measure explained 38.8% of total variance, the first residual component explained 3.9% of total variance in an eigenvalue of 1.9. Conclusions: The Rasch model provides excellent guidance for the development of short forms. Both CANTIT-75 and CANTIT-30 possess satisfactory internal structure as a construct validity evidence in measuring the lexical tone identification ability of the Cantonese speakers.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0588,Speech perception in Mandarin-speaking children with cochlear implants: A systematic review,"Objective: This paper reviewed the literature on the trajectories and the factors significantly affecting post-implantation speech perception development in Mandarin-speaking children with cochlear implants (CIs). Design: A systematic literature search of textbooks and peer-reviewed published journal articles in online bibliographic databases was conducted. Study sample: PubMed, Scopus and Wiley online library were searched for eligible studies based on predefined inclusion and exclusion criteria. Results: A total of 14 journal articles were selected for this review. A number of consistent results were found. That is, children with CIs, as a group, exhibited steep improvement in early speech perception, from exhibiting few prelingual auditory behaviours before implantation to identifying sentences in noise after one year of CI use. After one to three years of CI use, children are expected to identify tones above chance and recognition of words in noise. In addition, early age at implantation, longer duration of CI use and higher maternal education level contributed to greater improvements in speech perception. Conclusions: Findings from this review will contribute to the establishment of appropriate short-term developmental goals for Mandarin-speaking children with CIs in mainland China and clinicians could use them to determine whether children have made appropriate progress with CIs.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0589,Mandarin tone recognition in English speakers with normal hearing and with cochlear implants,"Objective: Mandarin-speaking cochlear implant users have difficulty perceiving tonal changes in speech with current signal processing strategies. The purpose of this study was to evaluate whether English-speaking cochlear implant and normal hearing listeners can be trained to recognise closed-set Mandarin tones. The validity of using native-English speakers to evaluate Mandarin tone perception in cochlear implants was tested. Design: Two groups of native-English speaking participants were evaluated. All listeners were given training rounds and evaluation rounds in which their tonal identification was tested. The normal-hearing group was also tested with acoustic simulations of the traditional Continuous Interleaved Sampling (CIS) strategy. Study sample: Ten normal-hearing English speakers and seven cochlear implant listeners participated. Results: The normal-hearing group correctly identified unprocessed tones at 87% and CIS-processed tones at 58% on average. The cochlear implant listeners achieved 56% correct identification on average. Conclusions: This level of performance for native English speaking CI users was comparable to previous studies using native Mandarin-speaking CI listeners, which showed a mean of 59% in 19 CI users.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0590,Mandarin lexical tone recognition in bimodal cochlear implant users,"Objective:To assess the recognition of lexical tones in Mandarin-speaking bimodal cochlear implant (CI) subjects. Design:Lexical tone recognition in quiet and noise (SNR= +5 dB) was measured with electric stimulation (CI alone) or bimodal stimulation (CI + hearing aid (HA)). The recognition and confusion rates of the four tones (T1, T2, T3 and T4) were analysed. Spearman correlation analysis was performed to examine the relationship between hearing levels in the contralateral ear and bimodal benefits. Study sample:Twenty native Mandarin-speaking bimodal CI users, with ages ranging from 16-49 years. Results:Relative to the CI alone, mean tone recognition with the CI + HA improved significantly from 84.1-92.1% correct in quiet (+8 points) and from 57.9-73.1% correct in noise (+15.2 points). Tone confusions between T2 and T3 were the most prominent in all test conditions, and T4 tended to be labelled as T3 in noise. There was no significant correlation between the bimodal benefits for tone recognition and the unaided or HA-aided pure-tone thresholds at 0.25 kHz. Conclusion:Listeners with CI + HA exhibited significantly better tone recognition than with CI alone. The bimodal advantage for tone recognition was greater in noise than in quiet, perhaps due to a ceiling effect in quiet.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0591,Alpha synchronisation of acoustic responses in active listening is indicative of native language listening experience,"Objective Examine the effect of language experience on auditory evoked and oscillatory brain responses to lexical tone in passive (ACC) and active (P300) listening conditions. Design Language experience was evaluated using two groups, Mandarin- vs. English-listeners (with vs. without lexical tone experience). Two Mandarin lexical tones with pitch movement (T2 rising; T3 dipping) produced on the syllable /ba/ were used as stimuli. For passive listening, each tone was presented in a block. For active listening, each tone was the standard (80%) or deviant (20%) presented in two blocks. Presentation order was counterbalanced across participants in both tasks. Study sample 10 adult Mandarin-listeners and 13 Australian-English-listeners contributed to the data. Results Both global field power (GFP) and time frequency analysis (TFA) failed to detect group differences in passive listening conditions for the ACC response. In contrast, the active listening condition revealed significant group differences for T2. GFP showed a trending significance with larger GFP (less consistent responses) in English- than Mandarin-listeners. TFA showed significantly higher alpha synchronisation (more focussed attention) for Mandarin- compared to English-listeners. Conclusions Acoustic responses to speech is influenced by language experience but only during active listening, suggesting that focussed attention is linked to higher level language processes.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0592,Meta-analysis on lexical tone recognition in cochlear implant users,"ObjectiveLexical tone plays a vital role in speech communication in tonal languages. This study investigated lexical tone recognition in cochlear implant (CI) users, and identified potential factors that influence lexical tone recognition in the CI population.DesignWe conducted a systematic search across eleven major databases, evaluated the risk of bias in the included studies, and conducted five meta-analyses.Study sampleForty studies that utilised a multi-item alternative forced-choice paradigm were included to evaluate the performance of lexical tone recognition in CI users.ResultsCI users performed worse at recognising lexical tones than normal hearing (NH) controls. Furthermore, bimodal stimulation could benefit lexical tone recognition for CI users in both quiet and noisy conditions. Besides, the pooled results showed a negative correlation between tone recognition accuracy and age at implantation, as well as a positive correlation between tone recognition performance and the duration of CI experience.ConclusionsThis study indicates that CI users could not recognise lexical tones at the same level as the NH population. The bimodal intervention does have a more positive effect than unimodal implantation regardless of the listening environment. Moreover, earlier implantation and longer experience with the CI could facilitate lexical tone recognition.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0593,Learning Prosodic Focus from Continuous Speech Input: A Neural Network Exploration,"This study uses connectionist modeling to explore whether and how infants might learn prosodic focus directly from continuous speech input. Focus is a communicative function that serves to put emphasis on a particular part of an utterance, and it is mainly encoded by pitch variations. The acquisition of focus entails two major difficulties. The first is that focus-related pitch patterns are confounded by other linguistic functions that also use pitch for their encoding, such as lexical tone in a tone language. Second, speakers have different pitch ranges, which further confounds the focus related pitch patterns. In three simulations using self-organizing neural networks, we explored how focus may be learned from continuous acoustic signals in Mandarin that were produced with co-occurring lexical tones and by multiple speakers. We used sentence-sized F-0 contours as well as their velocity profiles (D-1) as training input. Results show that both F-0 and D-1 contours provide information for focus learning, but only the D-1-trained network adequately handled the variability introduced by cross-gender differences. The recognition rate was analogous to human performance. Implications of these findings for theories of language acquisition and adult speech perception are discussed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0594,Infants' Perception of the Intonation of Broad and Narrow Focus,"Infants perceive intonation contrasts early in development in contrast to lexical stress but similarly to lexical pitch accent. Previous studies have mostly focused on pitch height/direction contrasts; however, languages use a variety of pitch features to signal meaning, including differences in pitch timing. In this study, we investigate infants' perception of the prosodic contrast that cues the difference between all-new information (broad focus) and the highlighting of a particular word (narrow/contrastive focus) in European Portuguese (EP), and which has been described as having pitch timing as its key feature. Using a modified version of the visual habituation paradigm, EP learning infants discriminated this contrast at 12 months but not at 7 months, deviating from previous findings of a precocious ability to perceive pitch distinctions. These results suggest different developmental trajectories of the perception of different prosodic contrasts, underlining the importance of the nature of the cues signalling a given contrast in a given language.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0595,When is a Question a Question for Children and Adults?,"Terminal changes in fundamental frequency provide the most salient acoustic cues to declarative questions, but adults sometimes identify such questions from pre-terminal cues. In the present study, adults and 7- to 10-year-old children judged a single speaker's adult-and child-directed utterances as questions or statements in a gating task with word-length increments. Listeners of all ages successfully used pre-terminal cues to identify utterance type, often only the initial word, and they were more accurate for child-directed than adult-directed utterances. There were age-related differences in identification accuracy and number of words required for correct identification. Age differences were already apparent on the initial {first five) utterances, confirming adults' superior explicit knowledge of intonation patterns that signify questions and statements. Adults' performance improved over the course of the test session, reflecting taker-specific learning, but children exhibited no such learning.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0596,Exploiting Pitch Accent Information in Compound Processing: A Comparison between Adults and 6- to 7-Year-Old Children,"A noun can be potentially ambiguous as to whether it is a head on its own, or is a modifier of a Noun + Noun compound waiting for its head. This study investigates whether young children can exploit the prosodic information on a modifier constituent preceding the head to facilitate resolution of such ambiguity in Japanese. Evidence from English suggests that young speakers are not sensitive to compound stress in distinguishing between compounds and syntactic phrases unless the compound is very familiar (Good, 2008; Vogel & Raimy, 2002). This study concerns whether children in general have such limited capability to use prosodic cues to promptly compute a compound representation without the lexical boost, or whether they might show greater sensitivity to more categorical compound prosody such as that associated with the Compound Accent Rule (CAR) in Japanese. A previous study (Hirose & Mazuka, 2015) demonstrated that adult Japanese speakers can predict the compound structure prior to the head if the prosodic information on the modifier unambiguously signals that the CAR is being applied. The present study conducted the same online experiment with children (6- to 7-year-olds) and compared the time course of the effects with that of adults using permutation-based analysis (Maris & Oosternveld, 2007). The results reveal that children are sensitive to pitch accent information that facilitates the quicker processing of the compound or the single head noun representation compared to when such prosodic signals are less apparent, depending on the type of the lexical accent of the noun in question.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0597,Plasticity in Second Language Learning: The Case of Mandarin Tones,"Adults typically struggle to perceive non-native sound contrasts, especially those that conflict with their first language. Do the same challenges persist when the sound contrasts overlap but do not conflict? To address this question, we explored the acquisition of lexical tones. While tonal variations are present in many languages, they are only used contrastively in tonal languages. We investigated the perception of Mandarin tones by adults with differing experience with Mandarin, including naive listeners, classroom learners, and native speakers. Naive listeners discriminated Mandarin tones at above-chance levels, and performance significantly improved after just one month of classroom exposure. Additional evidence for plasticity came from advanced classroom learners, whose tonemic perception was indistinguishable from that of native speakers. The results suggest that unlike many other non-native contrasts, adults studying a language in the classroom can readily acquire the perceptual skills needed to discriminate Mandarin tones.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0598,Infants' Sensitivity to Lexical Tone and Word Stress in Their First Year: A Thai and English Cross-Language Study,"Non-tone language infants' native language recognition is based first on supra-segmental then segmental cues, but this trajectory is unknown for tone-language infants. This study investigated non-tone (English) and tone (Thai) language 6- to 10-month-old infants' preference for English vs. Thai one-syllable words (containing segmental and tone cues) and two-syllable words (additionally containing stress cues). A preference for their native one-syllable words was observed in each of the two groups of infants, but this was not the case for two-syllable words where Thai-learning infants showed no native-language preference. These findings indicate that as early as six months of age, infants acquiring tone- and non-tone languages identify their native language by relying solely on lexical tone cues, but tone language infants no longer show successful identification of their native language when two pitch-based cues co-occur in the signal.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0599,Genredynamics: a perceptual calculus of genre,"Prevailing theories of genre, derived primarily from literary and musical scholarship, differ in characteristics they ascribe to genre itself. Here, the temporally dynamic and culturally contingent nature of genre informs a computational framework that is reducible to extant theories of genre and connected to psychological theories of perceptual categorization. This framework, called genredynamics, interprets genres as perceptual categories in a space defined by aesthetic and sociocultural variables, and characterizes the behaviour and structure of genres using concepts from differential topology. Its existence demonstrates that disparate theoretical approaches to genre can be unified and implies that genre is best understood as both a psychological and musicological phenomenon. Classifications' temporal fluidity and incorporating sociocultural variables alongside sensory ones are necessary for this framework to be generalizable. Together, these theoretical results have broad implications for potential applications of genre theory, including the study of mental representations, social and cultural psychology, and cognition.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0600,Orthographic congruency effects in the suprasegmental domain: Evidence from Thai,"The influence of orthographic knowledge on lexical tone processing was examined by manipulating the congruency between the tone and the tone marker of Thai monosyllabic words presented in three metalinguistic tasks. In tone monitoring (Experiment 1) and same-different tone judgement (Experiment 2)-that is, tasks that require an explicit analysis of tone information-an orthographic congruency effect was observed: Better performance was found when both tone and tone marker led to the same response than when they led to opposite, competing responses. In rhyme judgement (Experiment 3), a metaphonological task that allows tone to be processed in a more natural way since it does not require explicit analysis of tone, the orthographic effect emerged only when the interstimulus interval was lengthened from 30 to 1,200 ms. In addition to demonstrating the generalization of the orthographic congruency effect to the suprasegmental domain in Thai, the present data also suggest relatively late and task-dependent activation of orthographic representations of tone.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0601,Early use of phonetic information in spoken word recognition: Lexical stress drives eye movements immediately,"For optimal word recognition listeners should use all relevant acoustic information as soon as it comes available. Using printed-word eye tracking we investigated when during word processing Dutch listeners use suprasegmental lexical stress information to recognize words. Fixations on targets such as oOCtopuso (capitals indicate stress) were more frequent than fixations on segmentally overlapping but differently stressed competitors (ookTObero) before segmental information could disambiguate the words. Furthermore, prior to segmental disambiguation, initially stressed words were stronger lexical competitors than noninitially stressed words. Listeners recognize words by immediately using all relevant information in the speech signal.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0602,Characterizing congenital amusia,"The ability to make sense of the music in our environment involves sophisticated cognitive mechanisms that, for most people, are acquired effortlessly and in early life. A special population of individuals, with a disorder termed congenital amusia, report lifelong difficulties in this regard. Exploring the nature of this developmental disorder provides a window onto the cognitive architecture of typical musical processing, as well as allowing a study of the relationship between processing of music and other domains, such as language. The present article considers findings concerning pitch discrimination, pitch memory, contour processing, experiential aspects of music listening in amusia, and emerging evidence concerning the neurobiology of the disorder. A simplified model of melodic processing is outlined, and possible loci of the cognitive deficit are discussed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0603,Examining the effects of variation in emotional tone of voice on spoken word recognition,"Emotional tone of voice (ETV) is essential for optimal verbal communication. Research has found that the impact of variation in nonlinguistic features of speech on spoken word recognition differs according to a time course. In the current study, we investigated whether intratalker variation in ETV follows the same time course in two long-term repetition priming experiments. We found that intratalker variability in ETVs affected reaction times to spoken words only when processing was relatively slow and difficult, not when processing was relatively fast and easy. These results provide evidence for the use of both abstract and episodic lexical representations for processing within-talker variability in ETV, depending on the time course of spoken word recognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0604,Stress matters revisited: A boundary change experiment,"Breen and Clifton (Stress matters: Effects of anticipated lexical stress on silent reading. Journal of Memory and Language, 2011, 64, 153-170) argued that readers' eye movements during silent reading are influenced by the stress patterns of words. This claim was supported by the observation that syntactic reanalysis that required concurrent metrical reanalysis (e.g., a change from the noun form of abstract to the verb form) resulted in longer reading times than syntactic reanalysis that did not require metrical reanalysis (e.g., a change from the noun form of report to the verb form). However, the data contained a puzzle: The disruption appeared on the critical word (abstract, report) itself, although the material that forced the part of speech change did not appear until the next region. Breen and Clifton argued that parafoveal preview of the disambiguating material triggered the revision and that the eyes did not move on until a fully specified lexical representation of the critical word was achieved. The present experiment used a boundary change paradigm in which parafoveal preview of the disambiguating region was prevented. Once again, an interaction was observed: Syntactic reanalysis resulted in particularly long reading times when it also required metrical reanalysis. However, now the interaction did not appear on the critical word, but only following the disambiguating region. This pattern of results supports Breen and Clifton's claim that readers form an implicit metrical representation of text during silent reading.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0605,Does tonal information affect the early stages of visual-word processing in Thai?,"Thai offers a unique opportunity to investigate the role of lexical tone processing during visual-word recognition, as tone is explicitly expressed in its script. In order to investigate the contribution of tone at the orthographic/phonological level during the early stages of word processing in Thai, we conducted a masked priming experimentusing both lexical decision and word naming tasks. For a given target word (e.g., ????/h?:?2/, room), five priming conditions were created: (a) identity (e.g., ????/h?:?2/), (b) same initial consonant, but with a different tone marker (e.g., ????/h?:?1/), (c) different initial consonant, but with the same tone marker (e.g., ????/s?:?2/), (d) orthographic control (different initial consonant, different tone marker; e.g., ????/s?:?1/), and (e) same tone homophony, but with a different initial consonant and different tone marker (e.g., ????/t(h)?:?2/). Results of the critical comparisons revealed that segmental information (i.e., consonantal information) appears to be more important than tone information (i.e., tone marker) in the early stages of visual-word processing in alphabetic, tonal languages like Thai. Thus, these findings may help constrain models of visual-word recognition and reading in tonal languages.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0606,Suprasegmental lexical stress cues in visual speech can guide spoken-word recognition,"Visual cues to the individual segments of speech and to sentence prosody guide speech recognition. The present study tested whether visual suprasegmental cues to the stress patterns of words can also constrain recognition. Dutch listeners use acoustic suprasegmental cues to lexical stress (changes in duration, amplitude, and pitch) in spoken-word recognition. We asked here whether they can also use visual suprasegmental cues. In two categorization experiments, Dutch participants saw a speaker say fragments of word pairs that were segmentally identical but differed in their stress realization (e.g., 'ca-vi from cavia guinea pig vs. ?ka-vi from kaviaar caviar). Participants were able to distinguish between these pairs from seeing a speaker alone. Only the presence of primary stress in the fragment, not its absence, was informative. Participants were able to distinguish visually primary from secondary stress on first syllables, but only when the fragment-bearing target word carried phrase-level emphasis. Furthermore, participants distinguished fragments with primary stress on their second syllable from those with secondary stress on their first syllable (e.g., pro-'jec from projector projector vs. ?pro-jec from projectiel projectile), independently of phrase-level emphasis. Seeing a speaker thus contributes to spoken-word recognition by providing suprasegmental information about the presence of primary lexical stress.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0607,Neural correlates of early-closure garden-path processing: Effects of prosody and plausibility,"Functional magnetic resonance imaging (fMRI) was used to investigate neural correlates of early-closure garden-path sentence processing and use of extrasyntactic information to resolve temporary syntactic ambiguities. Sixteen participants performed an auditory picture verification task on sentences presented with natural versus flat intonation. Stimuli included sentences in which the garden-path interpretation was plausible, implausible because of a late pragmatic cue, or implausible because of a semantic mismatch between an optionally transitive verb and the following noun. Natural sentence intonation was correlated with left-hemisphere temporal activation, but also with activation that suggests the allocation of more resources to interpretation when natural prosody is provided. Garden-path processing was associated with upregulation in bilateral inferior parietal and right-hemisphere dorsolateral prefrontal and inferior frontal cortex, while differences between the strength and type of plausibility cues were also reflected in activation patterns. Region of interest (ROI) analyses in regions associated with complex syntactic processing are consistent with a role for posterior temporal cortex supporting access to verb argument structure. Furthermore, ROI analyses within left-hemisphere inferior frontal gyrus suggest a division of labour, with the anterior-ventral part primarily involved in syntactic-semantic mismatch detection, the central part supporting structural reanalysis, and the posterior-dorsal part showing a general structural complexity effect.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0608,The role of tone and segmental information in visual-word recognition in Thai,"Tone languages represent a large proportion of the spoken languages of the world and yet lexical tone is understudied. Thai offers a unique opportunity to investigate the role of lexical tone processing during visual-word recognition, as tone is explicitly expressed in its script. We used colour words and their orthographic neighbours as stimuli to investigate facilitation (Experiment 1) and interference (Experiment 2) Stroop effects. Five experimental conditions were created: (a) the colour word (e. g kha: w/ [ white]), (b) tone different word (e. g khw/[ news]), (c) initial consonant phonologically same word (e. gkha: w/[ fishy]), where the initial consonant of the word was phonologically the same but orthographically different, (d) initial consonant different, tone same word (e. gha: w/yawn), where the initial consonant was orthographically different but the tone of the word was the same, and (e) initial consonant different, tone different word (e. gka: w/glue), where the initial consonant was orthographically different, and the tone was different. In order to examine whether tone information per se had a facilitative effect, we also included a colour congruent word condition where the segmental (S) information was different but the tone (T) matched the colour word (S-T+) in Experiment 2. Facilitation/interference effects were found for all five conditions when compared with a neutral control word. Results of the critical comparisons revealed that tone information comes into play at a later stage in lexical processing, and orthographic information contributes more than phonological information.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0609,Fundamental frequency variation within neonatal crying: Does ambient language matter?,"Objective: Evaluate the fundamental frequency (fo) variabilty of spontaneous cries produced by neonates with tonal (Lamnso) and non-tonal (German) ambient language. Study Design: Populational prospective study. Participants: A total of 21 German infants (10 male) and 21 Cameroon (Nso) infants (10 male) within the first week of life served as participants. Methods: Spontaneously uttered cries by each infant were audio recorded. The cries were acoustically analyzed and measures of fo variability (pitch sigma, fo fluctuation, fo range) were calculated. Cry duration and anthropometric measures were calculated as co-factors. Results: Significant group differences were found for all fo variability measures, whereas somatic measures did not differ. Cry duration also differed significantly between groups. The results were indicative of Cameroon (Nso) infants producing cries with more fo variability compared to German infants. Conclusion: Albeit further studies with a larger sample size are warranted, the data foster previous findings of an early imprinting effect of the ambient maternal language on cry fo.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0610,Intonation influences processing and recall of left-dislocation sentences by indicating topic vs. focus status of dislocated referent,"We tested the effects of two intonation contours on the processing and cued recall of German sentences with a left-dislocated subject vs. object: (i) a rising accent on the dislocated phrase, followed by a rising-falling hat contour on the main clause; (ii) a falling accent on the dislocated phrase, followed by a falling accent plus subsequent deaccentuation. The contours had differential effects depending on the grammatical function of the dislocated phrase (subject/object) and, for the recall, on the cue type for the recall (subject/object), in certain conditions overriding the subject-before-object preference normally found in processing. To account for the findings, we propose: (1) Contour (i) signals the topic status of the referent of the dislocated phrase. Contour (ii) signals that referent's focus status. (2) Topics are referents that serve as an address in a structured discourse representation in working memory under which information about that referent is stored. (3) Subjects are default topics, whereas objects are not, so that topic-marking an object is motivated, which results in an object-before-subject preference for sentences with topical objects during processing. (4) Retrieval of information from an address incurs a lower processing load if the appropriate address is cued than if some other referent is cued.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0611,Emphasising sound and meaning: pitch gestures enhance Mandarin lexical tone acquisition,"Lexical tones - pitches differentiating between word meanings in tonal languages - are particularly difficult for atonal language speakers to learn. To test the hypotheses of embodied cognition and spoken word recognition, we examined whether - and how - gesture could facilitate English speakers' discrimination between Mandarin words differing in lexical tone. Words were learned with gestures conveying tone pitch contours (pitch gestures), gestures conveying word meanings (semantic gestures) or no gestures. The results demonstrated that pitch gestures enhanced English speakers' discrimination between the meanings of Mandarin words differing in tone, whereas semantic gestures hindered their identification of tones in learned words. These findings indicate that the visuospatial features of pitch gestures strengthen the relationship between English speakers' representations of Mandarin lexical tones and word meanings, supporting the predictions of spoken word recognition and embodied cognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0612,Multi-level processing of phonetic variants in speech production and visual word processing: evidence from Mandarin lexical tones,"Two picture-word interference experiments provide new evidence on the nature of phonological processing in speech production and visual word processing. In both experiments, responses were significantly faster either when distractor and target matched in tone category, but had different overt realisations (toneme condition) or when target and distractor matched in overt realisation, but mismatched in tone category (contour condition). Tone 3 sandhi is an allophone of Beijing Mandarin Tone 3 (T3). Its contour is similar to another tone, Tone 2. In Experiment 1, sandhi picture naming was faster with contour (Tone 2) and toneme (low Tone 3) distractors, compared to control distractors. This indicates both category and context-specific representations are activated in sandhi word production. In Experiment 2, both contour (Tone 2) and toneme (low Tone 3) picture naming was facilitated by visually presented sandhi distractors, compared to controls, evidence that category and context-specific instantiated representations are automatically activated during processing of visually presented words. Combined, the results point to multi-level processing of phonology, whether words are overtly produced or processed visually. Interestingly, there were differences in the time course of effects.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0613,"Do syllable-specific tonal probabilities guide lexical access? Evidence from Mandarin, Shanghai and Cantonese speakers","An eye-tracking study investigated how the interaction between syllable frequency and syllable-specific tonal probability guides online lexical access in speakers of mutually unintelligible Chinese dialects with three disparate tonal systems. Mono-dialectal Mandarin speakers, bi-dialectal Shanghai-Mandarin speakers and bi-dialectal Cantonese-Mandarin speakers searched for target Mandarin syllable-tone combinations while their eye movements and mouse clicks were recorded. The results showed dialectal differences in online eye fixation patterns but not in offline mouse responses. For all groups, mouse clicks were fastest for infrequent syllables with most probable tones and slowest for infrequent syllables with least probable tones. In online eye movement responses, only mono-dialectal Mandarin speakers showed an interaction between syllable frequency and tonal probability. Mono-dialectal Mandarin speakers' fixations were fastest for infrequent syllables with probable tones and slowest for infrequent syllables with improbable tones. Mono-dialectal speakers also showed a greater amount of competition from the more probable segmental competitor when hearing improbable tones. Bi-dialectal speakers showed different timing in their integration of tonal probabilities. These findings suggest that highly bilingual speakers track and use Mandarin tonal probabilities, but their sensitivity to L2 tonal information may lag behind monolinguals for online word recognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0614,"Cross-domain correlation in pitch perception, the influence of native language","The current study explores how language experience may shape the correlation between lexical tone and musical pitch perception. A two domains (music and lexical tone) by two languages (tone, Mandarin Chinese and non-tone, Dutch) design is adopted. Participants were tested on their discrimination of Mandarin Chinese lexical tones, Montreal Battery of Evaluation of Amusia (MBEA), and Musical Ear Test (MET). The Chinese listeners outperformed the Dutch listeners on both MBEA and MET, but had comparable accuracies for the lexical tone discrimination. Importantly, a significant cross-domain correlation was only observed for the Dutch listeners but not for the Chinese listeners. For tone language listeners, once lexical tones have been acquired, native listeners perceive them as phonological categories, and split them from other pitch variations that do not play a phonemic role. Non-tone language listeners, on the other hand, perceive both lexical tones and musical pitch on a psycho-acoustical basis, hence exhibit a unified perception of pitch across the two domains.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0615,"Processing dependencies of segmental and suprasegmental information: effects of emotion, lexical tone, and consonant variation","Human speech carries segmental (phonemic) as well as suprasegmental (non-phonemic) detail. The current study aims to determine the extent to which processing dependencies between segmental and suprasegmental cues are influenced by phonological and communicative relevance in the native language. In the present study, processing interactions between segmental (consonant) and two types of suprasegmental cues (lexical tone and emotion) were examined in Mandarin and English speakers using a speeded classification paradigm. Results revealed a distinct pattern of responses for each group: for both tone and emotion, Mandarin speakers demonstrated mutual and symmetrical processing dependencies. By contrast, English speakers exhibited reduced interference from suprasegmental tone variation on segmental processing than vice versa. However, English speakers exhibited greater interference from suprasegmental emotion judgements on segmental processing than vice versa. Results point to cross-linguistic differences in how suprasegmental cues are weighted and are discussed in terms of current models of the adult lexicon.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0616,The interaction between phonological information and pitch type at pre-attentive stage: an ERP study of lexical tones,"Previous studies on the processing of lexical tones have typically confounded effects due to phonological information (different meanings of words signalled by syllables with different tonal categories) with effects due to specific acoustic information (pitch type: pitch height/pitch contour). The present study is designed to dissociate these two kinds of effects and further investigate the processing of lexical tones at pre-attentive stage by mismatch negativity (MMN). We chose level tones and contour tones in Cantonese to differentiate pitch height from pitch contour, and manipulated tonal category (within-category/across-category) to distinguish phonological information from acoustic information. The results showed clear interactions between tonal category and pitch type in MMN mean amplitude and peak latency, suggesting the interaction between phonological information and pitch type in the pre-attentive processing of lexical tones. These results are discussed in light of cognitive and neural mechanisms underlying auditory processing of lexical tones.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0617,Adaptation in Mandarin tone production with pitch-shifted auditory feedback: influence of tonal contrast requirements,"We investigated Mandarin speakers' control of lexical tone production with F0-perturbed auditory feedback. Subjects produced high level (T1), mid rising (T2), low dipping (T3), and high falling (T4) tones in conditions with (a) no perturbation, (b) T1 shifted down, (c) T1 shifted down and T3 shifted up, or (d) T1 shifted down and T3 shifted up but without producing other tones. Speakers and new subjects also completed a tone identification task with unaltered and F0-perturbed productions. With only T1 perturbed down, speakers adapted by raising F0 relative to no-perturbation. With simultaneous T1 down and T3 up perturbations, no T1 adaptation occurred, and T3 adaptation occurred only if T2 was also produced. Identification accuracy with stimuli representing adapted productions was comparable to baseline, but with simulated non-adapted productions it was reduced for T2 and T3. Thus, Mandarin speakers' adaptation to F0 perturbations is linguistically constrained and serves to maintain tone contrast.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0618,When is a wh-in-situ question identified in standard Persian?,"Previous literature demonstrated the influential role of prediction in processing speech [Brazil, 1981. The place of intonation in a discourse model. In C.Malcolm & M.Montgomery (Eds.), Studies in discourse analysis (pp.146-157). London: Routledge & Kegan Paul; Grosjean, 1983. How long is the sentence? Prediction and prosody in the on-line processing of language. Linguistics, 21, 501-529, 1996a. Using prosody to predict the end of sentences in English and French: Normal and brain damaged subjects. Language and Cognitive Processes, 11, 107-134; Snedeker & Trueswell, 2003. Using prosody to avoid ambiguity: Effects of speaker awareness and referential context. Journal of Memory and Language, 48, 103-130], and of prosody in predicting the eventual syntactic structure of ambiguous sentences [e.g. Snedeker & Trueswell, 2003. Using prosody to avoid ambiguity: Effects of speaker awareness and referential context. Journal of Memory and Language, 48, 103-130]. Wh-in-situ questions contain temporary syntactic ambiguity. One of the languages characterised by wh-in-situ questions is Persian. The current research adopted the gating paradigm [Grosjean, 1980. Spoken word recognition processes and the gating paradigm. Perception and Psychophysics, 28, 267-283] to investigate when distinctive prosodic cues of the pre-wh part enable correct identification of wh-in-situ questions in Persian. A perception experiment was designed in which gated stimuli were played to Persian native speakers in a forced-choice sentence identification task. In line with our expectation, correct identification responses were given from the beginning of the sentence. The result is discussed in the context of proposals regarding the need to integrate prosody and prediction into models of language and speech processing [Beach, 1991. The interpretation of prosodic patterns at points of syntactic structure ambiguity: Evidence for cue trading relations. Journal of Memory and Language, 30, 644-663; Grosjean, 1983. How long is the sentence? Prediction and prosody in the on-line processing of language. Linguistics, 21, 501-529, 1996a. Using prosody to predict the end of sentences in English and French: Normal and brain damaged subjects. Language and Cognitive Processes, 11, 107-134].",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0619,Toward an (even) more comprehensive model of speech production planning,"Since the publication of Speaking in 1989, with its extraordinary goal of modelling the entire process of human speech generation from message conceptualisation to articulation, encompassing results from a wide range of empirical studies, much new information has emerged about three aspects of speech production that were not clearly in focus at that time. This evidence has revealed 1) the systematic patterns of context-governed surface phonetic variation, and the active control of these patterns exercised by speakers and listeners, 2) the depth and pervasiveness of prosodic influences on those patterns, and 3) the close alignment of co-speech gestures with the prosodic structure of an utterance. This paper reviews some of that evidence, and suggests how its implications may constrain models of speech production planning, as those models become more comprehensive in their treatment of higher-level structures, and of aspects of the communicative act beyond the articulation of lexical and syntactic elements.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0620,Phonological competition in Mandarin spoken word recognition,"Most of the world's languages use both segment and lexical tone to distinguish word meanings. However, the few studies on spoken word recognition in tone languages show conflicting results concerning the relative contribution of (sub-)syllabic constituents, and the time course of how segmental and tonal information is utilised. In Experiments 1 & 2, participants listened to monosyllabic Mandarin words with the presence of a phonological competitor, which overlaps in either segmental syllable, onset and tone, rhyme and tone, or just tone. Eye movement results only confirmed the segmental syllable competition effect. Experiment 3 investigated the time course of segmental vs. tonal cue utilisation by manipulating their point of divergence (POD) and found that POD modulates the look trajectories of both segmental and tonal phonological competitors. While listeners can use both segmental and tonal information incrementally to constrain lexical activation, segmental syllable plays an advantageous role in Mandarin spoken word recognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0621,The cognitive processing of tone sandhi in different information structural status during dialogue comprehension,"Mandarin tone 3 (T3) sandhi is a phonological alternation where a T3 changes to a rising tone when followed by another T3. The present event-related potential (ERP) study examined how sandhi words are processed during dialogue comprehension. Following different wh-questions, the answers embedded a sandhi word in which the first syllable was either pronounced as post-sandhi T3, pre-sandhi T3, or mispronounced T4 at the focus or background position. The results showed that the naturalness rating decreased as the amplitude of a late positivity (LPC) increased in the order of post-sandhi, pre-sandhi, and mispronunciation conditions, reflecting a reanalysis process after encountering the unexpected tone. At the background (vs. focus) position, the effect was more posterior for the mispronunciation condition, and disappeared for the pre-sandhi condition. Our data suggest that the processing of sandhi words is not only influenced by their tonal realisation, but also moderated by their information structural status.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0622,"Development of neural discrimination of pitch across speech and music in the first year of life, a mismatch response study","This study focuses on the development of neural discrimination of pitch changes in speech and music by English-language adults and 4-, 8- and 12-month-old infants. Speech stimuli were Mandarin Chinese rising and dipping lexical tones and the musical stimuli were three-note melodies with pitch levels based on those of the lexical tones. Mismatch responses were elicited using a non-attentive oddball paradigm. Adults showed mismatch negativity (MMN) responses in both the lexical tone and music conditions. For infants, for the lexical tones, a positive-mismatch response (p-MMR) was observed at 4, 8, and 12 months, whereas for the musical tones, a p-MMR was found for the 4-month-olds, an MMN for the 12-month-olds, and no mismatch response, either positive or negative, for the 8-month-olds. No evidence of cross-domain correlation of the mismatch responses was found. These results suggest domain-specific development of mismatch responses to pitch change in the first year of life.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0623,The role of tonal information in speech prediction: evidence based on Chinese tone sandhi,"Some studies assume that comprehenders predict word-form information (e.g. phonology) for the upcoming word, while the others argued that such an ""all-aspects"" prediction is not necessary. We conducted an eye-tracking experiment and an event-related potential (ERP) experiment to investigate phonological prediction effect in Mandarin comprehension. In Experiment 1, comprehenders utilise the tone of the number (e.g. ""yi4[one]"") to predict the tone of the upcoming classifier and its corresponding object (e.g. ""yi4 zhang1 you2piao4[one ZHANG stamp]"", means ""a stamp""), leading to more fixation on the target object than the competitors (e.g. ""yi2 liang4 dan1che1[one LIANG bike]"", means ""a bike"") in the time window of the number. In Experiment 2, the mismatched tone induced a larger negative response than the matched tone (e.g. [yi2] vs. [yi4] for ""a stamp""). These findings provide evidence for the tonal prediction effect, in which lexical tone can serve as a valuable cue for predicting upcoming words.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0624,The time course of speech act recognition conveyed by speech prosody: a gating study,"Speech prosody is essential for conveying communicative intentions. Although neurophysiological data has shown that communicative functions conveyed through prosody are processed rapidly in the human brain, it is still unclear when and to what extent prosodic information is needed for the conscious speech act recognition as speech unfolds. Using a gating paradigm, we investigated the point at which listeners recognise the function of identical Italian sentences - whether they express a question or statement - based on vocal intonation. Comparing cross-spliced and natural sentences, we found that, rising or falling nuclear accentual movement on the sentence-final word seems to be the primary cue for recognition, with questions identified slightly later than statements. Furthermore, we discuss the limitations of splicing techniques in filtering out natural prosodic variations, the presence of a ""statement bias"" in perceiving incomplete sentences, along with a visual examination of interindividual responses. These findings offer valuable insights into the timing of conscious recognition of different communicative functions based on speech prosody.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0625,Effects of sandhi-based predictability in Kansai Japanese depend on markedness: a visual-world eye-tracking study,"During comprehension, various aspects of upcoming input can be anticipated. However, it is unclear to what extent listeners exploit suprasegmental information to facilitate lexical retrieval. In Japanese, a Kansai-dialect-specific pitch accent system and a sandhi rule create a situation where the final tone of certain modifiers is conditioned by the initial tone of the following noun, thereby making the upcoming tone predictable. Both Tokyo and Kansai speakers showed increased fixations on the target when its initial tone was predictable based on an all-L-tone modifier, suggesting that this ""predictability effect"" may reflect an increased anticipation of upcoming input that disrupts a marked L-tone sequence, without necessarily invoking the dialect-specific sandhi rule. However, Kansai speakers were quicker to identify the target in this condition, suggesting that they utilised dialect-specific knowledge about the lexically determined pitch accent of the target noun to facilitate processing, particularly when the modifier tone deviated from the default.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0626,A corpus-driven critical discourse analysis of news reports on disabled women,"Words engender consciousness, and news reporting, as an important medium for the dissemination of discourse, shapes public cognition imperceptibly. As a marginalized group in patriarchal societies, disabled women have received relatively little attention. Therefore, using a corpus-driven critical discourse analysis, this study investigates the overall media representation of disabled women in Chinese news reports (2013-2023), along with the underlying cognitive and social factors. It reveals that Chinese news reporting prefers the social model of disability, which tends to narrate from the perspective of society and caregivers. This is also evidenced by the use of different personal pronouns. Second, in the view of semantic preference, it is apparent that expressions associated with disabled women often carry derogatory and negative emotional connotations, such as 'left-behind' and 'poor'. However, the overall semantic prosody presented in the news is primarily positive, advocating for societal attention and assistance towards this group. Finally, through the analysis of thematic words and high-frequency collocations, Chinese reports touch upon topics related to sexuality and marriage in the news descriptions of intellectually disabled women, potentially hinting at the discourse rights of this segment of the population.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0627,Functional Connectivity Associated with Acoustic Stability During Vowel Production: Implications for Vocal-Motor Control,"Vowels provide the acoustic foundation of communication through speech and song, but little is known about how the brain orchestrates their production. Positron emission tomography was used to study regional cerebral blood flow (rCBF) during sustained production of the vowel /a/. Acoustic and blood flow data from 13, normal, right-handed, native speakers of American English were analyzed to identify CBF patterns that predicted the stability of the first and second formants of this vowel. Formants are bands of resonance frequencies that provide vowel identity and contribute to voice quality. The results indicated that formant stability was directly associated with blood flow increases and decreases in both left-and right-sided brain regions. Secondary brain regions (those associated with the regions predicting formant stability) were more likely to have an indirect negative relationship with first formant variability, but an indirect positive relationship with second formant variability. These results are not definitive maps of vowel production, but they do suggest that the level of motor control necessary to produce stable vowels is reflected in the complexity of an underlying neural system. These results also extend a systems approach to functional image analysis, previously applied to normal and ataxic speech rate that is solely based on identifying patterns of brain activity associated with specific performance measures. Understanding the complex relationships between multiple brain regions and the acoustic characteristics of vocal stability may provide insight into the pathophysiology of the dysarthrias, vocal disorders, and other speech changes in neurological and psychiatric disorders.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0628,Single word reading in developmental stutterers and fluent speakers,"Ten fluent speakers and nine developmental stutterers read isolated nouns aloud in a delayed reading paradigm. Cortical activation sequences were mapped with a whole-head magnetoencephalography system. The stutterers were mostly fluent in this task. Although the overt performance was essentially identical in the two groups, the cortical activation patterns showed clear differences, both in the evoked responses, time-locked to word presentation and mouth movement onset, and in task-related suppression of 20-Hz oscillations. Within the first 400 ms after seeing the word, processing in fluent speakers advanced from the left inferior frontal cortex (articulatory programming) to the left lateral central sulcus and dorsal premotor cortex (motor preparation). This sequence was reversed in the stutterers, who showed an early left motor cortex activation followed by a delayed left inferior frontal signal. Stutterers thus appeared to initiate motor programmes before preparation of the articulatory code. During speech production, the right motor/premotor cortex generated consistent evoked activation in fluent speakers but was silent in stutterers. On the other hand, suppression of motor cortical 20-Hz rhythm, reflecting task-related neuronal processing, occurred bilaterally in both groups. Moreover, the suppression was right-hemisphere dominant in stutterers, as opposed to left-hemisphere dominant in fluent speakers. Accordingly, the right frontal cortex of stutterers was highly active during speech production but did not generate synchronous time-locked responses. The speech-related 20-Hz suppression concentrated in the mouth area in fluent speakers, but was evident in both the hand and mouth areas in stutterers. These findings may reflect imprecise functional connectivity within the right frontal cortex and incomplete segregation between the adjacent hand and mouth motor representations in stutterers during speech production. A network including the left inferior frontal cortex and the right motor/premotor cortex, likely to be relevant in merging linguistic and affective prosody with articulation during fluent speech, thus appears to be partly dysfunctional in developmental stutterers.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0629,"Intonation processing in congenital amusia: discrimination, identification and imitation","This study investigated whether congenital amusia, a neuro-developmental disorder of musical perception, also has implications for speech intonation processing. In total, 16 British amusics and 16 matched controls completed five intonation perception tasks and two pitch threshold tasks. Compared with controls, amusics showed impaired performance on discrimination, identification and imitation of statements and questions that were characterized primarily by pitch direction differences in the final word. This intonation-processing deficit in amusia was largely associated with a psychophysical pitch direction discrimination deficit. These findings suggest that amusia impacts upon one's language abilities in subtle ways, and support previous evidence that pitch processing in language and music involves shared mechanisms.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0630,Congenital amusia in speakers of a tone language: association with lexical tone agnosia,"Congenital amusia is a neurogenetic disorder that affects the processing of musical pitch in speakers of non-tonal languages like English and French. We assessed whether this musical disorder exists among speakers of Mandarin Chinese who use pitch to alter the meaning of words. Using the Montreal Battery of Evaluation of Amusia, we tested 117 healthy young Mandarin speakers with no self-declared musical problems and 22 individuals who reported musical difficulties and scored two standard deviations below the mean obtained by the Mandarin speakers without amusia. These 22 amusic individuals showed a similar pattern of musical impairment as did amusic speakers of non-tonal languages, by exhibiting a more pronounced deficit in melody than in rhythm processing. Furthermore, nearly half the tested amusics had impairments in the discrimination and identification of Mandarin lexical tones. Six showed marked impairments, displaying what could be called lexical tone agnosia, but had normal tone production. Our results show that speakers of tone languages such as Mandarin may experience musical pitch disorder despite early exposure to speech-relevant pitch contrasts. The observed association between the musical disorder and lexical tone difficulty indicates that the pitch disorder as defining congenital amusia is not specific to music or culture but is rather general in nature.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0631,Neural Correlates of Voice Perception in Newborns and the Influence of Preterm Birth,"Maternal voice is a highly relevant stimulus for newborns. Adult voice processing occurs in specific brain regions. Voice-specific brain areas in newborns and the relevance of an early vocal exposure on these networks have not been defined. This study investigates voice perception in newborns and the impact of prematurity on the cerebral processes. Functional magnetic resonance imaging (fMRI) and high-density electroencephalography (EEG) were used to explore the brain responses to maternal and stranger female voices in full-term newborns and preterm infants at term-equivalent age (TEA). fMRI results and the EEG oddball paradigm showed enhanced processing for voices in preterms at TEA than in full-term infants. Preterm infants showed additional cortical regions involved in voice processing in fMRI and a late mismatch response for maternal voice, considered as a first trace of a recognition process based on memory representation. Full-term newborns showed increased cerebral activity to the stranger voice. Results from fMRI, oddball, and standard auditory EEG paradigms highlighted important change detection responses to novelty after birth. These findings suggest that the main components of the adult voice-processing networks emerge early in development. Moreover, an early postnatal exposure to voices in premature infants might enhance their capacity to process voices.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0632,Irreversible Specialization for Speech Perception in Early International Adoptees,"In early childhood, the human brain goes through a period of tuning to native speech sounds but retains remarkable flexibility, allowing the learning of new languages throughout life. However, little is known about the stability over time of early neural specialization for speech and its influence on the formation of novel language representations. Here, we provide evidence that early international adoptees, who lose contact with their native language environment after adoption, retain enhanced sensitivity to a native lexical tone contrast more than 15 years after being adopted to Sweden from China, in the absence of any pretest familiarization with the stimuli. Changes in oscillatory brain activity showed how adoptees resort to inhibiting the processing of defunct phonological representations, rather than forgetting or replacing them with new ones. Furthermore, neurophysiological responses to native and nonnative contrasts were not negatively correlated, suggesting that native language retention does not interfere with the acquisition of adoptive phonology acquisition. These results suggest that early language experience provides strikingly resilient specialization for speech which is compensated for through inhibitory control mechanisms as learning conditions change later in life.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0633,Neurophysiological evidence for goal-oriented modulation of speech perception,"Speech perception depends on the dynamic interplay of bottom-up and top-down information along a hierarchically organized cortical network. Here, we test, for the first time in the human brain, whether neural processing of attended speech is dynamically modulated by task demand using a context-free discrimination paradigm. Electroencephalographic signals were recorded during 3 parallel experiments that differed only in the phonological feature of discrimination (word, vowel, and lexical tone, respectively). The event-related potentials (ERPs) revealed the task modulation of speech processing at approximately 200 ms (P2) after stimulus onset, probably influencing what phonological information to retain in memory. For the phonological comparison of sequential words, task modulation occurred later at approximately 300 ms (N3 and P3), reflecting the engagement of task-specific cognitive processes. The ERP results were consistent with the changes in delta-theta neural oscillations, suggesting the involvement of cortical tracking of speech envelopes. The study thus provides neurophysiological evidence for goal-oriented modulation of attended speech and calls for speech perception models incorporating limited memory capacity and goal-oriented optimization mechanisms.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0634,Cortical specialization associated with native speech category acquisition in early infancy,"This study investigates neural processes in infant speech processing, with a focus on left frontal brain regions and hemispheric lateralization in Mandarin-speaking infants' acquisition of native tonal categories. We tested 2- to 6-month-old Mandarin learners to explore age-related improvements in tone discrimination, the role of inferior frontal regions in abstract speech category representation, and left hemisphere lateralization during tone processing. Using a block design, we presented four Mandarin tones via [ta] and measured oxygenated hemoglobin concentration with functional near-infrared spectroscopy. Results showed age-related improvements in tone discrimination, greater involvement of frontal regions in older infants indicating abstract tonal representation development and increased bilateral activation mirroring native adult Mandarin speakers. These findings contribute to our broader understanding of the relationship between native speech acquisition and infant brain development during the critical period of early language learning.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0635,Functional alterations of lateral temporal cortex for processing voice prosody in adults with autism spectrum disorder,"The human auditory system includes discrete cortical patches and selective regions for processing voice information, including emotional prosody. Although behavioral evidence indicates individuals with autism spectrum disorder (ASD) have difficulties in recognizing emotional prosody, it remains understudied whether and how localized voice patches (VPs) and other voice-sensitive regions are functionally altered in processing prosody. This fMRI study investigated neural responses to prosodic voices in 25 adult males with ASD and 33 controls using voices of anger, sadness, and happiness with varying degrees of emotion. We used a functional region-of-interest analysis with an independent voice localizer to identify multiple VPs from combined ASD and control data. We observed a general response reduction to prosodic voices in specific VPs of left posterior temporal VP (TVP) and right middle TVP. Reduced cortical responses in right middle TVP were consistently correlated with the severity of autistic symptoms for all examined emotional prosodies. Moreover, representation similarity analysis revealed the reduced effect of emotional intensity in multivoxel activation patterns in left anterior superior temporal cortex only for sad prosody. These results indicate reduced response magnitudes to voice prosodies in specific TVPs and altered emotion intensity-dependent multivoxel activation patterns in adult ASDs, potentially underlying their socio-communicative difficulties.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0636,Distinct frontal regions subserve evaluation of linguistic and emotional aspects of speech intonation,"In addition to the propositional content of verbal utterances, significant linguistic and emotional information is conveyed by the tone of speech. To differentiate brain regions subserving processing of linguistic and affective aspects of intonation, discrimination of sentences differing in linguistic accentuation and emotional expressiveness was evaluated by functional magnetic resonance imaging. Both tasks yielded rightward lateralization of hemodynamic responses at the level of the dorsolateral frontal cortex as well as bilateral thalamic and temporal activation. Processing of linguistic and affective intonation, thus, seems to be supported by overlapping neural networks comprising partially right-sided brain regions. Comparison of hemodynamic activation during the two different tasks, however, revealed bilateral orbito-frontal responses restricted to the affective condition as opposed to activation of the left lateral inferior frontal gyrus confined to evaluation of linguistic intonation. These findings indicate that distinct frontal regions contribute to higher level processing of intonational information depending on its communicational function. In line with other components of language processing, discrimination of linguistic accentuation seems to be lateralized to the left inferior-lateral frontal region whereas bilateral orbito-frontal areas subserve evaluation of emotional expressiveness.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0637,Hemispheric shifts of sound representation in auditory cortex with conceptual listening,"The weak field specificity and the heterogeneity of neuronal filters found in any given auditory cortex field does not substantiate the view that such fields are merely descriptive maps of sound features. But field mechanisms were previously shown to support behaviourally relevant classification of sounds. Here the prediction was tested in human auditory cortex (AC) that classification-tasks rather than the stimulus class per se determine which auditory cortex area is recruited. By presenting the same set of frequency-modulations we found that categorization of their pitch direction (rising versus falling) increased functional magnetic resonance imaging activation in right posterior AC compared with stimulus exposure and in contrast to left posterior AC dominance during categorization of their duration (short versus long). Thus, top-down influences appear to select not only auditory cortex areas but also the hemisphere for specific processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0638,Implicitly Perceived Vocal Attractiveness Modulates Prefrontal Cortex Activity,"Social interactions involve more than ""just"" language. As important is a more primitive nonlinguistic mode of communication acting in parallel with linguistic processes and driving our decisions to a much higher degree than is generally suspected. Amongst the ""honest signals"" that influence our behavior is perceived vocal attractiveness. Not only does vocal attractiveness reflect important biological characteristics of the speaker, it also influences our social perceptions according to the ""what sounds beautiful is good"" phenomenon. Despite the widespread influence of vocal attractiveness on social interactions revealed by behavioral studies, its neural underpinnings are yet unknown. We measured brain activity while participants listened to a series of vocal sounds (""ah"") and performed an unrelated task. We found that voice-sensitive auditory and inferior frontal regions were strongly correlated with implicitly perceived vocal attractiveness. While the involvement of auditory areas reflected the processing of acoustic contributors to vocal attractiveness (""distance to mean"" and spectrotemporal regularity), activity in inferior prefrontal regions (traditionally involved in speech processes) reflected the overall perceived attractiveness of the voices despite their lack of linguistic content. These results suggest the strong influence of hidden nonlinguistic aspects of communication signals on cerebral activity and provide an objective measure of this influence.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0639,Repetition Suppression in the Left Inferior Frontal Gyrus Predicts Tone Learning Performance,"Do individuals differ in how efficiently they process non-native sounds? To what extent do these differences relate to individual variability in sound-learning aptitude? We addressed these questions by assessing the sound-learning abilities of Dutch native speakers as they were trained on non-native tone contrasts. We used fMRI repetition suppression to the non-native tones to measure participants' neuronal processing efficiency before and after training. Although all participants improved in tone identification with training, there was large individual variability in learning performance. A repetition suppression effect to tone was found in the bilateral inferior frontal gyri (IFGs) before training. No whole-brain effect was found after training; a region-of-interest analysis, however, showed that, after training, repetition suppression to tone in the left IFG correlated positively with learning. That is, individuals who were better in learning the non-native tones showed larger repetition suppression in this area. Crucially, this was true even before training. These findings add to existing evidence that the left IFG plays an important role in sound learning and indicate that individual differences in learning aptitude stem from differences in the neuronal efficiency with which non-native sounds are processed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0640,Task-General and Acoustic-Invariant Neural Representation of Speech Categories in the Human Brain,"A significant neural challenge in speech perception includes extracting discrete phonetic categories from continuous and multidimensional signals despite varying task demands and surface-acoustic variability. While neural representations of speech categories have been previously identified in frontal and posterior temporal-parietal regions, the task dependency and dimensional specificity of these neural representations are still unclear. Here, we asked native Mandarin participants to listen to speech syllables carrying 4 distinct lexical tone categories across passive listening, repetition, and categorization tasks while they underwent functional magnetic resonance imaging (fMRI). We used searchlight classification and representational similarity analysis (RSA) to identify the dimensional structure underlying neural representation across tasks and surface-acoustic properties. Searchlight classification analyses revealed significant ""cross-task"" lexical tone decoding within the bilateral superior temporal gyrus (STG) and left inferior parietal lobule (LIPL). RSA revealed that the LIPL and LSTG, in contrast to the RSTG, relate to 2 critical dimensions (pitch height, pitch direction) underlying tone perception. Outside this core representational network, we found greater activation in the inferior frontal and parietal regions for stimuli that are more perceptually similar during tone categorization. Our findings reveal the specific characteristics of fronto-tempo-parietal regions that support speech representation and categorization processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0641,Effects of the temporal fine structure in different frequency bands on Mandarin tone perception,"This letter evaluates the relative contributions of temporal fine structure cues in various frequency bands to Mandarin tone perception using novel ""auditory chimaeras"". Our results confirm the importance of temporal fine structure cues to lexical tone perception and the dominant region of lexical tone perception is found, namely the second to fifth harmonics can contribute no less than the fundamental frequency itself.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0642,Neural correlates of embodied and vibratory mechanisms associated with emotional prosody production,"Despite a large body of literature on the psychological and brain mechanisms of vocal emotion perception, less is known about expression and production mechanisms, especially the vibrations originating in the vocal cords and their role in emotional voice production. In the present study, we aimed to fill this gap. Participants were asked to produce angry, happy, and neutral tone emotional vocalizations in different production conditions ('normal,' 'whisper,' and 'silent articulation'). An accelerometer recorded the vibrations on the throat, close to the vocal folds. The results highlight the crucial role of vocal tract vibrations in multisensory integration during emotional prosody production. Crucially, Production and its interaction with Emotion revealed significant effects in motor, somatosensory, insular, and inferior frontal cortices. Results also showed effects of the emotion with activations in the bilateral temporal voice areas, the inferior frontal gyri, as well as motor and supplementary motor areas. Exploratory analysis revealed that emotional vocal tract vibrations correlate with activations in multisensory integration regions (insula, inferior frontal cortex, and cerebellum). We propose that vocal tract vibrations could implicitly affect bodily self-consciousness and, therefore, the representation of one's own emotions related to emotional vocal production.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0643,'Inner voices': the cerebral representation of emotional voice cues described in literary texts,"While non-verbal affective voice cues are generally recognized as a crucial behavioral guide in any day-to-day conversation their role as a powerful source of information may extend well beyond close-up personal interactions and include other modes of communication such as written discourse or literature as well. Building on the assumption that similarities between the different 'modes' of voice cues may not only be limited to their functional role but may also include cerebral mechanisms engaged in the decoding process, the present functional magnetic resonance imaging study aimed at exploring brain responses associated with processing emotional voice signals described in literary texts. Emphasis was placed on evaluating 'voice' sensitive as well as task- and emotion-related modulations of brain activation frequently associated with the decoding of acoustic vocal cues. Obtained findings suggest that several similarities emerge with respect to the perception of acoustic voice signals: results identify the superior temporal, lateral and medial frontal cortex as well as the posterior cingulate cortex and cerebellum to contribute to the decoding process, with similarities to acoustic voice perception reflected in a 'voice'-cue preference of temporal voice areas as well as an emotion-related modulation of the medial frontal cortex and a task-modulated response of the lateral frontal cortex.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0644,Neural decoding of discriminative auditory object features depends on their socio-affective valence,"Human voices consist of specific patterns of acoustic features that are considerably enhanced during affective vocalizations. These acoustic features are presumably used by listeners to accurately discriminate between acoustically or emotionally similar vocalizations. Here we used high-field 7T functional magnetic resonance imaging in human listeners together with a so-called experimental 'feature elimination approach' to investigate neural decoding of three important voice features of two affective valence categories (i.e. aggressive and joyful vocalizations). We found a valence-dependent sensitivity to vocal pitch (f0) dynamics and to spectral high-frequency cues already at the level of the auditory thalamus. Furthermore, pitch dynamics and harmonics-to-noise ratio (HNR) showed overlapping, but again valence-dependent sensitivity in tonotopic cortical fields during the neural decoding of aggressive and joyful vocalizations, respectively. For joyful vocalizations we also revealed sensitivity in the inferior frontal cortex (IFC) to the HNR and pitch dynamics. The data thus indicate that several auditory regions were sensitive to multiple, rather than single, discriminative voice features. Furthermore, some regions partly showed a valence-dependent hypersensitivity to certain features, such as pitch dynamic sensitivity in core auditory regions and in the IFC for aggressive vocalizations, and sensitivity to high-frequency cues in auditory belt and parabelt regions for joyful vocalizations.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0645,Neural bases of social communicative intentions in speech,"Our ability to understand others' communicative intentions in speech is key to successful social interaction. Indeed, misunderstanding an 'excuse me' as apology, while meant as criticism, may have important consequences. Recent behavioural studies have provided evidence that prosody, that is, vocal tone, is an important indicator for speakers' intentions. Using a novel audio-morphing paradigm, the present functional magnetic resonance imaging study examined the neurocognitive mechanisms that allow listeners to 'read' speakers' intents from vocal prosodic patterns. Participants categorized prosodic expressions that gradually varied in their acoustics between criticism, doubt, and suggestion. Categorizing typical exemplars of the three intentions induced activations along the ventral auditory stream, complemented by amygdala and mentalizing system. These findings likely depict the stepwise conversion of external perceptual information into abstract prosodic categories and internal social semantic concepts, including the speaker's mental state. Ambiguous tokens, in turn, involved cingulo-opercular areas known to assist decision-making in case of conflicting cues. Auditory and decision-making processes were flexibly coupled with the amygdala, depending on prosodic typicality, indicating enhanced categorization efficiency of overtly relevant, meaningful prosodic signals. Altogether, the results point to a model in which auditory prosodic categorization and socio-inferential conceptualization cooperate to translate perceived vocal tone into a coherent representation of the speaker's intent.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0646,Elicitation of N400m in sentence comprehension due to lexical prosody incongruity,"The role of lexical prosody in the semantic integration of spoken sentences consisting of a quiz stem and an answer word was investigated analyzing the event-related magnetic response, N400m. Three conditions regarding the relations between the quiz and the answer word were prepared: pitch-accent violation, phonemic violation and no violation. Both the pitch-accent and phonemic violations elicited significant N400m without any significant differences in the peak latency and magnitude of the equivalent current dipoles, suggesting that the role of pitch-accent in semantic integration is equivalent to that of phonemes. However, the rate of violation detection and the successful N400m source estimation were lower for the pitch-accent violation than for the phonemic violation, suggesting differential neural processes for the phonemic and prosodic cues. NeuroReport 12:1753-1756 (C) 2001 Lippincott Williams & Wilkins.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0647,An electrophysiological response to different pitch contours in words,"A spoken word with more than one syllable contains a specific stress pattern found to be processed during spoken word recognition. The present study investigated the word's pitch contour as a single auditory parameter that marks stress. Event-related brain potentials (ERPs) were recorded while subjects made decisions to artificially pitch manipulated words. ERPs revealed that pitch contours are discriminated already within the first syllable of a word. Furthermore, behavioral responses for words with incorrect pitch contours were longer than for words with correct pitch contours. The results suggest that the pitch contour is an auditory feature of the spoken word that a listener automatically processes during spoken word recognition. NeuroReport 12:3189-3191 (C) 2001 Lippincott Williams & Wilkins.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0648,Neural network for encoding immediate memory in phonological processing,"The aim of this fMRI study was to identify neuroanatomical substrates of immediate memory underlying phonological processing. To distinguish encoding of immediate memory from rehearsal, participants were required to match tones from the first and last positions of a three-syllable list to their following probes in an immediate-recognition paradigm. The first position task included intervening distractors between the target and probe. Increased activations were found in the left inferior frontal gyrus, right lateral cerebellum, and medial frontal gyrus for the target tone in first position. This network mediates articulatory encoding in immediate-response, and articulatory rehearsal in delayed-response paradigms. These findings support a working memory model in which rehearsal is optional, while encoding is an obligatory component of the phonological loop.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0649,Perception and production of mandarin tones in prelingually deaf children with cochlear implants,"Objective: Mandarin is a lexical tone language in which four tones are crucial for determining lexical meanings. Acquisition of such a tone system may be challenging to prelingually deaf children with cochlear implants because, as recent studies have shown, cochlear implant devices are ineffective in encoding voice pitch information required for tone recognition. This study aimed to investigate Mandarin tone production and perception skills of children with cochlear implants. Design: Thirty prelingually deaf children with cochlear implants, ages 6;0 (yr;mo) to 12;6, participated. These children received their implants at an average age of 5;8, with a range from 2;3 to 10;3. The average length of their cochlear implant experience was 3;7, with a range from 1;7 to 6;5. Tasks of tone production and tone identification involved a pictorial protocol of 48 words containing the targeted tones in either monosyllabic or disyllabic forms. Results: The average scores for tone production was 53.09% (SD = 15.42), and for tone identification was 72.88% (SD = 19.68; chance level = 50%). Significant differences were found in the percentages across the production or identification of tone types or tone pairs. The children with exceptional performance in tone production tended to also perform well in tone identification. The children's performance levels in tone identification and production were also discussed in relation to the factors of age at implantation and length of cochlear implant experience. Conclusions: The present results suggest that the majority of prelingually deaf children with cochlear implants did not master Mandarin tone production. However, a small group of participants demonstrated nearly perfect skills of Mandarin tone production in addition to tone perception. Thus, it is necessary to consider factors other than the device's limitations to explain these high levels of performance in the perception and production of Mandarin lexical tones.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0650,Specificity of experience-dependent pitch representation in the brainstem,"Crosslanguage comparisons of brainstem-evoked potentials have revealed experience-dependent plasticity in pitch representation for curvilinear f(0) contours representative of Mandarin tones. To assess the tolerance limits of this experience-dependent selectivity, we evaluated cross-linguistically (Chinese, English) the pitch strength and tracking accuracy of linear rising and falling f(0) ramps representative of Mandarin tones 2 and 4. No crosslanguage differences in pitch strength or accuracy were observed for either tone, indicating that stimuli with linear rising/falling ramps elicit homogeneous pitch representations at the level of the brainstem regardless of language experience. We conclude that pitch extraction at the brainstem level is critically dependent on specific dimensions of pitch contours that native speakers have been exposed to in natural speech contexts.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0651,Melodic Pitch Perception and Lexical Tone Perception in Mandarin-Speaking Cochlear Implant Users,"Objectives: To examine the relationship between lexical tone perception and melodic pitch perception in Mandarin-speaking cochlear implant (CI) users and to investigate the influence of previous acoustic hearing on CI users' speech and music perception. Design: Lexical tone perception and melodic contour identification (MCI) were measured in 21 prelingual and 11 postlingual young (aged 6-26 years) Mandarin-speaking CI users. Lexical tone recognition was measured for four tonal patterns: tone 1 (flat F0), tone 2 (rising F0), tone 3 (falling-rising F0), and tone 4 (falling F0). MCI was measured using nine five-note melodic patterns that contained changes in pitch contour, as well as different semitone spacing between notes. Results: Lexical tone recognition was generally good (overall mean = 81% correct), and there was no significant difference between subject groups. MCI performance was generally poor (mean = 23% correct). MCI performance was significantly better for postlingual (mean = 32% correct) than for prelingual CI participants (mean = 18% correct). After correcting for outliers, there was no significant correlation between lexical tone recognition and MCI performance for prelingual or postlingual CI participants. Age at deafness was significantly correlated with MCI performance only for postlingual participants. CI experience was significantly correlated with MCI performance for both prelingual and postlingual participants. Duration of deafness was significantly correlated with tone recognition only for prelingual participants. Conclusions: Despite the prevalence of pitch cues in Mandarin, the present CI participants had great difficulty perceiving melodic pitch. The availability of amplitude and duration cues in lexical tones most likely compensated for the poor pitch perception observed with these CI listeners. Previous acoustic hearing experience seemed to benefit postlingual CI users' melodic pitch perception. Longer CI experience was associated with better MCI performance for both subject groups, suggesting that CI users' music perception may improve as they gain experience with their device.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0652,Auditory Discrimination of Lexical Stress Patterns in Hearing-Impaired Infants with Cochlear Implants Compared with Normal Hearing: Influence of Acoustic Cues and Listening Experience to the Ambient Language,"Objectives: To assess discrimination of lexical stress pattern in infants with cochlear implant (CI) compared with infants with normal hearing (NH). While criteria for cochlear implantation have expanded to infants as young as 6 months, little is known regarding infants' processing of suprasegmental-prosodic cues which are known to be important for the first stages of language acquisition. Lexical stress is an example of such a cue, which, in hearing infants, has been shown to assist in segmenting words from fluent speech and in distinguishing between words that differ only the stress pattern. To date, however, there are no data on the ability of infants with CIs to perceive lexical stress. Such information will provide insight to the speech characteristics that are available to these infants in their first steps of language acquisition. This is of particular interest given the known limitations that the CI device has in transmitting speech information that is mediated by changes in fundamental frequency. Design: Two groups of infants participated in this study. The first group included 20 profoundly hearing-impaired infants with CI, 12 to 33 months old, implanted under the age of 2.5 years (median age of implantation = 14.5 months), with 1 to 6 months of CI use (mean = 2.7 months) and no known additional problems. The second group of infants included 48 NH infants, 11 to 14 months old with normal development and no known risk factors for developmental delays. Infants were tested on their ability to discriminate between nonsense words that differed on their stress pattern only (/doti/ versus /doti/ and /doti/ versus /doti/) using the visual habituation procedure. The measure for discrimination was the change in looking time between the last habituation trial (e.g., /doti/) and the novel trial (e.g., /doti/). Results: (1) Infants with CI showed discrimination between lexical stress pattern with only limited auditory experience with their implant device, (2) discrimination of stress patterns in infants with CI was reduced compared with that of infants with NH, (3) both groups showed directional asymmetry in discrimination, that is, increased discrimination from the uncommon to the common stress pattern in Hebrew (/doti/ versus /doti/) compared with the reversed condition. Conclusions: The CI device transmitted sufficient acoustic information (amplitude, duration, and fundamental frequency) to allow discrimination between stress patterns in young hearing-impaired infants with CI. The present pattern of results is in support of a discrimination model in which both auditory capabilities and ""top-down"" interactions are involved. That is, the CI infants detected changes between stressed and unstressed syllables after which they developed a bias for the more common weak-strong stress pattern in Hebrew. The latter suggests that infants with CI were able to extract the statistical distribution of stress patterns by listening to the ambient language even after limited auditory experience with the CI device. To conclude, in relation to processing of lexical stress patterns, infants with CI followed similar developmental milestones as hearing infants thus establishing important prerequisites for early language acquisition.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0653,The Advances in Hearing Rehabilitation and Cochlear Implants in China,"Hearing loss (HL) is a common sensory impairment in humans, with significant economic and social impacts. With nearly 20% of the world's population, China has focused on economic development and health awareness to improve the care for its hearing-impaired population. Recently, the Chinese government has initiated national programs such as the China Disabled Persons Federation to fund prevention, treatment, and rehabilitation of hearing impairment. Newborn hearing screening and auditory rehabilitation programs in China have expanded exponentially with government support. While facing many challenges and overcoming obstacles, cochlear implantation (CI) programs in China have also experienced considerable growth. This review discusses the implementation of CI programs for HL in China and presents current HL data including epidemiology, newborn hearing screening, and determination of genetic etiologies. Sharing the experience in Chinese auditory rehabilitation and CI programs will shine a light on the developmental pathway of healthcare infrastructure to meet emerging needs of the hearing-impaired population in other developing countries.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0654,Cantonese Tone Perception for Children Who Use a Hearing Aid and a Cochlear Implant in Opposite Ears,"Objectives: The ability to recognize tones is vital for speech perception in tonal languages. Cantonese has six tones, which are differentiated almost exclusively by pitch cues (tones 1 to 6). The differences in pitch contours among the tones are subtle, making Cantonese a challenging language for cochlear implant users. The addition of a hearing aid has been shown to improve speech perception in nontonal languages and in Mandarin Chinese. This study (1) investigates the Cantonese tone perception ability of children who use a cochlear implant and a hearing aid in opposite ears; (2) evaluates the effect of varying pitch height and pitch contour cues on Cantonese tone perception for these children; and (3) compares the Cantonese tone perception ability for using a hearing aid and a cochlear implant together versus an implant alone. Design: Eight native Cantonese speaking children using a cochlear implant and a hearing aid in opposite ears were assessed for tone perception and word identification. The tone perception test involved discriminating and ranking tone pairs from natural and artificially manipulated Cantonese tones with various pitch heights and/or pitch contours. The word identification test involved identifying Cantonese words in a four-alternative forced-choice task. All tests were performed in two device conditions: (1) cochlear implant and hearing aid together and (2) implant alone. Results: Seven of the 8 subjects performed significantly above chance in both tests using the cochlear implant alone. Results showed that both pitch height and/or pitch direction were important perceptual cues for implant users. Perception for some tones was improved by increasing the pitch height differences between the tones. The ability to discriminate and rank the tone 2/tone 5 contrast and the tone 4/tone 6 contrast was poor, as the tones in these contrasts are similar in pitch contours and onset frequencies. No significant improvement was observed after artificially increasing the pitch offset differences between the tones in the tone 2/tone 5 and the tone 4/tone 6 contrasts. Tone perception results were significantly better with the addition of the hearing aid in the nonimplanted ear compared with using the implant alone; however, word identification results were not significantly different between using the implant alone and using both the hearing aid and the implant together. None of the subjects performed worse in tone perception or in word identification when the hearing aid was added. Conclusions: Reduced ability to perceive pitch contour cues, even when artificially exaggerated, may explain some of the difficulties in Cantonese word recognition for implant users. The addition of a contralateral hearing aid could be beneficial for Cantonese tone perception for some individuals with a unilateral implant. The results encouraged Cantonese speakers to trial a hearing aid in the nonimplanted ear when using a cochlear implant.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0655,"Temporal Fine Structure Processing, Pitch, and Speech Perception in Adult Cochlear Implant Recipients","Objectives: The aim of the study was to investigate the link between temporal fine structure (TFS) processing, pitch, and speech perception performance in adult cochlear implant (CI) recipients, including bimodal listeners who may benefit better low-frequency (LF) temporal coding in the contralateral ear. Design: The study participants were 43 adult CI recipients (23 unilateral, 6 bilateral, and 14 bimodal listeners). Two new LF pitch perception tests-harmonic intonation (HI) and disharmonic intonation (DI)-were used to evaluate TFS sensitivity. HI and DI were designed to estimate a difference limen for discrimination of tone changes based on harmonic or inharmonic pitch glides. Speech perception was assessed using the newly developed Italian Sentence Test with Adaptive Randomized Roving level (STARR) test where sentences relevant to everyday contexts were presented at low, medium, and high levels in a fluctuating background noise to estimate a speech reception threshold (SRT). Results: Although TFS and STARR performances in the majority of CI recipients were much poorer than those of hearing people reported in the literature, a considerable intersubject variability was observed. For CI listeners, median just noticeable differences were 27.0 and 147.0 Hz for HI and DI, respectively. HI outcomes were significantly better than those for DI. Median STARR score was 14.8 dB. Better performers with speech reception thresholds less than 20 dB had a median score of 8.6 dB. A significant effect of age was observed for both HI/DI tests, suggesting that TFS sensitivity tended to worsen with increasing age. CI pure-tone thresholds and duration of profound deafness were significantly correlated with STARR performance. Bimodal users showed significantly better TFS and STARR performance for bimodal listening than for their CI-only condition. Median bimodal gains were 33.0 Hz for the HI test and 95.0 Hz for the DI test. DI outcomes in bimodal users revealed a significant correlation with unaided hearing thresholds for octave frequencies lower than 1000 Hz. Median STARR scores were 17.3 versus 8.1 dB for CI only and bimodal listening, respectively. STARR performance was significantly correlated with HI findings for CI listeners and with those of DI for bimodal listeners. Conclusions: LF pitch perception was found to be abnormal in the majority of adult CI recipients, confirming poor IFS processing of CIs. Similarly, the STARR findings reflected a common performance deterioration with the HI/DI tests, suggesting the cause probably being a lack of access to TFS information. Contralateral hearing aid users obtained a remarkable bimodal benefit for all tests. Such results highlighted the importance of TFS cues for challenging speech perception and the relevance to everyday listening conditions. HI/DI and STARR tests show promise for gaining insights into how TFS and speech perception are being limited and may guide the customization of CI program parameters and support the fine tuning of bimodal listening.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0656,Evaluation of the Optimized Pitch and Language Strategy in Cochlear Implant Recipients,"Objectives: The Optimized Pitch and Language (OPAL) strategy enhances pitch perception through coding of fundamental frequency (F0) amplitude modulation information in the stimulus envelope delivered to a cochlear implant. Previous research using a prototype of the strategy demonstrated significant benefits in musical pitch and lexical tone discrimination tasks with no degradation in speech recognition when compared with the clinical Advanced Combination Encoder (ACE) strategy in a small group of subjects. Based on those studies, a modified version of the strategy was implemented in the commercial Nucleus CP900 series processor. The aims of the present study were to establish whether the CP900 OPAL implementation continued to provide improved F0 pitch perception in a speech intonation task with no degradation to speech perception in quiet and noise, when compared with the clinical ACE strategy in a larger cohort of subjects. Further aims were to evaluate fitting procedures and subject acclimatization to the strategy after take-home experience. Design: Twenty experienced adult cochlear implant recipients were enrolled in the study. Two subjects withdrew during the study leaving 18 sets of data for analysis. A repeated-measures single-subject design with take-home experience was used to test for improved speech intonation perception using OPAL compared with ACE and for comparable performance between strategies for open-set word recognition in quiet at two presentation levels, sentence recognition in adaptive 4-talker babble noise, and speech intelligibility ratings. The stimulation rate employed for OPAL was 1200 pulses per second/channel which was higher than the default clinical rate of 900 pulses per second/channel used for ACE by all subjects in the present study. Two variations of the OPAL ""F0 restore gain"" (the gain applied to restore the loudness of modulated channels) were investigated: ""custom"" measured per subject and ""default"" which was the average of all subject custom gains. Results: A significant group mean benefit on the intonation test of 8.5% points was shown for OPAL compared with ACE. There was a significant period of adaptation to OPAL with significantly poorer sentence in noise scores acutely and after only 2 weeks of take-home experience. After 4 weeks of take-home experience, comparable word perception in quiet and sentence perception in noise for OPAL were obtained. Furthermore, there was good subject acceptability in the field with comparable speech intelligibility ratings between strategies. Results of the fitting procedure showed that OPAL did not require any additional steps compared with fitting of ACE. A default F0 restore gain provided comparable outcomes to a custom gain setting. Conclusions: The CP900 OPAL implementation provided a significant benefit to perception of speech intonation when compared with ACE. Comparable speech perception (in quiet and noise) and subjective ratings of speech intelligibility between strategies were also achieved after a period of acclimatization. These outcomes are consistent with results of earlier studies using prototype versions of the strategy and reaffirm its potential for improvement of F0 pitch perception in speech while preserving coding of segmental speech information. Furthermore, the OPAL strategy can be programmed into subject's processors using the same fitting procedures used for ACE thereby simplifying its adoption in clinical settings.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0657,Factors Affecting Bimodal Benefit in Pediatric Mandarin-Speaking Chinese Cochlear Implant Users,"Objectives: While fundamental frequency (F0) cues are important to both lexical tone perception and multitalker segregation, F0 cues are poorly perceived by cochlear implant (CI) users. Adding low-frequency acoustic hearing via a hearing aid in the contralateral ear may improve CI users' F0 perception. For English-speaking CI users, contralateral acoustic hearing has been shown to improve perception of target speech in noise and in competing talkers. For tonal languages such as Mandarin Chinese, F0 information is lexically meaningful. Given competing F0 information from multiple talkers and lexical tones, contralateral acoustic hearing may be especially beneficial for Mandarin-speaking CI users' perception of competing speech. Design: Bimodal benefit (CI+hearing aid - CI-only) was evaluated in 11 pediatric Mandarin-speaking Chinese CI users. In experiment 1, speech recognition thresholds (SRTs) were adaptively measured using a modified coordinated response measure test; subjects were required to correctly identify 2 keywords from among 10 choices in each category. SRTs were measured with CI-only or bimodal listening in the presence of steady state noise (SSN) or competing speech with the same (M+M) or different voice gender (M+F). Unaided thresholds in the non-CI ear and demographic factors were compared with speech performance. In experiment 2, SRTs were adaptively measured in SSN for recognition of 5 keywords, a more difficult listening task than the 2-keyword recognition task in experiment 1. Results: In experiment 1, SRTs were significantly lower for SSN than for competing speech in both the CI-only and bimodal listening conditions. There was no significant difference between CI-only and bimodal listening for SSN and M+F (p > 0.05); SRTs were significantly lower for CI-only than for bimodal listening for M+M (p < 0.05), suggesting bimodal interference. Subjects were able to make use of voice gender differences for bimodal listening (p < 0.05) but not for CI-only listening (p > 0.05). Unaided thresholds in the non-CI ear were positively correlated with bimodal SRTs for M+M (p < 0.006) but not for SSN or M+F. No significant correlations were observed between any demographic variables and SRTs (p > 0.05 in all cases). In experiment 2, SRTs were significantly lower with two than with five keywords (p < 0.05). A significant bimodal benefit was observed only for the 5-keyword condition (p < 0.05). Conclusions: With the CI alone, subjects experienced greater interference with competing speech than with SSN and were unable to use voice gender difference to segregate talkers. For the coordinated response measure task, subjects experienced no bimodal benefit and even bimodal interference when competing talkers were the same voice gender. A bimodal benefit in SSN was observed for the five-keyword condition but not for the two-keyword condition, suggesting that bimodal listening may be more beneficial as the difficulty of the listening task increased. The present data suggest that bimodal benefit may depend on the type of masker and/or the difficulty of the listening task.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0658,The Role of Lexical Tone Information in the Recognition of Mandarin Sentences in Listeners With Hearing Aids,"Objectives: Lexical tone information provides redundant cues for the recognition of Mandarin sentences in listeners with normal hearing in quiet conditions. The contribution of lexical tones to Mandarin sentence recognition in listeners with hearing aids (HAs) is unclear. This study aimed to remove lexical tone information and examine the effects on Mandarin sentence intelligibility in HA users. The second objective was to investigate the contribution of cognitive abilities (i.e., general cognitive ability, working memory, and attention) on Mandarin sentence perception when the presentation of lexical tone information was mismatched. Design: A text-to-speech synthesis engine was used to manipulate Mandarin sentences into three test conditions: (1) a Normal Tone test condition, where no alterations were made to lexical tones within sentences; (2) a Flat Tone test condition, where lexical tones were all changed into tone 1 (i.e., the flat tone); and (3) a Random Tone test condition, where each word in test sentences was randomly assigned one of four Mandarin lexical tones. The manipulated sentence signals were presented to 32 listeners with HAs in both quiet and noisy environments at an 8 dB signal to noise ratio. Results: Speech intelligibility was reduced significantly (by approximately 40 percentage points) in the presence of mismatched lexical tone information in both quiet and noise. The difficulty in correctly identifying sentences with mismatched lexical tones among adults with hearing loss was significantly greater than that of adults with normal hearing. Cognitive function was not significantly related to a decline in speech recognition scores. Conclusions: Contextual and other phonemic cues (i.e., consonants and vowels) are inadequate for HA users to perceive sentences with mismatched lexical tone contours in quiet or noise. Also, HA users with better cognitive function could not compensate for the loss of lexical tone information. These results highlight the importance of accurately representing lexical tone information for Mandarin speakers using HAs.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0659,Longer Cochlear Implant Experience Leads to Better Production of Mandarin Tones for Early Implanted Children,"Objectives: Children with cochlear implants (CIs) face challenges in acquiring tones, since CIs do not transmit pitch information effectively. It has been suggested that longer CI experience provides additional benefits for children implanted early, enabling them to achieve language abilities similar to that of normal-hearing (NH) children (Colletti 2009). Mandarin is a tonal language with four lexical tones and a neutral tone (T0), characterized by distinct pitch and durational patterns. It has been suggested that early implantation (i.e., before 2 years) greatly benefits the acquisition of Mandarin tones by children with CIs (Tang et al. 2019c). In this study, we extend those findings to investigate the effect of CI experience on the acquisition of Mandarin tones for children implanted early. We asked the extent to which they were able to produce distinct pitch and durational patterns of both lexical tones and T0 as a function of CI experience, and the extent to which their tonal productions were acoustically like that of NH children. Design: Forty-four NH 3-year olds and 28 children implanted with CIs between 1 and 2 years, aged 3 to 7, were recruited. The children with CIs were grouped according to the length of CI experience: 3 to 6 years, 2 to 3 years, and 1 to 2 years. Lexical tone and T0 productions were elicited using a picture-naming task. Tonal productions from the children with CIs were acoustically analyzed and compared with those from the NH children. Results: Children with 3 to 6 years of CI experience were able to produce distinct pitch and durational patterns for both lexical tones and T0, with NH-like acoustic realizations. Children with 2 to 3 years of CI experience were also able to produce the expected tonal patterns, although their productions were not yet NH-like. Those with only 1 to 2 years of CI experience, however, were not yet able to produce the distinct acoustic patterns for either lexical tones or T0. Conclusions: These results provide acoustic evidence demonstrating that, when Mandarin-speaking children are implanted before the age of 2, only those with 3 to 6 years of experience were able to produce NH-like tones, including both lexical tone and T0. Children with shorter CI experience (less than 3 years) were unable to produce distinct acoustic patterns for the different tones. This suggests that at least 3 years of CI experience is still needed for early implanted children to acquire tonal distinctions similar to those of NH 3-year olds.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0660,Pitch Accuracy of Vocal Singing in Deaf Children With Bimodal Hearing and Bilateral Cochlear Implants,"Objectives: The purpose of the present study was to investigate the pitch accuracy of vocal singing in children with severe to profound hearing loss who use bilateral cochlear implants (CIs) or bimodal devices [CI at one ear and hearing aid (HA) at the other] in comparison to similarly-aged children with normal-hearing (NH). Design: The participants included four groups: (1) 26 children with NH, (2) 13 children with bimodal devices, (3) 31 children with bilateral CIs that were implanted sequentially, and (4) 10 children with bilateral CIs that were implanted simultaneously. All participants were aged between 7 and 11 years old. Each participant was recorded singing a self-chosen song that was familiar to him or her. The fundamental frequencies (F0) of individual sung notes were extracted and normalized to facilitate cross-subject comparisons. Pitch accuracy was quantified using four pitch-based metrics calculated with reference to the target music notes: mean note deviation, contour direction, mean interval deviation, and F0 variance ratio. A one-way ANOVA was used to compare listener-group difference on each pitch metric. A principal component analysis showed that the mean note deviation best accounted for pitch accuracy in vocal singing. A regression analysis examined potential predictors of CI children's singing proficiency using mean note deviation as the dependent variable and demographic and audiological factors as independent variables. Results: The results revealed significantly poorer performance on all four pitch-based metrics in the three groups of children with CIs in comparison to children with NH. No significant differences were found among the three CI groups. Among the children with CIs, variability in the vocal singing proficiency was large. Within the group of 13 bimodal users, the mean note deviation was significantly correlated with their unaided pure-tone average thresholds (r = 0.582, p = 0.037). The regression analysis for all children with CIs, however, revealed no significant demographic or audiological predictor for their vocal singing performance. Conclusion: Vocal singing performance in children with bilateral CIs or bimodal devices is not significantly different from each other on a group level. Compared to children with NH, the pediatric bimodal and bilateral CI users, in general, demonstrated significant deficits in vocal singing ability. Demographic and audiological factors, known from previous studies to be associated with good speech and language development in prelingually-deafened children with CIs, were not associated with singing accuracy for these children.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0661,Sustainable Benefits of High Variability Phonetic Training in Mandarin-speaking Kindergarteners With Cochlear Implants: Evidence From Categorical Perception of Lexical Tones,"Objectives:Although pitch reception poses a great challenge for individuals with cochlear implants (CIs), formal auditory training (e.g., high variability phonetic training [HVPT]) has been shown to provide direct benefits in pitch-related perceptual performances such as lexical tone recognition for CI users. As lexical tones in spoken language are expressed with a multitude of distinct spectral, temporal, and intensity cues, it is important to determine the sources of training benefits for CI users. The purpose of the present study was to conduct a rigorous fine-scale evaluation with the categorical perception (CP) paradigm to control the acoustic parameters and test the efficacy and sustainability of HVPT for Mandarin-speaking pediatric CI recipients. The main hypothesis was that HVPT-induced perceptual learning would greatly enhance CI users' ability to extract the primary pitch contours from spoken words for lexical tone identification and discrimination. Furthermore, individual differences in immediate and long-term gains from training would likely be attributable to baseline performance and duration of CI use. Design:Twenty-eight prelingually deaf Mandarin-speaking kindergarteners with CIs were tested. Half of them received five sessions of HVPT within a period of 3 weeks. The other half served as control who did not receive the formal training. Two classical CP tasks on a tonal continuum from Mandarin tone 1 (high-flat in pitch) to tone 2 (mid-rising in pitch) with fixed acoustic features of duration and intensity were administered before (pretest), immediately after (posttest), and 10 weeks posttraining termination (follow-up test). Participants were instructed to either label a speech stimulus along the continuum (i.e., identification task) or determine whether a pair of stimuli separated by zero or two steps from the continuum was the same or different (i.e., discrimination task). Identification function measures (i.e., boundary position and boundary width) and discrimination function scores (i.e., between-category score, within-category score, and peakedness score) were assessed for each child participant across the three test sessions. Results:Linear mixed-effects (LME) models showed significant training-induced enhancement in lexical tone categorization with significantly narrower boundary width and better between-category discrimination in the immediate posttest over pretest for the trainees. Furthermore, training-induced gains were reliably retained in the follow-up test 10 weeks after training. By contrast, no significant changes were found in the control group across sessions. Regression analysis confirmed that baseline performance (i.e., boundary width in the pretest session) and duration of CI use were significant predictors for the magnitude of training-induced benefits. Conclusions:The stringent CP tests with synthesized stimuli that excluded acoustic cues other than the pitch contour and were never used in training showed strong evidence for the efficacy of HVPT in yielding immediate and sustained improvement in lexical tone categorization for Mandarin-speaking children with CIs. The training results and individual differences have remarkable implications for developing personalized computer-based short-term HVPT protocols that may have sustainable long-term benefits for aural rehabilitation in this clinical population.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0662,Outcomes Using the Optimized Pitch and Language Strategy Versus the Advanced Combination Encoder Strategy in Mandarin-Speaking Cochlear Implant Recipients,"Objectives:The experimental Optimized Pitch and Language (OPAL) strategy enhances coding of fundamental frequency (F0) information in the temporal envelope of electrical signals delivered to channels of a cochlear implant (CI). Previous studies with OPAL have explored performance on speech and lexical tone perception in Mandarin- and English-speaking CI recipients. However, it was not clear which cues to lexical tone (primary and/or secondary) were used by the Mandarin CI listeners. The primary aim of the present study was to investigate whether OPAL provides improved recognition of Mandarin lexical tones in both quiet and noisy environments compared with the Advanced Combination Encoder (ACE) strategy. A secondary aim was to investigate whether, and to what extent, removal of secondary (duration and intensity envelope) cues to lexical tone affected Mandarin tone perception.Design:Thirty-two CI recipients with an average age of 24 (range 7 to 57) years were enrolled in the study. All recipients had at least 1 year of experience using ACE. Each subject attended two testing sessions, the first to measure baseline performance, and the second to evaluate the effect of strategy after provision of some take-home experience using OPAL. A minimum take-home duration of approximately 4 weeks was prescribed in which subjects were requested to use OPAL as much as possible but were allowed to also use ACE when needed. The evaluation tests included recognition of Mandarin lexical tones in quiet and in noise (signal to noise ratio [SNR] +5 dB) using naturally produced tones and duration/intensity envelope normalized versions of the tones; Mandarin sentence in adaptive noise; Mandarin monosyllabic and disyllabic word in quiet; a subset of Speech, Spatial, and Qualities of hearing questionnaire (SSQ, speech hearing scale); and subjective preference for strategy in quiet and noise.Results:For both the natural and normalized lexical tone tests, mean scores for OPAL were significantly higher than ACE in quiet by 2.7 and 2.9%-points, respectively, and in noise by 7.4 and 7.2%-points, respectively. Monosyllabic word recognition in quiet using OPAL was significantly higher than ACE by approximately 7.5% points. Average SSQ ratings for OPAL were significantly higher than ACE by approximately 0.5 points on a 10-point scale. In quiet conditions, 14 subjects preferred OPAL, 7 expressed a preference for ACE, and 9 reported no preference. Compared with quiet, in noisy situations, there was a stronger preference for OPAL (19 recipients), a similar preference for ACE (7 recipients), while fewer expressed no preference. Average daily take-home use of ACE and OPAL was 4.9 and 7.1 hr, respectively.Conclusions:For Mandarin-speaking CI recipients, OPAL provided significant improvements to lexical tone perception for natural and normalized tones in quiet and noise, monosyllabic word recognition in quiet, and subjective ratings of speech intelligibility. Subjects accessed both primary and secondary cues to lexical tone for perception in quiet and noise conditions. The benefits of lexical tone recognition were attributed to enhanced F0 rate cues encoded by OPAL, especially in a noisy environment. The OPAL strategy was well accepted by many of the Mandarin-speaking CI recipients.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0663,Frequency-specific temporal envelope and periodicity components for lexical tone identification in Cantonese,"Objectives: Temporal envelope and periodicity components (TEPC) in the speech signal have potentials to offer important cues for speech recognition especially in tonal languages. The aims of this study are: (i) to investigate the degree of contributions of TEPC to lexical tone identification in Cantonese; and (ii) to investigate whether or not the contributions vary among different frequency bands. The results of these investigations would reveal if there are any frequency-specific TEPC that are important for lexical tone identification. Design: TEPC of monosyllable words carrying different lexical tones, were extracted by the method of full-wave rectification and low-pass filtering. They were used to modulate a speech spectrum noise to create the test stimuli. Thus the stimuli contain only temporal envelope and periodicity components but no temporal fine structures of the original speech signal. Multiple sets of stimuli were created with different combinations of TEPC modulated frequency bands, Eighteen adult subjects with normal hearing participated in the study. Results: Lexical tone identification was the best when only the TEPC from the two high frequency bands (1-2 kHz and 2-4 kHz) of the original signal were provided, but the worst when only the TEPC from the two low frequency bands (60-500 Hz and 500-1000 Hz) were provided. The findings suggested that high frequency bands are carrying TEPC which are important for lexical-tone identification. Lexical tone identification performance was better for the male stimuli than the female ones. Conclusions: The results indicate the potential on improving speech recognition in tonal languages by manipulating TEPC via new signal processing algorithms in hearing prosthesis.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0664,Lexical tone recognition with an artificial neural network,"Objectives: Tone production is particularly important for communicating in tone languages such as Mandarin Chinese. In the present study, an artificial neural network was used to recognize tones produced by adult native speakers. The purposes of the study were (1) to test the sensitivity of the neural network to speaker variation typically in adult speaker groups, (2) to evaluate two normalization procedures to overcome the effects of speaker variation, and (3) to compare tone recognition performance of the neural network with that of the human listeners. Design: A feedforward multilayer neural network was used. Twenty-nine adult native Mandarin Chinese speakers were recruited to record tone samples. The F0 contours of the vowel part of the 1044 monosyllabic words recorded were extracted using an autocorrelation method. Samples from the F0 contours were used as inputs to the neural network. The efficacy of the neural network was first tested by varying the number of inputs and the number of neurons in the hidden layer from 1 to 16. The sensitivity of the neural network to speaker variation was tested by (1) using the raw F0 data from speech tokens of a number of randomly drawn speakers that varied from 1 to 29, (2) using the raw F0 data from speech tokens of either male-only or female-only speakers, and (3) using two sets of normalized F0 data (i.e., tone 1-based normalization and first-order derivative) from speech tokens from a number of randomly drawn speakers that varied from 1 to 29. The recognition performance of the neural network under several experimental conditions was compared with the corresponding recognition performance of 10 normal-hearing, native Mandarin Chinese speaking adult listeners. Results: Three inputs and four hidden neurons were found to be sufficient for the neural network to perform at about 85% correct using speech samples without normalization. The performance of the neural network was affected by variation across speakers particularly between genders. Using the tone 1-based normalization procedure, the performance of the neural network improved significantly. The recognition accuracy of the neural network as a whole or for each tone was comparable with that of the human listeners. Conclusions: The neural network can be used to evaluate the tone production of Mandarin Chinese speaking adults with human listener-like recognition accuracy. The tone 1-based normalization procedure improves the performance of the neural network to human listener-like accuracy. The success of our neural network in recognizing tones from multiple speakers supports its utility for evaluating tone production. Further testing of the neural network with hearing-impaired speakers might reveal its potential use for clinical evaluation of tone production.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0665,Lexical Tone Perception with HiResolution and HiResolution 120 Sound-Processing Strategies in Pediatric Mandarin-Speaking Cochlear Implant Users,"Objectives: Lexical tone recognition tends to be poor in cochlear implant users. The HiResolution (HiRes) sound-processing strategy is designed to better preserve temporal fine structure, or the detailed envelope information, of an acoustic signal. The newer HiRes 120 strategy builds on HiRes by increasing the amount of potential spectral information delivered to the implant user. The purpose of this study was to examine lexical tone recognition in native Mandarin Chinese-speaking children with cochlear implants using the HiRes and HiRes 120 sound-processing strategies. Tone recognition performance was tested with HiRes at baseline and then after up to 6 mo of HiRes 120 experience in the same subjects. Design: Twenty prelingually deafened, native Mandarin-speaking children. with ages ranging from 3.5 to 16.5 yr, participated. All children completed a computerized tone contrast test on three occasions: (1) using HiRes immediately before conversion to HiRes 120 (baseline), (2) 1 mo after conversion, and (3) 3 mo after conversion. Twelve of the 20 children also were tested 6 mo after conversion. In addition, the parents of 18 children completed a questionnaire at the 3-mo follow-up visit regarding the preference of sound-processing strategies and the children's experience related to various aspects of auditory perception and speech production using HiRes 120. Results: As a group, no statistically significant differences were seen between the tone recognition scores using HiRes and HiRes 120. Individual scores showed great variability. Tone recognition performance ranged from chance (50% correct) to nearly perfect. Using the conventional HiRes strategy, 6 of the 20 children achieved high-level tone recognition performance (i.e., >= 90% correct), whereas 7 performed at a level not significantly different from chance (50-60% correct). At the final test, either 3 or 6 mo after conversion, all children achieved tone recognition performance with HiRes 120 that was equal to or better than that with HiRes, although some children's tone recognition performance was worse initially at the 1 or 3 mo follow-up intervals than at baseline. Eight of the 20 children showed statistically significant improvement in tone recognition performance with HiRes 120 on at least one of the follow-up tests. Age at implantation was correlated with tone recognition performance at all four test intervals. Parents of most of the children indicated that the children preferred HiRes 120 more than HiRes. Conclusions: As a group, HiRes 120 did not provide significantly improved lexical tone recognition compared to HiRes, at least throughout the length of the study (up to 6 mo). There were large individual differences in lexical tone recognition among the prelingually deafened, native Mandarin-speaking children with cochlear implants using either HiRes or HiRes 120. Six of the 20 children performed at or near ceiling in the baseline HiRes condition. Of the remainder, approximately half showed significantly better tone recognition when subsequently tested with HiRes 120, although the extent to which this improvement may be attributable to factors other than the change in processing strategy (e.g., general development) is unknown. The children who benefited most from HiRes 120 tended to be those who were implanted at younger ages.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0666,A Follow-Up Study on Music and Lexical Tone Perception in Adult Mandarin-Speaking Cochlear Implant Users,"Objective: The aim was to evaluate the development of music and lexical tone perception in Mandarin-speaking adult cochlear implant (CI) users over a period of 1 year. Study Design: Prospective patient series. Setting: Tertiary hospital and research institute. Patients: Twenty five adult CI users, with ages ranging from 19 to 75 years old, participated in a year-long follow-up evaluation. There were also 40 normal hearing adult subjects who participated as a control group to provide the normal value range. Interventions: Musical sounds in cochlear implants (Mu. S. I. C.) test battery was undertaken to evaluate music perception ability. Mandarin Tone Identification in Noise Test (M-TINT) was used to assess lexical tone recognition. The tests for CI users were completed at 1, 3, 6, and 12 months after the CI switch-on. Main Outcomes Measures: Quantitative and statistical analysis of their results from music and tone perception tests. Results: The performance of music perception and tone recognition both demonstrated an overall improvement in outcomes during the entire 1-year follow-up process. The increasing trends were obvious in the early period especially in the first 6 months after switch-on. There was a significant improvement in the melody discrimination (p<0.01), timbre identification (p <0.001), tone recognition in quiet (p<0.0001), and in noise (p<0.0001). Conclusions: Adult Mandarin-speaking CI users show an increasingly improved performance on music and tone perception during the 1-year follow-up. The improvement was the most prominent in the first 6 months of CI use. It is essential to strengthen the rehabilitation training within the first 6 months.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0667,Cochlear Implant Outcomes in the Chinese-Speaking Adult Population: A Systematic Review,"ObjectiveThe purpose of this study is to perform a systematic review of speech perception outcomes in the Chinese-speaking adult cochlear implant (CI) population.Databases ReviewedPubMed, EMbase, and Scopus.MethodsA comprehensive English literature search was performed with MeSH search terms, keywords, and phrases. Literature written in a language other than English was not included. Full-text articles were screened by two blinded reviewers and adjudicated by a third. Relevant outcomes and demographic data were extracted. Qualitative summaries were performed of the demographics and assessment tools. Speech perception outcomes were assessed with quantitative measures.ResultsForty-four studies (n = 467) notable for marked heterogeneity in speech perception assessment utilized and reporting of relevant patient demographics were included. Mean duration of deafness among studies reporting this metric was 10.27 years (range, 0.08-49; SD, 7.70; n = 250), with 80% of subjects reporting >5 years' duration of deafness and only 19 subjects (7.6%) with ConclusionWithin the English-language literature, there is marked heterogeneity and lack of standardization regarding speech perception outcomes, tests utilized, and reported patient demographics in the Chinese-speaking adult CI population. Most Chinese-speaking CI users for whom data were available had prolonged duration of deafness before implantation. This study may serve as an initial reference for providers counseling Chinese-language CI candidates and who may be interested in adopting these tests, while highlighting the need for continued efforts to measure speech perception outcomes after CI for tonal language speakers.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0668,Lexical Tone Perception Ability of Profoundly Hearing-Impaired Children: Performance of Cochlear Implant and Hearing Aid Users,"Objective: In tone languages such as Cantonese, a change in tone denotes a change in lexical meaning. The present study investigates the functional benefit of hearing devices in assisting tone perception among children with profound hearing impairment. Subjects: Fifty-two children with profound hearing loss were categorized into two groups based on their primary type of hearing device - a hearing aid group and cochlear implant group. Methods: A 75-item tone identification test covering all 15 Cantonese tone contrast pairs was administered to each subject under two conditions unaided (hearing devices turned off) and aided (devices turned on). The proportion of correct responses was computed as the total score for all items and subtotal contrast scores for each of the 15 tone contrasts. Results: The results indicated no significant differences between the children wearing hearing aids and those with cochlear implants under the unaided or the aided condition (z = 0.91, p = 0.36; z = -0.40, p = 0.69, respectively). Regardless of the type of device used, the total scores under the aided condition were higher than those under the unaided condition (z = -3.55 for the hearing aid group; z = -4.87 for the cochlear implant group, both ps <0.01). Conclusion: Children wearing hearing devices generally have a satisfactory functional gain in tone perception. No major observable difference was noted between children using cochlear implants and those using hearing aids. Tone contrast pairs with dissimilar fundamental frequency at onset and dissimilar tone contour patterns were more easily identified.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0669,Speech Intonation and Melodic Contour Recognition in Children With Cochlear Implants and With Normal Hearing,"Background: Cochlear implant (CI) users have difficulty perceiving some intonation cues in speech and melodic contours because of poor frequency selectivity in the cochlear implant signal. Objectives: To assess perceptual accuracy of normal hearing (NH) children and pediatric CI users on speech intonation (prosody), melodic contour, and pitch ranking, and to determine potential predictors of outcomes. Hypothesis: Does perceptual accuracy for speech intonation or melodic contour differ as a function of auditory status (NH, CI), perceptual category (falling versus rising intonation/contour), pitch perception, or individual differences (e. g., age, hearing history)? Method: NH and CI groups were tested on recognition of falling intonation/contour versus rising intonation/contour presented in both spoken and melodic (sung) conditions. Pitch ranking was also tested. Outcomes were correlated with variables of age, hearing history, HINT, and CNC scores. Results: The CI group was significantly less accurate than the NH group in spoken (CI, M = 63.1%; NH, M = 82.1%) and melodic (CI, M = 61.6%; NH, M = 84.2%) conditions. The CI group was more accurate in recognizing rising contour in the melodic condition compared with rising intonation in the spoken condition. Pitch ranking was a significant predictor of outcome for both groups in falling intonation and rising melodic contour; age at testing and hearing history variables were not predictive of outcomes. Conclusion: Children with CIs were less accurate than NH children in perception of speech intonation, melodic contour, and pitch ranking. However, the larger pitch excursions of the melodic condition may assist in recognition of the rising inflection associated with the interrogative form.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0670,Relationship Between Tone Perception and Production in Prelingually Deafened Children With Cochlear Implants,"Hypothesis: Performance in tone perception and production are correlated in prelingually deafened pediatric cochlear implant (CI) users across individuals. Demographic variables, such as age at implantation, contribute to the performance variability. Background: Poor representation of pitch information in CI devices hinders pitch perception and affects perception of lexical tones in cochlear implant users who speak tonal languages. Methods: One hundred ten Mandarin-speaking, prelingually deafened CI subjects and 125 typically developing, normal-hearing subjects were recruited from Beijing, China. Lexical tone perception was measured using a computerized tone contrast test. Tone production was judged by native Mandarin-speaking adult listeners as well as analyzed acoustically and with an artificial neural network. A general linear model analysis was performed to determine factors that accounted for performance variability. Results: CI subjects scored similar to 67% correct on the lexical tone perception task. The degree of differentiation of tones produced by the CI group was significantly lower than the control group as revealed by acoustic analysis. Tone production performance assessed by the neural network was highly correlated with that evaluated by human listeners. There was a moderate correlation between the overall tone perception and production performance across CI subjects. Duration of implant use and age at implantation jointly explained similar to 29% of the variance in the tone perception performance. Age at implantation was the only significant predictor for tone production performance in the CI subjects. Conclusion: Tone production performance in pediatric CI users is dependent on accurate perception. Early implantation predicts a better outcome in lexical tone perception and production.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0671,The effect of lexical tone experience on English intonation perception in Mandarin-speaking cochlear-implanted children,"To examine the effect of lexical tone experience on English intonation perception in Mandarin-speaking cochlear-implanted children during second language acquisition in Taiwan. A retrospective cohort study. A tertiary referred center. Fourteen children with cochlear implant (CI) in the experimental group, and 9 normal hearing children in the control group were enrolled in this study. Cochlear implantation and hearing rehabilitation. Two speech recognition accuracies were examined: (1) Lexical tone recognition (4-alternative forced choice, AFC), (2) English Sentence Intonation (2AFC). The overall accuracies for tone perception are 61.13% (standard deviation, SD = 10.84%) for CI group and 93.82% (SD = 1.80%) for normal hearing group. Tone 4 and Tone 1 were more easily to be recognized than tone 2 and tone 3 in the pediatric CI recipients (cCI) group. In English intonation perception, the overall accuracies are 61.82% (SD = 16.85%) for CI group, and 97.59% (SD = 4.73%) for normal hearing group. Significant high correlation (R = .919, P <= .000) between lexical tone perception and English intonation perception is noted. There is no significant difference for English intonation perception accuracies between Mandarin-speaking cCI (61.82%) and English-speaking cCI (70.13%, P = .11). Mandarin-speaking cochlear-implanted children showed significant deficits in perception of lexical tone and English intonation relative to normal hearing children. There was no tonal language benefit in Mandarin-speaking cochlear-implanted children's English intonation perception, compared to the English-speaking cochlear-implanted peers. For cochlear-implanted children, better lexical tone perception comes with better English intonation perception. Enhancing Mandarin prosodic perception for cochlear-implanted children may benefit their command of intonation in English.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0672,Cortical Sensorimotor Control in Vocalization: A Functional Magnetic Resonance Imaging Study,"Background: Verbal communication is a human feature and volitional vocalization is its basis. However, little is known regarding the cortical areas involved in human vocalization. Methods: Therefore, functional magnetic resonance imaging at 3 Tesla was performed in 16 healthy adults to evaluate brain activations related to voice production. The main experiments included tasks involving motor control of laryngeal muscles with and without intonation. In addition, reference mappings of the sensorimotor hand area and the auditory cortices were performed. Results: Related to vocalization, in addition to activation of the most lateral aspect of the primary sensorimotor cortex close to the Sylvian fissure (M1c), we found activations medially (M1a) and laterally (M1b) of the well-known sensorimotor hand area. Moreover, the supplementary motor area and the anterior cingulate cortex were activated. Conclusions: Although M1a could be ascribed to motor control of breathing, M1b has been associated with laryngeal motor control. Consequently, even though M1c represents a laryngeal sensorimotor area, its exclusiveness as suggested previously could not be confirmed. Activations in the supplementary motor area and anterior cingulate cortex were ascribed to ""vocal-motor planning."" The present data provide the basis for further functional magnetic resonance imaging studies in patients with neurological laryngeal disorders.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0673,"Lexical Morphology: Structure, Process, and Development","Recent work has demonstrated the importance of derivational morphology to later language development and has led to a consensus that derivation is a lexical process. In this review, derivational morphology is discussed in terms of lexical representation models from both linguistic and psycholinguistic perspectives. Input characteristics, including types of frequency (lexical, surface, affix, and relative) and transparency (semantic, phonological, and orthographic), are examined as key factors that affect processing and acquisition. We introduce the possibility that lexical prosody and syllabic characteristics are relevant to lexical representation and affix separability, and we propose that derivational morphemes can emerge to different degrees in a system that is sensitive to both sound and meaning. Finally, morphological development with a focus on children's sensitivity to input characteristics is briefly reviewed, and we conclude with a perspective of how lexical representation can be a framework for derived word study in therapeutic or educational settings.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0674,Articulatory-based Phonemic Paraphasia in Conduction Aphasia: A Dysfunction in Phoneme-to-Articulation Conversion Uncovered Through Crossed Aphasia,"Phonemic paraphasia, a common characteristic of conduction aphasia, has traditionally been attributed to phonological representation dysfunction. An alternative hypothesis posits that phonemic paraphasia arises from difficulty converting phonemes into their corresponding articulatory maneuvers. However, detailed case studies supporting this theory have been lacking. In this report, we present the case of a 61-year-old right-handed man with right temporo-parietal infarction who exhibited crossed aphasia characterized by typical conduction aphasia symptoms (eg, relatively fluent speech with intact comprehension, frequent phonemic paraphasia, and pronounced difficulties in oral repetition) in the absence of distorted articulation, syllable segmentation, and prosody impairment. Despite the frequent occurrence of phonemic paraphasia and articulatory challenges, our patient's phonological representations remained relatively intact. His phonemic paraphasia was often self-corrected to produce correct responses, a feature known as conduit d'approche. During the oral repetition of individual mora (ie, the smallest unit of speech in Japanese), we observed that the patient consistently traced the corresponding Hiragana phonetic symbol accurately, despite his difficulties in articulation. We substantiated this phenomenon through objective assessment and posit that it resulted from an unusual separation of language functions in crossed aphasia-specifically, a disconnection between phonological representations in the right temporo-parietal cortex and speech articulation engrams in the left hemisphere. In this case of conduction aphasia, articulatory-based phonemic paraphasia may be viewed as an inability to convert phonemes into the appropriate articulatory maneuvers rather than as phonological representation dysfunction or apraxia of speech.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0675,Tonal and vowel information processing in Chinese spoken word recognition: an event-related potential study,"In the present study, the time course of tonal and vowel information processing of the spoken words in Mandarin Chinese was investigated using a delayed-response paradigm. Idiomatic materials, providing semantically highly constraining contexts, were utilized. Besides being presented normally, the terminal monosyllabic words in idioms were manipulated with Tonal, Vowel, or Triple violations (i.e. with consonantal, tonal, and vowel mismatches). Event-related potential results showed that all three violations elicited larger widespread negativities in comparison with the Control condition, with the Triple violation effect starting first from 150ms, then the Vowel violation, and the Tonal violation being the latest. The different starting times of the violation effects suggest that the access of tonal information is slower than that of vowel information, even though the lexical tones are very important in distinguishing the meaning of Chinese words.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0676,Evidence of both brainstem and auditory cortex involvement in categorical perception for Chinese lexical tones,"The categorical perception of lexical tones is important to understand tonal languages. Recent studies have provided electrophysiological evidence for the categorical perception of lexical tones at the cortical level; however, whether neural correlates exist at subcortical levels remain unknown. In this study, by using across-category and within-category lexical tone contrasts with the equivalent physical interval, we recorded deviance detection activities at both the brainstem (reflected by frequency following response) and cortical levels (reflected by mismatch negativity) simultaneously. We found that significantly enhanced intertrial phase-locking of frequency following response s was observed only during the across-category deviance detection, which indicates that phonological differences could be detected at the level of brainstem. In addition, the across-category deviants induced stronger mismatch negativity than within-category deviants. For the first time, our results demonstrate that neural correlates of categorical perception of lexical tones exist even in the brainstem, and suggests that both cortical and subcortical processes are involved in the coding and categorization of tonal information.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0677,Experience-dependent neural plasticity is sensitive to shape of pitch contours,"Language experience is known to modulate the preattentive processing of linguistically relevant pitch contours when presented in the speech domain. To assess if experience-dependent effects are specific to speech, we evaluated the mismatch negativity response to nonspeech homologs (iterated rippled noise) of such curvilinear pitch contours (Mandarin: Tone 1, 'high levels'; Tone 2, 'high rising') by Chinese and English listeners as well as to a pitch contour that was a linear approximation of Tone 2 ('linear ascending ramps). Mandarin speakers showed larger mismatch negativity responses than English to the curvilinear pitch contours only. These results suggest that experience-dependent neural plasticity in early cortical processing of linguistically relevant pitch contours is sensitive to naturally occurring pitch dimensions but not specific to speech per se.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0678,Event-related potential evidence of processing lexical pitch-accent in auditory Japanese sentences,"Neural mechanisms that underlie the processing of lexical pitch-accent in auditory Japanese were investigated by using event-related potentials. Native speakers of Japanese listened to two types of short sentences, both consisting of a noun and a verb. The sentences ended with a verb with either congruous or incongruous pitch-accent pattern, where pitch-accent violations occur at the verb in the incongruent condition. The event-related potentials of the incongruent condition showed an increased widespread negativity that started 400 ms after the onset of the deviant lexical item and lasted for about 400 ms. These results suggest that the negativity evoked by violations in lexical-pitch accent indicates electrophysiological evidence for the online processing of lexical-pitch accent in auditory Japanese. NeuroReport 20:1270-1274 (C) 2009 Wolters Kluwer Health vertical bar Lippincott Williams & Wilkins.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0679,Hemispheric asymmetries in phonological processing of tones versus segmental units,"The aim of this functional magnetic resonance imaging study is to identify neuroanatomical substrates underlying phonological processing of segmental (consonant, rhyme) and suprasegmental (tone) units. An auditory verbal recognition paradigm was used in which native speakers of Mandarin Chinese were required to match a phonological unit that occurs in a list of three syllables to the corresponding unit of a following probe. The results show that hemispheric asymmetries arise depending on the type of phonological unit. In direct contrasts between phonological units, tones, relative to consonants and rhymes, yield increased activation in frontoparietal areas of the right hemisphere. This finding indicates that the cortical circuitry subserving lexical tones differs from that of consonants or rhymes. NeuroReport 21:690-694 (C) 2010 Wolters Kluwer Health vertical bar Lippincott Williams & Wilkins.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0680,Linguistic status of timbre influences pitch encoding in the brainstem,"The aim of this experiment is to assess the effects of the linguistic status of timbre on pitch processing in the brainstem. Brainstem frequency following responses were evoked by the Mandarin high-rising lexical tone superimposed on a native vowel quality ([i]), nonnative vowel quality ([oe]), and iterated rippled noise ( nonspeech). Results revealed that voice fundamental frequency magnitudes were larger when concomitant with a native vowel quality compared with either nonnative vowel quality or nonspeech timbre. Such experience-dependent effects suggest that subcortical sensory encoding of pitch interacts with timbre in the human brainstem. As a consequence, responses of the perceptual system can be differentially shaped to pitch patterns in relation to the linguistic status of their concomitant timbre. NeuroReport 22:801-803 (C) 2011 Wolters Kluwer Health vertical bar Lippincott Williams & Wilkins.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0681,HIDDEN MARKOV MODEL FOR MANDARIN LEXICAL TONE RECOGNITION,,,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0682,Classification of Thai tone sequences in syllable-segmented speech using the analysis-by-synthesis method,,,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0683,Statistical prosodic modeling: From corpus design to parameter estimation,"The increasing availability of carefully designed and collected speech corpora opens up new possibilities for the statistical estimation of formal multivariate prosodic models. At Apple Computer, statistical prosodic modeling exploits the Victoria corpus, recently created to broadly support ongoing speech synthesis research and development. This corpus is composed of five constituent parts. each designed to cover a specific aspect of speech synthesis: polyphones, prosodic contests, reiterant speech, function word sequences, and continuous speech. This paper focuses on the use of the Victoria corpus in the statistical estimation of duration and pitch models for Apple's next-generation test-to-speech system in Macintosh OS X. Duration modeling relies primarily on the subcorpus of prosodic contexts, which is instrumental tb uncover empirical evidence in favor of a piece-wise linear transformation in the well-known sums-of-products approach. Pitch modeling relies primarily on the subcorpus of reiterant speech, which makes possible the optimization of superpositional pitch models with more accurate underlying smooth contours. Experimental results illustrate the improved prosodic representation resulting from these new duration and pitch models.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0684,Emotional Voice Conversion Using a Hybrid Framework With Speaker-Adaptive DNN and Particle-Swarm-Optimized Neural Network,"We propose a hybrid network-based learning framework for speaker-adaptive vocal emotion conversion, tested on three different datasets (languages), namely, EmoDB (German), IITKGP (Telugu), and SAVEE (English). The optimized learning model introduced is unique because of its ability to synthesize emotional speech with an acceptable perceptive quality while preserving speaker characteristics. The multilingual model is extremely beneficial in scenarios wherein emotional training data from a specific target speaker are sparsely available. The proposed model uses speaker-normalized mel-generalized cepstral coefficients for spectral training with data adaptation using the seed data from the target speaker. The fundamental frequency (F0) is transformed using a wavelet synchrosqueezed transform prior to mapping to obtain a sharpened time & x2013;frequency representation. Moreover, a feedforward artificial neural network, together with particle swarm optimization, was used for F0 training. Additionally, static-intensity modification was also performed for each test utterance. Using the framework, we were able to capture the spectral and pitch contour variabilities of emotional expression better than with other state-of-the-art methods used in this study. Considering the overall performance scores across datasets, an average melcepstral distortion (MCD) of 4.98 and root mean square error (RMSE-F0) of 10.67 were obtained in objective evaluations, and an average comparative mean opinion score (CMOS) of 3.57 and speaker similarity score of 3.70 were obtained for the proposed framework. Particularly, the best MCD of 4.09 (EmoDB-happiness) and RMSE-F0 of 9.00 (EmoDB-anger) were obtained, along with the maximum CMOS of 3.7 and speaker similarity of 4.6, thereby highlighting the effectiveness of the hybrid network model.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0685,The Application Analysis of Neural Network Techniques on Lexical Tone Rehabilitation of Mandarin-Speaking Patients With Post-Stroke Dysarthria,"The Objectives of this study are (1) to evaluate tone production in Mandarin-speaking patients with post-stroke dysarthria (PSD) using an artificial neural network (ANN), (2) to investigate the efficacy of recognition performance of the ANN model contrast to the human listeners and the convolutional neural network (CNN) model, and (3) to explore rehabilitation application of the artificial intelligence recognition for lexical tone production disorder with PSD. The subjects include two groups of native Mandarin speaking adults: 31 patients with PSD and 42 normal-speaking adults (NA) in a similar age range as controls. Each subject was recorded producing a list of 7 Mandarin monosyllables with 4 tones (i.e., a total of 28 tokens). The fundamental frequency (F0) of each monosyllable was extracted using auto-correlation algorithm. The ANN was trained with F0 data of the tone tokens from the NA, to generate the final model. The recognition rates of the human ears, ANN model, and CNN model were 87.78 & x0025; & x00B1; 8.96 & x0025; (mean & x00B1; SD), 89.11 & x0025; & x00B1;11.80 & x0025;, 65.91 & x0025; & x00B1; 8.79 & x0025; respectively for tone production of NA group; 70.28 & x0025; & x00B1; 17.61 & x0025;, 63.35 & x0025; & x00B1; 17.40 & x0025;, 34.71 & x0025; & x00B1; 6.92 & x0025; respectively for tone production of PSD group. For PSD group, there was significant correlation between the performance of the ANN model and human listeners (r & x003D; 0.826, P & x003C; 0.001). However, the performance of CNN model was not correlated with that of the human ears (r & x003D; & x2212;0.108, P & x003D; 0.562). Thus, the experiments show that ANN is more objective and efficient, which could replace human listeners in the assessment of lexical tone production disorder in Mandarin-speaking patients with PSD. Furthermore, using ANN may reduce the heterogeneity of rehabilitation evaluation among different speech therapists and may give the feedback for achievement of rehabilitation treatment more accurately.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0686,RFGETT-TTS: Robust Fine-Grained Expressivity Transfer With Transformer for Text-to-Speech Synthesis,"Neural text-to-speech (TTS) research has advanced significantly, yielding various approaches that generate speech with enhanced naturalness. Despite these strides, synthesizing expressive speech remains a significant challenge due to the complex and variable nature of explicit human prosody. In this paper, we presented an effective TTS approach that transfers expressivity from a reference speech to a target spoken text. The proposed approach, Robust Fine-Grained Expressivity Transfer with Transformer, RFGETT-TTS, extends and enhances the Transformer-based text-to-speech architecture to enable transfer of expressivity from a reference utterance to synthesized speech. Key components of RFGETT-TTS include: 1) a prosody extraction module that aligns prosodic features with phoneme inputs, providing stable prosodic representations to the NTTS system, 2) an expressivity aggregation module that encodes and fuses expressive characteristics into a single expressive vector using convolutional layers and normalization, 3) a Multi-Head Cross-Attention mechanism that aligns and integrates linguistic inputs with expressivity features derived from the reference utterance, and 4) the speaker module that encodes speaker identity representation. The Transformer encoder's output is then fused with speaker embeddings and used to condition the Transformer decoder. Extensive evaluations on ESD and EmoV-DB datasets demonstrate that RFGETT-TTS outperforms four TTS models in expressivity transfer while maintaining high-quality of synthesized speech, validated by both objective and subjective evaluation metrics. Objective metric like Mel Cepstral Distortion shows that RFGETT-TTS achieves better than four models with a gap up to 0.18, while statistical subjective results on Expressivity Mean Opinion Score show significant difference (p-value < 0.05) than four models.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0687,Voice Conversion for Persons with Amyotrophic Lateral Sclerosis,"Amyotrophic lateral sclerosis (ALS) results in progressive paralysis of voluntary muscles throughout the body. As speech deteriorates, individuals rely on pre-programmed messages available on commercial speech generating devices to communicate using one of the generic electronic voices on the device. To replace these generic voices and restore vocal identity, our aim is to develop personalized voices for people with ALS via the approach of voice conversion. The task is challenging because very few people have large quantities of their premorbid healthy speech recorded. Therefore, we have to rely on small quantities of dysarthric speech concomitant with an individual's disease stage. Further, progressive fatigue prohibits acquisition of large speech datasets and individuals display a range of dysarthria severities resulting from breathing, voice, articulation, resonance, and prosody disturbances. As the first step to address these problems, we use healthy source speakers and propose the approach of combining a structured sparse spectral transform with multiple linear regression-based frequency warping prediction for spectral conversion, and interpolating the transformed spectral frames for speech rate modification. Our experimental data included four healthy source speakers from the ARCTIC dataset, and four target ALS speakers with mild to severe dysarthria, forming 16 speaker pairs. Subjective listening evaluations showed that on average, (i) the proposed approach improved speech intelligibility by about 80% over the target speakers' speech, (ii) the converted voice was 3 times more similar to the target speakers' speech than to the source speakers' speech, and (iii) the converted speech quality was close to the MOS scale ""good"" relative to the source speakers' speech being ""excellent.""",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0688,B-Spline Model Order Selection With Optimal MDL Criterion Applied to Speech Fundamental Frequency Stylization,"In the speech processing field, stylization of fundamental frequency F-0 has been subjected to numerous works. Models proposed in the literature rely on knowledge stemming from phonology and linguistics. We propose an approach that deals with the issue of F-0 curve stylization requiring as few linguistic assumptions as possible and in the framework of B-spline models. A B-spline model, characterized by a sequence of knots with which control points are associated, enables the formalization of discontinuities in the derivatives of the observed values sequence. Beyond the implementation of a B-spline model to stylize an open curve sampled using a constant step, we address the problem of the optimal model order choice. We propose to use a parsimony criterion based on a minimum description length (MDL) approach, in order to optimize the number of knots. We derive several criteria relying on bounds estimated from parameter values. We demonstrate the optimality of these choices in the theoretical MDL framework. We introduce a notion of variable precision of parameters which enables a good compromise between the modeling precision and degrees of freedom of the estimated models. Experiments are performed on a French speech corpus and compare three MDL criteria. The use of both B-spline model and MDL methodology enables an efficient modeling of F-0 curves and provides an RMS error around 1 Hz while allowing a relatively high compression rate about 40%.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0689,Automatic Variation of the Degree of Articulation in New HMM-Based Voices,"This paper focuses on the automatic modification of the degree of articulation (hypo and hyperarticulation) of an existing standard neutral voice in the framework of HMM-based speech synthesis. Hypo and hyperarticulation refer to the production of speech respectively with a reduction and an increase of the articulatory efforts compared to the neutral style. Starting from a source speaker for which neutral, hypo and hyperarticulated speech data are available, statistical transformations are computed during the adaptation of the neutral speech synthesizer. These transformations are then applied to a new target speaker for which no hypo or hyperarticulated recordings are available. Four statistical methods are investigated, differing in the speaking style adaptation technique (model-space Linear Scaling LS vs. CMLLR) and in the speaking style transposition approach (phonetic vs. acoustic correspondence) they use. The efficiency of these techniques is assessed for the transposition of prosody and of filter coefficients separately. Besides we investigate which representation of the spectral envelope is the most suited for this purpose: MGC, LSP, PARCOR and LAR coefficients. Subjective evaluations are performed in order to determine which statistical transformation method achieves the highest performance in terms of segmental quality, reproduction of the articulation degree and speaker identity preservation. The most successful method is finally used for automatically modifying the degree of articulation of existing standard neutral voices.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0690,Automatic Detection of Articulatory-Based Disfluencies in Primary Progressive Aphasia,"Speech corpora are collections of textual data derived from human verbal output and speech signals that can be processed from a variety of perspectives, including formal or semantic content, to serve analyses of different levels of linguistic organisation (phonemic, morphosyntactic, lexico-semantic and content information, prosody and intonation) and to serve analyses of important phenomena such as speech fluency and errors (non-fluencies). We focus on transcribing speech along with non-fluencies or dysfluencies, the detection of which plays an important role in the diagnosis of primary progressive aphasia, where we specifically examine articulation-based dysfluencies in nfvPPA speech. In this work, we propose SSDM 2.0, which is built on top of the current state-of-the-art system of dysfluency detection [1] and tackles its shortcomings via four main contributions: (1) We propose a novel Neural Articulatory Flow for deriving highly scalable, dysfluency-aware speech representations. (2) We develop a full-stack connectionist subsequence aligner to capture all major dysfluency types. (3) We introduce a mispronunciation prompt pipeline and consistency learning into LLMs to enable in-context dysfluency learning. (4) We curate and open-source Libri-Co-Dys (Lian et al., 2024), the largest co-dysfluency corpus to date. (5) We also present SSDM-L, a modular, non-end-to-end, lightweight model designed for clinical deployment. In clinical experiments on pathological speech transcription, we tested SSDM 2.0 using nfvPPA corpus primarily characterized by articulatory dysfluencies. Overall, SSDM 2.0 outperforms SSDM and all other dysfluency transcription models by a large margin.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0691,Recognizing Stress Using Semantics and Modulation of Speech and Gestures,"This paper investigates how speech and gestures convey stress, and how they can be used for automatic stress recognition. As a first step, we look into how humans use speech and gestures to convey stress. In particular, for both speech and gestures, we distinguish between stress conveyed by the intended semantic message (e.g. spoken words for speech, symbolic meaning for gestures), and stress conveyed by the modulation of either speech and gestures (e.g. intonation for speech, speed and rhythm for gestures). As a second step, we use this decomposition of stress as an approach for automatic stress prediction. The considered components provide an intermediate representation with intrinsic meaning, which helps bridging the semantic gap between the low level sensor representation and the high level context sensitive interpretation of behavior. Our experiments are run on an audiovisual dataset with service-desk interactions. The final goal is having a surveillance system that would notify when the stress level is high and extra assistance is needed. We find that speech modulation is the best performing intermediate level variable for automatic stress prediction. Using gestures increases the performance and is mostly beneficial when speech is lacking. The two-stage approach with intermediate variables performs better than baseline feature level or decision level fusion.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0692,"Hidden Bawls, Whispers, and Yelps: Can Text Convey the Sound of Speech, Beyond Words?","Whether a word was bawled, whispered, or yelped, captions will typically represent it in the same way. If they are your only way to access what is being said, subjective nuances expressed in the voice will be lost. Since so much of communication is carried by these nuances, we posit that if captions are to be used as an accurate representation of speech, embedding visual representations of paralinguistic qualities into captions could help readers use them to better understand speech beyond its mere textual content. This paper presents a model for processing vocal prosody (its loudness, pitch, and duration) and mapping it into visual dimensions of typography (respectively, font-weight, baseline shift, and letter-spacing), creating a visual representation of these lost vocal subtleties that can be embedded directly into the typographical form of text. An evaluation was carried out where participants were exposed to this speech-modulated typography and asked to match it to its originating audio, presented between similar alternatives. Participants (n=117) were able to correctly identify the original audios with an average accuracy of 65%, with no significant difference when showing them modulations as animated or static text. Additionally, participants' comments showed their mental models of speech-modulated typography varied widely.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0693,DurFlex-EVC: Duration-Flexible Emotional Voice Conversion Leveraging Discrete Representations Without Text Alignment,"Emotional voice conversion (EVC) involves modifying various acoustic characteristics, such as pitch and spectral envelope, to match a desired emotional state while preserving the speaker's identity. Existing EVC methods often rely on text transcriptions or time-alignment information and struggle to handle varying speech durations effectively. In this paper, we propose DurFlex-EVC, a duration-flexible EVC framework that operates without the need for text or alignment information. We introduce a unit aligner that models contextual information by aligning speech with discrete units representing content, eliminating the need for text or speech-text alignment. Additionally, we design a style autoencoder that effectively disentangles content and emotional style, allowing precise manipulation of the emotional characteristics of the speech. We further enhance emotional expressiveness through a hierarchical stylize encoder that applies the target emotional style at multiple hierarchical levels, refining the stylization process to improve the naturalness and expressiveness of the converted speech. Experimental results from subjective and objective evaluations demonstrate that our approach outperforms baseline models, effectively handling duration variability and enhancing emotional expressiveness in the converted speech.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0694,"Automatic prosodic event detection using acoustic, lexical, and syntactic evidence","With the advent of prosody annotation standards such as tones and break indices (ToBI), speech technologists and linguists alike have been interested in automatically detecting prosodic events in speech. This is because the prosodic tier provides an additional layer of information over the short-term segment-level features and lexical representation of an utterance. As the prosody of an utterance is closely tied to its syntactic and semantic content in addition to its lexical content, knowledge of the prosodic events within and across utterances can assist spoken language applications such as automatic speech recognition and translation. On the other hand, corpora annotated with prosodic events are useful for building natural-sounding speech synthesizers. In this paper, we build an automatic detector and classifier for prosodic events in American English, based on their acoustic, lexical, and syntactic correlates. Following previous work in this area, we focus on accent (prominence, or ""stress"") and prosodic phrase boundary detection at the syllable level. Our experiments achieved a performance rate of 86.75% agreement on the accent detection task, and 91.61% agreement on the phrase boundary detection task on the Boston University Radio News Corpus.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0695,Exploiting acoustic and syntactic features for automatic prosody labeling in a maximum entropy framework,"In this paper, we describe a maximum entropy-based automatic prosody labeling framework that exploits both language and speech information. We apply the proposed framework to both prominence and phrase structure detection within the Tones and Break Indices (ToBI) annotation scheme. Our framework utilizes novel syntactic features in the form of supertags and a quantized acoustic-prosodic feature representation that is similar to linear parameterizations of the prosodic contour. The proposed model is trained discriminatively and is robust in the selection of appropriate features for the task of prosody detection. The proposed maximum entropy acoustic-syntactic model achieves pitch accent and,boundary tone detection accuracies of 86.0% and 93.1% on the Boston University Radio News corpus, and, 79.8% and 90.3% on the Boston Directions corpus. The phrase structure detection through prosodic break index labeling provides accuracies of 84% and 87% on the two corpora, respectively. The reported results are significantly better than previously reported results and demonstrate the strength of maximum entropy model in jointly modeling simple lexical, syntactic, and acoustic features for automatic prosody labeling.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0696,Speech Analysis and Synthesis Based on Dynamic Modes,"In this paper, the source-filter model of speech production is adapted to represent the speech signal as the superposition and convolution of a dynamic source and resonant modes. The aim is to increase the resolution of the time-instantaneous-frequency representation of each of the individual contributions of different sections of the human phonatory system. We present a framework based on dynamic mode predictors and filters, which are adapted, using gradient-based techniques, to track the modal dynamics of speech yielding a representation which is free from quasi-stationary assumptions thus allowing flexible manipulation of the speech signal. Several examples are offered including intonation modifications to illustrate the potential of the proposed approach.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0697,Prosodic Realization of Rhetorical Structure in Chinese Discourse,"The research reported in this paper is an acoustic experiment attempting to elucidate the relationship between prosodic variation and rhetorical structure in discourse. Based on Rhetorical Structure Theory, facets of discourse structure such as hierarchy, relation, and the relative importance of discourse segment were identified. Five speakers of standard Chinese were recorded reading ten paragraphs with two repetitions. Boundary pause duration, f0 max, f0 min, and pitch range of the segments were measured. It was found that speakers realized longer pauses at boundaries of higher hierarchy. Furthermore, compared with segments linked by nucleus-satellite relation, segments linked by multinuclear relation were found to have wider pre-boundary pitch range. Additionally, important segments were found to be articulated with wider pitch range than unimportant segments. These results suggest that rhetorical structure is reliably conveyed by prosodic parameters in standard Chinese.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0698,Symbolic Modeling of Prosody: From Linguistics to Statistics,"The assignment of prosodic events (accent and phrasing) from the text is crucial in text-to-speech synthesis systems. This paper addresses the combination of linguistic and metric constraints for the assignment of prosodic events in text-to-speech synthesis. First, a linguistic processing chain is used to provide a rich linguistic description of a text. Then, a novel statistical representation based on a hierarchical HMM (HHMM) is used to model the prosodic structure of a text: the root layer represents the text, each intermediate layer a sequence of intermediate phrases, the pre-terminal layer the sequence of accents, and the terminal layer the sequence of linguistic contexts. For each intermediate layer, a segmental HMM and information fusion are used to fuse the linguistic and metric constraints for the segmentation of a text into phrases. A set of experiments conducted on multi-speaker databases with various speaking styles reports that: the rich linguistic representation improves drastically the assignment of prosodic events, and the fusion of linguistic and metric constraints significantly improves over standard methods for the segmentation of a text into phrases. These constitute substantial advances that can be further used to model the speech prosody of a speaker, a speaking style, and emotions for text-to-speech synthesis.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0699,"Score-Informed Analysis of Tuning, Intonation, Pitch Modulation, and Dynamics in Jazz Solos","Both the collection and analysis of large music repertoires constitute major challenges within musicological disciplines such as jazz research. Automatic methods of music analysis based on audio signal processing have the potential to assist researchers and to accelerate the transcription and analysis of music recordings significantly. In this paper, we propose a framework for analyzing improvised monophonic solos in multi-instrumental jazz recordings with special focus on reed and brass instruments. The analysis algorithms rely on prior score-information, which is taken from high quality manual solo transcriptions. Following an initial solo and accompaniment source separation, we propose algorithms for tone-wise extraction of fundamental frequency and intensity contours. Based on this fine-grained representation of recorded jazz solos, we perform several exploratory experiments motivated by questions relating to jazz research in order to analyze the use of expressive stylistic devices such as intonation, pitch modulation, and dynamics in jazz solos. The results show that a score-informed audio analysis of jazz recordings can provide valuable insights into the individual stylistic characteristics of jazz musicians.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0700,Group Sparse Representation With WaveNet Vocoder Adaptation for Spectrum and Prosody Conversion,"The statistical approach to voice conversion typically consists of a feature conversion module followed by a vocoder. So far, the feature conversion studies are mainly focused on the conversion of spectrum. However, speaker identity is also characterized by prosodic features, such as fundamental frequency (F0) and energy contour among others. In this paper, we study the transformation of speaker characteristics both in terms of spectrum and prosody. We propose two novel techniques that effectively use a limited amount of source-target training data and leverage a large general speech corpus to improve the voice conversion quality. First, we study the phonetic sparse representation under the group sparsity mathematical formulation. We use phonetic posteriorgrams (PPGs) together with spectral and prosody features to form tandem feature in the phonetic dictionary. The tandem feature allow us to estimate an activation matrix that is less dependent on source speakers, thus providing a better voice conversion quality. Second, we study the use of WaveNet vocoder that can be trained on general speech corpus from multiple speakers and adapted on target speaker data to improve the vocoding quality. We benefit from the large general speech databases that are used to train the PPG generator, and the WaveNet vocoder. The experiments show that the proposed conversion framework outperforms the traditional spectrum and prosody conversion techniques in both objective and subjective evaluations.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0701,Emotional Voice Conversion Using Dual Supervised Adversarial Networks With Continuous Wavelet Transform F0 Features,"In emotional voice conversion (VC) tasks, it is difficult to deal with a simple representation of fundamental frequency (F0), which is the most important feature in emotional voice representation. In order to address this issue, we propose the adaptive scales continuous wavelet transform (ADS-CWT) method to systematically capture F0 features of different temporal levels, which can represent different prosodic aspects, ranging from micro-prosody to sentences. Moreover, in an emotional VC task, each dataset is paired with the labeled emotional voice and neutral voice, which can be regarded as a dual task. Owing to, first, dual supervised learning's ability to improve the training performances by using the leveraging probabilistic connection between the dual tasks to enhance the learning from labeled data and, second, generative adversarial networks' (GANs') ability to mitigate the over-smoothing problem caused in the low-level data space when converting the acoustic features, we further present a novel training framework for emotional VC using GANs combined with dual supervised learning, named as dual supervised adversarial networks. In emotional VC experiments, we confirmed the high similarity performance of our method when using limited labeled data for emotional VC. Our method achieves good and consistent performance, in both objective and subjective evaluations.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0702,An Overview of Voice Conversion and Its Challenges: From Statistical Modeling to Deep Learning,"Speaker identity is one of the important characteristics of human speech. In voice conversion, we change the speaker identity from one to another, while keeping the linguistic content unchanged. Voice conversion involves multiple speech processing techniques, such as speech analysis, spectral conversion, prosody conversion, speaker characterization, and vocoding. With the recent advances in theory and practice, we are now able to produce human-like voice quality with high speaker similarity. In this article, we provide a comprehensive overview of the state-of-the-art of voice conversion techniques and their performance evaluation methods from the statistical approaches to deep learning, and discuss their promise and limitations. We will also report the recent Voice Conversion Challenges (VCC), the performance of the current state of technology, and provide a summary of the available resources for voice conversion research.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0703,Automatic Detection of Affective Flattening in Schizophrenia: Acoustic Correlates to Sound Waves and Auditory Perception,"Affective flattening is a typical negative symptom in schizophrenia that causes a diminution of normal behaviors and functions in schizophrenic patients. In this work, an automatic algorithm of schizophrenia detection is proposed. This algorithm includes three newly proposed features. These features establish a representation for the production and perception of schizophrenic speech, called the K-Sf (kurtosis skewness fraction), SPI (sound perception indicator), and EBMC (enhanced bilateral matching coefficient). The K-Sf is proposed to reflect the overall distribution of speech segments. The SPI evaluates the relation between the speech composition and the sound perception. The EBMC feature is proposed based on the theory of air pressure oscillations which could reflect the details of air modulation in the vocal tract. Experiments evaluating the discriminative capabilities of the three features are conducted using a speech dataset that is collected from 56 participants (28 schizophrenic patients and 28 healthy controls) and an ensemble classifier. Comparative experiments with SVM classifiers are also conducted. The discrimination accuracies of patients and control subjects using the K-Sf, SPI, EBMC, and the ensemble classifier are in the range of 76.8-92.9%, 82.1-92.8%, and 80.4-91.1%, respectively. When the three features are combined, the discrimination results range from 89.3% to 94.6%. The experimental results indicate that the three features have stronger robustness and better discrimination capability than those previous features relating to the detection of flat affect.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0704,Decoupling Speaker-Independent Emotions for Voice Conversion via Source-Filter Networks,"Emotional voice conversion (VC) aims to convert a neutral voice to an emotional one while retaining the linguistic information and speaker identity. We note that the decoupling of emotional features from other speech information (such as content, speaker identity, etc.) is the key to achieving promising performance. Some recent attempts of speech representation decoupling on the neutral speech cannot work well on the emotional speech, due to the more complex entanglement of acoustic properties in the latter. To address this problem, here we propose a novel Source-Filter-based Emotional VC model (SFEVC) to achieve proper filtering of speaker-independent emotion cues from both the timbre and pitch features. Our SFEVC model consists of multi-channel encoders, emotion separate encoders, pre-trained speaker-dependent encoders, and the corresponding decoder. Note that all encoder modules adopt a designed information bottleneck auto-encoder. Additionally, to further improve the conversion quality for various emotions, a novel training strategy based on the 2D Valence-Arousal (VA) space is proposed. Experimental results show that the proposed SFEVC along with a VA training strategy outperforms all baselines and achieves the state-of-the-art performance in speaker-independent emotional VC with nonparallel data.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0705,Deep Learning Approaches in Topics of Singing Information Processing,"Singing, the vocal productionof musical tones, is one of the most important elements of music. Addressing the needs of real-world applications, the study of technologies related to singing voices has become an increasingly active area of research. In this paper, we provide a comprehensive overview of the recent developments in the field of singing information processing, specifically in the topics of singing skill evaluation, singing voice synthesis, singing voice separation, and lyrics synchronization and transcription. We will especially focus on deep learning approaches including modern representation learning techniques for singing voices. We will also provide an overview of contributions in public datasets for singing voice research.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0706,Prosody Modelling With Pre-Trained Cross-Utterance Representations for Improved Speech Synthesis,"When humans speak multiple utterances in a continuous manner, the prosodic features generated in each utterance are related to those in its neighbouring utterances. Such cross-utterance (CU) dependencies are often ignored by the current neural text-to-speech (TTS) systems, which reduces the naturalness and expressiveness of the synthesized speeches. In this article, we propose to improve the prosody modelling ability of neural TTS systems using pre-trained CU acoustic and text representations. Such CU acoustic representations are derived using the Wav2Vec 2.0 model (W2V2) from the synthesized audios of the past utterances, while the CU text representations are extracted using the Bidirectional Encoder Representation from Transformers (BERT) model from the scripts of the future utterances. Experimental results on a Mandarin audiobook and an English audiobook showed the naturalness and expressiveness of the synthesized audios were significantly improved by incorporating such pre-trained W2V2 and BERT CU representations into the Fastspeech2 TTS framework.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0707,AudioLM: A Language Modeling Approach to Audio Generation,"We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0708,TEAR: A Cross-Modal Pre-Trained Text Encoder Enhanced by Acoustic Representations for Speech Synthesis,"Text encoders play an important role in text-to-speech (TTS) by analyzing text input and converting it into linguistic representations. In order to generate expressive speech from text, pre-training text encoders on large amounts of data has recently become a solution to generate richer and more effective linguistic representations. However, existing pre-trained text encoders only use the self-supervised target on the text data, without considering the relationship between text and speech modalities during the pre-training stage. In this paper, we propose TEAR, a cross-modal pre-trained Text Encoder enhanced by Acoustic Representations for TTS. In addition to conventional text pre-training, TEAR incorporates speech pre-training to extract semantic and prosody-related acoustic representations from speech. Then, TEAR introduces a novel cross-modal pre-training task for the text encoder, termed acoustics-aware joint prediction. This task leverages the acoustic representations generated by the preceding speech pre-training, enabling the linguistic representation to perceive and comprehend prosody during the encoding process. In our implementation, TEAR was pre-trained on 130 million unlabeled Chinese and English sentences, as well as 740,000 Chinese text-speech pairs. The results of the downstream TTS experiments on three expressive TTS datasets indicate that the proposed TEAR can encode more effective and comprehensive linguistic representations compared to the text-only pre-trained encoders, leading to the generation of more natural speech.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0709,CrossSpeech plus plus : Cross-Lingual Speech Synthesis With Decoupled Language and Speaker Generation,"The goal of this work is to generate natural speech in multiple languages while maintaining the same speaker identity, a task known as cross-lingual speech synthesis. A key challenge of cross-lingual speech synthesis is the language-speaker entanglement problem, which causes the quality of cross-lingual systems to lag behind that of intra-lingual systems. In this paper, we propose CrossSpeech++, which effectively disentangles language and speaker information and significantly improves the quality of cross-lingual speech synthesis. To this end, we break the complex speech generation pipeline into two simple components: language-dependent and speaker-dependent generators. The language-dependent generator produces linguistic variations that are not biased by specific speaker attributes. The speaker-dependent generator models acoustic variations that characterize speaker identity. By handling each type of information in separate modules, our method can effectively disentangle language and speaker representation. We conduct extensive experiments using various metrics, and demonstrate that CrossSpeech++ achieves significant improvements in cross-lingual speech synthesis, outperforming existing methods by a large margin.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0710,Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction,"We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0711,FluentEditor2: Text-Based Speech Editing by Modeling Multi-Scale Acoustic and Prosody Consistency,"Text-based speech editing (TSE) allows users to edit speech by modifying the corresponding text directly without altering the original recording. Current TSE techniques often focus on minimizing discrepancies between generated speech and reference within edited regions during training to achieve fluent TSE performance. However, the generated speech in the edited region should maintain acoustic and prosodic consistency with the unedited region and the original speech at both the local and global levels. To maintain speech fluency, we propose a new fluency speech editing scheme based on our previous FluentEditor model, termed FluentEditor2, by modeling the multi-scale acoustic and prosody consistency training criterion in TSE training. Specifically, for local acoustic consistency, we propose hierarchical local acoustic smoothness constraint to align the acoustic properties of speech frames, phonemes, and words at the boundary between the generated speech in the edited region and the speech in the unedited region. For global prosody consistency, we propose contrastive global prosody consistency constraint to keep the speech in the edited region consistent with the prosody of the original utterance. Extensive experiments on the VCTK and LibriTTS datasets show that FluentEditor2 surpasses existing neural networks-based TSE methods, including Editspeech, Campnet, A3T, FluentSpeech, and our Fluenteditor, in both subjective and objective. Ablation studies further highlight the contributions of each module to the overall effectiveness of the system.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0712,Recognizing Tonal and Nontonal Mandarin Sentences for EEG-Based Brain-Computer Interface,"Most current research has focused on nontonal languages such as English. However, more than 60% of the world's population speaks tonal languages. Mandarin is the most spoken tonal languages in the world. Interestingly, the use of tone in tonal languages may represent different meanings of words and reflect feelings, which is very different from nontonal languages. The objective of this study is to determine whether a spoken Mandarin sentence with or without tone can be distinguished by analyzing electroencephalographic (EEG) signals. We first constructed a new Brain Research Center Speech (BRCSpeech) database to recognize Mandarin. The EEG data of 14 participants were recorded, while they articulated preselected sentences. To the best of our knowledge, this is the first study to apply the method of asymmetric feature extraction method for speech recognition using EEG signals. This study shows that the feature extraction method of rational asymmetry (RASM) can achieve the best accuracy in the classification of cross-subjects. In addition, our proposed binomial variable algorithm methodology can achieve 98.82% accuracy in cross-subject classification. Furthermore, we demonstrate that the use of eight channels [(F7, F8), (C5, C6), (P5, P6), and (O1, O2)] can achieve an accurate of 94.44%. This study explores the neurophysiological correlation of Mandarin pronunciation, which can help develop a tonal language synthesis system based on BCI in the future.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0713,A Social Condition-Enhanced Network for Recognizing Power Distance Using Expressive Prosody and Intrinsic Brain Connectivity,"Culture is the social norm that often dictates a person's thoughts, decision-making, and social behaviors during interaction at an individual level. In this study, we present a computational framework that automatically assesses an individual culture attribute of power distance (PDI), i.e., the measure to describe one's acceptance of social status, power and authority in organizations through multimodal modeling of a participant's expressive prosodic structures and brain connectivity using a social condition-enhanced network. In specific, we propose a joint learning approach of center-loss embedding network architecture that learns to ""centerize"" the embedding space given a particular social interaction condition to enhance the PDI discriminability of the representation. Our proposed method achieves 88.5% and 73.1% in binary classification task of recognizing low versus high power distance on prosodic and fMRI modality separately. After performing multimodal fusion, it improves to 96.2% of 2-class recognition rate (7.7% relative improvement). Further analyses reveal that average and standard deviation of speech energy are significantly correlated with power distance index; the right middle cingulate cortex (MCC) of brain region achieves the best recognition accuracy demonstrating its role in processing a person's belief about power distance.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0714,Disentanglement of Prosody Representations via Diffusion Models and Scheduled Gradient Reversal,"Prosody plays a fundamental role in human speech and communication, facilitating intelligibility and conveying emotional and cognitive states. Extracting accurate prosodic information from speech is vital for building assistive technology, such as controllable speech synthesis, speaking style transfer, and speech emotion recognition (SER). However, it is challenging to disentangle speaker-independent prosody representations since prosodic attributes, such as intonation, excessively entangle with speaker-specific attributes, e.g., pitch. In this article, we propose a novel model, called Diffsody, to disentangle and refine prosody representations: 1) to disentangle prosody representations, we leverage the expressive generative ability of a diffusion model by conditioning it on quantified semantic information and pretrained speaker embeddings. Additionally, a prosody encoder automatically learns prosody representations used for spectrogram reconstruction in an unsupervised fashion; and 2) to refine and learn speaker-invariant prosody representations, a scheduled gradient reversal layer (sGRL) is proposed and integrated into the prosody encoder of Diffsody. We extensively evaluate Diffsody through qualitative and quantitative means. t-SNE visualization and speaker verification experiments demonstrate the efficacy of the sGRL method in preventing speaker-specific information leakage. Experimental results on speaker-independent SER and automatic depression detection (ADD) tasks demonstrate that Diffsody can efficiently factorize speaker-independent prosody representations, resulting in a significant boost in SER and ADD. In addition, Diffsody synergistically integrates with the semantic representation model WavLM, which leads to a discernibly elevated performance, outperforming contemporary methods in both SER and ADD tasks. Furthermore, the Diffsody model exhibits promising potential for various practical applications, such as voice or style conversion. Some audio samples can be found on our https://leyuanqu.github.io/Diffsody/demo website.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0715,HierSpeech plus plus : Bridging the Gap Between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-Shot Speech Synthesis,"Large language model (LLM)-based speech synthesis has been widely adopted in zero-shot speech synthesis. However, they require a large-scale data and possess the same limitations as previous autoregressive speech models, including slow inference speed and lack of robustness. This article proposes HierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech (TTS) and voice conversion (VC). We verified that hierarchical speech synthesis frameworks could significantly improve the robustness and expressiveness of the synthetic speech. Furthermore, we significantly improve the naturalness and speaker similarity of synthetic speech even in zero-shot speech synthesis scenarios. For TTS, we adopt the text-to-vec (TTV) framework, which generates a self-supervised speech representation and an F0 representation based on text representations and prosody prompts. Then, HierSpeech++ generates speech from the generated vector, F0, and voice prompt. We further introduce a high-efficient speech super-resolution (SpeechSR) framework from 16 to 48 kHz. The experimental results demonstrated that the hierarchical variational autoencoder could be a strong zero-shot speech synthesizer given that it outperforms LLM-based and diffusion-based models. Moreover, we achieved the first human-level quality zero-shot speech synthesis. Audio samples and source code are available at https://github.com/hierspeechpp/code",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0716,Loudness Contour Can Influence Mandarin Tone Recognition: Vocoder Simulation and Cochlear Implants,"Lexical tone recognition with current cochlear implants (CI) remains unsatisfactory due to significantly degraded pitch-related acoustic cues, which dominate the tone recognition by normal-hearing (NH) listeners. Several secondary cues (e.g., amplitude contour, duration, and spectral envelope) that influence tone recognition in NH listeners and CI users have been studied. This work proposes a loudness contour manipulation algorithm, namely Loudness-Tone (L-Tone), to investigate the effects of loudness contour on Mandarin tone recognition and the effectiveness of using loudness cue to enhance tone recognition for CI users. With L-Tone, the intensity of sound samples is multiplied by gain values determined by instantaneous fundamental frequencies (F0s) and pre-defined gain-F0 mapping functions. Perceptual experiments were conducted with a four-channel noise-band vocoder simulation in NH listeners and with CI users. The results suggested that 1) loudness contour is a useful secondary cue for Mandarin tone recognition, especially when pitch cues are significantly degraded; 2) L-Tone can be used to improve Mandarin tone recognition in both simulated and actual CI-hearing without significant negative effect on vowel and consonant recognition. L-Tone is a promising algorithm for incorporation into real-time CI processing and off-line CI rehabilitation training software.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0717,Automated Dysarthria Severity Classification: A Study on Acoustic Features and Deep Learning Techniques,"Assessing the severity level of dysarthria can provide an insight into the patient's improvement, assist pathologists to plan therapy, and aid automatic dysarthric speech recognition systems. In this article, we present a comparative study on the classification of dysarthria severity levels using different deep learning techniques and acoustic features. First, we evaluate the basic architectural choices such as deep neural network (DNN), convolutional neural network, gated recurrent units and long short-term memory network using the basic speech features, namely, Mel-frequency cepstral coefficients (MFCCs) and constant-Q cepstral coefficients. Next, speech-disorder specific features computed from prosody, articulation, phonation and glottal functioning are evaluated on DNN models. Finally, we explore the utility of low-dimensional feature representation using subspace modeling to give i-vectors, which are then classified using DNN models. Evaluation is done using the standard UA-Speech and TORGO databases. By giving an accuracy of 93.97% under the speaker-dependent scenario and 49.22% under the speaker-independent scenario for the UA-Speech database, the DNN classifier using MFCC-based i-vectors outperforms other systems.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0718,Successful lexical tone production of Mandarin Chinese autistic children with intellectual impairment,"BackgroundAtypical speech prosody has been commonly found among autistic children. Yet it remains unknown whether prosody impairment originates from poor pitch ability in general or whether it is the result of the difficulty in understanding and using prosody for communicative purposes. AimsTo investigate whether native Mandarin Chinese-speaking autistic children with intellectual impairment were able to accurately produce native lexical tones, which are pitch patterns that distinguish word meaning lexically and serve little social purpose. Methods & ProceduresUsing a picture-naming task, thirteen 8-13-year-old Mandarin Chinese-speaking autistic children with intellectual impairment were tested on their production of Chinese lexical tones. Chronical age-matched typically developing (TD) children were included as the control group. Perceptual assessment and phonetic analyses were conducted with the produced lexical tones. Outcomes & ResultsThe majority of the lexical tones produced by the autistic children were perceived as accurate by adult judges. Phonetic analysis of the pitch contours found no significant difference between the two groups, and the autistic children and TD children used the phonetic features in comparable ways when differentiating the lexical tones. However, the lexical tone accuracy rate was lower among the autistic children than among the TDs, and the larger individual difference was observed among the autistic children than the TD children. Conclusions & ImplicationsThese results indicate that autistic children are able to produce the global contours of the lexical tones, and pitch deficits do not seem to qualify as a core feature of autism. What this paper addsWhat is already known on the subjectAtypical prosody has been considered a maker of the speech of autistic children, and meta-analysis found a significant difference in mean pitch and pitch range between TD children and autistic children. Yet it remains unknown whether the pitch deficits are the result of impaired perceptual-motoric ability or if they reflect failure in learning sentential prosody, which requires an understanding of the interlocutors' mind. In addition, research on pitch ability of autistic children with intellectual disabilities has been scarce, and whether these children are able to produce pitch variation is largely unknown. What this paper adds to existing knowledgeWe tested native Mandarin Chinese autistic children with intellectual impairment on their production of native lexical tones. The lexical tones in Chinese are pitch variations realized on individual syllables that distinguish lexical meaning, but they do not serve social pragmatic purposes. We found that although these autistic children had only developed limited spoken language, the majority of their lexical tones were perceived as accurate. They were able to use the phonetic features in comparable ways with the TD children when distinguishing the lexical tones. What are the potential or actual clinical implications of this work?It seems unlikely that pitch processing at the lexical level is fundamentally impaired in autistic children, and pitch deficits do not seem to qualify for a core feature of their speech. Practitioners should be cautious when using pitch production as a clinical marker for autistic children.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0719,Exploring word-referent mapping in Mandarin-speaking late-talkers at 33 months and its language predictors at 27 months: An eye-tracking study,"Background and Aims This longitudinal study investigated the language skills, phonological working memory and lexical-tone perception of Mandarin-speaking late-talkers (LTs) and those with typical language development (TLD) at 27 months, while also examining their connections with novel word-referent mapping (W-R mapping) through eye-tracking at 33 months. Methods and Procedures Participants included 22 Mandarin-speaking 27-month-old LTs and 22 toddlers with TLD. Data on expressive and receptive language abilities, as well as phonological working memory and lexical-tone perception, were collected when participants were 27 months old. An eye-tracking paradigm was further employed during the word-learning tasks, which included W-R mapping and word-identification test (W-I test) phases at 33 months. Multilevel models were used to analyse participants' gaze pattern trajectories. Outcomes and Results At 27 months, LT toddlers exhibited poorer language skills (receptive: p = 0.015, expressive: p < 0.001), lexical-tone perception (p < 0.001) and phonological working memory (p < 0.001) compared to those with TLD, even after considering maternal educational level and participants' fine motor ability. During the W-I test phase, we observed that LT toddlers showed a slower increase in fixations on the novel target image while listening to the corresponding novel word over time, compared to TLD toddlers (linear: p = 0.011, quadratic: p = 0.007) after adding confounders. Further, expressive language ability at 27 months old was a predictor of their newly established W-R mappings at 33 months old (p = 0.016). Additionally, the toddler's phonological working memory and lexical-tone perception were associated with their expressive language ability (p = 0.001 and < 0.001). Conclusions and Implications These findings indicate that the novel W-R mapping is not as robust in LTs as in TLDs, and the skills necessary for word learning share similarities with a wide range of expressive language abilities. Moreover, poor expressive language abilities were associated with deficits in lexical processing abilities; that is, phonological working memory and lexical-tone perception. These findings suggest the need for interventions aimed at improving LTs' lexical processing abilities to strengthen their lagging word-learning skills at toddlerhood.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0720,Double dissociations in reading comprehension difficulties among Chinese-English bilinguals and their association with tone awareness,"Poor comprehenders have reading comprehension difficulties but normal word recognition ability. Here, we report the first study, which investigated (i) the dissociation and (ii) the prevalence of L1-L2 reading comprehension difficulties, and (iii) the levels of key metalinguistic skills in poor comprehenders among Chinese-English bilingual children. From a sample of 124 Chinese-English second graders, we identified 18 poor comprehenders (six Chinese, six English, and six in both Chinese and English). We matched these with six average comprehenders of comparable age and word reading abilities. Multivariate analysis of covariance and univariate F tests revealed that poor Chinese comprehenders and poor English comprehenders had significantly lower levels of Chinese lexical tone awareness than average readers even after controlling for nonverbal intelligence. No significant differences emerged on scores for segmental phonological awareness and vocabulary knowledge, either in Chinese and English or on English lexical stress sensitivity. These findings were discussed in relation to the universal view of reading, cross language prosodic transfer and the simple view of reading.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0721,Cross-linguistic contributions of acoustic cues and prosodic awareness to first and second language vocabulary knowledge,"Background Several studies have revealed that prosody contributes to reading acquisition. However, the relation between awareness of prosodic patterns and different facets of language ability (e.g., vocabulary knowledge) in school-age children remains unclear. This study measured awareness of prosodic patterns using non-speech and speech stimuli. Methods Hierarchical regression equations were computed to examine links among auditory cues (e.g., amplitude rise time, pitch contour and interval), language-specific prosodic awareness and children's vocabulary knowledge in Mandarin as a first language (L1) and English as a second language (L2) after controlling for age and nonverbal IQ. Results Results revealed that (1) amplitude envelope rise time discrimination predicted Mandarin L1 and English L2 vocabulary knowledge, (2) Mandarin tone perception and rhyme awareness did not predict Mandarin L1 vocabulary and (3) English rhyme awareness better predicted English L2 vocabulary than did stress production. Conclusion Our findings suggest that (1) amplitude rise time, which signals syllable boundaries, is a cross-linguistic predictor of vocabulary knowledge and (2) the development of English L2 vocabulary may depend on phonological more than prosodic awareness.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0722,The music of speech: Music training facilitates pitch processing in both music and language,"The main aim of the present experiment was to determine whether extensive musical training facilitates pitch contour processing not only in music but also in language. We used a parametric manipulation of final notes or words' fundamental frequency (F0), and we recorded behavioral and electrophysiological data to examine the precise time Course of pitch processing. We compared professional musicians and nonmusicians. Results revealed that within both domains, musicians detected weak F0 manipulations better than nonmusicians. Moreover, F0 manipulations within both music and language elicited similar variations in brain electrical potentials, with overall shorter onset latency for musicians than for nonmusicians. Finally, the scalp distribution of an early negativity in the linguistic task varied with musical expertise, being largest over temporal sites bilaterally for musicians and largest centrally and over left temporal sites for nonmusicians. These results are taken as evidence that extensive musical training influences the perception of pitch contour in spoken language.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0723,An ERP correlate of metrical stress in spoken word recognition,"Rhythmic properties of spoken language such as metrical stress, that is, the alternation of strong and weak syllables, are important in speech recognition of stress-timed languages such as Dutch and English. Nineteen subjects Listened passively to or discriminated actively between sequences of bisyllabic Dutch words, which started with either a weak or a strong syllable. Weak-initial words, which constitute 12% of the Dutch lexicon, evoked more negativity than strong-initial words in the interval between P2 and N400 components of the auditory event-related potential. This negativity was denoted as N325. The N325 was larger during stress discrimination than during passive Listening. N325 was also larger when a weak-initial word followed a sequence of strong-initial words than when it followed words with the same stress pattern. The latter difference was larger for listeners who performed well on stress discrimination. It was concluded that the N325 is probably a manifestation of the extraction of metrical stress from the acoustic Signal and its transformation into task requirements.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0724,Insights into reading processes through investigating diversity,"The general goal of reading is to obtain meaning from what is written on the page or screen. The numerous scripts around the world used to write different languages vary in terms of what aspects of language are encoded in the written form. The aim of the current review is to examine some recent research on lesser-studied orthographies, in particular Thai, to illustrate the benefits of comparative studies in building a greater understanding of what processes are common or distinct when reading diverse writing systems. Three areas of reading are focused on where there is substantial variation across orthographies: (1) reading with and without interword spaces, (2) flexibility in letter position coding and initial letter position advantage, and (3) the role of lexical tone when reading. In order to effectively read a script, readers need to attend to the critical features of the script that interface with the particular language of the speaker. For example, in scripts with interword spaces, these salient visual cues form clear word boundaries, whereas in unspaced scripts other orthography-specific cues need to be identified and utilised. Furthermore, comparative research shows that letter position encoding varies across languages, which is shaped by the characteristics of the orthography. Finally, research on Thai and Chinese indicates that tone takes a secondary role in comparison to segmental information (consonantal and vowel information) and appears to be processed at a later stage.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0725,From Flexibility to Constraint: The Contrastive Use of Lexical Tone in Early Word Learning,"Infants must develop both flexibility and constraint in their interpretation of acceptable word forms. The current experiments examined the development of infants' lexical interpretation of non-native variations in pitch contour. Fourteen-, 17-, and 19-month-olds (Experiments 1 and 2, N=72) heard labels for two novel objects; labels contained the same syllable produced with distinct pitch contours (Mandarin lexical tones). The youngest infants learned the label-object mappings, but the older groups did not, despite being able to discriminate pitch differences in an object-free task (Experiment 3, N=14). Results indicate that 14-month-olds remain flexible regarding what sounds make meaningful distinctions between words. By 17-19months, experience with a nontonal native language constrains infants' interpretation of lexical tone.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0726,Flexibility in Bilingual Infants' Word Learning,"The present experiments tested bilingual infants' developmental narrowing for the interpretation of sounds that form words. These studies addressed how language specialization proceeds when the environment provides varied and divergent input. Experiment 1 (N=32) demonstrated that bilingual 14- and 19-month-olds learned a pair of object labels consisting of the same syllable produced with distinct pitch contours (rising and falling). Infants' native languages did not use pitch contour to differentiate words. In Experiment 2 (N=16), 22-month-old bilinguals failed to learn the labels. These results conflict with the developmental trajectory of monolinguals, who fail to learn pitch contour contrasts as labels at 17-19months (Hay, Graf Estes, Wang, & Saffran, 2015). Bilingual infants exhibited a prolonged period of flexibility in their interpretation of potential word forms.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0727,A New View of Language Development: The Acquisition of Lexical Tone,"Research in first language development draws disproportionately from nontone languages. Such research is often presumed to reveal developmental universals in spite of the fact that most languages are tone languages. Recent research in the acquisition of tone languages points to a distinct course of development as compared to nontone languages. Our purpose is to provide an integrated review of research on lexical tone acquisition. First, the linguistic properties and origins of tone languages are described. Following this, research on the acquisition of tones in perception and production is reviewed and integrated. Possible reasons for the uniqueness of tone in language acquisition are discussed. Finally, theoretical advances promised by further research on tone acquisition and specific research directions are proposed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0728,Development of categorical speech perception in Mandarin-speaking children and adolescents,"Although children develop categorical speech perception at a very young age, the maturation process remains unclear. A cross-sectional study in Mandarin-speaking 4-, 6-, and 10-year-old children, 14-year-old adolescents, and adults (n = 104, 56 males, all Asians from mainland China) was conducted to investigate the development of categorical perception of four Mandarin phonemic contrasts: lexical tone contrast Tone 1-2, vowel contrast /u/-/i/, consonant aspiration contrast /p/-/p(h)/, and consonant formant transition contrast /p/-/t/. The results indicated that different types of phonemic contrasts, and even the identification and discrimination of the same phonemic contrast, matured asynchronously. The observation that tone and vowel perception are achieved earlier than consonant perception supports the phonological saliency hypothesis.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0729,Implicit Association Test (IAT) Studies Investigating Pitch-Shape Audiovisual Cross-modal Associations Across Language Groups,"Previous studies have shown that Chinese speakers and non-Chinese speakers exhibit different patterns of cross-modal congruence for the lexical tones of Mandarin Chinese, depending on which features of the pitch they attend to. But is this pattern of language-specific listening a conscious cultural strategy or an automatic processing effect? If automatic, does it also apply when the same pitch contours no longer sound like speech? Implicit Association Tests (IATs) provide an indirect measure of cross-modal association. In a series of IAT studies, conducted with participants with three kinds of language backgrounds (Chinese-dominant bilinguals, Chinese balanced bilinguals, and English speakers with no Chinese experience) we find language-specific congruence effects for Mandarin lexical tones but not for matched sine-wave stimuli. That is, for linguistic stimuli, non-Chinese speakers show advantages for pitch-height congruence (high-pointy, low-curvy); no congruence effects were found for Chinese speakers. For non-linguistic stimuli, all participant groups showed advantages for pitch-height congruence. The present findings suggest that non-lexical tone congruence (high-pointy, low-curvy) is a basic congruence pattern, and the acquisition of a language with lexical tone can alter this perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0730,The Importance of Linguistic Factors: He Likes Subject Referents,"We report the results of one visual-world eye-tracking experiment and two referent selection tasks in which we investigated the effects of information structure in the form of prosody and word order manipulation on the processing of subject pronouns er and der in German. Factors such as subject-hood, focus, and topicality, as well as order of mention have been linked to an increased probability of certain referents being selected as the pronoun's antecedent and described as increasing this referent's prominence, salience, or accessibility. The goal of this study was to find out whether pronoun processing is primarily guided by linguistic factors (e.g., grammatical role) or nonlinguistic factors (e.g., first-mention), and whether pronoun interpretation can be described in terms of referents' ""prominence"" / ""accessibility"" / ""salience."" The results showed an overall subject preference for er, whereas der was affected by the object role and focus marking. While focus increases the attentional load and enhances memory representation for the focused referent making the focused referent more available, ultimately it did not affect the final interpretation of er, suggesting that ""prominence"" or the related concepts do not explain referent selection preferences. Overall, the results suggest a primacy of linguistic factors in determining pronoun resolution.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0731,Influences of vowel and tone variation on emergent word knowledge: a cross-linguistic investigation,"To learn words, infants must be sensitive to native phonological contrast. While lexical tone predominates as a source of phonemic contrast in human languages, there has been little investigation of the influences of lexical tone on word learning. The present study investigates infants' sensitivity to tone mispronunciations in two groups of infants. For one group (Chinese learners), tone is phonemic in their native language, and for the second group (English learners), tone is non-phonemic and constituted suprasegmental variation. In Experiment 1, English learners were trained on novel word-object pairings and tested on their recognition of correct pronunciations, tone and vowel mispronunciations of these words at 18 and 24months. In Experiment 2a, bilingual English-Chinese learners were tested on a similar task translated into Chinese at the same age intervals. Results demonstrate that non-tonal learners treated tonal and vowel substitutions alike as mispronunciations at 18months but only treated vowel substitutions as mispronunciations at 24months. Tonal learners treated both tonal and vowel substitutions as mispronunciations at both ages. In Experiment 2b, bilingual non-tone language learners were tested on the same set of tasks replicating a similar set of results as monolingual non-tone language learners (Experiment 1). Findings point to an early predisposition to treat tone as a defining characteristic of words regardless of its lexical relevance at 18months. Between 18 and 24months, learners appear to ascribe lexical relevance to tone in a language-specific manner. The current study identifies the influences of tone variation on memories for newly learned words and the time period during which lexical tone - a highly frequent constituent of human languages - actually becomes lexical for early learners. Findings are contextualized with prevailing models of the developing lexicon.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0732,Tracking independence and merging of prosodic and phonemic processing across infancy,"Recent evidence suggests division of labor in phonological analysis underlying speech recognition. Adults and children appear to decompose the speech stream into phoneme-relevant information and into syllable stress. Here we investigate whether both speech processing streams develop from a common path in infancy, or whether there are two separate streams from early on. We presented stressed and unstressed syllables (spoken primes) followed by initially stressed early learned disyllabic German words (spoken targets). Stress overlap and phoneme overlap between the primes and the initial syllable of the targets varied orthogonally. We tested infants 3, 6 and 9 months after birth. Event-related potentials (ERPs) revealed stress priming without phoneme priming in the 3-month-olds; phoneme priming without stress priming in the 6-month-olds; and phoneme priming, stress priming as well as an interaction of both in 9-month-olds. In general the present findings reveal that infants start with separate processing streams related to syllable stress and to phoneme-relevant information; and that they need to learn to merge both aspects of speech processing. In particular the present results suggest (i) that phoneme-free prosodic processing dominates in early infancy; (ii) that prosody-free phoneme processing dominates in middle infancy; and (iii) that both types of processing are operating in parallel and can be merged in late infancy.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0733,"The development of tone discrimination in infancy: Evidence from a cross-linguistic, multi-lab report","We report the findings of a multi-language and multi-lab investigation of young infants' ability to discriminate lexical tones as a function of their native language, age and language experience, as well as of tone properties. Given the high prevalence of lexical tones across human languages, understanding lexical tone acquisition is fundamental for comprehensive theories of language learning. While there are some similarities between the developmental course of lexical tone perception and that of vowels and consonants, findings for lexical tones tend to vary greatly across different laboratories. To reconcile these differences and to assess the developmental trajectory of native and non-native perception of tone contrasts, this study employed a single experimental paradigm with the same two pairs of Cantonese tone contrasts (perceptually similar vs. distinct) across 13 laboratories in Asia-Pacific, Europe and North-America testing 5-, 10- and 17-month-old monolingual (tone, pitch-accent, non-tone) and bilingual (tone/non-tone, non-tone/non-tone) infants. Across the age range and language backgrounds, infants who were not exposed to Cantonese showed robust discrimination of the two non-native lexical tone contrasts. Contrary to this overall finding, the statistical model assessing native discrimination by Cantonese-learning infants failed to yield significant effects. These findings indicate that lexical tone sensitivity is maintained from 5 to 17 months in infants acquiring tone and non-tone languages, challenging the generalisability of the existing theoretical accounts of perceptual narrowing in the first months of life.RESEARCH HIGHLIGHTSThis is a multi-language and multi-lab investigation of young infants' ability to discriminate lexical tones.This study included data from 13 laboratories testing 5-, 10-, and 17-month-old monolingual (tone, pitch-accent, non-tone) and bilingual (tone/non-tone, non-tone/non-tone) infants.Overall, infants discriminated a perceptually similar and a distinct non-native tone contrast, although there was no evidence of a native tone-language advantage in discrimination.These results demonstrate maintenance of tone discrimination throughout development.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0734,Event-related potentials of familiar monosyllabic words with unexpected lexical tones: A picture-word study of Mandarin-speaking preschoolers with and without a history of late talking,"This study examined how Mandarin-speaking preschoolers with and without a history of late talking (LT) process familiar monosyllabic words with unexpected lexical tones, focusing on both phonological and semantic violations. This study initially enrolled 64 Mandarin-speaking toddlers: 31 with a history of LT (mean age: 27.67 months) and 33 without a history of LT (non-LT) (mean age: 27.85 months). Event-related potentials were recorded at the age of 4 years during a picture-word mismatch task (LT mean age: 51.36 months; non-LT mean age: 51.20 months); in this task, the participants were presented with auditory words either matching (Tone 3) or mismatching with images in terms of their lexical tones; the mismatches encompassed acoustically dissimilar (Tone 1) and similar (Tone 2) mismatches. A significant difference in the phonological mapping negativity (PMN) responses to Tones 1 and 3 was observed only in the non-LT group. However, differences in the N400 responses to Tones 1 and 3 remained consistent across both groups. In addition, greater differences in the PMN responses between Tones 1 and 3 were associated with higher language proficiency during the preschool period. The PMN response serves as an indicator of neural correlates in lexical tone processing, reflecting challenges encountered by preschoolers with a history of LT when processing the lexical tones of familiar words. Furthermore, the PMN response was correlated with concurrent language abilities. These findings indicate the importance of early tonal perception development for Mandarin speakers with a history of LT.Research Highlights Preschoolers with a history of late talking (LT), similar to preschoolers without such a history, can establish word expectations and detect the lexical tone violation in real time. However, those with a history of LT require additional time to process acoustic cues and differentiate between word semantics based on lexical tone information. The phonological mapping negativity response serves as an indicator of neural correlates in lexical tone processing, reflecting challenges encountered by preschoolers with a history of LT when processing the lexical tones of familiar words. The present findings indicate the importance of early intervention for Mandarin speakers with a history of LT, with an emphasis on lexical tone processing from toddlerhood.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0735,Cortical oscillations and entrainment in speech processing during working memory load,"Neuronal oscillations are thought to play an important role in working memory (WM) and speech processing. Listening to speech in real-life situations is often cognitively demanding but it is unknown whether WM load influences how auditory cortical activity synchronizes to speech features. Here, we developed an auditory n-back paradigm to investigate cortical entrainment to speech envelope fluctuations under different degrees of WM load. We measured the electroencephalogram, pupil dilations and behavioural performance from 22 subjects listening to continuous speech with an embedded n-back task. The speech stimuli consisted of long spoken number sequences created to match natural speech in terms of sentence intonation, syllabic rate and phonetic content. To burden different WM functions during speech processing, listeners performed an n-back task on the speech sequences in different levels of background noise. Increasing WM load at higher n-back levels was associated with a decrease in posterior alpha power as well as increased pupil dilations. Frontal theta power increased at the start of the trial and increased additionally with higher n-back level. The observed alpha-theta power changes are consistent with visual n-back paradigms suggesting general oscillatory correlates of WM processing load. Speech entrainment was measured as a linear mapping between the envelope of the speech signal and low-frequency cortical activity (< 13 Hz). We found that increases in both types of WM load (background noise and n-back level) decreased cortical speech envelope entrainment. Although entrainment persisted under high load, our results suggest a top-down influence of WM processing on cortical speech entrainment.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0736,Concurrent affective and linguistic prosody with the same emotional valence elicits a late positive ERP response,"Change in linguistic prosody generates a mismatch negativity response (MMN), indicating neural representation of linguistic prosody, while change in affective prosody generates a positive response (P3a), reflecting its motivational salience. However, the neural response to concurrent affective and linguistic prosody is unknown. The present paper investigates the integration of these two prosodic features in the brain by examining the neural response to separate and concurrent processing by electroencephalography (EEG). A spoken pair of Swedish words-['fa:s epsilon n] phase and ['fa:s epsilon n] damn-that differed in emotional semantics due to linguistic prosody was presented to 16 subjects in an angry and neutral affective prosody using a passive auditory oddball paradigm. Acoustically matched pseudowords['va:s epsilon m] and ['va:s epsilon m]-were used as controls. Following the constructionist concept of emotions, accentuating the conceptualization of emotions based on language, it was hypothesized that concurrent affective and linguistic prosody with the same valence-angry ['fa:s epsilon n] damn-would elicit a unique late EEG signature, reflecting the temporal integration of affective voice with emotional semantics of prosodic origin. In accordance, linguistic prosody elicited an MMN at 300-350 ms, and affective prosody evoked a P3a at 350-400 ms, irrespective of semantics. Beyond these responses, concurrent affective and linguistic prosody evoked a late positive component (LPC) at 820-870 ms in frontal areas, indicating the conceptualization of affective prosody based on linguistic prosody. This study provides evidence that the brain does not only distinguish between these two functions of prosody but also integrates them based on language and experience.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0737,Infants' Perception of Intonation: Is It a Statement or a Question?,"The ability to distinguish phonetic variations in speech that are relevant to meaning is essential for infants' language development. Previous studies into the acquisition of prosodic categories have focused on lexical stress, lexical pitch accent, or lexical tone. However, very little is known about the developmental course of infants' perception of linguistic intonation. In this study, we investigate infants' perception of the correlates of the statement/yes-no question contrast in a language that marks this sentence type distinction only by prosodic means, European Portuguese (EP). Using a modified version of the visual habituation paradigm, EP-learning infants at 5-6 and 8-9months were able to successfully discriminate segmentally varied, single-prosodic word intonational phrases presented with statement or yes-no question intonation, demonstrating that they are sensitive to the prosodic cues marking this distinction as early as 5months and maintain this sensitivity throughout the first year. These results suggest the presence of precocious discrimination abilities for intonation across segmental variation, similarly to previous reports for lexical pitch accent, but unlike previous findings for word stress.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0738,The Use of Pitch Accent in Word-Object Association by Monolingual Japanese Infants,"This study investigated the lexical use of Japanese pitch accent in Japanese-learning infants. A word-object association task revealed that 18-month-old infants succeeded in learning the associations between two nonsense objects paired with two nonsense words minimally distinguished by pitch pattern (Experiment 1). In contrast, 14-month-old infants failed (Experiment 2). Eighteen-month-old infants succeeded even for sounds that contained only the prosodic information (Experiment 3). However, a subsequent experiment revealed that 14-month-old infants succeeded in an easier single word-object task using pitch contrast (Experiment 4). These findings indicate that pitch pattern information is robustly available to 18-month-old Japanese monolingual infants in a minimal pair word-learning situation, but only partially accessible in the same context for 14-month-old infants.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0739,A Cross-linguistic Examination of Toddlers' Interpretation of Vowel Duration,"Languages differ in their phonological use of vowel duration. For the child, learning how duration contributes to lexical contrast is complicated because segmental duration is implicated in many different linguistic distinctions. Using a language-guided looking task, we measured English and Dutch 21-month-olds' recognition of familiar words with normal or manipulated vowel durations. Dutch but not English learners were affected by duration changes, even though distributions of short and long vowels in both languages are similar, and English uses vowel duration as a cue to (for example) consonant coda voicing. Additionally, we found that word recognition in Dutch toddlers was affected by shortening but not lengthening of vowels, matching an asymmetry also found in Dutch adults. Considering the subtlety of the cross-linguistic difference in the input, and the complexity of duration as a phonetic feature, our results suggest a strong capacity for phonetic analysis in children before their second birthday.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0740,"Sing for me, Mama! Infants' discrimination of novel vowels in song","When adults speak or sing with infants, they sound differently than in adult communication. Infant-directed (ID) communication helps caregivers to regulate infants' emotions and helps infants to process speech information, at least from ID-speech. However, it is largely unclear whether infants might also process speech information presented in ID-singing. Therefore, we examined whether infants discriminate vowels in ID-singing, as well as potential differences with ID-speech. Using an alternating trial preference procedure, infants aged 4-6 and 8-10 months were tested on their discrimination of an unfamiliar non-native vowel contrast presented in ID-like speech and singing. Relying on models of early speech sound perception, we expected that infants in their first half year of life would discriminate the vowels, in contrast to older infants whose non-native sound perception should deteriorate, at least in ID-like speech. Our results showed that infants of both age groups were able to discriminate the vowels in ID-like singing, while only the younger group discriminated the vowels in ID-like speech. These results show that infants process speech sound information in song from early on. They also hint at diverging perceptual or attentional mechanisms guiding infants' sound processing in ID-speech versus ID-singing toward the end of the first year of life.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0741,Sisterhood and tonal scaling,"This paper discusses central aspects of the effects of hierarchical structure on tonal scaling in intonation. The core results of a number of phonetic studies on this topic, by Ladd, by van den Berg, Gussenhoven and Rietveld, as well as experimental results of our own, are reviewed. We review the suggestions of this earlier work and argue for an addition to the theory. The principle 'The deeper the steeper' says that downstep among sister nodes is relatively larger if these sister-nodes are relatively more deeply embedded in the prosodic representation.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0742,Word recognition and cognitive profiles of Chinese pre-school children at risk for dyslexia through language delay or familial history of dyslexia,"Background: This study sought to identify cognitive abilities that might distinguish Hong Kong Chinese kindergarten children at risk for dyslexia through either language delay or familial history of dyslexia from children who were not at risk and to examine how these abilities were associated with Chinese word recognition. The cognitive skills of interest were syllable awareness, tone detection, rapid automatised naming, visual skill, and morphological awareness. Method: We recruited 36 children whose sibling had been previously diagnosed with dyslexia (familial risk group) and 36 children who were initially reported to have difficulties in preschool literacy acquisition by either teachers or parents and subsequently found to demonstrate clinical at-risk factors in aspects of language by paediatricians (language delayed group); the mean age of these groups was approximately 61 months. Thirty-six children with no such risk factors were matched by age, IQ, and parents' education to the at-risk groups. All children were tested on cognitive skills and Chinese word recognition. Results: Compared to the controls, children in the language delayed group scored significantly lower on all measures, whereas children in the familial risk group performed significantly worse only on tone detection, morphological awareness, and Chinese word recognition. In regression analyses, word recognition was best explained by morphological awareness, tone detection and visual skill. Conclusions: Languag-erelated measures are strongly associated with early reading development and impairment in Hong Kong Chinese children. Tests of tone detection and morphological awareness may be important clinical tools for diagnosing risk for reading problems in young Chinese children. In contrast, Chinese language delay may be associated with broader cognitive impairments as found previously in various Indo-European languages (e.g., Bishop &, Snowling, 2004).",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0743,Perception of tone and aspiration contrasts in Chinese children with dyslexia,"Previous research has shown a relationship between speech perception and dyslexia in alphabetic writing. In these studies speech perception was measured using phonemes, a prominent feature of alphabetic languages. Given the primary importance of lexical tone in Chinese language processing, we tested the extent to which lexical tone and aspiration, two fundamental dimensions of Cantonese speech not represented in writing, would distinguish dyslexic from non-dyslexic 8-year-old Chinese children. Tone and aspiration were tested in addition to other phonological processing skills across groups to determine the importance of different aspects of phonological sensitivity in relation to reading disability. Dyslexic children and age-matched and reading-level controls were tested on their categorical perception of minimal pairs contrasting in tone and aspiration, phonological awareness, rapid digit naming, and Chinese reading abilities. While performing similarly to reading-level controls, dyslexic children perceived tone and aspiration contrasts less categorically and accurately than age-matched controls. They also performed more poorly than the age-matched controls on rapid digit naming and a measure of phonological awareness testing children's sensitivity to different grain size units. Dyslexia in non-alphabetic Chinese correlates with the categorical organization and accuracy of Cantonese speech perception, along the tone and aspiration dimensions. This association with reading is mediated by its association with phonological awareness. Therefore, dyslexia is universally at least partly a function of basic speech and phonological processes independent of whether the speech dimensions in question are coded in writing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0744,Universality of categorical perception deficit in developmental dyslexia: an investigation of Mandarin Chinese tones,"Background: While previous studies have shown that children affected by dyslexia exhibit a deficit in categorical perception of segmental features in alphabetic languages, it remains unclear whether the categorical perception deficit generalizes to nonalphabetic languages at the suprasegmental level. In this study, we investigated the occurrence of categorical perception deficit in Mandarin lexical tones in Chinese children with dyslexia. Methods: Both behavioral and electrophysiological measures were taken to compare Chinese dyslexic children with age-matched controls. Auditory event-related potentials were collected with a passive listening oddball paradigm. Results: Behavioral data showed that dyslexic children perceived lexical tone contrasts less categorically and less precisely than age-matched controls. Consistent with the behavioral data, the across-category tone contrast elicited larger mismatch negativity than the within-category distinction in the left hemisphere for the age-matched controls but not for the dyslexic children. Conclusion: The behavioral and electrophysiological results demonstrate impaired categorical perception of lexical tones in Chinese children with dyslexia. Our findings support the hypothesis that children affected by dyslexia have a general deficit in categorical perception of speech, which generalizes to nonalphabetic languages at the suprasegmental level.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0745,Perception of Mandarin Tones: The Effect of L1 Background and Training,"This study investigates whether native Hmong speakers' first language (L1) lexical tone experience facilitates or interferes with their perception of Mandarin tones and whether training is effective for perceptual learning of second (L2) tones. In Experiment 1, 3 groups of beginning level learners of Mandarin with different L1 prosodic background (Hmong, Japanese, and English) took a perception test on Mandarin tones. Both the English and Japanese groups outperformed the Hmong group in perceptual accuracy of Mandarin tones. In Experiment 2, 18 learners with different L1 background received either perception training only or perception with production training on Mandarin tones for 6 hours within 34 weeks. Both training paradigms were effective for perceptual learning of Mandarin tone contrasts as the two training groups' perceptual accuracy improved significantly at posttest compared with a control group. Although Hmong speakers initially had more difficulties in perception of Mandarin tones than the other 2 groups, they are by no means disadvantaged by their L1 prosodic background as they gain L2 experience after intensive training.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0746,The Semantics of Prosody: Acoustic and Perceptual Evidence of Prosodic Correlates to Word Meaning,"This investigation examined whether speakers produce reliable prosodic correlates to meaning across semantic domains and whether listeners use these cues to derive word meaning from novel words. Speakers were asked to produce phrases in infant-directed speech in which novel words were used to convey one of two meanings from a set of antonym pairs (e.g., big/small). Acoustic analyses revealed that some acoustic features were correlated with overall valence of the meaning. However, each word meaning also displayed a unique acoustic signature, and semantically related meanings elicited similar acoustic profiles. In two perceptual tests, listeners either attempted to identify the novel words with a matching meaning dimension (picture pair) or with mismatched meaning dimensions. Listeners inferred the meaning of the novel words significantly more often when prosody matched the word meaning choices than when prosody mismatched. These findings suggest that speech contains reliable prosodic markers to word meaning and that listeners use these prosodic cues to differentiate meanings. That prosody is semantic suggests a reconceptualization of traditional distinctions between linguistic and nonlinguistic properties of spoken language.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0747,Phonological Abstraction in Processing Lexical-Tone Variation: Evidence From a Learning Paradigm,"There is a growing consensus that the mental lexicon contains both abstract and word-specific acoustic information. To investigate their relative importance for word recognition, we tested to what extent perceptual learning is word specific or generalizable to other words. In an exposure phase, participants were divided into two groups; each group was semantically biased to interpret an ambiguous Mandarin tone contour as either tone1 or tone2. In a subsequent test phase, the perception of ambiguous contours was dependent on the exposure phase: Participants who heard ambiguous contours as tone1 during exposure were more likely to perceive ambiguous contours as tone1 than participants who heard ambiguous contours as tone2 during exposure. This learning effect was only slightly larger for previously encountered than for not previously encountered words. The results speak for an architecture with prelexical analysis of phonological categories to achieve both lexical access and episodic storage of exemplars.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0748,'For the Inscape's Sake': Sounding the Self in the Metres of Gerard Manley Hopkins,"This article is part of a cluster that draws material from the recent conference Metre Matters: New Approaches to Prosody, 1780-1914. It comprises an introduction by Jason David Hall and six articles presented at the conference, whose aim was to address renewed scholarly interest in versification and form across the long nineteenth century, as well as some of the methodologies underpinning it. The papers included in the cluster look both to the minutiae of Romantic and Victorian metres and to their cultural intertexts. The conference, hosted by the University of Exeter's Centre for Victorian Studies, was held 35 July 2008. The cluster is made up of the following articles: Jason David Hall, Metre, History, Context: Introduction to the Metre Matters Cluster. Emma Mason and Rhian Williams, Reciprocal Scansion in Wordsworth's There Was a Boy. Ross Wilson, Robert Browning's Compounds. Margaret A. Loose, The Internationalism of Ernest Jones's Dialectical Prosody. Nancy Jiwon Cho, Gender and Authority in British Women Hymn-Writers Use of Metre, 1760-1900. Ashley Miller, Involuntary Metrics and the Physiology of Memory. Summer Star, For the Inscape's Sake: Sounding the Self in the Metres of Gerard Manley Hopkins. This article explores the nature of (and makes an argument for) the relationship between Gerard Manley Hopkins's interests in the primary otherness in language and his philosophy of selfhood. To consider both the crucial and problematic nature of such a union, I contextualize Hopkins's own interest in primitive language with the 'natural language' theories that influenced and bourgeoned in the nineteenth century, arguing that Hopkins's 'sprung rhythm' was his own contribution to the search for the primitive roots of human speech. Hopkins's metrical expressions of physical movement in his poems with human subjects (specifically in the ` St. Dorothea' poems and ` Harry Ploughman') are a representation of the deep distinctiveness of subjective experience: the experience of ` pitch', which Hopkins theorized in his sermons, or of one's own instress. In this way, metre itself was for the poet a manifestation of an original union between the physical, bodily, and spiritual realms - enacting incarnation. Yet how can we reconcile the poeticizing of the pitch of another with Hopkins's own insistence on the utter and unapproachable difference of that other's own 'self-taste'? Responding to resonances between this problem and modern ethical theory, particularly Merleau-Ponty's notion of how we conceive of otherness through physical observation, I consider Hopkins's metrical expressions of physical embodiment as a possible answer to their own ethical problem. Looking into his often-employed concept of ` sake', defined by Hopkins himself as 'that being which a thing has outside itself ', this article finally compares the sounding of this human pitch, of hearing its echo, to Hopkins's religious faith in language as a manifestation of divine being in the world - and in a divine distinctiveness of self destined to transcend mortal limits.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0749,Evidence for Plasticity in White-Matter Tracts of Patients with Chronic Broca's Aphasia Undergoing Intense Intonation-based Speech Therapy,"Recovery from aphasia can be achieved through recruitment of either perilesional brain regions in the affected hemisphere or homologous language regions in the nonlesional hemisphere. For patients with large left-hemisphere lesions, recovery through the right hemisphere may be the only possible path. The right-hemisphere regions most likely to play a role in this recovery process are the superior temporal lobe (important for auditory feedback control), premotor regions/posterior inferior frontal gyrus (important for planning and sequencing of motor actions and for auditory-motor mapping), and the primary motor cortex (important for execution of vocal motor actions). These regions are connected reciprocally via a major fiber tract called the arcuate fasciculus (AF), however, this tract is not as well developed in the right hemisphere as it is in the dominant left. We tested whether an intonation-based speech therapy (i.e., melodic intonation therapy [MIT]), which is typically administered in an intense fashion with 75-80 daily therapy sessions, would lead to changes in white-matter tracts, particularly the AF Using diffusion tensor imaging (DTI), we found a significant increase in the number of AF fibers and AF volume comparing post- with pretreatment assessments in six patients that could not be attributed to scan-to-scan variability. This suggests that intense, long-term MIT leads to remodeling of the right AF and may provide an explanation for the sustained therapy effects that were seen in these six patients.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0750,Tonal Language Processing in Congenital Amusia,"Twenty amusic and 20 control speakers of French were presented with pairs of Mandarin lexical tones to discriminate as same or different. Results revealed that even if the amusic group performed significantly below the control group, the scores of the two groups largely overlapped, with only 15% of the amusic group performing outside the normal variations. Thus, the findings suggest a modest transfer of deficit between music and speech, which in turn calls for further work in order to identify the nature of the mediating factors.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0751,Acuity of mental representations of pitch,"Singing in one's mind or forming expectations about upcoming notes both require that mental images of one or more pitches will be generated. As with other musical abilities, the acuity with which such images are formed might be expected to vary across individuals and may depend on musical training. Results from several behavioral tasks involving intonation judgments indicate that multiple memory systems contribute to the formation of accurate mental images for pitch, and that the functionality of each is affected by musical training. Electrophysiological measures indicate that the ability to form accurate mental images is associated with greater engagement of auditory areas and associated error-detection circuitry when listeners imagine ascending scales and make intonation judgments about target notes. A view of auditory mental images is espoused in which unified mental image representations are distributed across multiple brain areas. Each brain area helps shape the acuity of the unified representation based on current behavioral demands and past experience.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0752,Language and Music in the Musician Brain,"Results of numerous experiments conducted over the past 15 years by using behavioural as well as brain imaging methods have shown that musical expertise influences brain anatomy, brain functions and behaviour. The musician' brain is thus considered as a very good model of brain plasticity. Moreover, many results have demonstrated that musical expertise not only impacts on music processing but also on several aspects of speech processing including lexical pitch, sentence intonation and the metric structure of words. Conversely, recent results indicated that linguistic expertise with tone or quantity languages such as Mandarin Chinese, Thai, Finnish and Japanese, influences the processing of harmonic tones and musical intervals. We discuss possible interpretations of these findings in terms of common processing of the acoustic parameters involved in music and speech and in terms of bidirectional transfer of training effects between music and speech processing.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0753,Predicting Operational Rater-Type Classifications Using Rasch Measurement Theory and Random Forests: A Music Performance Assessment Perspective,"The purpose of this study was to build a Random Forest supervised machine learning model in order to predict musical rater-type classifications based upon a Rasch analysis of raters' differential severity/leniency related to item use. Raw scores (N = 1,704) from 142 raters across nine high school solo and ensemble festivals (grades 9-12) were collected using a 29-item Likert-type rating scale embedded within five domains (tone/intonation, n = 6; balance, n = 5; interpretation, n = 6; rhythm, n = 6; and technical accuracy, n = 6). Data were analyzed using a Many Facets Rasch Partial Credit Model. An a priori k-means cluster analysis of 29 differential rater functioning indices produced a discrete feature vector that classified raters into one of three distinct rater-types: (a) syntactical rater-type, (b) expressive rater-type, or (c) mental representation rater-type. Results of the initial Random Forest model resulted in an out-of-bag error rate of 5.05%, indicating that approximately 95% of the raters were correctly classified. After tuning a set of three hyperparameters (n(tree), m(try), and node size), the optimized model demonstrated an improved out-of-bag error rate of 2.02%. Implications for improvements in assessment, research, and rater training in the field of music education are discussed.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0754,Consequences of brain tumour resection on emotion recognition,"Emotion processing impairments are common in patients undergoing brain surgery for fronto-temporal tumour resection, with potential consequences on social interactions. However, evidence is controversial concerning side and site of lesions causing such deficits. This study investigates visual and auditory emotion recognition in brain tumour patients with the aim of clarifying which lesion sites are related to impairments in emotion processing from different modalities. Thirty-four patients were evaluated, before and after surgery, on facial expression and emotional prosody recognition; voxel-based lesion-symptom mapping (VLSM) analyses were performed on patients' post-surgery MRI images. Results showed that patients' performance decreased after surgery in both visual and auditory modalities, but, in general, recovered 3 months after surgery. In facial expression recognition, left brain-damaged patients showed greater post-surgery deterioration than right brain-damaged ones, whose performance specifically decreased for sadness and fear. VLSM analysis revealed two segregated areas in the left hemisphere accounting for post-surgery scores for happy (fronto-temporo-insular region) and surprised (middle frontal gyrus and inferior fronto-occipital fasciculus) facial expressions. Our findings demonstrate that surgical removal of tumours in the fronto-temporal region produces impairment in facial emotion recognition with an overall recovery at 3 months, suggesting a partially different representation of positive and negative emotions in the left and right hemispheres for visually - but not auditory - presented emotions; moreover, we show that deficits in specific expression recognition are associated with discrete lesion locations.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0755,Pitch Ability As an Aptitude for Tone Learning,"Tone languages such as Mandarin use voice pitch to signal lexical contrasts, presenting a challenge for second/foreign language (L2) learners whose native languages do not use pitch in this manner. The present study examined components of an aptitude for mastering L2 lexical tone. Native English speakers with no previous tone language experience completed a Mandarin word learning task, as well as tests of pitch ability, musicality, L2 aptitude, and general cognitive ability. Pitch ability measures improved predictions of learning performance beyond musicality, L2 aptitude, and general cognitive ability and also predicted transfer of learning to new talkers. In sum, although certain nontonal measures help predict successful tone learning, the central components of tonal aptitude are pitch-specific perceptual measures.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0756,"Hearing and Seeing Tone Through Color: An Efficacy Study of Web-Based, Multimodal Chinese Tone Perception Training","Multimodal approaches have been shown to be effective for many learning tasks. In this study, we compared the effectiveness of five multimodal methods for second language (L2) Mandarin tone perception training: three single-cue methods (number, pitch contour, color) and two dual-cue methods (color and number, color and pitch contour). A total of 303 true novice learners of L2 Mandarin (native speakers of English) completed a 3-week online training program. Results from pretests as well as immediate and delayed posttests indicated that multimodal training aided L2 learners' tone perception, with a small, practical advantage for pitch contours and numbers over color coding. Dual-cue methods did not yield better learning than single-cue training. Thus, the additive benefits of multimodal input (i.e., auditory and visual) did not extend to instruction featuring doubled visual input (i.e., visual and visual). We argue for embedding color in visuals in a way that helps make abstract information concrete.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0757,English Learners' Use of Segmental and Suprasegmental Cues to Stress in Lexical Access: An Eye-Tracking Study,"This study investigated the use of segmental and suprasegmental cues to lexical stress in word recognition by Mandarin-speaking English learners, Korean-speaking English learners, and native English listeners. Unlike English and Mandarin, Korean does not have lexical stress. Participants completed a visual-world eye-tracking experiment that examined whether listeners' word recognition is constrained by suprasegmental cues to stress alone or by a combination of segmental and suprasegmental cues. Results showed that English listeners used both suprasegmental cues alone and segmental and suprasegmental cues together to recognize English words, with the effect of stress being greater for combined cues. Conversely, Mandarin listeners used stress in lexical access only when stress was signaled by suprasegmental cues alone, and Korean listeners did so only when stress was signaled by segmental and suprasegmental cues together. These results highlight the importance of a cue-based approach to the study of stress in word recognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0758,Statistical Regularities Affect the Perception of Second Language Speech: Evidence From Adult Classroom Learners of Mandarin Chinese,"This study investigated how adult second language (L2) learners of Mandarin Chinese use knowledge of phonological and lexical statistical regularities when acoustic information is insufficient for word recognition. A gating task was used to test intermediate L2 learners at two time points across a semester of classroom learning. Native Mandarin speakers (tested once) served as a control group. Mixed-effects modeling revealed that upon hearing truncated speech, L2 learners, like native speakers, identified high token frequency syllable-tone combinations more accurately than low token frequency syllable-tone combinations. Error analysis of correct syllable/incorrect tone responses revealed that native speakers made specific probability-based errors. L2 learners primarily demonstrated more acoustic-based errors but exhibited a trend toward greater probability-based errors during the second test. These findings are interpreted in light of L2 speech learning models that emphasize a statistical learning mechanism. Open Practices This article has been awarded an Open Materials badge. All materials are publicly accessible via the Open Science Framework at . Learn more about the Open Practices badges from the Center for Open Science: .",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0759,Behavioral and Neural Responses to Tone Errors in Foreign-Accented Mandarin,"Previous event-related potentials (ERP) research has investigated how foreign accent modulates listeners' neural responses to lexical-semantic and morphosyntactic errors. We extended this line of research to consider whether pronunciation errors in Mandarin Chinese are processed differently when a foreign-accented speaker makes them relative to when a native-accented speaker makes them (a conceptual replication using the materials from Pelzl et al., 2019). We evaluated behavioral judgments, the N400, and late positive component while native speakers listened to native and foreign-accented sentences containing tone and rhyme pronunciation errors. We observed effects that suggested that the participants were prone to detect errors in foreign-accented speech more often in sentences with no critical word deviation but also were less likely to reject critical tone errors produced by the foreign-accented speaker. ERP results showed a main effect of accent on late positive components that suggested a difference in degree for sensitivity to foreign-accented compared to native-accented pronunciation errors rather than a completely different response pattern. We found no effect of accent on N400s, with statistically significant differences between tone and rhyme errors regardless of speaker accent.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0760,"Incidental Nonspeech Auditory Learning Scaffolds Phonetic, Category, and Word Learning in a Foreign Language Classroom","There is considerable lab-based evidence for successful incidental learning, in which a learner's attention is directed away from the to-be-learned stimulus and towards another stimulus. In this study, we extend incidental learning research into the language learning classroom. Three groups of adult second language (L2) learners (N = 52) engaged in structured classroom Mandarin learning took part in an 8-week study. One group served as a classroom-only control group. The second group underwent additional intentional auditory training involving Mandarin speech and explicit feedback. The third group underwent additional incidental learning combined with nonspeech ""perceptual building block"" categories-categories that share critical perceptual dimensions with target L2 speech categories but that are not perceived as speech. We demonstrate that when supplemented with structured classroom learning, incidental learning involving nonspeech analogs promotes phonetic, category, and word learning equivalent to learning from more traditional intentional auditory training.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0761,The life and times of focus alternatives: Tracing the activation of alternatives to a focused constituent in language comprehension,"The function of focus is to activate a set of alternatives, providing the locus for focus-sensitive particles like only. In the past decade, psycholinguistic research has shown that listeners entertain a set of alternatives in online language comprehension, similar to the algorithm stipulated by Alternative Semantics. The purpose of the present review is to gain a comprehensive picture of the role of focus alternatives in utterance comprehension and interpretation. Specifically, we focus on how the processing of focus particles interacts with alternatives activated by focus. We show that focus marking activates a network of related concepts, but over time, only those that can be considered as focus alternatives in the relevant context of the utterance are retained in the mental representation of the discourse. Focus particles, in turn, increase the competition between the focused element and its alternatives during the initial stages of comprehension, helping contextual restriction of the set of alternatives. Overall, the studies presented in this review show that focus does not only guide a listener in determining which alternatives are relevant for the purpose of conversation, it also plays a fundamental role in the memory representation of a discourse.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0762,Do Explicit Instruction and High Variability Phonetic Training Improve Nonnative Speakers' Mandarin Tone Productions?,"This study examines the putative benefits of explicit phonetic instruction, high variability phonetic training, and their effects on adult nonnative speakers' Mandarin tone productions. Monolingual first language (L1) English speakers (n = 80), intermediate second language (L2) Mandarin learners (n = 40), and L1 Mandarin speakers (n = 40) took part in a multiday Mandarin-like artificial language learning task. Participants were asked to repeat a syllable-tone combination immediately after hearing it. Half of all participants were exposed to speech from 1 talker (low variability) while the other half heard speech from 4 talkers (high variability). Half of the L1 English participants were given daily explicit instruction on Mandarin tone contours, while the other half were not. Tone accuracy was measured by L1 Mandarin raters (n = 104) who classified productions according to their perceived tonal category. Explicit instruction of tone contours facilitated L1 English participants' production of rising and falling tone contours. High variability input alone had no main effect on participants' productions but interacted with explicit instruction to improve participants' productions of high-level tone contours. These results motivate an L2 tone production training approach that consists of explicit tone instruction followed by gradual exposure to more variable speech.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0763,The Origin of the Dominant: Schoenberg's 'Strong Progression' and the Realisation of Implied Virtual Pitches,"In major-minor tonality, V implies I, and rising fourths, falling thirds and rising seconds between successive chord roots are more common than falling fourths, rising thirds and falling seconds respectively. Possible explanations involve history (in two-part medieval counterpoint, harmonic major sixths resolved to octaves - maintained in V-I); the rising leading note (V-I includes it, IV-I does not); acoustic speculation (in V-I, the third harmonic of 1$\hat 1$ resolves to the second); voice leading (in V7-I, a tritone resolves to a third by contrary step); melodic closure (a rising melodic leap implies subsequent falling steps, and melodies often end with 2$\hat 2$-1$\hat 1$, harmonised V-I); root newness (chords 'progress' if the second chord's root is not part of the first); and tonal stability (V is less stable than IV, giving it a stronger 'urge' to resolve). An additional possibility combines the 'virtual-pitch' theory of Ernst Terhardt with the 'implication-realisation' theory of Leonard B. Meyer and Eugene Narmour. Major and minor triads imply subsidiary virtual pitches (missing fundamentals; see Rameau's 'fundamental bass') at third and fifth intervals below the root. These weakly perceived pitches are realised in the next chord if the root falls by a third or rises a fourth or major second, creating a feeling of forward progression - while also facilitating intonation for singers and melodic instrumentalists. The theory correctly predicts that successive harmonic complex (but not pure) tones an octave apart sound more similar if rising, and rising octaves are more common than falling in melody. It also explains why Classical modulations are asymmetrical in the opposite direction, major keys tending to modulate to V (not IV) and minor to III (not VI): accidental flats tend to be more perceptually salient or stable than sharps.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0764,Auditory-motor mapping training: Testing an intonation-based spoken language treatment for minimally verbal children with autism spectrum disorder,"We tested an intonation-based speech treatment for minimally verbal children with autism (auditory-motor mapping training, AMMT) against a nonintonation-based control treatment (speech repetition therapy, SRT). AMMT involves singing, rather than speaking, two-syllable words or phrases. In time with each sung syllable, therapist and child tap together on electronic drums tuned to the same pitches, thus coactivating shared auditory and motor neural representations of manual and vocal actions, and mimicking the ""babbling and banging"" stage of typical development. Fourteen children (three females), aged 5.0-10.8, with a mean Autism Diagnostic Observation Schedule-2 score of 22.9 (SD = 2.5) and a mean Kaufman Speech Praxis Test raw score of 12.9 (SD = 13.0) participated in this trial. The main outcome measure was percent syllables approximately correct. Four weeks post-treatment, AMMT resulted in a mean improvement of +12.1 (SE = 3.8) percentage points, compared to +2.8 (SE = 5.7) percentage points for SRT. This between-group difference was associated with a large effect size (Cohen's d = 0.82). Results suggest that simultaneous intonation and bimanual movements presented in a socially engaging milieu are effective factors in AMMT and can create an individualized, interactive music-making environment for spoken-language learning in minimally verbal children with autism.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0765,Segmental and suprasegmental features in speech perception in Cantonese-speaking second graders: An ERP study,"Using a multiple-deviant oddball paradigm, this study examined second graders' brain responses to Cantonese speech. We aimed to address the question of whether a change in a consonant or lexical tone could be automatically detected by children. We measured auditory mismatch responses to place of articulation and voice onset time (VOT), reflecting segmental perception, as well as Cantonese lexical tones including level tone and contour tone, reflecting suprasegmental perception. The data showed that robust mismatch negativities (MMNs) were elicited by all deviants in the time window of 300-500ms in second graders. Moreover, relative to the standard stimuli, the VOT deviant elicited a robust positive mismatch response, and the level tone deviant elicited a significant MMN in the time window of 150-300ms. The findings suggest that Hong Kong second graders were sensitive to neural discriminations of speech sounds both at the segmental and suprasegmental levels.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0766,Effects of native language experience on Mandarin lexical tone processing in proficient second language learners,"Learning the acoustic and phonological information in lexical tones is significant for learners of tonal languages. Although there is a wealth of knowledge from studies of second language (L2) tone learning, it remains unclear how L2 learners process acoustic versus phonological information differently depending on whether their first language (L1) is a tonal language. In the present study, we first examined proficient L2 learners of Mandarin with tonal and nontonal L1 in a behavioral experiment (identifying a Mandarin tonal continuum) to construct tonal contrasts that could differentiate the phonological from the acoustic information in Mandarin lexical tones for the L2 learners. We then conducted an ERP experiment to investigate these learners' automatic processing of acoustic and phonological information in Mandarin lexical tones by mismatch negativity (MMN). Although both groups of L2 learners showed similar behavioral identification features for the Mandarin tonal continuum as native speakers, L2 learners with nontonal L1, as compared with both native speakers and L2 learners with tonal L1, showed longer reaction time to the tokens of the Mandarin tonal continuum. More importantly, the MMN data further revealed distinct roles of acoustic and phonological information on the automatic processing of L2 lexical tones between the two groups of L2 learners. Taken together, the results indicate that the processing of acoustic and phonological information in L2 lexical tones may be modulated by L1 experience with a tonal language. The theoretical implications of the current study are discussed in light of models of L2 speech learning.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0767,The effect of lexical status on prosodic processing in infants learning a fixed stress language,"In speech processing, in the first year of life, prosody and phoneme-relevant aspects serve different functions. Recent studies have assumed that the two aspects become integrated at around 9 months of age. The present study investigates the effect of lexical status on stress processing in a fixed stress language. We hypothesize that lexicality modulates stress processing, and where the stress cue is in conflict with the lexical status (legal deviant condition), we will observe differences in age indicating the stage of integration. We tested 69 6 and 10 month-old infants in an acoustic oddball event-related potential paradigm. A frequent word stimulus (baba) and a pseudoword (bebe) were used with legal versus illegal stress. We systematically swapped the standard and deviant roles of the different stress variants in two conditions. In the illegal deviant condition in the case of the word stimulus, the response pattern typical for the pseudoword (an MMR to the absence of the stress cue) was missing. This implies the suppression effect of lexicality. In the legal deviant condition, negative MMR (N-MMR) in the second time window indicated a facilitation effect of lexicality in both age groups. As only the 6-month-olds produced an N-MMR in the first time window, we concluded that in a fixed stress language, integration starts at 6 months but is only completed by the age of 10 months. Our results show that lexical status modulates stress processing at word level in a highly regularly stressed language in which stable, long-term language-specific stress representation exists from early infancy.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0768,Relative importance of temporal envelope and fine structure in lexical-tone perception,"The relative importance of temporal envelope and fine structure in speech and music perception was investigated by Smith et al. [Nature (London) 416, 87-90 (2002)] using ""auditory chimera"" in which the envelope from one sound was paired with the fine structure of another. Smith et al. found that, when 4 to 16 frequency bands were used, recognition of English speech was dominated by the envelope, whereas recognition of melody was dominated by the fine structure. In the present study, Mandarin Chinese monosyllables were divided into 4, 8, or 16 frequency bands and the tine structure and envelope of one tone pattern were exchanged with those of another tone pattern of the same monosyllable. Five normal-hearing native Mandarin Chinese speakers completed a four-alternative forced-choice tone-identification task. In the vast majority of trials, subjects based their identification of the monosyllables on the fine structure rather than the envelope. Thus, the relative importance of envelope and fine structure for lexical-tone perception resembled that for melody recognition rather than that for English speech recognition. Delivering fine-structure information in cochlear implant stimulation could be particularly beneficial for lexical-tone perception. (C) 2003 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0769,Enhancement of temporal periodicity cues in cochlear implants: Effects on prosodic perception and vowel identification,"Standard continuous interleaved sampling processing, and a modified processing strategy designed to enhance temporal cues to voice pitch, were compared on tests of intonation perception, and vowel perception, both in implant users and in acoustic simulations. In standard processing, 400 Hz low-pass envelopes modulated either pulse trains (implant users) or noise carriers (simulations). In the modified strategy, slow-rate envelope modulations, which convey dynamic spectral variation crucial for speech understanding, were extracted by low-pass filtering (32 Hz). In addition, during voiced speech, higher-rate temporal modulation in each channel was provided by 100% amplitude-modulation by a sawtooth-like wave form whose periodicity followed the fundamental frequency (FO) of the input. Channel levels were determined by the product of the lower- and higher-rate modulation components. Both in acoustic simulations and in implant users, the ability to use intonation information to identify sentences as question or statement was significantly better with modified processing. However, while there was no difference in vowel recognition in the acoustic simulation, implant users performed worse with modified processing both in vowel recognition and in formant frequency discrimination. It appears that, while enhancing pitch perception, modified processing harmed the transmission of spectral information. (c) 2005 Acoustical Society of America.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0770,Extrinsic context affects perceptual normalization of lexical tone,"The present study explores the use of extrinsic context in perceptual normalization for the purpose of identifying lexical tones in Cantonese. In each of four experiments, listeners were presented with a target word embedded in a semantically neutral sentential context. The target word was produced with a mid level tone and it was never modified throughout the study, but on any given trial the fundamental frequency of part or all of the context sentence was raised or lowered to varying degrees. The effect of perceptual normalization of tone was quantified as the proportion of non-mid level responses given in F0-shifted contexts. Results showed that listeners' tonal judgments (i) were proportional to the degree of frequency shift, (ii) were not affected by non-pitch-related differences in talker, (iii) and were affected by the frequency of both the preceding and following context, although (iv) following context affected tonal decisions more strongly than did preceding context. These findings suggest that perceptual normalization of lexical tone may involve a ""moving window"" or ""running average"" type of mechanism, that selectively weights more recent pitch information over older information, but does not depend on the perception of a single voice. (c) 2006 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0771,Constrained tone transformation technique for separation and combination of Mandarin tone and intonation,"This paper addresses a classical but important problem: The coupling of lexical tones and sentence intonation in tonal languages, such as Chinese, focusing particularly on voice fundamental frequency (F-0) contours of speech. It is important because it forms the basis of speech synthesis technology and prosody analysis. We provide a solution to the problem with a constrained tone transformation technique based on structural modeling of the F-0 contours. This consists of transforming target values in pairs from norms to variants. These targets are intended to sparsely specify the prosodic contributions to the F0 contours, while the alignment of target pairs between norms and variants is based on underlying lexical tone structures. When the norms take the citation forms of lexical tones, the technique makes it possible to separate sentence intonation from observed F0 contours. When the norms take normative F0 contours, it is possible to measure intonation variations from the norms to the variants, both having identical lexical tone structures. This paper explains the underlying scientific and linguistic principles and presents an algorithm that was implemented on computers. The method's capability of separating and combining tone and intonation is evaluated through analysis and re-synthesis of several hundred observed F0 contours. (c) 2006 Acoustical Society of America.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0772,Evidence for attractors in English intonation,"Although the pitch of the human voice is continuously variable, some linguists contend that intonation in speech is restricted to a small, limited set of patterns. This claim is tested by asking subjects to mimic a block of 100 randomly generated intonation contours and then to imitate themselves in several successive sessions. The produced f(0) contours gradually converge towards a limited set of distinct, previously recognized basic English intonation patterns. These patterns are ""attractors"" in the space of possible, intonation English contours. The convergence does not occur immediately. Seven of the ten participants show continued convergence toward their attractors after the first iteration. Subjects retain and use information beyond phonological contrasts, suggesting that intonational phonology is not a complete description of their mental representation of intonation. (c) 2006 Acoustical Society of America.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0773,Is fundamental frequency a cue to aspiration in initial stops?,"One production and one perception experiment were conducted to investigate the interaction of consonant voicing and fundamental frequency at, the onset of voicing (onset f(0)) in Cantonese, a tonal language. Consonantal voicing in English can affect onset f(0) up to 100 ms after voicing onset, but existing research provides inconclusive information regarding the effects of voicing on f(0) in tonal languages where f(0) variability is constrained by the demands of the lexical tone system. Previous research on consonantal effects on onset f(0) provides two contrasting theories: These effects may be automatic, resulting from physiological constraints inherent to the speech production mechanism or they may be controlled, produced as part of a process of cue enhancement for the perception of laryngeal contrasts. Results of experiment 1 showed that consonant aspiration affects onset f(0) in Cantonese only within the first 10 ms following voicing onset, comparable to results for other tonal languages. Experiment 2 showed that Cantonese listeners can use differences in onset f(0) to cue perception of the voicing contrast, but the minimum extent of f(0) perturbation necessary for this is greater than is found in Cantonese production, and comparable to that observed in acoustic studies of nontonal languages. These results suggest that consonantal effects on onset f(0) are at least partially controlled by talkers, but that their role in the perception of voicing/aspiration may be a consequence of language independent properties of audition rather than listeners' experience with the phonological contrasts of a specific language. (c) 2006 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0774,Simulating the acquisition of lexical tones from continuous dynamic input,"Infants develop phonetic categories by simply being exposed to adult speech. It remains unclear, however, how they handle the extensive variability inherent to speech, and how they process multiple linguistic functions that share the same acoustic parameters. Across four neural network simulations of lexical tone acquisition, self-organizing maps were trained with continuous speech input of increasing variability. Robust tonal categorization was achieved by tracking the velocity profiles of fundamental frequency contours. This result suggests that continuous speech signal carries sufficient categorical information that can be directly processed, and that dynamic acoustic information can be used for resolving the variability problem. (c) 2007 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0775,Tone recognition in continuous Cantonese speech using supratone models,"This paper studies automatic tone recognition in continuous Cantonese speech. Cantonese is a major Chinese dialect that is known for being rich in tones. Tone information serves as a useful knowledge source for automatic speech recognition of Cantonese. Cantonese tone recognition is difficult because the tones have similar shapes of pitch contours. The tones are differentiated mainly by their relative pitch heights. In natural speech, the pitch level of a tone may shift up and down and the F0 ranges of different tones overlap with each other, making them acoustically indistinguishable within the domain of a syllable. Our study shows that the relative pitch heights are largely preserved between neighboring tones. A novel method of supratone modeling is proposed for Cantonese tone recognition. Each supratone model characterizes the F0 contour of two or three tones in succession. The tone sequence of a continuous utterance is formed as an overlapped concatenation of supratone units. The most likely tone sequence is determined under phonological constraints on syllable-tone combinations. The proposed method attains an accuracy of 74.68% in speaker-independent tone recognition experiments. In particular, the confusion among the tones with similar contour shapes is greatly resolved. (C) 2007 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0776,Acoustic cues to lexical segmentation: A study of resynthesized speech,"It has been posited that the role of prosody in lexical segmentation is elevated when the speech signal is degraded or unreliable. Using predictions from Cutler and Norris' [J. Exp. Psychol. Hum. Percept. Perform. 14, 113-121 (1988)] metrical segmentation strategy hypothesis as a framework, this investigation examined how individual suprasegmental and segmental cues to syllabic stress contribute differentially to the recognition of strong and weak syllables for the purpose of lexical segmentation. Syllabic contrastivity was reduced in resynthesized phrases by systematically (i) flattening the fundamental frequency (F0) contours, (ii) equalizing vowel durations, (iii) weakening strong vowels, (iv) combining the two suprasegmental cues., i.e., F0 and duration, and (v) combining the manipulation of all cues. Results indicated that, despite similar decrements in overall intelligibility, F0 flattening and the weakening of strong vowels had a greater impact on lexical segmentation than did equalizing vowel duration. Both combined-cue conditions resulted in greater decrements in intelligibility, but with no additional negative impact on lexical segmentation. The results support the notion of F0 variation and vowel quality as primary conduits for stress-based segmentation and suggest that the effectiveness of stress-based segmentation with degraded speech must be investigated relative to the suprasegmental and segmental impoverishments occasioned by each particular degradation. (c) 2007 Acoustical Society of America.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0777,Seeing pitch: Visual information for lexical tones of Mandarin-Chinese,"Mandarin perceivers were tested in visual lexical-tone identification before and after learning. Baseline performance was only slightly above chance, although there appeared to be some visual information in the speakers' neck and head movements. When participants were taught to use this visible information in two experiments, visual tone identification improved significantly. There appears to be a relationship between the production of lexical tones and the visible movements of the neck, head, and mouth, and this information can be effectively used after a short training session. (C) 2008 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0778,Identification of Mandarin tones by English-speaking musicians and nonmusicians,"This study examined Mandarin tone identification by 36 English-speaking musicians and 36 nonmusicians and musical note identification by the musicians. In the Mandarin task, participants were given a brief tutorial on Mandarin tones and identified the tones of the syllable sa produced by 32 speakers. The stimuli included intact syllables and acoustically modified syllables with limited F0 information. Acoustic analyses showed considerable overlap in F0 range among the tones due to the presence of multiple speakers. Despite no prior experience with Mandarin, the musicians identified intact tones at 68% and silent-center tones at 54% correct, both exceeding chance (25%). The musicians also outperformed the nonmusicians, who identified intact tones at 44% and silent-center tones at 36% correct. These results indicate musical training facilitated lexical tone identification, although the facilitation varied as a function of tone and the type of acoustic input. In the music task, the musicians listened to synthesized musical notes of three timbres and identified the notes without a reference pitch. Average identification accuracy was at chance level even when multiple semitone errors were allowed. Since none of the musicians possessed absolute pitch, the role of absolute pitch in Mandarin tone identification remains inconclusive. (C) 2008 Acoustical Society of America. [DOI: 10.1121/1.2990713]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0779,Modeling tone and intonation in Mandarin and English as a process of target approximation,"This paper reports the development of a quantitative target approximation (qTA) model for generating F-0 contours of speech. The qTA model simulates the production of tone and intonation as a process of syllable-synchronized sequential target approximation [Xu, Y. (2005). ""Speech melody as articulatorily implemented communicative functions,"" Speech Commun. 46, 220-251]. It adopts a set of biomechanical and linguistic assumptions about the mechanisms of speech production. The communicative functions directly modeled are lexical tone in Mandarin and lexical stress in English and focus in both languages. The qTA model is evaluated by extracting function-specific model parameters from natural speech via supervised learning (automatic analysis by synthesis) and comparing the F-0 contours generated with the extracted parameters to those of natural utterances through numerical evaluation and perceptual testing. The F-0 contours generated by the qTA model with the learned parameters were very close to the natural contours in terms of root mean square error, rate of human identification of tone, and focus and judgment of naturalness by human listeners. The results demonstrate that the qTA model is both an effective tool for research on tone and intonation and a potentially effective system for automatic synthesis of tone and intonation.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0780,"Identifying isolated, multispeaker Mandarin tones from brief acoustic input: A perceptual and acoustic study","Lexical tone identification relies primarily on the processing of F0. Since F0 range differs across individuals, the interpretation of F0 usually requires reference to specific speakers. This study examined whether multispeaker Mandarin tone stimuli could be identified without cues commonly considered necessary for speaker normalization. The sa syllables, produced by 16 speakers of each gender, were digitally processed such that only the fricative and the first six glottal periods remained in the stimuli, neutralizing the dynamic F0 contrasts among the tones. Each stimulus was presented once, in isolation, to 40 native listeners who had no prior exposure to the speakers' voices. Chi-square analyses showed that tone identification accuracy exceeded chance as did tone classification based on F0 height. Acoustic analyses showed contrasts between the high- and low-onset tones in F0, duration, and two voice quality measures (F1 bandwidth and spectral tilt). Correlation analyses showed that F0 covaried with the voice quality measures and that tone classification based on F0 height also correlated with these acoustic measures. Since the same acoustic measures consistently distinguished the female from the male stimuli, gender detection may be implicated in F0 height estimation when no context, dynamic F0, or familiarity with speaker voices is available. (C) 2009 Acoustical Society of America. [DOI:10.1121/1.3050322]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0781,Cantonese tone recognition with enhanced temporal periodicity cues,"This study investigated the contributions of temporal periodicity cues and the effectiveness of enhancing these cues for Cantonese tone recognition in noise. A multichannel noise-excited vocoder was used to simulate speech processing in cochlear implants. Ten normal-hearing listeners were tested. Temporal envelope and periodicity cues (TEPCs) below 500 Hz were extracted from four frequency bands: 60-500, 500-1000, 1000-2000, and 2000-4000 Hz. The test stimuli were obtained by combining TEPC-modulated noise signals from individual bands. For periodicity enhancement, temporal fluctuations in the range 20-500 Hz were replaced by a sinusoid with frequency equal to the fundamental frequency of original speech. Tone identification experiments were carried out using disyllabic word carriers. Results showed that TEPCs from the two high-frequency bands were more important for tone identification than TEPCs from the low-frequency bands. The use of periodicity-enhanced TEPCs led to consistent improvement of tone identification accuracy. The improvement was more significant at low signal-to-noise ratios, and more noticeable for female than for male voices. Analysis of error distributions showed that the enhancement method reduced tone identification errors and did not show any negative effect on the recognition of segmental structures.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0782,General perceptual contributions to lexical tone normalization,"Within tone languages that use pitch variations to contrast meaning, large variability exists in the pitches produced by different speakers. Context-dependent perception may help to resolve this perceptual challenge. However, whether speakers rely on context in contour tone perception is unclear; previous studies have produced inconsistent results. The present study aimed to provide an unambiguous test of the effect of context on contour lexical tone perception and to explore its underlying mechanisms. In three experiments, Mandarin listeners' perception of Mandarin first and second (high-level and mid-rising) tones was investigated with preceding speech and non-speech contexts. Results indicate that the mean fundamental frequency (f0) of a preceding sentence affects perception of contour lexical tones and the effect is contrastive. Following a sentence with a higher-frequency mean f0, the following syllable is more likely to be perceived as a lower frequency lexical tone and vice versa. Moreover, non-speech precursors modeling the mean spectrum of f0 also elicit this effect, suggesting general perceptual processing rather than articulatory-based or speaker-identity-driven mechanisms. (C) 2009 Acoustical Society of America. [DOI: 10.1121/1.3125342]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0783,Individual variability in cue-weighting and lexical tone learning,"Speech sound patterns can be discerned using multiple acoustic cues. The relative weighting of these cues is known to be language-specific. Speech-sound training in adults induces changes in cue-weighting such that relevant acoustic cues are emphasized. In the current study, the extent to which individual variability in cue weighting contributes to differential success in learning to use foreign sound patterns was examined. Sixteen English-speaking adult participants underwent a sound-to-meaning training paradigm, during which they learned to incorporate Mandarin linguistic pitch contours into words. In addition to cognitive tests, measures of pitch pattern discrimination and identification were collected from all participants. Reaction time data from the discrimination task was subjected to 3-way multidimensional scaling to extract dimensions underlying tone perception. Two dimensions relating to pitch height and pitch direction were found to underlie non-native tone space. Good learners attended more to pitch direction relative to poor learners, before and after training. Training increased the ability to identify and label pitch direction. The results demonstrate that variability in the ability to successfully learn to use pitch in lexical contexts can be explained by pre-training differences in cue-weighting. (C) 2010 Acoustical Society of America. [DOI: 10.1121/1.3445785]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0784,The perception of intonation questions and statements in Cantonese,"In tone languages there are potential conflicts in the perception of lexical tone and intonation, as both depend mainly on the differences in fundamental frequency (F0) patterns. The present study investigated the acoustic cues associated with the perception of sentences as questions or statements in Cantonese, as a function of the lexical tone in sentence final position. Cantonese listeners performed intonation identification tasks involving complete sentences, isolated final syllables, and sentences without the final syllable (carriers). Sensitivity (d' scores) were similar for complete sentences and final syllables but were significantly lower for carriers. Sensitivity was also affected by tone identity. These findings show that the perception of questions and statements relies primarily on the F0 characteristics of the final syllables (local F0 cues). A measure of response bias (c) provided evidence for a general bias toward the perception of statements. Logistic regression analyses showed that utterances were accurately classified as questions or statements by using average F0 and F0 interval. Average F0 of carriers (global F0 cue) was also found to be a reliable secondary cue. These findings suggest that the use of F0 cues for the perception of intonation question in tonal languages is likely to be language-specific. (C) 2011 Acoustical Society of America. [DOI: 10.1121/1.3531840]",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0785,Evidence for the central origin of lexical tone normalization (L),"Huang and Holt [(2009). J. Acoust. Soc. Am. 125, 3983-3994] suggest that listeners may dynamically tune lexical tone perception via general auditory sensitivity to the mean f0 of the preceding context, effectively normalizing pitch differences across talkers. The present experiments further examine the effect using the missing-f0 phenomenon as a means of determining the level of auditory processing at which lexical tone normalization occurs. Speech contexts filtered to remove or mask low-frequency f0 energy elicited contrastive context effects. Central, rather than peripheral, auditory processes may be responsible for the context-dependence that has been considered to be lexical tone normalization. (C) 2011 Acoustical Society of America. [DOI: 10.1121/1.3543994]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0786,Predicting the intelligibility of vocoded and wideband Mandarin Chinese,"Due to the limited number of cochlear implantees speaking Mandarin Chinese, it is extremely difficult to evaluate new speech coding algorithms designed for tonal languages. Access to an intelligibility index that could reliably predict the intelligibility of vocoded (and non-vocoded) Mandarin Chinese is a viable solution to address this challenge. The speech-transmission index (STI) and coherence- based intelligibility measures, among others, have been examined extensively for predicting the intelligibility of English speech but have not been evaluated for vocoded or wideband (non-vocoded) Mandarin speech despite the perceptual differences between the two languages. The results indicated that the coherence-based measures seem to be influenced by the characteristics of the spoken language. The highest correlation (r = 0.91-0.97) was obtained in Mandarin Chinese with a weighted coherence measure that included primarily information from high-intensity voiced segments (e. g., vowels) containing F0 information, known to be important for lexical tone recognition. In contrast, in English, highest correlation was obtained with a coherence measure that included information from weak consonants and vowel/ consonant transitions. A band-importance function was proposed that captured information about the amplitude envelope contour. A higher modulation rate (100 Hz) was found necessary for the STI-based measures for maximum correlation (r = 0.94-0.96) with vocoded Mandarin and English recognition. (C) 2011 Acoustical Society of America. [DOI: 10.1121/1.3570957]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0787,Perception of musical and lexical tones by Taiwanese-speaking musicians,"This study explored the relationship between music and speech by examining absolute pitch and lexical tone perception. Taiwanese-speaking musicians were asked to identify musical tones without a reference pitch and multispeaker Taiwanese level tones without acoustic cues typically present for speaker normalization. The results showed that a high percentage of the participants (65% with an exact match required and 81% with one-semitone errors allowed) possessed absolute pitch, as measured by the musical tone identification task. A negative correlation was found between occurrence of absolute pitch and age of onset of musical training, suggesting that the acquisition of absolute pitch resembles the acquisition of speech. The participants were able to identify multispeaker Taiwanese level tones with above-chance accuracy, even though the acoustic cues typically present for speaker normalization were not available in the stimuli. No correlations were found between the performance in musical tone identification and the performance in Taiwanese tone identification. Potential reasons for the lack of association between the two tasks are discussed. (C) 2011 Acoustical Society of America. [DOI: 10.1121/1.3596473]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0788,Perception of intonation in Mandarin Chinese,"There is a tendency across languages to use a rising pitch contour to convey question intonation and a falling pitch contour to convey a statement. In a lexical tone language such as Mandarin Chinese, rising and falling pitch contours are also used to differentiate lexical meaning. How, then, does the multiplexing of the F-0 channel affect the perception of question and statement intonation in a lexical tone language? This study investigated the effects of lexical tones and focus on the perception of intonation in Mandarin Chinese. The results show that lexical tones and focus impact the perception of sentence intonation. Question intonation was easier for native speakers to identify on a sentence with a final falling tone and more difficult to identify on a sentence with a final rising tone, suggesting that tone identification intervenes in the mapping of F-0 contours to intonational categories and that tone and intonation interact at the phonological level. In contrast, there is no evidence that the interaction between focus and intonation goes beyond the psychoacoustic level. The results provide insights that will be useful for further research on tone and intonation interactions in both acoustic modeling studies and neurobiological studies. (C) 2011 Acoustical Society of America. [DOI: 10.1121/1.3651818]",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0789,Fine-grained pitch processing of music and speech in congenital amusia,"Congenital amusia is a lifelong disorder of music processing that has been ascribed to impaired pitch perception and memory. The present study tested a large group of amusics (n = 17) and provided evidence that their pitch deficit affects pitch processing in speech to a lesser extent: Fine-grained pitch discrimination was better in spoken syllables than in acoustically matched tones. Unlike amusics, control participants performed fine-grained pitch discrimination better for musical material than for verbal material. These findings suggest that pitch extraction can be influenced by the nature of the material (music vs speech), and that amusics' pitch deficit is not restricted to musical material, but extends to segmented speech events. (C) 2011 Acoustical Society of America. [DOI: 10.1121/1.3658447]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0790,Sine-wave speech recognition in a tonal language,"It is hypothesized that in sine-wave replicas of natural speech, lexical tone recognition would be severely impaired due to the loss of F0 information, but the linguistic information at the sentence level could be retrieved even with limited tone information. Forty-one native Mandarin-Chinese-speaking listeners participated in the experiments. Results showed that sine-wave tone-recognition performance was on average only 32.7% correct. However, sine-wave sentence-recognition performance was very accurate, approximately 92% correct on average. Therefore the functional load of lexical tones on sentence recognition is limited, and the high-level recognition of sine-wave sentences is likely attributed to the perceptual organization that is influenced by top-down processes. (C) 2012 Acoustical Society of America",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0791,Faciliation of Mandarin tone perception by visual speech in clear and degraded audio: Implications for cochlear implants,"Cochlear implant (CI) users in tone language environments report great difficulty in perceiving lexical tone. This study investigated the augmentation of simulated cochlear implant audio by visual (facial) speech information for tone. Native speakers of Mandarin and Australian English were asked to discriminate between minimal pairs of Mandarin tones in five conditions: Auditory-Only, Auditory-Visual, CI-simulated Auditory-Only, CI-simulated Auditory-Visual, and Visual-Only (silent video). Discrimination in CI-simulated audio conditions was poor compared with normal audio, and varied according to tone pair, with tone pairs with strong non-F0 cues discriminated the most easily. The availability of visual speech information also improved discrimination in the CI-simulated audio conditions, particularly on tone pairs with strong durational cues. In the silent Visual-Only condition, both Mandarin and Australian English speakers discriminated tones above chance levels. Interestingly, tone-naive listeners outperformed native listeners in the Visual-Only condition, suggesting firstly that visual speech information for tone is available, and may in fact be under-used by normal-hearing tone language perceivers, and secondly that the perception of such information may be language-general, rather than the product of language-specific learning. This may find application in the development of methods to improve tone perception in CI users in tone language environments. (C) 2012 Acoustical Society of America. [DOI: 10.1121/1.3672703]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0792,"INTONATION OF SOLO VIOLIN PERFORMANCE WITH REFERENCE TO EQUALLY TEMPERED, PYTHAGOREAN, AND JUST INTONATIONS","The purpose of this study was to deter-mine which musical scale best models the solo performances of violinists when they play diatonic scales of C major very slowly and without frequency vibrato as accurately as possible. Eight professional violinists played without stopping three adjacent scales in ascending order (from C4 to C7), followed immediately by an analogous return to the initial note C4 in descending order. Results show that when violin performances are analyzed taking into account the context of the scale in which they were played, Pythagorean and equally tempered scales are equally good models for the description of performances. Just intonation fits the data significantly less well. When the data analysis is limited to the intervals between separate pairs of notes, not taking into consideration the context of the scale in which they were played, observed interval sizes are almost identical to the arithmetic means of the corresponding interval sizes as defined in Pythagorean and equally tempered intonations. Octave intervals calculated by summing the sizes of performed adjacent major and minor seconds within octaves, show an average stretching of 3.9 cents. However, the arithmetic mean of computed interval sizes between two C's, deviates only 0.3 cents from the theoretical tonal extent of 1200 cents. Results suggest that when scales of C major are played, the tonic C is adopted as an absolute cognitive reference point.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0793,Speaker normalization in the perception of Mandarin Chinese tones,"This study investigated speaker normalization in perception of Mandarin tone 2 (midrising) and tone 3 (low-falling-rising) by examining listeners' use of F0 range as a cue to speaker identity. Two speakers were selected such that lone 2 of the low-pitched speaker and tone 3 of rite high-pitched speaker occurred at equivalent F0 heights. Production and perception experiments determined that turning paint (or inflection point of the tone), and Delta F0 (the difference in F0 between onset and turning point) distinguished the two tones. Three tone continua varying in either turning point, Delta F0, or both acoustic dimensions, were then appended to a natural precursor phrase from each of the two speakers. Results showed identification shifts such that identical stimuli were identified as low tones for the high precursor condition, but as high tones for the low precursor condition, Stimuli varying in turning point showed no significant shift, suggesting that listeners normalize only when the precursor varies in the same dimension as the stimuli, The magnitude of the shift was greater for stimuli varying only in Delta F0, as compared to stimuli varying in both turning point and Delta F0, indicating that normalization effects are reduced for stimuli more closely matching natural speech. (C) 1997 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0794,Pitch accent in spoken-word recognition in Japanese,"Three experiments addressed the question of whether pitch-accent information may be exploited in the process of recognizing spoken words in Tokyo Japanese. In a two-choice classification task, listeners judged from which of two words, differing in accentual structure, isolated syllables had been extracted (e.g., ka from baka HL or gaka LH); most judgments were correct, and listeners' decisions were correlated with the fundamental frequency characteristics of the syllables. In a gating experiment, listeners heard initial fragments of words and guessed what the words were; their guesses overwhelmingly had the same initial accent structure as the gated word even when only the beginning CV of the stimulus (e.g., na- from nagasa HLL or nagashi LHH) was presented. In addition, listeners were more confident in guesses with the same initial accent structure as the stimulus than in guesses with different accent. In a lexical decision experiment, responses to spoken words (e.g., ame HL) were speeded by previous presentation of the same word (e.g., ame HL) but not by previous presentation of a word differing only in accent (e.g., ame LH). Together these findings provide strong evidence that accentual information constrains the activation and selection of candidates for spoken-word recognition. (C) 1999 Acoustical Society of America. [S0001-4966(99)03003-9].",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0795,Analysis and synthesis of intonation using the Tilt model,"This paper introduces the Tilt intonational model and describes how this model can be used to automatically analyze and synthesize intonation. In the model, intonation is represented as a linear sequence of events, which can be pitch accents or boundary tones. Each event is characterized by continuous parameters representing amplitude, duration, and tilt (a measure of the shape of the event). The paper describes an event detector, in effect an intonational recognition system, which produces a transcription of an utterance's intonation. The features and parameters of the event detector are discussed and performance figures are shown on a variety of read and spontaneous speaker independent conversational speech databases. Given the event locations, algorithms are described which produce an automatic analysis of each event in terms of the Tilt parameters. Synthesis algorithms are also presented which generate F0 contours from Tilt representations. The accuracy of these is shown by comparing synthetic F0 contours to real F0 contours. The paper concludes with an extensive discussion on linguistic representations of intonation and gives evidence that the Tilt model goes a long way to satisfying the desired goals of such a representation in that it has the right number of degrees of freedom to be able to describe and synthesize intonation accurately. (C) 2000 Acoustical Society of America. [S0001-4966(00)01802-6].",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0796,The influence of linguistic and musical experience on Cantonese word learning,"Adult non-native speech perception is subject to influence from multiple factors, including linguistic and extralinguistic experience such as musical training. The present research examines how linguistic and musical factors influence non-native word identification and lexical tone perception. Groups of native tone language (Thai) and non-tone language listeners (English), each subdivided into musician and non-musician groups, engaged in Cantonese tone word training. Participants learned to identify words minimally distinguished by five Cantonese tones during training, also completing musical aptitude and phonemic tone identification tasks. First, the findings suggest that either musical experience or a tone language background leads to significantly better non-native word learning proficiency, as compared to those with neither musical training nor tone language experience. Moreover, the combination of tone language and musical experience did not provide an additional advantage for Thai musicians above and beyond either experience alone. Musicianship was found to be more advantageous than a tone language background for tone identification. Finally, tone identification and musical aptitude scores were significantly correlated with word learning success for English but not Thai listeners. These findings point to a dynamic influence of musical and linguistic experience, both at the tone dentification level and at the word learning stage. (C) 2012 Acoustical Society of America. [http://dx.doi.org/10.1121/1.4714355]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0797,Post-low bouncing in Mandarin Chinese: Acoustic analysis and computational modeling,"Post-low bouncing is a phenomenon whereby after reaching a very low pitch in a low lexical tone, F-0 bounces up and then gradually drops back in the following syllables. This paper reports the results of an acoustic analysis of the phenomenon in two Mandarin Chinese corpora and presents a simple mechanical model that can effectively simulate this bouncing effect. The acoustic analysis shows that most of the F-0 dynamic features profiling the bouncing effect strongly correlate with the amount of F-0 lowering in the preceding low-tone syllable, and that the additional F-0 raising commences at the onset of the first post-low syllable. Using the quantitative Target Approximation model, this bouncing effect was simulated by adding an acceleration adjustment to the initial F-0 state of the first post-low syllable. A highly linear relation between F-0 lowering and estimated acceleration adjustment was found. This relation was then used to effectively simulate the bouncing effect in both the neutral tone and the full tones. The results of the analysis and simulation are consistent with the hypothesis that the bouncing effect is due to a temporary perturbation of the balance between antagonistic forces in the laryngeal control in producing a very low pitch. (C) 2012 Acoustical Society of America. [http://dx.doi.org/10.1121/1.4725762]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0798,Unequal effects of speech and nonspeech contexts on the perceptual normalization of Cantonese level tones,"Context is important for recovering language information from talker-induced variability in acoustic signals. In tone perception, previous studies reported similar effects of speech and nonspeech contexts in Mandarin, supporting a general perceptual mechanism underlying tone normalization. However, no supportive evidence was obtained in Cantonese, also a tone language. Moreover, no study has compared speech and nonspeech contexts in the multi-talker condition, which is essential for exploring the normalization mechanism of inter-talker variability in speaking F0. The other question is whether a talker's full F0 range and mean F0 equally facilitate normalization. To answer these questions, this study examines the effects of four context conditions (speech/nonspeech x F0 contour/mean F0) in the multi-talker condition in Cantonese. Results show that raising and lowering the F0 of speech contexts change the perception of identical stimuli from mid level tone to low and high level tone, whereas nonspeech contexts only mildly increase the identification preference. It supports the speech-specific mechanism of tone normalization. Moreover, speech context with flattened F0 trajectory, which neutralizes cues of a talker's full F0 range, fails to facilitate normalization in some conditions, implying that a talker's mean F0 is less efficient for minimizing talker-induced lexical ambiguity in tone perception. (C) 2012 Acoustical Society of America. [http://dx.doi.org/10.1121/1.4731470]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0799,The separation between music and speech: Evidence from the perception of Cantonese tones,"This study investigates the relationship between music and speech, testing whether musical training has any facilitatory effects on native tone language speakers. Some Cantonese tone pairs are merging in recent years. The merging subjects have poorer general lexical tone perception than the control subjects. Previous studies showed that musical training facilitates lexical tone perception of nontone language speakers, but it is unclear if the same is true for tone language speakers. Three groups of listeners (standard Cantonese, merging Cantonese, nontone) with and without musical training participated in AX discrimination tasks of Cantonese monosyllables and pure tones resynthesized from Cantonese lexical tones. Results show that while musical training enhances lexical tone perception of nontone listeners, it has little influence on Cantonese listeners. The findings suggest that the linguistic use of tones is more fundamental and more robust than musical tones. Our results are compatible with the idea that linguistic and musical mechanisms belong to separate but overlapping domains. (C) 2012 Acoustical Society of America. [http://dx.doi.org/10.1121/1.4747010]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0800,Toddlers' recognition of noise-vocoded speech,"Despite their remarkable clinical success, cochlear-implant listeners today still receive spectrally degraded information. Much research has examined normally hearing adult listeners' ability to interpret spectrally degraded signals, primarily using noise-vocoded speech to simulate cochlear implant processing. Far less research has explored infants' and toddlers' ability to interpret spectrally degraded signals, despite the fact that children in this age range are frequently implanted. This study examines 27-month-old typically developing toddlers' recognition of noise-vocoded speech in a language-guided looking study. Children saw two images on each trial and heard a voice instructing them to look at one item (""Find the cat!""). Full-spectrum sentences or their noise-vocoded versions were presented with varying numbers of spectral channels. Toddlers showed equivalent proportions of looking to the target object with full-speech and 24- or 8-channel noise-vocoded speech; they failed to look appropriately with 2-channel noise-vocoded speech and showed variable performance with 4-channel noise-vocoded speech. Despite accurate looking performance for speech with at least eight channels, children were slower to respond appropriately as the number of channels decreased. These results indicate that 2-yr-olds have developed the ability to interpret vocoded speech, even without practice, but that doing so requires additional processing. These findings have important implications for pediatric cochlear implantation. (C) 2013 Acoustical Society of America. [http://dx.doi.org/10.1121/1.4770241]",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0801,On-line perception of Mandarin Tones 2 and 3: Evidence from eye movements,"Using the visual world paradigm, the present study investigated on-line processing of fine-grained pitch information prior to lexical access in a tone language; specifically how lexical tone perception of Mandarin Tones 2 and 3 was influenced by the pitch height of the tone at onset, turning point, and offset. Native speakers of Mandarin listened to manipulated tone tokens and selected the corresponding word from four visually presented words (objects in Experiment 1 and characters in Experiment 2) while their eye movements were monitored. The results showed that 87% of ultimate tone judgments were made according to offset pitch height. Tokens with high offset pitch were identified as Tone 2, and low offset pitch as Tone 3. A low turning point pitch served as a pivotal cue for Tone 3, and prompted more eye fixations on Tone 3 items, until the offset pitch directed significantly more fixations to the final tone choice. The findings support the view that lexical tone perception is an incremental process, in which pitch height at critical points serves as an important cue. (C) 2013 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0802,Effects of tone training on Cantonese tone-word learning,The present study examined the effect of improving lexical tone identification abilities on Cantonese tone-word learning. Native English non-musicians received training on Cantonese tones before learning the meanings of words distinguished by these tones. Their results were compared to English non-musicians and musicians who received no tone training. The tone-trainees obtained a similar level of word identification proficiency as musicians by the end of training and were significantly better than non-tone trained non-musicians. These results lend support for phonetic-phonological-lexical continuity in learning because enhancing listeners' perception of lower-level tonal information significantly contributed to success in a higher-level linguistic task. (C) 2013 Acoustical Society of America,,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0803,Just noticeable difference of tone pitch contour change for English- and Chinese-native listeners,"Just noticeable differences of tone pitch contour discrimination were examined for young English- and Mandarin Chinese-native listeners to examine categorical features of tone perception for the two groups of listeners. Three types of stimuli were used: A Mandarin Chinese vowel, an English vowel, and tonal glides. Level, rising, and falling tones within or across tone boundaries served as the standard stimuli to measure thresholds of tone pitch discrimination. Performance was equivalent between English-and Chinese-native listeners for level tones, but significantly differed for rising and falling tones, regardless of the type of stimuli. English listeners showed significantly lower thresholds at the offset of F0 shifts than Chinese listeners, while Chinese listeners discriminated tone pitch changes at the onset with significantly lower thresholds than their English peers. These psychophysical results, combined with tone perception reported in other studies, indicated that Mandarin-native listeners perceived lexical tones in a categorical manner, based on their lexical tone experience, whereas English-native listeners perceived tones on a psychophysical base. (C) 2013 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0804,Perception of pitch height in lexical and musical tones by English-speaking musicians and nonmusicians,"The purpose of this study was to explore the music-speech relationship by examining pitch height perception in lexical and musical tones. English-speaking musicians and nonmusicians identified multispeaker Taiwanese level tones without typical cues for speaker normalization. The musicians also identified note names of piano, viola, and pure tones without a reference pitch. In the Taiwanese task, both the musicians and nonmusicians were able to identify tone height above chance, but only for tones at the extremes of the speakers' overall vocal range. The musicians only had a slight advantage over the nonmusicians. In the music task, none of the musicians met the criterion for absolute pitch. Timbre did not affect how accurately the musical tones were identified. No correlations were found between performance in the Taiwanese task and that in the music task. It was concluded that musicians' advantage in lexical tone perception arose from the ability to track F0 contours. The ability to identify pitch height in lexical tones appears to involve calibrating acoustic input according to gender-specific, internally stored pitch templates. (C) 2014 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0805,The effect of language experience on perceptual normalization of Mandarin tones and non-speech pitch contours,"Context-dependent pitch perception helps listeners recognize tones produced by speakers with different fundamental frequencies (f0s). The role of language experience in tone normalization remains unclear. In this cross-language study of tone normalization, native Mandarin and English listeners were asked to recognize Mandarin Tone 1 (high-flat) and Tone 2 (mid-rising) with a preceding Mandarin sentence. To further test whether context-dependent pitch perception is speech-specific or domain-general, both language groups were asked to identify non-speech flat and rising pitch contours with a preceding non-speech flat pitch contour. Results showed that both Mandarin and English listeners made more rising responses with non-speech than with speech stimuli, due to differences in spectral complexity and listening task between the two stimulus types. English listeners made more rising responses than Mandarin listeners with both speech and non-speech stimuli. Contrastive context effects (more rising responses in the high-f0 context than in the low-f0 context) were found with both speech and non-speech stimuli for Mandarin listeners, but not for English listeners. English listeners' lack of tone experience may have caused more rising responses and limited use of context f0 cues. These results suggest that context-dependent pitch perception in tone normalization is domain-general, but influenced by long-term language experience. (C) 2014 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0806,The effect of context duration on Mandarin listeners' tone normalization,"Tone normalization has been observed in Mandarin listeners, who contrastively adjust tone recognition using context pitch cues. This study tested the effect of context duration on Mandarin tone normalization. The target tones varied from Tone 1 (high-flat) to Tone 2 (mid-rising). The preceding phrase was modified to have different durations with 160- or 200-Hz mean fundamental frequencies (F0s). The results showed that the high-F0 context elicited significantly more Tone-2 responses than the low-F0 context, even when the contexts were 125 ms. The contrastive context effect saturated with the 250-ms contexts, indicating a 250-ms critical context duration for robust tone normalization. (C) 2014 Acoustical Society of America",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0807,The role of spectro-temporal fine structure cues in lexical-tone discrimination for French and Mandarin listeners,"The role of spectro-temporal modulation cues in conveying tonal information for lexical tones was assessed in native-Mandarin and native-French adult listeners using a lexical-tone discrimination task. The fundamental frequency (F0) of Thai tones was either degraded using an 8-band vocoder that reduced fine spectral details and frequency-modulation cues, or extracted and used to modulate the F0 of click trains. Mandarin listeners scored lower than French listeners in the discrimination of vocoded lexical tones. For click trains, Mandarin listeners outperformed French listeners. These preliminary results suggest that the perceptual weight of the fine spectro-temporal modulation cues conveying F0 information is enhanced for adults speaking a tonal language. (C) 2014 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0808,Effect of musical experience on learning lexical tone categories,"Previous studies suggest that musicians show an advantage in processing and encoding foreign-language lexical tones. The current experiments examined whether musical experience influences the perceptual learning of lexical tone categories. Experiment I examined whether musicians with no prior experience of tonal languages differed from nonmusicians in the perception of a lexical tone continuum. Experiment II examined whether short-term perceptual training on lexical tones altered the perception of the lexical tone continuum differentially in English-speaking musicians and nonmusicians. Results suggested that (a) musicians exhibited higher sensitivity overall to tonal changes, but perceived the lexical tone continuum in a manner similar to nonmusicians (continuously), in contrast to native Mandarin speakers (categorically); and (b) short-term perceptual training altered perception; however, there were no significant differences between the effects of training on musicians and nonmusicians. (C) 2015 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0809,Just-noticeable difference of tone pitch contour change for Mandarin congenital amusics,"Congenital amusia is a neuro-developmental disorder that may affect the processing of both music pitch and lexical tone. In the present study, the just-noticeable differences (JNDs) of tone pitch contour change were examined for three groups of Mandarin-native listeners: amusics with (tone agnosics) and without lexical tone difficulties (pure amusics), and matched controls. Tone agnosics showed significantly larger JNDs than normal controls, while pure amusics performed comparably with the controls. These results suggest that only those amusics with behavioral lexical tone deficits might be psychophysically impaired in pitch contour discrimination. (C) 2015 Acoustical Society of America",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0810,Higher-level linguistic categories dominate lower-level acoustics in lexical tone processing,"Native tonal-language speakers exhibit reduced sensitivity to lexical tone differences within, compared to across, categories (higher-level linguistic category influence). Yet, sensitivity is enhanced among musically trained, non-tonal-language-speaking individuals (lower-level acoustics processing influence). The current study investigated the relative contribution of higher- and lower-level influences when both are present. Seventeen Mandarin musicians completed music pitch and lexical tone discrimination tasks. Similar to English musicians [Zhao and Kuhl (2015). J. Acoust. Soc. Am. 137(3), 1452-1463], Mandarin musicians' overall sensitivity to lexical tone differences was associated with music pitch score, suggesting lower-level contributions. However, the musician's sensitivities to lexical tone pairs along a continuum were similar to Mandarin non-musicians, reflecting dominant higher- level influences. (C) 2015 Acoustical Society of America",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0811,Context effects on second-language learning of tonal contrasts,"Studies of lexical tone learning generally focus on monosyllabic contexts, while reports of phonetic learning benefits associated with input variability are based largely on experienced learners. This study trained inexperienced learners on Mandarin tonal contrasts to test two hypotheses regarding the influence of context and variability on tone learning. The first hypothesis was that increased phonetic variability of tones in disyllabic contexts makes initial tone learning more challenging in disyllabic than monosyllabic words. The second hypothesis was that the learnability of a given tone varies across contexts due to differences in tonal variability. Results of a word learning experiment supported both hypotheses: tones were acquired less successfully in disyllables than in monosyllables, and the relative difficulty of disyllables was closely related to contextual tonal variability. These results indicate limited relevance of monosyllable-based data on Mandarin learning for the disyllabic majority of the Mandarin lexicon. Furthermore, in the short term, variability can diminish learning; its effects are not necessarily beneficial but dependent on acquisition stage and other learner characteristics. These findings thus highlight the importance of considering contextual variability and the interaction between variability and type of learner in the design, interpretation, and application of research on phonetic learning. (C) 2015 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0812,Varying irrelevant phonetic features hinders learning of the feature being trained,"Learning to distinguish nonnative words that differ in a critical phonetic feature can be difficult. Speech training studies typically employ methods that explicitly direct the learner's attention to the relevant nonnative feature to be learned. However, studies on vision have demonstrated that perceptual learning may occur implicitly, by exposing learners to stimulus features, even if they are irrelevant to the task, and it has recently been suggested that this task-irrelevant perceptual learning framework also applies to speech. In this study, subjects took part in a seven-day training regimen to learn to distinguish one of two nonnative features, namely, voice onset time or lexical tone, using explicit training methods consistent with most speech training studies. Critically, half of the subjects were exposed to stimuli that varied not only in the relevant feature, but in the irrelevant feature as well. The results showed that subjects who were trained with stimuli that varied in the relevant feature and held the irrelevant feature constant achieved the best learning outcomes. Varying both features hindered learning and generalization to new stimuli. (C) 2016 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0813,The influence of native-language tones on lexical access in the second language,"When listening to speech in a second language, bilinguals' perception of acoustic-phonetic properties is often influenced by the features that are important in the native language of the bilingual. Furthermore, changes in the perception of segmental contrasts due to L1 experience can influence L2 lexical access during comprehension. The present study investigates whether the effect of L1 experience on L2 processing seen at the segmental level extends to suprasegmental processing. In an eye-tracking task, Mandarin-English bilinguals heard an auditorily presented English word and selected which of two visually presented Chinese characters represented the correct Mandarin translation. The pitch contour of the spoken word was manipulated to either match or mismatch the lexical tone of the Mandarin translation. Results revealed that bilinguals were significantly faster to correctly identify the target and made earlier eye movements to targets when the suprasegmental information of the word spoken in English matched that of its Mandarin translation. The findings provide compelling evidence for bilinguals' sensitivity to suprasegmental tone information, even when listening to a non-tonal language. These results have important implications for the effect of L1 experience on L2 lexical access and language interaction in bilinguals, and are consistent with a highly interactive account of language processing. (C) 2016 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0814,Internal and contextual cues to tone perception in Medumba,"This study presents results of an identification experiment with speakers of Medimba, a Grassfields Bantu language, aimed at evaluating the relative effects of f(0) and duration in cuing tonal contrasts, as well as the role of lexical vs non-speech pitch contexts in biasing tone perception. Results show that duration is a cue for tone perception, with the influence of duration strongest where target f(0) values were lower. Lexical tone perception is also sensitive to the identity of a preceding lexical tone, but not to the presence of a preceding non-speech pure tone. (C) 2016 Acoustical Society of America",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0815,Pitch perception and production in congenital amusia: Evidence from Cantonese speakers,"This study investigated pitch perception and production in speech and music in individuals with congenital amusia (a disorder of musical pitch processing) who are native speakers of Cantonese, a tone language with a highly complex tonal system. Sixteen Cantonese-speaking congenital amusics and 16 controls performed a set of lexical tone perception, production, singing, and psychophysical pitch threshold tasks. Their tone production accuracy and singing proficiency were subsequently judged by independent listeners, and subjected to acoustic analyses. Relative to controls, amusics showed impaired discrimination of lexical tones in both speech and non-speech conditions. They also received lower ratings for singing proficiency, producing larger pitch interval deviations and making more pitch interval errors compared to controls. Demonstrating higher pitch direction identification thresholds than controls for both speech syllables and piano tones, amusics nevertheless produced native lexical tones with comparable pitch trajectories and intelligibility as controls. Significant correlations were found between pitch threshold and lexical tone perception, music perception and production, but not between lexical tone perception and production for amusics. These findings provide further evidence that congenital amusia is a domain-general language-independent pitch-processing deficit that is associated with severely impaired music perception and production, mildly impaired speech perception, and largely intact speech production. (C) 2016 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0816,Categorical perception of lexical tones by English learners of Mandarin Chinese,"Whether native speakers of non-tonal languages can acquire categorical representations of lexical tones remains controversial. This study investigates the acquisition of lexical tone categories by native English speakers learning Mandarin Chinese as a foreign language by comparing the categorical perception (CP) of lexical tones between three groups of listeners: (1) native English speakers who had taken advanced Mandarin courses in colleges; (2) native English speakers with no knowledge of Mandarin Chinese; and (3) native Mandarin speakers. Two tonal continua derived from natural speech within carrier phrases were created through interpolation within two tonal contrasts (tone 1/tone 4, T1/T4; tone 2/tone 3, T2/T3). Results showed categorical-like perception of tones by native Mandarin speakers. The inexperienced English speakers performed near chance on discrimination tasks and showed significantly broader identification boundaries than the other two groups. The learners of Mandarin showed similar pattern of CP to native Mandarin speakers, but with higher overall discrimination scores. Findings suggest that CP of lexical tone may be available to advanced second language learners. (C) 2016 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0817,Low-frequency fine-structure cues allow for the online use of lexical stress during spoken-word recognition in spectrally degraded speech,"English listeners use suprasegmental cues to lexical stress during spoken-word recognition. Prosodic cues are, however, less salient in spectrally degraded speech, as provided by cochlear implants. The present study examined how spectral degradation with and without low-frequency fine-structure information affects normal-hearing listeners' ability to benefit from suprasegmen-tal cues to lexical stress in online spoken-word recognition. To simulate electric hearing, an eight-channel vocoder spectrally degraded the stimuli while preserving temporal envelope infor-mation. Additional lowpass-filtered speech was presented to the opposite ear to simulate bimodal hearing. Using a visual world paradigm, listeners' eye fixations to four printed words (target, competitor, two distractors) were tracked, while hearing a word. The target and competi-tor overlapped segmentally in their first two syllables but mismatched suprasegmentally in their first syllables, as the initial syllable received primary stress in one word and secondary stress in the other (e. g., ""admiral,"" ""admi'ration""). In the vocoder-only condition, listeners were unable to use lexical stress to recognize targets before segmental information disambiguated them from competitors. With additional lowpass-filtered speech, however, listeners efficiently processed prosodic information to speed up online word recognition. Low-frequency fine-structure cues in simulated bimodal hearing allowed listeners to benefit from suprasegmental cues to lexical stress during word recognition. (C) 2017 Acoustical Society of America.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0818,Normalization of lexical tones and nonlinguistic pitch contours: Implications for speech-specific processing mechanism,"Context is indispensable for accurate tone perception, especially when the target tone system is as complex as that of Cantonese. However, not all contexts are equally beneficial. Speech contexts are usually more effective in improving lexical tone identification than nonspeech contexts matched in pitch information. Some potential factors which may contribute to these unequal effects have been proposed but, thus far, their plausibility remains unclear. To shed light on this issue, the present study compares the perception of lexical tones and their nonlinguistic counterparts under specific contextual (speech, nonspeech) and attentional (with/without focal attention) conditions. The results reveal a prominent congruency effect-target sounds tend to be identified more accurately when embedded in contexts of the same nature (speech/nonspeech). This finding suggests that speech and nonspeech sounds are partly processed by domain-specific mechanisms and that information from the same domain can be integrated more effectively than that from different domains. Therefore, domain-specific processing of speech could be the most likely cause of the unequal context effect. Moreover, focal attention is not a prerequisite for extracting contextual cues from speech and nonspeech during perceptual normalization. This finding implies that context encoding is highly automatic for native listeners. (C) 2017 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0819,Pitch shape modulates the time course of tone vs pitch-accent identification in Mandarin Chinese,"In Mandarin Chinese pitch is used to express both lexical meanings via tones and sentence-level meanings via pitch-accents raising the question of which information is processed first. While research with meaningful sentence materials suggested a general processing advantage of tone over pitch-accents, research on pure tones and nonce speech in pre-attentive processing found that the f0-shape led to timing and site processing differences. The current study reconciles these results by exploring whether the tone advantage found in meaningful speech materials is modulated by the f0-shape by establishing via a gating paradigm the relative timing of tone and pitch-accent identification. Target words containing static (T1) and dynamic (T2, T4) tones were embedded into meaningful sentences and were divided into 50 ms gates which were added incrementally either from the left-or right-edge of the target word. Results showed that dynamic targets had either a tone or pitch-accent advantage contingent on the direction of gate processing. In contrast, for static T1 targets, tone and pitch-accent were identified simultaneously regardless of the direction of gate processing. Altogether, these results indicate that the f0-shape, as defined by pitch dimensions of f0 and pitch range, mediates the timing of tone and pitch-accent identification in meaningful speech supporting highly interactive models of speech perception. (C) 2017 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0820,Relative contributions of acoustic temporal fine structure and envelope cues for lexical tone perception in noise,"Previous studies have shown that lexical tone perception in quiet relies on the acoustic temporal fine structure (TFS) but not on the envelope (E) cues. The contributions of TFS to speech recognition in noise are under debate. In the present study, Mandarin tone tokens were mixed with speech-shaped noise (SSN) or two-talker babble (TTB) at five signal-to-noise ratios (SNRs; -18 to +6 dB). The TFS and E were then extracted from each of the 30 bands using Hilbert transform. Twenty-five combinations of TFS and E from the sound mixtures of the same tone tokens at various SNRs were created. Twenty normal-hearing, native-Mandarin-speaking listeners participated in the tone-recognition test. Results showed that tone-recognition performance improved as the SNRs in either TFS or E increased. The masking effects on tone perception for the TTB were weaker than those for the SSN. For both types of masker, the perceptual weights of TFS and E in tone perception in noise was nearly equivalent, with E playing a slightly greater role than TFS. Thus, the relative contributions of TFS and E cues to lexical tone perception in noise or in competing-talker maskers differ from those in quiet and those to speech perception of non-tonal languages. (C) 2017 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0821,Beyond lexical meaning: The effect of emotional prosody on spoken word recognition,"This study employs an auditory-visual associative priming paradigm to test whether non-emotional words uttered in emotional prosody (e.g., pineapple spoken in angry prosody or happy prosody) facilitate recognition of semantically emotional words (e.g., mad, upset or smile, joy). The results show an affective priming effect between emotional prosody and emotional words independent of lexical carriers of the prosody. Learned acoustic patterns in speech (e.g., emotional prosody) map directly to social concepts and representations, and this social information influences the spoken word recognition process. (C) 2017 Acoustical Society of America",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0822,Intelligibility of naturally produced and synthesized Mandarin speech by cochlear implant listeners,"Mandarin is a tonal language, and it is important to preserve lexical tone information in synthesized speech. With natural speech, Chinese cochlear implant (CI) users have difficulty perceiving voice pitch cues important for lexical tone perception; it is unclear whether this difficulty persists in Mandarin synthesized speech. In this study, intelligibility of naturally produced and synthesized Mandarin speech was measured in Chinese CI listeners; intelligibility was also measured in a control group of normal-hearing (NH) listeners. Five synthesized voices were selected to represent different talker genders (male, female, child), speaking rates (normal, slow), and speaking styles (emotional, accent). The data showed that while modern Mandarin text-to-speech (TTS) systems can provide perfect speech intelligibility for NH listeners, overall intelligibility was much poorer for CI than for NH listeners. CI performance was significantly poorer with synthesized speech than with natural speech (p< 0.001). CI listeners were highly sensitive to the ""extra-atypical"" synthesized emotional and accented speech. Performance with each of the synthesized speech types was significantly correlated with performance with natural speech in CI users (p< 0.01 in all cases). While modern TTS systems offer educational and communication benefits to CI users and hearing-impaired individuals, the selection of synthesized voices should be carefully considered in education applications of TTS for hearing-impaired individuals, especially CI children, since poor intelligibility performance may affect language learning. (C) 2018 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0823,Effects of fundamental frequency contour on understanding Mandarin sentences in bimodal hearing simulations,"Fundamental frequency (F0) contour carries important information for understanding a tonal language. The present work assessed the effects of F0 contour on understanding Mandarin sentences in bimodal hearing simulations, including three conditions of acousticonly, electric-only, and combined stimulations. Test stimuli were synthesized Mandarin sentences, each word with a normal, flat, or randomly assigned lexical tone, and presented to normal-hearing Mandarin-speaking listeners to recognize. Experimental results showed that changing F0 contour significantly affected the perception of Mandarin sentences under all conditions of acoustic-only, electric-only, and combined stimulations. The combined-stimulation advantage was only observed for test stimuli with the normal F0 contour. (C) 2018 Acoustical Society of America",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0824,Impaired perceptual normalization of lexical tones in Cantonese-speaking congenital amusics,"Human listeners perceive speech sounds relative to acoustic cues in context. In this study the authors examined how congenital amusia, a pitch-processing disorder, affects perceptual normalization of lexical tones according to the distribution of F0 cues in context. Sixteen Cantonese-speaking amusics and 16 controls were tested on the effects of shifting F0 level in four types of contexts on tone perception: nonspeech, reversed speech, semantically anomalous speech, and meaningful speech contexts. Performance of controls replicated previous studies, showing contrastive changes of tone perception according to the shifted F0 level of anomalous and meaningful contexts, which were native speech contexts with phonological cues to estimate a talker's tone space. Effects of nonspeech and reversed contexts were small and inconsistent, and tone perception performance varied depending on the typicality of a talker's F0 range. In contrast to controls, amusics showed reduced context effects in anomalous and meaningful contexts, but largely comparable context effects in nonspeech and reversed contexts, indicating a deficit of amusics in tone normalization through phonological cues in native speech contexts. These findings suggest that the ability to perceive speech sounds relative to acoustic cues in context is not a universal endowment, and that this ability is impaired substantially in amusics. (C) 2018 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0825,Tone language experience modulates the effect of long-term musical training on musical pitch perception,"Long-term musical training is widely reported to enhance music pitch perception. However, it remains unclear whether tone language experience influences the effect of long-term musical training on musical pitch perception. The present study addressed this question by testing 30 Cantonese and 30 non-tonal language speakers, each divided equally into musician and non-musician groups, on pitch height and pitch interval discrimination. Musicians outperformed non-musicians among non-tonal language speakers, but not among Cantonese speakers on the pitch height discrimination task. However, musicians outperformed non-musicians among Cantonese speakers, but not among non-tonal language speakers on the pitch interval discrimination task. These results suggest that the effect of long-term musical training on musical pitch perception is shaped by tone language experience and varies across different pitch perception tasks. (C) 2018 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0826,Effects of age and duration of deafness on Mandarin speech understanding in competing speech by normal-hearing and cochlear implant children,"Due to poor perception of fundamental frequency (F0) cues that are important for lexical tone perception and talker segregation, pediatric Chinese cochlear implant (CI) users may be especially susceptible to informational masking. Here, speech recognition thresholds (SRTs) were measured in steady noise or competing speech in Mandarin-speaking CI and normal-hearing (NH) children. CI children were more susceptible to informational masking and were unable to use F0 cues to segregate talkers. SRTs were significantly correlated with chronological age in NH children and with duration of deafness in CI children, suggesting that auditory deprivation may limit developmental processes important for talker segregation. (C) 2018 Acoustical Society of America",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0827,Lower-level acoustics underlie higher-level phonological categories in lexical tone perception,"The pitch-processing deficit associated with congenital amusia has been shown to be transferable to lexical tone processing. However, it remains unclear whether the tone perception difficulties of amusics are merely due to the domain-general deficit in acoustic processing or additionally caused by impaired higher-level phonological operations. Answers to this question can shed light on the influence of lower-level acoustic processing on higher-level phonological processing. Using a modified categorical perception paradigm, the present study indicates that the acoustic processing deficit systematically extends to higher-level phonological processing. These findings suggest that lower-level acoustics underlie higher-level phonological categories in lexical tone perception. (C) 2018 Acoustical Society of America",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0828,Context integration deficit in tone perception in Cantonese speakers with congenital amusia,"Congenital amusia is a neuro-developmental disorder of pitch processing. This study investigated how this deficit affects lexical tone perception with and without context. Twenty-three Cantonese-speaking amusics and 23 controls were tested on the identification of high-variation tone stimuli in isolation vs in a carrier sentence. The controls generally achieved a higher accuracy with context than in isolation, suggesting that speech context facilitated tone identification. In contrast, amusics generally failed to benefit from the context, despite some variation among different tones. These findings provide insights into the underlying deficits of amusia, revealing a context integration deficit of tone perception in amusia. (C) 2018 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0829,Pitch-shift responses as an online monitoring mechanism during level tone production,"This paper investigates whether pitch-shift responses can be modulated as a function of level tone height in Taiwanese Southern Min (TSM). Twenty-six native TSM speakers were recruited and asked to produce three TSM words that differed in tone on the first syllable but had the same mid-level tone on the second syllable (hence, HM, MM, and LM). The pitch-shift stimuli appeared at 100 ms after vocalization onset and lasted for 200 ms. The magnitudes of the pitch-shift stimuli were +/- 250 cents for HM, +250/-150 cents for MM, and +/- 150 cents for LM, in order to overlap the shifted pitch with another lexical tone. The results show that larger pitch-shift peak amplitudes were elicited when the H level tone of the HM word was downshifted 250 cents to the M level and when the L level tone of the LM word was upshifted 150 cents to the M level tone. However, no significant direction effect was found for the MM word. The M level tone might be perceived non-categorically by native TSM speakers. Overall, the findings suggest that the magnitudes of pitch-shift responses may have to do with the degree of categorical perception. (C) 2019 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0830,The influence of tone language experience and speech style on the use of intonation in language discrimination,"This work tests whether listeners' use of suprasegmental information in speech perception is modulated by language background and speech style. Native Mandarin (tone language) and Malay (non-tone language) listeners completed an AX language discrimination task with four levels of signal degradation and two speech styles. Listeners in both groups showed more benefit from pitch information in read than in spontaneous speech. Mandarin listeners showed a greater benefit than Malay listeners from the inclusion of f0 information in a segmentally degraded signal, suggesting that experience with lexical tone may extend to increased attention and/or sensitivity to phrase-level pitch contours. (C) 2019 Acoustical Society of America",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0831,The effect of F0 contour on the intelligibility of Mandarin Chinese for hearing-impaired listeners,"Dynamic F0 contour plays an important role in recognizing speech. The present work examined the effect of F0 contour on speech intelligibility for hearing-impaired listeners for Mandarin Chinese in quiet, in steady noise, and in two-talker competing speech. The intelligibility of two types of natural speech was measured: single-Tone speech with relatively flat F0 contours and multi-Tone speech with time-varying F0 contours. The speech rate and mean F0 of speech materials were carefully controlled to avoid effects other than F0 contour on the speech intelligibility. Results showed that intelligibility was significantly higher for speech with a flat F0 contour than that with a dynamic F0 contour at a low signal-to-masker ratio in both speech-spectrum noise and two-talker masker.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0832,Contributions of lexical tone to Mandarin sentence recognition in hearing-impaired listeners under noisy conditions,"Mandarin sentence recognition using natural-tone and flat-tone sentences was tested in 22 subjects with sensorineural hearing loss (SNHL) and 25 listeners with normal hearing (NH) in quiet, speech-shaped noise, and two-talker-babble conditions. While little effects of flat tones on sentence recognition were seen in the NH listeners when the signal-to-noise ratio (SNR) was >= 0dB, the SNHL listeners showed decreases in flat-tone-sentence recognition in quiet and at +5-dB SNR. Such declined performance was correlated with their degrees of hearing loss. Lexical tone contributes greatly to sentence recognition in hearing-impaired listeners in both quiet and in noise listening conditions.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0833,Discrimination and identification of lexical tones and consonants in Mandarin-speaking children using cochlear implants,"Mandarin-speaking adults using cochlear implants (CI) experience more difficulties in perceiving lexical tones than consonants. This problem may result from the fact that CIs provide relatively sufficient temporal envelope information for consonant perception in quiet environments, but do not convey the fine spectro-temporal information considered to be necessary for accurate pitch perception. Another possibility is that Mandarin speakers with post-lingual hearing loss have developed language-specific use of these acoustic cues, impeding lexical tone processing under CI conditions. To investigate this latter hypothesis, syllable discrimination and word identification abilities for Mandarin consonants (place and manner) and lexical-tone contrasts (tones 1 vs 3 and 1 vs 2) were measured in 15 Mandarin-speaking children using CIs and age-matched children with normal hearing (NH). In the discrimination task, only children using CIs exhibited significantly lower scores for consonant place contrasts compared to other contrasts, including lexical tones. In the word identification task, children using CIs showed lower performance for all contrasts compared to children with NH, but they both showed specific difficulties with tone 1 vs 2 contrasts. This study suggests that Mandarin-speaking children using CIs are able to discriminate and identify lexical tones and, perhaps more surprisingly, have more difficulties when discriminating consonants. (C) 2019 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0834,Production-perception relationship of Mandarin tones as revealed by critical perceptual cues,"The relationship of lexical tone production and perception has not been well studied. Using Mandarin tone, this research tests the hypothesis that a production-perception link is revealed by critical perceptual cues. The critical status of perceptual tonal cues was determined by perceptual cue weights, showing fundamental frequency (F0) contour as being more critical than height. Then, tone production features were examined for critical F0 contour (slope, curvature, turning-point location) and non-critical F0 height (mean, onset) cues. A production-perception correlation was found for F0 contour but not height cues, suggesting that critical perceptual cues dictate the relationship between production and perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0835,Mandarin tone perception in multiple-talker babbles and speech-shaped noise,"Lexical tone recognition in multiple-talker babbles (N = 1, 2, 4, 8, 10, or 12) and in speech-shaped noise at different signal-to-noise ratios (SNRs = -18 to -6 dB) were tested in 30 normal-hearing native Mandarin-speaking listeners. Results showed that tone perception was robust to noise. The performance curve as a function of N was non-monotonic. The breakpoint at which the performance plateaued was N = 8 for all SNRs tested with a slight improvement at N > 8 at -6 and -9 dB SNR.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0836,Effects of vowel coproduction on the timecourse of tone recognition,"Vowel contrasts tend to be perceived independently of pitch modulation, but it is not known whether pitch can be perceived independently of vowel quality. This issue was investigated in the context of a lexical tone language, Mandarin Chinese, using a printed word version of the visual world paradigm. Eye movements to four printed words were tracked while listeners heard target words that differed from competitors only in tone (test condition) or also in onset consonant and vowel (control condition). Results showed that the timecourse of tone recognition is influenced by vowel quality for high, low, and rising tones. For these tones, the time for the eyes to converge on the target word in the test condition (relative to control) depended on the vowel with which the tone was coarticulated with /a/ and /i/ supporting faster recognition of high, low, and rising tones than /u/. These patterns are consistent with the hypothesis that tone-conditioned variation in the articulation of /a/ and /i/ facilitates rapid recognition of tones. The one exception to this general pattern-no effect of vowel quality on falling tone perception-may be due to fortuitous amplification of the harmonics relevant for pitch perception in this context.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0837,"Musicians show enhanced perception, but not production, of native lexical tones","Many studies have reported a musical advantage in perceiving lexical tones among non-native listeners, but it is unclear whether this advantage also applies to native listeners, who are likely to show ceiling-like performance and thus mask any potential musical advantage. The ongoing tone merging phenomenon in Hong Kong Cantonese provides a unique opportunity to investigate this as merging tone pairs are reported to be difficult to differentiate even among native listeners. In the present study, native Cantonese musicians and non-musicians were compared based on discrimination and identification of merging Cantonese tone pairs to determine whether a musical advantage in perception will be observed, and if so, whether this is seen on the phonetic and/or phonological level. The tonal space of the subjects' lexical tone production was also compared. Results indicated that the musicians outperformed the non-musicians on the two perceptual tasks, as indexed by a higher accuracy and faster reaction time, particularly on the most difficult tone pair. In the production task, however, there was no group difference in various indices of tonal space. Taken together, musical experience appears to facilitate native listeners' perception, but not production, of lexical tones, which partially supports a music-to-language transfer effect. (C) 2020 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0838,The differential effects of vowel and onset consonant lengthening on speech segmentation: Evidence from Taiwanese Southern Min,"A review of previous speech segmentation research suggests the prediction that listeners of Taiwanese Southern Min (TSM), a lexical tone language, would exploit vowel lengthening and syllable-onset consonant lengthening to locate word ends and beginnings, respectively. Yet, correlations between segment duration and tone identity in tone languages along with some TSM-specific phonological phenomena may work against such use. Two artificial language learning experiments examined TSM listeners' use of the lengthening cues. The listeners heard the words of an artificial language (e.g., /ba.nu.me/) repeated continuously and identified them in a subsequent two-alternative forced-choice test. Experiment I revealed that their segmentation benefits from and only from word-initial onset lengthening or word-final vowel lengthening, supporting the prediction. Experiment II further demonstrated that these two cues in combination synergistically support segmentation at least when compared to word-initial onset lengthening alone, consistent with previous findings regarding complementary cues. These results furnish additional evidence that vowel and onset consonant lengthening affect segmentation in different ways, possibly reflecting a functional division between vowels and consonants that is supported by some prosody-computing mechanism. Additionally, vowel lengthening seems to affect segmentation to a greater extent than onset consonant lengthening. Possible explanations for this and further issues are discussed.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0839,Adjustment of cue weighting in speech by speakers and listeners: Evidence from amplitude and duration modifications of Mandarin Chinese tone,"Speech contrasts are signaled by multiple acoustic dimensions, but these dimensions are not equally diagnostic. Moreover, the relative diagnosticity, or weight, of acoustic dimensions in speech can shift in different communicative contexts for both speech perception and speech production. However, the literature remains unclear on whether, and if so how, talkers adjust speech to emphasize different acoustic dimensions in the context of changing communicative demands. Here, we examine the interplay of flexible cue weights in speech production and perception across amplitude and duration, secondary non-spectral acoustic dimensions for phonated Mandarin Chinese lexical tone, across natural speech and whispering, which eliminates fundamental frequency contour, the primary acoustic dimension. Phonated and whispered Mandarin productions from native talkers revealed enhancement of both duration and amplitude cues in whispered, compared to phonated speech. When nonspeech amplitude-modulated noises modeled these patterns of enhancement, identification of the noises as Mandarin lexical tone categories was more accurate than identification of noises modeling phonated speech amplitude and duration cues. Thus, speakers exaggerate secondary cues in whispered speech and listeners make use of this information. Yet, enhancement is not symmetric among the four Mandarin lexical tones, indicating possible constraints on the realization of this enhancement. (C) 2022 Acoustical Society of America.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0840,Asymmetrical roles of segment and pitch accent in Japanese spoken word recognition,"This study examines the roles of segment and pitch accent in Japanese spoken word recognition. In a lexical decision task, it replicates the finding of Cutler and Otake [(1999) J. Acoust. Soc. Am. 105(3), 1877-1888] that pitch accent restricts word activation with a more comprehensive, rigorous experimental design. Furthermore, results uncover an asymmetrical role of segment and pitch accent in word recognition in Japanese: words primed by a pitch accent-matching prime are recognized more slowly and less accurately than words primed by a segment-matching prime.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0841,"Creaky voice identification in Mandarin: The effects of prosodic position, tone, pitch range and creak locality","Creaky voice, a non-modal aperiodic phonation that is often associated with low pitch targets, has been found to not only correlate linguistically with prosodic boundary, tonal categories, and pitch range, but also socially with age, gender, and social status. However, it is still not clear whether co-varying factors such as prosodic boundary, pitch range, and tone could, in turn, affect listeners' identification of creak. To fill this gap, this current study examines how creaky voice is identified in Mandarin through experimental data, aiming to enhance our understanding of cross-linguistic perception of creaky voice and, more broadly, speech perception in multi-variable contexts. Our results reveal that in Mandarin, creak identification is context-dependent: factors including prosodic position, tone, pitch range, and the amount of creak all affect how Mandarin listeners identify creak. This reflects listeners' knowledge about the distribution of creak in linguistically universal (e.g., prosodic boundary) and language-specific (e.g., lexical tone) environments.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0842,The form and function processing of lexical tone and intonation in tone-language-speaking children with autism spectrum disorder,"Studies on how the form versus function aspect of tone and intonation is processed by autistic individuals have mainly focused on speakers of non-tonal languages (e.g., English) with equivocal results. While the samples' heterogeneous cognitive abilities may be contributing factors, the phenotype of tone and intonation processing in autism may also vary with one's language background. Thirty-eight cognitively able autistic and 32 non-autistic Mandarin-speaking children completed tone and intonation perception tasks, each containing a function and form condition. Results suggested that the abilities to discriminate tone and intonation were not impaired at either the form or function level in these autistic children, and that these abilities were positively associated with one another in both autistic and non-autistic groups. The more severe the autism symptoms, the worse the form- and function-level of tone and intonation processing. While enhanced tone and intonation processing has been found in a subgroup of autistic children, it may not be a general characteristic of the autistic population with long-term tone language experience. These findings reveal typical tone and intonation processing at both the form and function levels in cognitively able Mandarin-speaking autistic children and provide evidence for associated tone and intonation processing abilities across levels.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0843,Acquisition of Mandarin tones by Canadian first graders: Effect of prior exposure to tonal and non-tonal languages,"This study examines the tone productions of school-aged children with and without a tonal language background who are learning Mandarin as a second language (L2) or heritage language in Mandarin-English bilingual schools in Western Canada. Tones are frequently identified as one of the most challenging aspects of phonology for Mandarin L2 learners to acquire. In this study, tone productions of bilingual children from three home language backgrounds, English, Cantonese, and Mandarin Chinese, were compared for transcribed accuracy using mixed effects logistic regression. In addition, the fundamental frequency contours of correct tone productions were fitted with generalized additive mixed models to analyse the acoustic differences between groups. Error patterns were also analysed for possible Cantonese substitutions. Our results suggest that children with a Cantonese background are more accurate in tone productions than children with an English language background, but they also made more errors than their peers with a Mandarin language background. These findings suggest that a tonal language background could result in positive transfer among school-age children who are in the early stages of learning Mandarin as an L2.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0844,Acoustic characteristics of infant- and foreigner-directed speech with Mandarin as the target language,"The quality of speech input influences the efficiency of L1 and L2 acquisition. This study examined modifications in infant-directed speech (IDS) and foreigner-directed speech (FDS) in Standard Mandarin-a tonal language-and explored how IDS and FDS features were manifested in disyllabic words and a longer discourse. The study aimed to determine which characteristics of IDS and FDS were enhanced in comparison with adult-directed speech (ADS), and how IDS and FDS differed when measured in a common set of acoustic parameters. For words, it was found that tone-bearing vowel duration, mean and range of fundamental frequency (F0), and the lexical tone contours were enhanced in IDS and FDS relative to ADS, except for the dipping Tone 3 that exhibited an unexpected lowering in FDS, but no modification in IDS when compared with ADS. For the discourse, different aspects of temporal and F0 enhancements were emphasized in IDS and FDS: the mean F0 was higher in IDS whereas the total discourse duration was greater in FDS. These findings add to the growing literature on L1 and L2 speech input characteristics and their role in language acquisition.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0845,High variability phonetic training facilitates perception-to-production transfer in Mandarin-speaking children with cochlear implants: An acoustic investigation,"This study primarily aimed to evaluate the effectiveness of high variability phonetic training (HVPT) for children with cochlear implants (CIs) via the cross-modal transfer of perceptual learning to lexical tone production, a scope that has been largely neglected by previous training research. Sixteen CI participants received a five-session HVPT within a period of three weeks, whereas another 16 CI children were recruited without receiving any formal training. Lexical tone production was assessed with a picture naming task before the provision (pretest) and immediately after (posttest) and ten weeks after (follow-up test) the completion of the training protocol. The production samples were coded and analyzed acoustically. Despite considerable distinctions from the typical baselines of normal-hearing peers, the trained CI children exhibited significant improvements in Mandarin tone production from pretest to posttest in pitch height of T1, pitch slope of T2, and pitch curvature of T3. Moreover, the training-induced acoustic changes in the concave characteristic of the T3 contour was retained ten weeks after training termination. This study represents an initial acoustic investigation on HVPT-induced benefits in lexical tone production for the pediatric CI population, which provides valuable insights into applying this perceptual training technique as a viable tool in clinical practices.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0846,Predicting the intelligibility of Mandarin Chinese with manipulated and intact tonal information for normal-hearing listeners,"Objective indices for predicting speech intelligibility offer a quick and convenient alternative to behavioral measures of speech intelligibility. However, most such indices are designed for a specific language, such as English, and they do not take adequate account of tonal information in speech when applied to languages like Mandarin Chinese (hereafter called Mandarin) for which the patterns of fundamental frequency (F0) variation play an important role in distinguishing speech sounds with similar phonetic content. To address this, two experiments with normal-hearing listeners were conducted examining: (1) The impact of manipulations of tonal information on the intelligibility of Mandarin sentences presented in speech-shaped noise (SSN) at several signal-to-noise ratios (SNRs); (2) The intelligibility of Mandarin sentences with intact tonal information presented in SSN, pink noise, and babble at several SNRs. The outcomes were not correctly predicted by the Hearing Aid Speech Perception Index (HASPI-V1). A new intelligibility metric was developed that used one acoustic feature from HASPI-V1 plus Hilbert time envelope and temporal fine structure information from multiple frequency bands. For the new metric, the Pearson correlation between obtained and predicted intelligibility was 0.923 and the root mean square error was 0.119. The new metric provides a potential tool for evaluating Mandarin intelligibility.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0847,Individual differences in the distributional learning and overnight consolidation of the Mandarin level-falling tone contrast,"In perceptual studies, musicality and pitch aptitude have been implicated in tone learning, while vocabulary size has been implicated in distributional (segment) learning. Moreover, working memory plays a role in the overnight consolidation of explicit-declarative L2 learning. This study examines how these factors uniquely account for individual differences in the distributional learning and consolidation of an L2 tone contrast, where learners are tonal language speakers, and the training is implicit. Following a previous study investigating distributional tone learning, 66 L1-Cantonese participants who learned and consolidated a Mandarin level-falling tone contrast through distributional exposure were measured in a pitch threshold task, Montreal Battery of Evaluation of Amusia, Mandarin Peabody Picture Vocabulary Test, and an Operation Span task. Pitch threshold predicted immediate learning improvement while working memory predicted overnight consolidation by a bimodal group (not a unimodal group). The findings imply that pitch aptitude may be important in encoding stepwise tonal tokens, and the predictive power of working memory in overnight consolidation extends to implicit tone learning. Meanwhile, musical aptitude may not confer an additional advantage for speakers with native-tone experiences, and learners with a larger L2 vocabulary size might have resisted adaptation to distributional exposure because of robust L2 tonal representations.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0848,The affective iconicity of lexical tone: Evidence from standard Chinesea),"Previous studies suggested that pitch characteristics of lexical tones in Standard Chinese influence various sensory perceptions, but whether they iconically bias emotional experience remained unclear. We analyzed the arousal and valence ratings of bi-syllabic words in two corpora (Study 1) and conducted an affect rating experiment using a carefully designed corpus of bi-syllabic words (Study 2). Two-alternative forced-choice tasks further tested the robustness of lexical tones' affective iconicity in an auditory nonce word context (Study 3). Hierarchical linear models, generalized linear mixed models, and cross-validation were employed to understand the relationship between lexical tones and the emotional responses of tone-carrying words. Results consistently indicated that words with a falling-falling tonal sequence, both real and nonce words, received higher arousal ratings than those with rising-rising and rising-low tones. Only in nonce words, the high-high sequence was more likely to be associated with the low-arousal option; the falling-falling tone sequence was more often linked to negative-valence choice, while high-high and rising-rising tones with positive-valence. These findings, though subtle, suggest that the use of pitch in lexical tones influences emotional responses during the processing of tone-carrying words, pointing to an inherent iconic quality in lexical tones that may subtly shape speakers' emotional experiences.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0849,Sparse representation of speech using an atomic speech modela),"Speech perception has been extensively studied using degradation algorithms such as channel vocoding, mosaic speech, and pointillistic speech. Here, an ""atomic speech model"" is introduced to generate unique sparse time-frequency patterns. It processes speech signals using a bank of bandpass filters, undersamples the signals, and reproduces each sample using a Gaussian-enveloped tone (a Gabor atom). To examine atomic speech intelligibility, adaptive speech reception thresholds (SRTs) are measured as a function of atom rate in normal-hearing listeners, investigating the effects of spectral maxima, binaural integration, and single echo. Experiment 1 showed atomic speech with 4 spectral maxima out of 32 bands remained intelligible even at a low rate under 80 atoms per second. Experiment 2 showed that when atoms were nonoverlappingly assigned to both ears, the mean SRT increased (i.e., worsened) compared to the monaural condition, where all atoms were assigned to one ear. Individual data revealed that a few listeners could integrate information from both ears, performing comparably to the monaural condition. Experiment 3 indicated higher mean SRT with a 100 ms echo delay than that with shorter delays (e.g., 50, 25, and 0 ms). These findings demonstrate the utility of the atomic speech model for investigating speech perception and its underlying mechanisms.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0850,"Context effects on lexical tone categorization in quiet and noisy conditions by young, middle-aged, and older individuals","Previous studies focused on how contexts affect the recognition of lexical tones, primarily among healthy young adults in a quiet environment. However, little is known about how senescence and cognitive decline influence lexical tone normalization in adverse listening conditions. This study aims to explore how F0 shifts of the preceding context affect lexical tone identification across different age groups in quiet and noisy conditions. Twenty-two Mandarin-speaking young adults, 22 middle-aged adults, and 21 older adults with mild cognitive impairment (MCI) participated in tone identification tasks with and without speech contexts. The identification tasks with contexts were conducted in quiet and babble noise with signal-to-noise ratios (SNRs) set at 5 and 0 dB. Results showed that contextual F0 cues exerted an equal impact on lexical tone normalization across all three age groups in the quiet environment. Nevertheless, under SNRs of 5 and 0 dB, noise nullified such an effect. Moreover, working memory was negatively correlated with the size of lexical tone normalization in the older group. These findings suggest that context effects on Mandarin tone normalization tend to be resistant to senescence and MCI but susceptible to babble noise, offering further insights into the cognitive processing mechanisms underlying speech normalization.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0851,Recognition of spoken words with mispronounced lexical prosody in Japanese,"Lexical prosody plays a crucial role in Japanese spoken word recognition. However, listeners of Japanese can still recognize spoken words easily even when they are pronounced with mispronounced lexical prosody, i.e., prosody that differs from their lexical knowledge. The present study investigated how listeners recognize spoken words pronounced with mispronounced lexical pitch accent in Japanese. Two cross-modal semantic priming experiments addressed the process of lexical access during listening to prosodically mispronounced words under a sentential context. When words were presented with mispronounced prosody, semantic priming effects that reflected the access to contextually congruent words were obtained not under interstimulus interval (ISI) = 0 ms (experiment 1) but under ISI = 750 ms (experiment 2). These results suggested that a prosodically mispronounced input temporarily disturbed access to the appropriate meaning of the word, whereas appropriate access was achieved later within 750 ms with reference to the contextual information rather than the mispronounced lexical prosodic information. Thus, contextual information overrides lexical prosodic information to recognize spoken words appropriately even when lexical prosody is mispronounced.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0852,The impact of segmental familiarity on incidental suprasegmental category learning,"Previous research on tone category learning has suggested that the learners' familiarity with the segmental composition of tokens can impact learning. However, the impact of segmental familiarity on tone category learning has seldom been tested. Also, the learning of novel tone categories in natural speech has been studied primarily under explicit learning paradigms. In the current study, we expand the use of incidental learning paradigms from synthesized sound category learning to natural speech to examine the impact of segmental familiarity on tone category learning. In one online session, three groups of native English speakers from multiple countries learned four Thai tone categories presented in tokens with familiar or unfamiliar vowel segments. Results demonstrated that incidental learning is a quick and effective method for studying novel sound category acquisition, with learners in each group achieving up to 100% identification accuracy at test. Using this paradigm, we observed a small impact of segmental familiarity on learning, suggesting that its influence may be highly granular, depending on the type and complexity of segmental features in the stimuli.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0853,ASPM-lexical tone association in speakers of a tone language: Direct evidence for the genetic-biasing hypothesis of language evolution,"How language has evolved into more than 7000 varieties today remains a question that puzzles linguists, anthropologists, and evolutionary scientists. The genetic-biasing hypothesis of language evolution postulates that genes and language features coevolve, such that a population that is genetically predisposed to perceiving a particular linguistic feature would tend to adopt that feature in their language. Statistical studies that correlated a large number of genetic variants and linguistic features not only generated this hypothesis but also specifically pinpointed a linkage between ASPM and lexical tone. However, there is currently no direct evidence for this association and, therefore, the hypothesis. In an experimental study, we provide evidence to link ASPM with lexical tone perception in a sample of over 400 speakers of a tone language. In addition to providing the first direct evidence for the genetic-biasing hypothesis, our results have implications for further studies of linguistic anthropology and language disorders.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0854,Neural mechanisms for lexical processing in dogs,"During speech processing, human listeners can separately analyze lexical and intonational cues to arrive at a unified representation of communicative content. The evolution of this capacity can be best investigated by comparative studies. Using functional magnetic resonance imaging, we explored whether and how dog brains segregate and integrate lexical and intonational information. We found a left-hemisphere bias for processing meaningful words, independently of intonation; a right auditory brain region for distinguishing intonationally marked and unmarked words; and increased activity in primary reward regions only when both lexical and intonational information were consistent with praise. Neuralmechanisms to separately analyze and integrate word meaning and intonation in dogs suggest that this capacity can evolve in the absence of language.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0855,Intonational speech prosody encoding in the human auditory cortex,"Speakers of all human languages regularly use intonational pitch to convey linguistic meaning, such as to emphasize a particular word. Listeners extract pitch movements from speech and evaluate the shape of intonation contours independent of each speaker's pitch range. We used high-density electrocorticography to record neural population activity directly from the brain surface while participants listened to sentences that varied in intonational pitch contour, phonetic content, and speaker. Cortical activity at single electrodes over the human superior temporal gyrus selectively represented intonation contours. These electrodes were intermixed with, yet functionally distinct from, sites that encoded different information about phonetic features or speaker identity. Furthermore, the representation of intonation contours directly reflected the encoding of speaker-normalized relative pitch but not absolute pitch.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0856,INTEGRATING APPLIED LINGUISTICS WITH ARTIFICIAL INTELLIGENCE-ENABLED ARABIC TEXT-TO-SPEECH SYNTHESIZER,"Currently, Text-to-Speech (TTS) or speech synthesis, the ability of the complex system to generate a human-like sounding voice from the written text, is becoming increasingly popular in speech processing in various complex systems. TTS is the artificial generation of human speech. A classical TTS system translates a language text into a waveform. Several English TTS systems produce human-like, mature, and natural speech synthesizers. On the other hand, other languages, such as Arabic, have just been considered. The present Arabic speech synthesis solution is of low quality and slow, and the naturalness of synthesized speech is lower than that of English synthesizers. Also, they lack crucial primary speech factors, including rhythm, intonation, and stress. Several studies have been proposed to resolve these problems, integrating using concatenative techniques like parametric or unit selection methods. This paper proposes an Applied Linguistics with Artificial Intelligence-Enabled Arabic Text-to-Speech Synthesizer (ALAI-ATTS) model. This ALAI-ATTS technique includes three essential components: data preprocessing through phonetization and diacritization, Extreme Learning Machine (ELM)-based speech synthesis, and Grey Wolf Fractals Optimization (GWO)-based parameter tuning. Initially, the data preprocessing step includes diacritization, where diacritics are restored to unvoweled text to ensure correct pronunciation, followed by phonetization, translating the text into its phonetic representation. Then, the ELM-based speech synthesis model uses the processed dataset for speech generation. ELMs, well known for their excellent generalization performance and fast learning speed, are especially suitable for real-time TTS applications, balancing high-quality speech output and computational efficiency. Lastly, the GWO methodology is employed to tune the parameters of the ELM. The simulation outcomes validate that the ALAI-ATTS technique considerably enhances the intelligibility and naturalness of Arabic synthesized speech compared to existing approaches. The experimental results of the ALAI-ATTS technique portrayed a lesser value of 3.48, 0.15 and 1.37, 0.25 under WER and DER.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0857,Do Prosody and Embodiment Influence the Perceived Naturalness of Conversational Agents' Speech?,"For conversational agents' speech, either all possible sentences have to be prerecorded by voice actors or the required utterances can be synthesized. While synthesizing speech is more flexible and economic in production, it also potentially reduces the perceived naturalness of the agents among others due to mistakes at various linguistic levels. In our article, we are interested in the impact of adequate and inadequate prosody, here particularly in terms of accent placement, on the perceived naturalness and aliveness of the agents. We compare (1) inadequate prosody, as generated by off-the-shelf text-to-speech (TTS) engines with synthetic output; (2) the same inadequate prosody imitated by trained human speakers; and (3) adequate prosody produced by those speakers. The speech was presented either as audio-only or by embodied, anthropomorphic agents, to investigate the potential masking effect by a simultaneous visual representation of those virtual agents. To this end, we conducted an online study with 40 participants listening to four different dialogues each presented in the three Speech levels and the two Embodiment levels. Results confirmed that adequate prosody in human speech is perceived as more natural (and the agents are perceived as more alive) than inadequate prosody in both human (2) and synthetic speech (1). Thus, it is not sufficient to just use a human voice for an agents' speech to be perceived as natural-it is decisive whether the prosodic realisation is adequate or not. Furthermore, and surprisingly, we found no masking effect by speaker embodiment, since neither a human voice with inadequate prosody nor a synthetic voice was judged as more natural, when a virtual agent was visible compared to the audio-only condition. On the contrary, the human voice was even judged as less ""alive"" when accompanied by a virtual agent. In sum, our results emphasize, on the one hand, the importance of adequate prosody for perceived naturalness, especially in terms of accents being placed on important words in the phrase, while showing, on the other hand, that the embodiment of virtual agents plays a minor role in the naturalness ratings of voices.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0858,PhonemeVec: A Phoneme-Level Contextual Prosody Representation For Speech Synthesis,"Recently, fine-grained prosody representations have emerged and attracted growing attention to address the one-to-many problem in text-to-speech (TTS). In this article, we propose the PhonemeVec, a pre-trained prosody representations with considering the contextual information. To obtain the contextual prosody representations, we improve the data2vec framework according to the characteristics of prosody to extract the PhonemeVec from the low-band mel-spectrogram, and pre-train on a 960 hours Chinese corpus with high quality and diverse pronunciation. PhonemeVec is subsequently integrated into FastSpeech2, supervising the prosody modeling of the text encoder. Experiments conducted on the Blizzard Challenge 2019 dataset show that the integration of PhonemeVec results in the synthesis of more natural speech. Additionally, objective evaluations confirm that the application of PhonemeVec reduces the distortions between the generated speech and original recordings in terms of duration and F0.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0859,Cracking Prosody in Articulatory Phonology,"Articulatory Phonology advances an account of phonological structure in which dynamically defined vocal tract tasks-gestures-are simultaneously and isomorphically units of cognitive representation and units of physical action. This paradigm has fundamentally altered our understanding of the linguistic representation of words. This article reviews the relatively recent incorporation of prosody into Articulatory Phonology. A capsule review of the Articulatory Phonology theoretical framework is presented, and the notions of phrasal and prominence organization are introduced as the key aspects of linguistic prosodic structure under consideration. Parameter dynamics, activation dynamics, and prosodic modulation gestures, such as the p-gesture, are outlined. The review is extended to touch on rhythm, intonation, and pauses and to consider innovations for integrating multiple aspects of prosodic structure under this dynamical approach. Finally, a range of questions emerges, crystallizing outstanding issues ranging from the abstract and theoretical to the interactive and functional.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0860,Representation of the temporal envelope of sounds in the human brain,"The cerebral representation of the temporal envelope of sounds was studied in five normal-hearing subjects using functional magnetic resonance imaging. The stimuli were white noise, sinusoidally amplitude-modulated at frequencies ranging from 4 to 256 Hz. This range includes low AM frequencies (up to 32 Hz) essential for the perception of the manner of articulation and syllabic rate, and high AM frequencies (above 64 Hz) essential for the perception of voicing and prosody. The right lower brainstem (superior olivary complex), the right inferior colliculus, the left medial geniculate body, Heschl's gyrus, the superior temporal gyrus, the superior temporal sulcus, and the inferior parietal lobule were specifically responsive to AM. Global tuning curves in these regions suggest that the human auditory system is organized as a hierarchical filter bank, each processing level responding preferentially to a given AM frequency, 256 Hz for the lower brainstem, 32-256 Hz for the inferior colliculus, 16 Hz for the medial geniculate body, 8 Hz for the primary auditory cortex, and 4-8 Hz for secondary regions. The time course of the hemodynamic responses showed sustained and transient components with reverse frequency dependent patterns: the lower the AM frequency the better the fit with a sustained response model, the higher the AM frequency the better the fit with a transient response model. Using cortical maps of best modulation frequency, we demonstrate that the spatial representation of AM frequencies varies according to the response type. Sustained responses yield maps of low frequencies organized in large clusters. Transient responses yield maps of high frequencies represented by a mosaic of small clusters. Very few voxels were tuned to intermediate frequencies (32-64 Hz). We did not find spatial gradients of AM frequencies associated with any response type. Our results suggest that two frequency ranges (up to 16 and 128 Hz and above) are represented in the cortex by different response types. However, the spatial segregation of these two ranges is not systematic. Most cortical regions were tuned to low frequencies and only a few to high frequencies. Yet, voxels that show a preference for low frequencies were also responsive to high frequencies. Overall, our study shows that the temporal envelope of sounds is processed by both distinct (hierarchically organized series of filters) and shared (high and low AM frequencies eliciting different responses at the same cortical locus) neural substrates. This layout suggests that the human auditory system is organized in a parallel fashion that allows a degree of separate routing for groups of AM frequencies conveying different information and preserves a possibility for integration of complementary features in cortical auditory regions.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0861,The Influence of Tone Inventory on ERP without Focal Attention: A Cross-Language Study,"This study investigates the effect of tone inventories on brain activities underlying pitch without focal attention. We find that the electrophysiological responses to across-category stimuli are larger than those to within-category stimuli when the pitch contours are superimposed on nonspeech stimuli; however, there is no electrophysiological response difference associated with category status in speech stimuli. Moreover, this category effect in nonspeech stimuli is stronger for Cantonese speakers. Results of previous and present studies lead us to conclude that brain activities to the same native lexical tone contrasts are modulated by speakers' language experiences not only in active phonological processing but also in automatic feature detection without focal attention. In contrast to the condition with focal attention, where phonological processing is stronger for speech stimuli, the feature detection (pitch contours in this study) without focal attention as shaped by language background is superior in relatively regular stimuli, that is, the nonspeech stimuli. The results suggest that Cantonese listeners outperform Mandarin listeners in automatic detection of pitch features because of the denser Cantonese tone system.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0862,The Benefits of Residual Hair Cell Function for Speech and Music Perception in Pediatric Bimodal Cochlear Implant Listeners,"Objective. The aim of this study was to investigate the benefits of residual hair cell function for speech and music perception in bimodal pediatric Mandarin-speaking cochlear implant (CI) listeners. Design. Speech and music performance was measured in 35 Mandarin-speaking pediatric CI users for unilateral (CI-only) and bimodal listening. Mandarin speech perception was measured for vowels, consonants, lexical tones, and sentences in quiet. Music perception was measured for melodic contour identification (MCI). Results. Combined electric and acoustic hearing significantly improved MCI and Mandarin tone recognition performance, relative to CI-only performance. For MCI, performance was significantly better with bimodal listening for all semitone spacing conditions (p < 0 05 in all cases). For tone recognition, bimodal performance was significantly better only for tone 2 (rising; p < 0 05). There were no significant differences between CI-only and CI + HA for vowel, consonant, or sentence recognition. Conclusions. The results suggest that combined electric and acoustic hearing can significantly improve perception of music and Mandarin tones in pediatric Mandarin-speaking CI patients. Music and lexical tone perception depends strongly on pitch perception, and the contralateral acoustic hearing coming from residual hair cell function provided pitch cues that are generally not well preserved in electric hearing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0863,A Neurophysiological Study of Musical Pitch Identification in Mandarin-Speaking Cochlear Implant Users,"Music perception in cochlear implant (CI) users is far from satisfactory, not only because of the technological limitations of current CI devices but also due to the neurophysiological alterations that generally accompany deafness. Early behavioral studies revealed that similar mechanisms underlie musical and lexical pitch perception in CI-based electric hearing. Although neurophysiological studies of the musical pitch perception of English-speaking CI users are actively ongoing, little such research has been conducted with Mandarin-speaking CI users; as Mandarin is a tonal language, these individuals require pitch information to understand speech. The aim of this work was to study the neurophysiological mechanisms accounting for the musical pitch identification abilities of Mandarin-speaking CI users and normal-hearing (NH) listeners. Behavioral and mismatch negativity (MMN) data were analyzed to examine musical pitch processing performance. Moreover, neurophysiological results from CI users with good and bad pitch discrimination performance (according to the just-noticeable differences (JND) and pitch-direction discrimination (PDD) tasks) were compared to identify cortical responses associated with musical pitch perception differences. The MMN experiment was conducted using a passive oddball paradigm, with musical tone C4 (262 Hz) presented as the standard and tones D4 (294 Hz), E4 (330 Hz), G#4 (415 Hz), and C5 (523 Hz) presented as deviants. CI users demonstrated worse musical pitch discrimination ability than did NH listeners, as reflected by larger JND and PDD thresholds for pitch identification, and significantly increased latencies and reduced amplitudes in MMN responses. Good CI performers had better MMN results than did bad performers. Consistent with findings for English-speaking CI users, the results of this work suggest that MMN is a viable marker of cortical pitch perception in Mandarin-speaking CI users.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0864,Mancunian intonation and intonational representation,"There has been little systematic description of the intonation of English accents other than RP and General American. In the first part of this article the characteristics of the tones of Mancunian intonation are described together with a functional categorisation of these tones, in which a dichotomy is proposed between Open and Closed varieties. In the second part the description is related to the current model of intonation known as ToBl and the inadequacies of a representation of Mancunian tones in a standard and a modified form of ToBl are revealed. A more radical modification of the ToBl approach based on tonal features is proposed. Copyright (C) 2001 S. Karger AG, Basel.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0865,Fundamental frequency as a perceptual cue for vowel identification in speakers with Parkinson's disease,"This study investigates the importance of fundamental frequency (F0) as a perceptual cue for identification of vowel targets produced by speakers with Parkinson's disease (PD). It has been suggested in the literature that F0 is a redundant cue for vowel identification in highly intelligible speech. For speakers with dysarthria who are having difficulty with segmental and suprasegmental aspects of production which result in ambiguous or conflicting cues in the acoustic signal, F0 may have increased perceptual importance for accurate identification of vowel targets. In the present study, F0 contours for single-word targets produced in sentence level material by 20 speakers with PD and 20 control speakers were synthetically modified in several different ways (i.e.,. attened and enhanced). Listener identification of vowel targets across the F0 conditions was recorded. The accuracy of vowel identification for the control group was not affected by the flattening of the F0 contour. For the speakers with PD, however, modi. cation of the F0 contour (flattening or enhancing) affected the accuracy with which listeners identified certain vowels. Differences in vowel identification were found primarily for the front vowels/I, epsilon, ae / along a high-low continuum. Copyright (c) 2006 S. Karger AG, Basel.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0866,Do Chinese Speakers Need a Specialized Cochlear Implant System?,"Background: Cochlear implants are a standard treatment option for the profoundly deaf, but have only recently become a treatment option in China. Chinese is a tonal language, and a change in the lexical tone almost always changes the meaning of a word. Methods: A critical review of the strategies for improving the outcome of cochlear implantation in Chinese speakers was made. Results: The introduction of cochlear implantation in China has stimulated the development of new speech perception tests, which are specific to Chinese, for adults and children. Research undertaken over the last decade in Hong Kong has demonstrated the successful acquisition of tones in adults and children who have received cochlear implants. Surgical approaches have been developed to address specific issues, including post-irradiation and infected ears. New areas of research into the enhancement of speech understanding through signal processing have been established. Conclusions: The findings in these new areas will form the basis of a cochlear implant system for Chinese people in the near future. Copyright (C) 2009 S. Karger AG, Basel",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0867,ARTICULATORY TIMING AND THE PROSODIC INTERPRETATION OF SYLLABLE DURATION,"A number of different prosodic effects (e.g. intonation-phrase-final position, the presence of stress or accent) increase syllable duration, as conventionally measured by the spacing of abrupt energy transitions in the acoustic signal. However, different prosodic contrasts may have different influences on syllable-internal articulatory organization. The present study examined the time course of vowel-related opening and closing mandibular gestures in four different prosodic contexts. For some prosodic effects, such as intonationphrase-final lengthening, longer acoustic durations were associated with a disproportionate lengthening of the latter part of the vocalic gesture. By contrast, the presence of nuclear stress was associated with a more even distribution of lengthening throughout the syllable. These results suggest that the rhythmic effects of different prosodic contrasts cannot be adequately modelled in terms of millisecond values of durational ratios for acoustic segments. It is proposed that a suitable phonological representation of the rhythms of stress and phrasing might describe them as the time course of a syllable''s phonetic sonority.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0868,INFORMATION FOR MANDARINE TONES IN THE AMPLITUDE CONTOUR AND IN BRIEF SEGMENTS,"While the tones of Mandarin are conveyed mainly by the F0 contour, they also differ consistently in duration and in amplitude contour. The contribution of these factors was examined by using signal-correlated noise stimuli, in which natural speech is manipulated so that it has no F0 or formant structure but retains its original amplitude contour and duration. Tones 2, 3 and 4 were perceptible from just the amplitude contour, even when duration was not also a cue. In two further experiments, the location of the critical information for the tones during the course of the syllable was examined by extracting small segments from each part of the original syllable. Tones 2 and 3 were often confused with each other, and segments which did not have much F0 change were most often heard as Tone 1. There were, though, also cases in which a low, unchanging pitch was heard as Tone 3, indicating a partial effect of register even in Mandarin. F0 was positively correlated with amplitude, even when both were computed on a pitch period basis. Taken together, the results show that Mandarin tones are realized in more than just the F0 pattern, that amplitude contours can be used b listeners as cues for tone identification, and that not every portion of the F0 pattern unambiguously indicates the original tone.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0869,Teaching intonation to young deaf children with the Intonation Meter,"Incorrect production of intonation contours is a common phonatory problem in prelingually, profoundly deaf speakers. To help deaf speakers improve this, a visual display system for teaching intonation has been developed. In this system, called the Intonation Meter, visual feedback of intonation is given as a continuous representation of the pitch contour containing only the perceptually relevant aspects of the intonation pattern. This pitch-contour representation is supposed to facilitate the interpretation of the visual feedback of the pitch contour. A study was carried out, using a Single-Subject Design, in which subjects alternately received intonation training by means of regular methods and intonation training by means of regular methods in which also use was made of the Intonation Meter, to evaluate the effectiveness of the Intonation Meter for teaching intonation to young deaf children. Prelingually profoundly deaf children aged 6 to 7 years and 9 to 11 years participated in this study. The results showed that the 9 to 11 year old children showed most progress when the Intonation Meter was used in intonation training whereas the 6 to 7 year olds progressed well irrespective of whether or not the Intonation Meter was used, which is in accordance with the theory of a critical period for language learning. Alternatively, it is hypothesized that the cognitive requirements of the visual feedback might be too advanced for very young children to be helpful in learning to produce certain pitch contours.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0870,Studies of Chinese Speakers with Dysarthria: Informing Theoretical Models,"Most theoretical models of dysarthria have been developed based on research using individuals speaking English or other Indo-European languages. Studies of individuals with dysarthria speaking other languages can allow investigation into the universality of such models, and the interplay between language-specific and language-universal aspects of dysarthria. In this article, studies of Cantonese-and Mandarin-Chinese speakers with dysarthria are reviewed. The studies focused on 2 groups of speakers: those with cerebral palsy and those with Parkinson's disease. Key findings are compared with similar studies of English speakers. Since Chinese is tonal in nature, the impact of dysarthria on lexical tone has received considerable attention in the literature. The relationship between tone [which involves fundamental frequency (F-0) control at the syllable level] and intonation (involving F-0 control at the sentential level) has received more recent attention. Many findings for Chinese speakers with dysarthria support earlier findings for English speakers, thus affirming the language-universal aspect of dysarthria. However, certain differences, which can be attributed to the distinct phonologies of Cantonese and Mandarin, highlight the language-specific aspects of the condition. Copyright (C) 2010 S. Karger AG, Basel",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0871,Pitch Range Variation in English Tonal Contrasts: Continuous or Categorical?,"The importance of pitch range variation for intonational meaning and theory is well known; however, whether pitch range is a phonetic dimension which is treated categorically in English remains unclear. To test this possibility, three intonation continua varying in pitch range were constructed which had endpoints with contrastive representations under autosegmental-metrical (AM) theory: H* vs. L+H*, H* with 'peak delay' vs. L*+H, and %H L* vs. L*. The prediction derived from AM theory was that the reproduction of continuous pitch range variation should show a discrete pattern reflecting a change in the phonological representation of tonal sequences and in the number of tonal targets across each continuum. Participants' reproductions of each stimulus set showed continuous variation in pitch range, suggesting that pitch range is a gradient phonetic dimension in English conveying semantic contrast, similar to the formant space for vowels. Moreover, the gradience observed in productions across all parts of the pitch range suggests that contours within each series had the same number of tonal targets. The results support a version of AM theory in which rises and falls are usually comprised of two tonal targets, with strictly monotonic f(0) interpolation between them. Copyright (C) 2010 S. Karger AG, Basel",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0872,Intonation Adapts to Lexical Tone: The Case of Kammu,"In this paper, we investigate how lexical tones interact with intonation, using data from the Austroasiatic language Kammu, one of few languages with two dialects whose only major phonological difference is the presence or absence of lexical tones. Northern (and Western) Kammu have developed tones in connection with the merger of voiceless and voiced initial consonants, while the non-tonal Eastern dialect kept the segmental opposition with no tones. We found the following prosodic hierarchy: (1) lexical tones, (2) phrase-final boundary tone, (3) focus marking. The results strongly suggest that the intonational systems of the two Kammu dialects are basically identical, and that the main differences between the dialects are adaptations of intonation patterns to the lexical tones when the identities of the tones are jeopardized. Copyright (C) 2012 S. Karger AG, Basel",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0873,Danish Stod: Laryngealization or Tone,"In the light of previous acoustic analyses of Danish stod and Danish intonation, we discuss two different phonological theories. In one, stod is an autonomous laryngeal syllable prosody. In the other, stod is the phonetic manifestation of an HL tonal pattern compressed within one syllable. The tonal representation is found to be contradicted by the phonetic reality, and it cannot account for the structurally determined alternation between non-stod and stod in inflection and derivation, nor for latent stod or stod in compounds. Furthermore, stod patterns are largely constant across regional varieties of Danish, but tonal patterns over the relevant structural domains are highly variable. Thus, stod may occur on any kind of tonal configuration, anywhere in the speaker's pitch range, a variability which is hard to reconcile with a fixed HL representation. Copyright (C) 2013 S. Karger AG, Basel",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0874,Stress Effects in Vowel Perception as a Function of Language-Specific Vocabulary Patterns,"Background/Aims: Evidence from spoken word recognition suggests that for English listeners, distinguishing full versus reduced vowels is important, but discerning stress differences involving the same full vowel (as in mu- from music or museum) is not. In Dutch, in contrast, the latter distinction is important. This difference arises from the relative frequency of unstressed full vowels in the two vocabularies. The goal of this paper is to determine how this difference in the lexicon influences the perception of stressed versus unstressed vowels. Methods: All possible sequences of two segments (diphones) in Dutch and in English were presented to native listeners in gated fragments. We recorded identification performance over time throughout the speech signal. The data were here analysed specifically for patterns in perception of stressed versus unstressed vowels. Results: The data reveal significantly larger stress effects (whereby unstressed vowels are harder to identify than stressed vowels) in English than in Dutch. Both language-specific and shared patterns appear regarding which vowels show stress effects. Conclusion: We explain the larger stress effect in English as reflecting the processing demands caused by the difference in use of unstressed vowels in the lexicon. The larger stress effect in English is due to relative inexperience with processing unstressed full vowels. (C) 2016 S. Karger AG, Basel",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0875,Neural Underpinnings of Early Speech Perception and Emergent Literacy,"Mismatch negativity (MMN) is an event-related potential component used to index automatic auditory change detection. Thus, MMN provides an excellent tool to assess the speech sensitivity of infants and children. Although MMN is well established in adults, the polarity and latency of mismatch responses (MMRs) in infants are highly inconsistent across studies. This paper aims to provide a comprehensive review of MMN studies of speech perception in early infancy. In particular, data from a series of MMN studies of Mandarin lexical tone, vowels, and initial consonants will be presented to demonstrate how phonological saliency, size of deviance, and neural maturation modulate MMRs in early infancy. These data suggest that MMN and positive MMRs index different functional characteristics and may provide information on when and how children's speech perception becomes automatic at different developmental stages. By using MMN to index sensitivity to speech discrimination, dyslexic children usually show reduced or absent MMN, which supports the relationship between phonological sensitivity and literacy. However, children with attention deficit/hyperactivity disorder showed the typical MMN, but attenuated P3a and enhanced late discriminative negativity. Taken together, the MMR characteristics, including amplitude, peak latency, and the transition of polarity, may be used to index the maturation of speech development and for the early identification of children with atypical language development. (C) 2019 S. Karger AG, Basel",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0876,The syntax and prosody of weak pronouns in Chamorro,"In the modular linguistic theory assumed by many generative linguists, phonology and syntax are interconnected but fundamentally independent components of grammar. The effects of syntax on phonology are mediated by prosodic structure, a representation of prosodic constituents calculated from syntactic structure but not isomorphic to it. Within this overall architecture, I investigate the placement of weak pronouns in the Austronesian language Chamorro. Certain Chamorro pronominals can be realized as prosodically deficient weak pronouns that typically occur right after the predicate. I show that these pronouns are second-position clitics whose placement is determined not syntactically, but prosodically: they occur after the leftmost phonological phrase of their intonational phrase. My analysis of these clitics assumes that lexical insertion is late and can affect and be affected by prosodic phrase formation-assumptions consistent with the view that the mutual interaction of phonology and syntax is confined to the postsyntactic operations that translate syntactic structure into prosodic structure.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0877,fMRI evidence for cortical modification during learning of Mandarin lexical tone,"Functional magnetic resonance imaging was employed before and after six native English speakers completed lexical tone training as part of a program to learn Mandarin as a second language. Language-related areas including Broca's area, Wernicke's area, auditory cortex, and supplementary motor regions were active in all subjects before and after training and did not vary in average location. Across all subjects, improvements in performance were associated with an increase in the spatial extent of activation in left superior temporal gyrus (Brodmann's area 22, putative Wernicke's area), the emergence of activity in adjacent Brodmann's area 42, and the emergence of activity in right inferior frontal gyrus ( Brodmann's area 44), a homologue of putative Broca's area. These findings demonstrate a form of enrichment plasticity in which the early cortical effects of learning a tone-based second language involve both expansion of preexisting language-related areas and recruitment of additional cortical regions specialized for functions similar to the new language functions.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0878,"Morphosyntax, prosody, and linking elements: The auditory processing of German nominal compounds","The morphosyntactic decomposition of German compound words and a proposed function of linking elements were examined during auditory processing using event-related brain potentials. In Experiment 1, the syntactic gender agreement was manipulated between a determiner and the initial compound constituent ( the ""nonhead'' constituent), and between a determiner and the last constituent (""head''). Although only the head is (morpho) syntactically relevant in German, both constituents elicited a left-anterior negativity if its gender was incongruent. This strongly suggests that compounds are morphosyntactically decomposed. Experiment 2 tested the function of those linking elements which are homophonous to plural morphemes. It has been previously suggested that these indicate the number of nonhead constituents. The number agreement was manipulated for both constituents analogous to Experiment 1. Number-incongruent heads, but not nonhead constituents, elicited an N400 and a subsequent broad negativity, suggesting that linking elements are not processed as plural morphemes. Experiment 3 showed that prosodic cues ( duration and fundamental frequency) are employed to differentiate between compounds and single nouns and, thereby, betwen linking elements and plural morphemes. Number-incongruent words elicited a broad negativity if they were produced with a single noun prosody; the same words elicited no event-related potential effect if produced with a compound prosody. A dual-route model can account for the influence of prosody on morphosyntactic processing.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0879,THE CORTICAL REPRESENTATION OF SPEECH,"In this study, we compare regional cerebral blood flow (rCBF) while French monolingual subjects listen to continuous speech in an unknown language, to lists of French words, or to meaningful and distorted stories in French. Our results show that, in addition to regions devoted to single-word comprehension, processing of meaningful stories activates the left middle temporal gyrus, the left and right temporal poles, and a superior prefrontal area in the left frontal lobe. Among these regions, only the temporal poles remain activated whenever sentences with acceptable syntax and prosody are presented.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0880,Musician children detect pitch violations in both music and language better than nonmusician children: Behavioral and electrophysiological approaches,"The idea that extensive musical training can influence processing in cognitive domains other than music has received considerable attention from the educational system and the media. Here we analyzed behavioral data and recorded event-related brain potentials ( ERPs) from 8-year-old children to test the hypothesis that musical training facilitates pitch processing not only in music but also in language. We used a parametric manipulation of pitch so that the final notes or words of musical phrases or sentences were congruous, weakly incongruous, or strongly incongruous. Musician children outperformed nonmusician children in the detection of the weak incongruity in both music and language. Moreover, the greatest differences in the ERPs of musician and nonmusician children were also found for the weak incongruity: whereas for musician children, early negative components developed in music and late positive components in language, no such components were found for nonmusician children. Finally, comparison of these results with previous ones from adults suggests that some aspects of pitch processing are in effect earlier in music than in language. Thus, the present results reveal positive transfer effects between cognitive domains and shed light on the time course and neural basis of the development of prosodic and melodic processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0881,Musicians detect pitch violation in a foreign language better than nonmusicians: Behavioral and electrophysiological evidence,"The aim of this study was to determine whether musical expertise influences the detection of pitch variations in a foreign language that participants did not understand. To this end, French adults, musicians and nonmusicians, were presented with sentences spoken in Portuguese. The final words of the sentences were prosodically congruous (spoken at normal pitch height) or incongruous (pitch was increased by 35% or 120%). Results showed that when the pitch deviations were small and difficult to detect (35%: weak prosodic incongruities), the level of performance was higher for musicians than for nonmusicians. Moreover, analysis of the time course of pitch processing, as revealed by the event-related brain potentials to the prosodically congruous and incongruous sentence-final words, showed that musicians were, on average, 300 msec faster than nonmusicians to categorize prosodically congruous and incongruous endings. These results are in line with previous ones showing that musical expertise, by increasing discrimination of pitch-a basic acoustic parameter equally important for music and speech prosody-does facilitate the processing of pitch variations not only in music but also in language. Finally, comparison with previous results [Schon, D., Magne, C., & Besson, M. The music of speech: Music training facilitates pitch processing in both music and language. Psychophysiolog, 41, 341-349, 2004] points to the influence of semantics on the perception of acoustic prosodic cues.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0882,Cross-domain Effects of Music and Language Experience on the Representation of Pitch in the Human Auditory Brainstem,"Neural encoding of pitch in the auditory brainstem is known to be shaped by long-term experience with language or music, implying that early sensory processing is subject to experience-dependent neural plasticity. In language, pitch patterns consist of sequences of continuous, curvilinear contours; in music, pitch patterns consist of relatively discrete, stair-stepped sequences of notes. The primary aim was to determine the influence of domain-specific experience (language vs. music) on the encoding of pitch in the brainstem. Frequency-following responses were recorded from the brainstem in native Chinese, English amateur musicians, and English nonmusicians in response to iterated rippled noise homologues of a musical pitch interval (major third; M3) and a lexical tone (Mandarin tone 2; T2) from the music and language domains, respectively. Pitch-tracking accuracy (whole contour) and pitch strength (50 msec sections) were computed from the brainstem responses using autocorrelation algorithms. Pitch-tracking accuracy was higher in the Chinese and musicians than in the nonmusicians across domains. Pitch strength was more robust across sections in musicians than in nonmusicians regardless of domain. In contrast, the Chinese showed larger pitch strength, relative to nonmusicians, only in those sections of T2 with rapid changes in pitch. Interestingly, musicians exhibited greater pitch strength than the Chinese in one section of M3, corresponding to the onset of the second musical note, and two sections within T2, corresponding to a note along the diatonic musical scale. We infer that experience-dependent plasticity of brainstem responses is shaped by the relative saliency of acoustic dimensions underlying the pitch patterns associated with a particular domain.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0883,Activating without Inhibiting: Left-edge Boundary Tones and Syntactic Processing,"Right-edge boundary tones have earlier been found to restrict syntactic processing by closing a clause for further integration of incoming words. The role of left-edge intonation, however, has received little attention to date. We show that Swedish left-edge boundary tones selectively facilitate the on-line processing of main clauses, the syntactic structure they are associated with. In spoken Swedish, main clauses are produced with a left-edge boundary tone, which is absent in subordinate clauses. Main and subordinate clauses are further distinguished syntactically by word order when containing sentence adverbs. The effects of tone and word order on the processing of embedded main, subordinate, and neutral clauses (lacking sentence adverbs) were measured using ERPs. A posterior P600 in embedded main clauses and a smaller P600 in subordinate clauses indicated that embedded clauses with sentence adverbs were structurally less expected than neutral clauses and thus were reanalyzed. The tone functioned as a cue for main clause word order, selectively reducing the P600 in embedded main clauses, without affecting the processing of subordinate or neutral clauses. Its perception was reflected in a right frontal P200 effect. The left-edge boundary tone thus seems to activate a main clause structure, albeit without suppressing alternative structures. The P600 was also preceded by a short positive effect in cases where a left-edge boundary tone was absent.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0884,"Language-universal Sensory Deficits in Developmental Dyslexia: English, Spanish, and Chinese","Studies in sensory neuroscience reveal the critical importance of accurate sensory perception for cognitive development. There is considerable debate concerning the possible sensory correlates of phonological processing, the primary cognitive risk factor for developmental dyslexia. Across languages, children with dyslexia have a specific difficulty with the neural representation of the phonological structure of speech. The identification of a robust sensory marker of phonological difficulties would enable early identification of risk for developmental dyslexia and early targeted intervention. Here, we explore whether phonological processing difficulties are associated with difficulties in processing acoustic cues to speech rhythm. Speech rhythm is used across languages by infants to segment the speech stream into words and syllables. Early difficulties in perceiving auditory sensory cues to speech rhythm and prosody could lead developmentally to impairments in phonology. We compared matched samples of children with and without dyslexia, learning three very different spoken and written languages, English, Spanish, and Chinese. The key sensory cue measured was rate of onset of the amplitude envelope (rise time), known to be critical for the rhythmic timing of speech. Despite phonological and orthographic differences, for each language, rise time sensitivity was a significant predictor of phonological awareness, and rise time was the only consistent predictor of reading acquisition. The data support a language-universal theory of the neural basis of developmental dyslexia on the basis of rhythmic perception and syllable segmentation. They also suggest that novel remediation strategies on the basis of rhythm and music may offer benefits for phonological and linguistic development.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0885,Influence of Musical Expertise on Segmental and Tonal Processing in Mandarin Chinese,"A same-different task was used to test the hypothesis that musical expertise improves the discrimination of tonal and segmental (consonant, vowel) variations in a tone language, Mandarin Chinese. Two four-word sequences (prime and target) were presented to French musicians and nonmusicians unfamiliar with Mandarin, and event-related brain potentials were recorded. Musicians detected both tonal and segmental variations more accurately than nonmusicians. Moreover, tonal variations were associated with higher error rate than segmental variations and elicited an increased N2/N3 component that developed 100 msec earlier in musicians than in nonmusicians. Finally, musicians also showed enhanced P3b components to both tonal and segmental variations. These results clearly show that musical expertise influenced the perceptual processing as well as the categorization of linguistic contrasts in a foreign language. They show positive music-to-language transfer effects and open new perspectives for the learning of tone languages.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0886,"Monolingual, Non-tone Bilingual, and Tone Bilingual Infants: Language Experiences Alter Speech and Nonspeech Perception","Studies on first-year infants' pitch perception have witnessed shifts of perceptual focus from acoustic to linguistic information and from a wide range of contrasts to those relevant to their native language. Nevertheless, how linguistic experience interacts with this developmental process remains an open question. This study compared the neural discrimination of speech/lexical and nonspeech/violin tone contrasts by 5- to 6- and 11- to 12-month-old infants across three types of language backgrounds: monolingual infants learning a non-tone language (Mono), bilingual infants learning two non-tone languages (Bi-NT), and bilingual infants learning a non-tone and a tone language (Bi-Tone). Although Mono infants do not show significant responses to the lexical tone contrast, both Bi-NT and Bi-Tone infants showed positive mismatch responses at both ages, indicating an enhancement effect brought by a complex language environment as early as 5 months after birth. Regarding the violin tone perception, distinct patterns were observed across language backgrounds: a perceptual decrease for Mono infants, no significant response for Bi-NT infants, and a perceptual increase for Bi-Tone infants over the first year. These patterns suggest that pitch perception may be affected across domains by language experiences at this stage, where interactions in cognitive processing between speech and nonspeech prosodic information may occur.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0887,Neural Evidence for Tonal Prediction: Multivariate Decoding of Predicted Tone Categories Using Functional Magnetic Resonance Imaging Data,"Predictive processing plays a central role in language comprehension, allowing listeners to generate predictions about upcoming linguistic input. Although considerable evidence supports segmental prediction, less is known about whether listeners can form predictions about suprasegmental features such as lexical tone. This study investigates whether listeners can generate and neurally represent predicted tonal information in the absence of auditory input. Using a Mandarin Chinese tone sandhi paradigm, we established tonal predictions based on sentence and visual context, recording brain activity with functional magnetic resonance imaging. Multivariate pattern analysis showed that predicted tonal categories could be decoded from brain activity even without tonal stimuli present. These representations were localized in auditory areas, articulatory motor regions, and the right cerebellum. We also found that predicted tone representations had distinct neural substrates compared to perceived tone representations. The study provides direct neural evidence that listeners can form representations of lexical tone in predictions of auditory input. The findings expand our understanding of suprasegmental prediction in speech and highlight the cerebellum's role in linguistic prediction.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0888,Silent Reading of Direct versus Indirect Speech Activates Voice-selective Areas in the Auditory Cortex,"In human communication, direct speech (e. g., Mary said: ""I'm hungry"") is perceived to be more vivid than indirect speech (e. g., Mary said [that] she was hungry). However, for silent reading, the representational consequences of this distinction are still unclear. Although many of us share the intuition of an ""inner voice,"" particularly during silent reading of direct speech statements in text, there has been little direct empirical confirmation of this experience so far. Combining fMRI with eye tracking in human volunteers, we show that silent reading of direct versus indirect speech engenders differential brain activation in voice-selective areas of the auditory cortex. This suggests that readers are indeed more likely to engage in perceptual simulations (or spontaneous imagery) of the reported speaker's voice when reading direct speech as opposed to meaning-equivalent indirect speech statements as part of a more vivid representation of the former. Our results may be interpreted in line with embodied cognition and form a starting point for more sophisticated interdisciplinary research on the nature of auditory mental simulation during reading.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0889,Challenges and Methods in Annotating Natural Speech for Neurolinguistic Research,"Spoken language is central to human communication, influencing cognition, learning, and social interactions. Despite its spontaneous nature, characterized by disfluencies, fillers, self-corrections and irregular syntax, it effectively serves its communicative purpose. Understanding how the brain processes natural language offers valuable insights into the neurobiology of language. Recent neuroscience advancements allow us to study neural processes in response to ongoing speech, requiring detailed, time-locked descriptions of speech material to capture the nuances of spoken language. While there are many speech-to-text tools available, obtaining a time-locked true verbatim transcript, reflecting everything that was uttered, requires additional effort to achieve an accurate representation. We demonstrate the challenges involved in the process of obtaining time-resolved annotation of spontaneous speech, by presenting two semi-automatic pipelines, developed for German and Hebrew but adaptable to other languages. The outputs of these pipelines enable analyses of the neural representation and processing of key linguistic features. We discuss the methodological challenges and opportunities posed by current state-of-the-art pipelines, and advocate for new lines of natural language processing research aimed at advancing our understanding of how the brain processes everyday language.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0890,Emerging Native-Similar Neural Representations Underlie Non-Native Speech Category Learning Success,"Learning non-native phonetic categories in adulthood is an exceptionally challenging task, characterized by large interindividual differences in learning speed and outcomes. The neurobiological mechanisms underlying the interindividual differences in the learning efficacy are not fully understood. Here we examine the extent to which training-induced neural representations of non-native Mandarin tone categories in English listeners (n = 53) are increasingly similar to those of the native listeners (n = 33) who acquired these categories early in infancy. We assess the extent to which the neural similarities in representational structure between non-native learners and native listeners are robust neuromarkers of interindividual differences in learning success. Using intersubject neural representational similarity (IS-NRS) analysis and predictive modeling on two functional magnetic resonance imaging datasets, we examined the neural representational mechanisms underlying speech category learning success. Learners' neural representations that were significantly similar to the native listeners emerged in brain regions mediating speech perception following training; the extent of the emerging neural similarities with native listeners significantly predicted the learning speed and outcome in learners. The predictive power of IS-NRS outperformed models with other neural representational measures. Furthermore, neural representations underlying successful learning were multidimensional but cost-efficient in nature. The degree of the emergent native-similar neural representations was closely related to the robustness of neural sensitivity to feedback in the frontostriatal network. These findings provide important insights into the experience-dependent representational neuroplasticity underlying successful speech learning in adulthood and could be leveraged in designing individualized feedback-based training paradigms that maximize learning efficacy.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0891,Early Development of Neural Speech Encoding Depends on Age but Not Native Language Status: Evidence From Lexical Tone,"We investigated the development of early-latency and long-latency brain responses to native and non-native speech to shed light on the neurophysiological underpinnings of perceptual narrowing and early language development. Specifically, we postulated a two-level process to explain the decrease in sensitivity to non-native phonemes toward the end of infancy. Neurons at the earlier stages of the ascending auditory pathway mature rapidly during infancy facilitating the encoding of both native and non-native sounds. This growth enables neurons at the later stages of the auditory pathway to assign phonological status to speech according to the infant's native language environment. To test this hypothesis, we collected early-latency and long-latency neural responses to native and non-native lexical tones from 85 Cantonese-learning children aged between 23 days and 24 months, 16 days. As expected, a broad range of presumably subcortical early-latency neural encoding measures grew rapidly and substantially during the first two years for both native and non-native tones. By contrast, long-latency cortical electrophysiological changes occurred on a much slower scale and showed sensitivity to nativeness at around six months. Our study provided a comprehensive understanding of early language development by revealing the complementary roles of earlier and later stages of speech processing in the developing brain.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0892,"Tunes and Tones: Music, Language, and Inhibitory Control","A debate is underway regarding the perceptual and cognitive benefits of bilingualism and musical experience. This study contributes to the debate by investigating auditory inhibitory control in English-speaking monolingual musicians, non-musicians, tone language bilinguals, and non-tone language bilinguals. We predicted that musicians and tone language bilinguals would demonstrate enhanced processing relative to monolinguals and other bilinguals. Groups of monolinguals (N = 22), monolingual musicians (N = 19), non-tone language bilinguals (N = 20) and tone language bilinguals (N = 18) were compared on auditory Stroop tasks to assess domain-transferable processing benefits (e.g. auditory inhibitory control) resulting from potentially shared underlying cognitive mechanisms (Patel, 2003; Bialystok & DePape, 2009). In one task, participants heard the words ""high"" and ""low"" presented in high or low pitches, and responded regarding the pitch of the stimuli as quickly as possible. In another task, participants heard the words ""rise"" or ""fall"" presented in rising or falling pitch contours, and responded regarding the contour of the stimuli as quickly as possible. Results suggest transferable auditory inhibitory control benefits for musicians across pitch and contour processing, but any possible enhanced processing for speakers of tone languages may be task-dependent, as lexical tone activation may interfere with pitch contour processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0893,On the representation and realization of the Ancient Greek acute Evidence from tone-tune mappings in Ancient Greek music,"This study addresses the phonological representation and phonetic realization of pitch patterns found on or near prosodically prominent syllables in Ancient Greek, namely, the distinction between the so-called ""acute"" and ""circumflex"" accents. Empirically, we investigate in detail the correlation between tones and tunes in the Delphic hymns (DAGM 20 and 21) on syllables capable of bearing a circumflex accent (i.e., syllables containing a long vowel or diphthong = VV -syllables). This data supports two major findings. First, VV -syllables with circumflex accent are significantly more likely to be set to a melism than VV -syllables that are acute, grave, or unaccented, and, moreover, the proportion of melismatic settings among acute, unaccented, and grave VVsyllables does not significantly differ. Second, circumflex melisms consistently (always or nearly so) fall in pitch (on average, by three semitones), whereas acute and unaccented melisms may either rise or fall (on average, by 1.5-2.25 semitones in either direction). Taken together, this data conforms to the usual description of the circumflex as a falling pitch, [H L], but speaks against claims that the acute constitutes a rising pitch ([L H], or High alone aligned with the latter portion of a VV -syllable, [0 H]). We instead conclude that the acute represents a single High pitch target phonologically mapped to the entirety of a VV -syllable, and discuss the implications for the phonological analysis of the prosody of Ancient Greek in light of the typology of contour tones.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0894,Tone perception in cantonese-speaking children with hearing aids,"Objectives: In this study we investigated the benefit of using hearing aids for Cantonese tone perception among children with various degrees of hearing impairment. Methods: Forty-eight children with moderate to profound hearing loss were investigated. They were required to perform a lexical tone perception test with recorded test stimuli presented at 65 dB in soundproof booths. To allow for comparison, the subjects performed the test under 2 conditions: with their hearing aids turned off (unaided condition) and with them turned on (aided condition). Results: The mean tone perception scores for the aided condition were higher than those for the unaided condition across all of the subject groups. Paired sample t-tests showed statistically significant improvement in tone perception in the moderate and severe hearing loss groups (p = .02 and p = .03, respectively). The result obtained from the moderately severe hearing loss group was marginally significant (p = .058). The improvement in tone perception in the profound hearing loss group was insignificant (p = .55). Conclusions: The use of a hearing aid is beneficial for Cantonese tone perception in children who have moderate to severe hearing impairment. When a hearing loss is greater than 90 dB, ie, in children who are classified as having profound hearing loss, a hearing aid is not effective in aiding Cantonese tone perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0895,Age Sensitivity in the Acquisition of Lexical Tone Production: Evidence From Children With Profound Congenital Hearing Impairment After Cochlear Implantation,"Objectives: This study investigated the effects of implant experience and age at implantation on the Cantonese tone production of children with cochlear implants The study also examined whether there was a particular age at which children were more responsive to acquiring tones Methods: The study included 45 children who had received unilateral cochlear implants at a mean age of 65 56 months The subjects were grouped according to their age at cochlear implantation and were assessed annually for 5 years thereafter A picture-naming task was used to measure their tone production performance Results: A simple effect of age at implantation was significant at all testing intervals except at the preoperative data point Children who were younger than 4 years of age when they received their implants scored significantly higher than did the 2 older groups at various testing intervals A significant simple effect of implant experience was also found Progress was most striking in children who received their implants before the age of 4 years Conclusions: For the most effective acquisition of Cantonese lexical tones, children should undergo early cochlear implantation For children who receive implants before the age of 4 years. benefits are noted in tone production ability in terms of a faster rate of improvement within a shorter period of time.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0896,A Tale of Two Writing Systems: Double Dissociation and Metalinguistic Transfer Between Chinese and English Word Reading Among Hong Kong Children,"This study investigated the rate of school-aged Chinese-English language learners at risk for reading difficulties in either Chinese or English only, or both, among second and fifth graders in Hong Kong. In addition, we examined the metalinguistic skills that distinguished those who were poor in reading Chinese from those who were poor in reading English. The prevalence of poor English readers among children identified to be poor in Chinese word recognition across the five participating schools was approximately 42% at Grade 2 and 57% at Grade 5. Across grades, children who were poor readers of both languages tended to have difficulties in phonological and morphological awareness. Poor readers of English only were found to manifest significantly poorer phonological awareness, compared to those who were poor readers of Chinese only; their average tone awareness score was also lower relative to normally developing controls. Apart from indicating possible dissociations between Chinese first language (L1) word reading and English second language (L2) word reading, these findings suggested that the degree to which different metalinguistic skills are important for reading in different writing systems may depend on the linguistic features of the particular writing system.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0897,Beyond Auditory Sensory Processing Deficits: Lexical Tone Perception Deficits in Chinese Children With Developmental Dyslexia,"Increasing evidence suggests that children with developmental dyslexia exhibit a deficit not only at the segmental level of phonological processing but also, by extension, at the suprasegmental level. However, it remains unclear whether such a suprasegmental phonological processing deficit is due to a difficulty in processing acoustic cues of speech rhythm, such as rise time and intensity. This study set out to investigate to what extent suprasegmental phonological processing (i.e., Cantonese lexical tone perception) and rise time sensitivity could distinguish Chinese children with dyslexia from typically developing children. Sixteen children with dyslexia and 44 age-matched controls were administered a Cantonese lexical tone perception task, psychoacoustic tasks, a nonverbal reasoning ability task, and word reading and dictation tasks. Children with dyslexia performed worse than controls on Cantonese lexical tone perception, rise time, and intensity. Furthermore, Cantonese lexical tone perception appeared to be a stable indicator that distinguishes children with dyslexia from controls, even after controlling for basic auditory processing skills. These findings suggest that suprasegmental phonological processing (i.e., lexical tone perception) is a potential factor that accounts for reading difficulty in Chinese.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0898,Bidirectional Cross-Linguistic Association of Phonological Skills and Reading Comprehension: Evidence From Hong Kong Chinese-English Bilingual Readers,"This study examined the roles of first-language (L1) Chinese and second-language (L2) English phonological skills in English and Chinese reading comprehension, respectively, and their association with reading comprehension difficulties among Hong Kong Chinese-English bilingual children. We tested 258 second graders on nonverbal intelligence, working memory, phonological skills, word reading, and reading comprehension, in both Chinese and English. Structural equation modeling analyses revealed that Chinese phonological skills contributed to English reading comprehension both directly and indirectly, through the mediation of English phonological skills and English word reading. In contrast, English phonological skills contributed only indirectly to Chinese reading comprehension through L1 Chinese phonological and word reading skills. Furthermore, poor Chinese readers, poor English readers, and poor readers in both Chinese and English exhibited lower levels of lexical tone awareness than average readers, even after controlling for nonverbal intelligence, word reading, and working memory. Poor Chinese readers outperformed poor English readers and poor readers in both Chinese and English on Chinese segmental phonological awareness, and their performance was comparable to average readers. These findings suggest that both suprasegmental and segmental phonological skills are critical to the development of reading comprehension across L1 Chinese and L2 English in Hong Kong Chinese-English bilingual children.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0899,The perception of lexical tone in Mambila,"The issue of the perception of lexical tone has been addressed mainly through studies of Southeast Asian languages which feature phonological contour tones as well as level tones. Little attention has been paid to African languages which have, almost exclusively, only level tones. This paper examines tone perception in Mambila, a Benue-Congo language with four level lexical tones. A categorization experiment was run to determine some of the salient aspects of the perceptual nature of these tones. Since the four tones are well defined with respect to production, we sought to determine whether this characteristic carried over into perception, the expectation being that experimental stimuli, on the basis of pitch height alone, would fall into four reasonably well defined categories. Results showed interesting differences across the four tones, with indications that the two Mid tones, T2 and T3, are perceptually different than the High (T1) and Low (T4) tones. The experiment was run a second time, using a group of native English listeners, to assess to what extent results for the Mambila listeners were determined by the perceptual structure of the Mambila tone system. A Signal Detection analysis was used, which revealed important differences between the two groups of listeners. Results are discussed in light of what is known about universal tendencies of tone systems and the historical development of the Mambila system.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0900,Voornaam is not (really) a homophone: Lexical prosody and lexical access in Dutch,"Four experiments examined Dutch listeners' use of suprasegmental information in spoken-word recognition. Isolated syllables excised from minimal stress pairs such as VOORnaam/voorNAAM could be reliably assigned to their source words. In lexical decision, no priming was observed from one member of minimal stress pairs to the other, suggesting that the pairs' segmental ambiguity was removed by suprasegmental information. Words embedded in nonsense strings were harder to detect if the nonsense string itself formed the beginning of a competing word, but a suprasegmental mismatch to the competing word significantly reduced this inhibition. The same nonsense strings facilitated recognition of the longer words of which they constituted the beginning, but again the facilitation was significantly reduced by suprasegmental mismatch. Together these results indicate that Dutch listeners effectively exploit suprasegmental cues in recognizing spoken words. Nonetheless, suprasegmental mismatch appears to be somewhat less effective in constraining activation than segmental mismatch.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0901,Constraints of lexical stress on lexical access in English: Evidence from native and non-native listeners,"Four cross-modal priming experiments and two forced-choice identification experiments investigated the use of suprasegmental cues to stress in the recognition of spoken English words, by native (English-speaking) and non-native (Dutch) listeners. Previous results had indicated that suprasegmental information was exploited in lexical access by Dutch but not by English listeners. For both listener groups, recognition of visually presented target words was faster, in comparison to a control condition, after stress-matching spoken primes, either monosyllabic (mus-from MUsic/muSEum) or bisyllabic (admi-from ADmiral / admiRAtion). For native, listeners, the effect of stress-mismatching bisyllabic primes was not different from that of control primes, but mismatching monosyllabic primes produced partial facilitation. For non-native listeners, both bisyllabic and monosyllabic stress-mismatching primes produced partial facilitation. Native English listeners thus can exploit suprasegmental information in spoken-word recognition, but information from two syllables is used more effectively than information from one syllable. Dutch listeners are less proficient at using suprasegmental information in English than in their native language, but, as in their native language, use mono- and bisyllabic information to an equal extent. In forced-choice identification, Dutch listeners outperformed native listeners at correctly assigning a monosyllabic fragment (e.g., mus-) to one of two words differing in stress.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0902,"Phonetic diversity, statistical learning, and acquisition of phonology","In learning to perceive and produce speech, children master complex language-specific patterns. Daunting language-specific variation is found both in the segmental domain and in the domain of prosody and intonation. This article reviews the challenges posed by results in phonetic typology and sociolinguistics for the theory of language acquisition. It argues that categories are initiated bottom-up from statistical modes in use of the phonetic space, and sketches how exemplar theory can be used to model the updating of categories once they are initiated. It also argues that bottom-up initiation of categories is successful thanks to the perception-production loop operating in the speech community. The behavior of this loop means that the superficial statistical properties of speech available to the infant indirectly reflect the contrastiveness and discriminability of categories in the adult grammar. The article also argues that the developing system is refined using internal feedback from type statistics over the lexicon, once the lexicon is well-developed. The application of type statistics to a system initiated with surface statistics does not cause a fundamental reorganization of the system. Instead, it exploits confluences across levels of representation which characterize human language and make bootstrapping possible.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0903,Pitch accent alignment in romance: Primary and secondary associations with metrical structure,"The article describes the contrastive possibilities of alignment of high accents in three Romance varieties, namely, Central Catalan, Neapolitan Italian, and Pisa Italian. The Romance languages analyzed in this article provide crucial evidence that small differences in alignment in rising accents should be encoded phonologically. To account for such facts within the AM model, the ""phonological anchoring'' as an extension of article develops the notion of the concept of secondary association originally proposed by Pierrehumbert and Beckman (1988), and later adopted by Grice (1995), Grice, Ladd, and Arvaniti (2000), and others to explain the behavior of edge tones. The Romance data represent evidence that not only peripheral edge tones seek secondary associations. We claim that the phonological representation of pitch accents should include two independent mechanisms to encode alignment properties with metrical structure: (1) encoding of the primary phonological association (or affiliation) between the tone and its tone-bearing unit; and (2), for some specific cases, encoding of the secondary phonological anchoring of tones to prosodic edges (moras, syllables, and prosodic words). The Romance data described in the article provide crucial evidence of mora-edge, syllable-edge, and word-edge H tonal associations.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0904,Does horse activate mother? Processing lexical tone in form priming,"Lexical tone languages make up the majority of all known languages of the world, but the role of tone in lexical processing remains unclear. In the present study, four form priming experiments examined the role of Mandarin tones in constraining lexical activation and the time course of the activation. When a prime and a target were related directly in form (e.g., lou3 'hug'-lou2 'hall'), competitors that differed from the prime in tone failed to be activated, indicating the use of tonal information to distinguish between segmentally identical words. When a prime and a target were not form-related but were related through a third word that was not actually presented (e.g., lou3 'hug'-jian4zhu0 'building', where lou3 is form-related to lou2 'hall', which was semantically related to jian4zhu0), a mismatch in tone prevented activation of minimal tone pairs at 250 ms interstimulus interval (ISI) but did not prevent activation at 50 ms ISI. These results indicate that tonal information is used on-line to reduce the number of activated candidates, but does not prevent the minimal tone pairs from being activated in the early phase of lexical activation.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0905,Identification of Acoustically Modified Mandarin Tones by Non-native Listeners,"This study investigated identification of fragmented Mandarin tones by non-native listeners. Monosyllabic Mandarin words were digitally processed to generate intact, silent-center, center-only, and onset-only syllables. The syllables were recorded with two carrier phrases such that the offset of the carrier tone and the onset of the target tone were either continuous or discontinuous in fundamental frequency (F0). The syllables were presented with an original carrier phrase, excised from the carrier phrase, or excised and cross-spliced with another carrier phrase. Response accuracy and reaction time were measured, and tone confusion patterns were analyzed. Overall, tone identification varied as a function of modification and tone. Intact and center-only syllables were identified more accurately than silent-center and onset-only syllables. Tone 2 was consistently the most challenging tone to identify. Although the performance level of the third-year students approached that of native listeners reported in Lee, Tao, and Bond (2008), the non-native listeners did not show evidence of using coarticulatory information. Nonetheless, the continuity or discontinuity in F0 between the carrier and target tones did affect tone identification, suggesting the influence of context in non-native tone identification.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0906,Contextual Evidence for the Representation of Pitch Accents in Standard Serbian,"This paper reports the results of an experiment that elicits contextual effects on Rising and Falling accents in Standard Serbian, with the goal of determining their acoustic correlates and their phonological representation. Materials systematically vary the distance between pitch accents, inducing ""tone crowding,"" in order to identify the phonetic dimensions that consistently distinguish the two pitch accent types, to examine the association between accents and the segmental string, as well as the timing relationship between accent minima and maxima, and to investigate the interaction between lexical accents and boundary tones. On the basis of the phonetic findings, a unified analysis of the phonological distribution and phonetic realization of Falling and Rising accents in Standard Serbian is proposed. It is proposed that both Rising and Falling accents consist of a single lexical High (H). The restricted distribution of the two accents emerges from the interaction of stress and tone: Falling accents are monosyllabic, such that stress and pitch prominence coincide; Rising accents are bisyllabic, such that the stressed syllable precedes the pitch-accented syllable. The phonetic differences between the Falling and Rising accents follow from the place of lexically designated H, the location of stress, and the effects of boundary tones. The larger issue we address concerns the phonological characterization of tone/stress interactions. Given the two general types of interactions, one in which the place of stress is predictable from the place of tone, and the other with the reversed direction of influence, we analyze Standard Serbian as belonging to the former type. While both types can be characterized in systems of tonal phonology, which allow free interaction of tone and stress, the type exemplified by Standard Serbian, with contrastive tonal specifications governing the distribution of stress, cannot be captured in an Autosegmental-Metrical (AM) framework, in which stress serves as anchor for tonal melodies.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0907,The Contribution of Segmental and Tonal Information in Mandarin Spoken Word Processing,"Two priming experiments examined the separate contribution of lexical tone and segmental information in the processing of spoken words in Mandarin Chinese. Experiment 1 contrasted four types of prime-target pairs: tone-and-segment overlap (ru4-ru4), segment-only overlap (ru3-ru4), tone-only overlap (sha4-ru4) and unrelated (qin1-ru4) in an auditory lexical decision task with 48 native Mandarin listeners. Experiment 2 further investigated the minimal segmental overlap needed to trigger priming when tonal information is present. Four prime-target conditions were contrasted: tone-and-segment overlap (ru4-ru4), only onset segment overlap (re4-ru4), only rime overlap (pu4-ru4) and unrelated (qin1-ru4) in an auditory lexical decision task with 68 native Mandarin listeners. The results showed significant priming effects when both tonal and segmental information overlapped or, although to a lesser extent, when only segmental information overlapped, with no priming found when only tones matched. Moreover, any partial segmental overlap, even with matching tonal cues, resulted in significant inhibition. These data clearly indicate that lexical tones are processed differently from segments, with syllabic structure playing a critical role. These findings are discussed in terms of the overall architecture of the processing system that emerges in Mandarin lexical access.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0908,Inferring Difficulty: Flexibility in the Real-time Processing of Disfluency,"Upon hearing a disfluent referring expression, listeners expect the speaker to refer to an object that is previously unmentioned, an object that does not have a straightforward label, or an object that requires a longer description. Two visual-world eye-tracking experiments examined whether listeners directly associate disfluency with these properties of objects, or whether disfluency attribution is more flexible and involves situation-specific inferences. Since in natural situations reference to objects that do not have a straightforward label or that require a longer description is correlated with both production difficulty and with disfluency, we used a mini-artificial lexicon to dissociate difficulty from these properties, building on the fact that recently learned names take longer to produce than existing words in one's mental lexicon. The results demonstrate that disfluency attribution involves situation-specific inferences; we propose that in new situations listeners spontaneously infer what may cause production difficulty. However, the results show that these situation-specific inferences are limited in scope: listeners assessed difficulty relative to their own experience with the artificial names, and did not adapt to the assumed knowledge of the speaker.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0909,Tune in to the Tone: Lexical Tone Identification is Associated with Vocabulary and Word Recognition Abilities in Young Chinese Children,"Lexical tone is one of the most prominent features in the phonological representation of words in Chinese. However, little, if any, research to date has directly evaluated how young Chinese children's lexical tone identification skills contribute to vocabulary acquisition and character recognition. The present study distinguished lexical tones from segmental phonological awareness and morphological awareness in order to estimate the unique contribution of lexical tone in early vocabulary acquisition and character recognition. A sample of 199 Cantonese children aged 5-6 years was assessed on measures of lexical tone identification, segmental phonological awareness, morphological awareness, nonverbal ability, vocabulary knowledge, and Chinese character recognition. It was found that lexical tone awareness and morphological awareness were both associated with vocabulary knowledge and character recognition. However, there was a significant relationship between lexical tone awareness and both vocabulary knowledge and character recognition, even after controlling for the effects of age, nonverbal ability, segmental phonological awareness and morphological awareness. These findings suggest that lexical tone is a key factor accounting for individual variance in young children's lexical acquisition in Chinese, and that lexical tone should be considered in understanding how children learn new Chinese vocabulary words, in either oral or written forms.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0910,"Constraints of Tones, Vowels and Consonants on Lexical Selection in Mandarin Chinese","Previous studies have shown that when speakers of European languages are asked to turn nonwords into words by altering either a vowel or consonant, they tend to treat vowels as more mutable than consonants. These results inspired the universal vowel mutability hypothesis: listeners learn to cope with vowel variability because vowel information constrains lexical selection less tightly and allows for more potential candidates than does consonant information. The present study extends the word reconstruction paradigm to Mandarin Chinese-a Sino-Tibetan language, which makes use of lexically contrastive tone. Native speakers listened to word-like nonwords (e.g., su3) and were asked to change them into words by manipulating a single consonant (e.g., tu3), vowel (e.g., si3), or tone (e.g., su4). Additionally, items were presented in a fourth condition in which participants could change any part. The participants' reaction times and responses were recorded. Results revealed that participants responded faster and more accurately in both the free response and the tonal change conditions. Unlike previous reconstruction studies on European languages, where vowels were changed faster and more often than consonants, these results demonstrate that, in Mandarin, changes to vowels and consonants were both overshadowed by changes to tone, which was the preferred modification to the stimulus nonwords, while changes to vowels were the slowest and least accurate. Our findings show that the universal vowel mutability hypothesis is not consistent with a tonal language, that Mandarin tonal information is lower-priority than consonants and vowels and that vowel information most tightly constrains Mandarin lexical access.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0911,Does Second Language Experience Modulate Perception of Tones in a Third Language?,"It is unclear what roles native language (L1) and second language (L2) play in the perception of lexical tones in a third language (L3). In tone perception, listeners with different language backgrounds use different fundamental frequency (F0). While English listeners use F0 height, Mandarin listeners rely more on F0 direction. The present study addresses whether knowledge of Mandarin, particularly as an L2, results in speakers' reliance on F0 direction in their perception of L3 (Cantonese) tones. Fifteen English-speaking L2 learners of Mandarin constituted the target group, and 15 English monolinguals and 15 native Mandarin speakers, with no background in other tonal languages, were included as control groups. All groups had to discriminate Cantonese tones either by distinguishing a contour tone from a level tone (F0 direction pair) or a level tone from another level tone (F0 height pair). The results showed that L2 learners patterned differently from both control groups by using F0 direction as well as F0 height under the influence of L1 and L2 experience. The acoustics of the tones also affected all listeners' discrimination. When L2 and L3 are similar in terms of the presence of lexical tone, L2 experience modulates the perception of L3 tones.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0912,Separability of Tones and Rhymes in Chinese Speech Perception: Evidence from Perceptual Migrations,"This study used the perceptual-migration paradigm to explore whether Mandarin tones and syllable rhymes are processed separately during Mandarin speech perception. Following the logic of illusory conjunctions, we calculated the cross-ear migration of tones, rhymes, and their combination in Chinese and English listeners. For Chinese listeners, tones migrated more than rhymes. For English listeners, the opposite pattern was found. The results lend empirical support to autosegmental theory, which claims separability and mobility between tonal and segmental representations. They also provide evidence that such representations and their involvement in perception are deeply shaped by a listener's linguistic experience.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0913,Early L2 Spoken Word Recognition Combines Input-Based and Knowledge-Based Processing,"This study examines the perceptual trade-off between knowledge of a language's statistical regularities and reliance on the acoustic signal during L2 spoken word recognition. We test how early learners track and make use of segmental and suprasegmental cues and their relative frequencies during non-native word recognition. English learners of Mandarin were taught an artificial tonal language in which a tone's informativeness for word identification varied according to neighborhood density. The stimuli mimicked Mandarin's uneven distribution of syllable+tone combinations by varying syllable frequency and the probability of particular tones co-occurring with a particular syllable. Use of statistical regularities was measured by four-alternative forced-choice judgments and by eye fixations to target and competitor symbols. Half of the participants were trained on one speaker, that is, low speaker variability while the other half were trained on four speakers. After four days of learning, the results confirmed that tones are processed according to their informativeness. Eye movements to the newly learned symbols demonstrated that L2 learners use tonal probabilities at an early stage of word recognition, regardless of speaker variability. The amount of variability in the signal, however, influenced the time course of recovery from incorrect anticipatory looks: participants exposed to low speaker variability recovered from incorrect probability-based predictions of tone more rapidly than participants exposed to greater variability. These results motivate two conclusions: early L2 learners track the distribution of segmental and suprasegmental co-occurrences and make predictions accordingly during spoken word recognition; and when the acoustic input is more variable because of multi-speaker input, listeners rely more on their knowledge of tone-syllable co-occurrence frequency distributions and less on the incoming acoustic signal.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0914,The Influence of Tonal and Atonal Bilingualism on Children's Lexical and Non-Lexical Tone Perception,"This study examined how bilingualism in an atonal language, in addition to a tonal language, influences lexical and non-lexical tone perception and word learning during childhood. Forty children aged 5;3-7;2, bilingual either in English and Mandarin or English and another atonal language, were tested on Mandarin lexical tone discrimination, level-pitch sine-wave tone discrimination, and learning of novel words differing minimally in Mandarin lexical tone. Mandarin-English bilingual children discriminated between and learned novel words differing minimally in Mandarin lexical tone more accurately than their atonal-English bilingual peers. However, Mandarin-English and atonal-English bilingual children discriminated between level-pitch sine-wave tones with similar accuracy. Moreover, atonal-English bilingual children showed a tendency to perceive differing Mandarin lexical and level-pitch sine-wave tones as identical, whereas their Mandarin-English peers showed no such tendency. These results indicate that bilingualism in a tonal language in addition to an atonal language-but not bilingualism in two atonal languages-allows for continued sensitivity to lexical tone beyond infancy. Moreover, they suggest that although tonal-atonal bilingualism does not enhance sensitivity to differences in pitch between sine-wave tones beyond infancy any more effectively than atonal-atonal bilingualism, it protects against the development of biases to perceive differing lexical and non-lexical tones as identical. Together, the results indicate that, beyond infancy, tonal-atonal bilinguals process lexical tones using different cognitive mechanisms than atonal-atonal bilinguals, but that both groups process level-pitch non-lexical tone using the same cognitive mechanisms.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0915,The Language-specific Use of Fundamental Frequency Rise in Segmentation of an Artificial Language: Evidence from Listeners of Taiwanese Southern Min,"Experience with native-language prosody encourages language-specific strategies for speech segmentation. Conflicting findings from previous research suggest that these strategies may not be abstracted away from the acoustic manifestation of prosodic features in the native speech. Using the artificial language learning paradigm, the current study explores this possibility in connection with listeners of a lexical tone language called Taiwanese Southern Min (TSM). In TSM, the only rising lexical tone occurs almost only on the final syllable of the language's tone sandhi domain and is phonetically associated with final lengthening. Based on these observations, Experiment I examined what constituted a sufficient finality cue for use by TSM listeners to support segmentation: (a) final fundamental frequency (F0) rise only; or (b) final F0 rise conjoined with final lengthening. The results showed that segmentation was inhibited by the former cue but facilitated by the latter. Experiment II showed that the facilitation cannot be attributed entirely to final lengthening, as a null effect was found when final lengthening was the sole prosodic cue to segmentation. It is thus assumed that acoustic details as fine-grained as the lengthening of the rising tone are involved in the modulation of the segmentation strategy whereby TSM listeners perceive F0 rise as signaling finality. The inhibitory effect of final F0 rise alone found in Experiment I motivated Experiment III, which revealed that initial F0 rise in the absence of lengthening cues improved TSM listeners' segmentation. It is speculated that such use of initial F0 rise might reflect a cross-linguistic segmentation solution.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0916,Chinese-English Speakers' Perception of Pitch in Their Non-Tonal Language: Reinterpreting English as a Tonal-Like Language,"Changing the F0-contour of English words does not change their lexical meaning. However, it changes the meaning in tonal languages such as Mandarin. Given this important difference and knowing that words in the two languages of a bilingual lexicon interact, the question arises as to how Mandarin-English speakers process pitch in their bilingual lexicon. The few studies that addressed this question showed that Mandarin-English speakers did not perceive pitch in English words as native English speakers did. These studies, however, used English words as stimuli failing to examine nonwords and Mandarin words. Consequently, possible pre-lexical effects and L1 transfer were not ruled out. The present study fills this gap by examining pitch perception in Mandarin and English words and nonwords by Mandarin-English speakers and a group of native English controls. Results showed the tonal experience of Chinese-English speakers modulated their perception of pitch in their non-tonal language at both pre-lexical and lexical levels. In comparison to native English controls, tonal speakers were more sensitive to the acoustic salience of F0-contours in the pre-lexical processing due to top-down feedback. At the lexical level, Mandarin-English speakers organized words in their two languages according to similarity criteria based on both F0 and segmental information, whereas only the segmental information was relevant to the control group. These results in perception together with consistently reported production patterns in previous literature suggest that Mandarin-English speakers process pitch in English as if it was a one-tone language.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0917,Stop Laryngeal Distinctions Driven by Contrastive Effects of Neighboring Tones,"This study examined contrastive effects of neighboring tones that give rise to a systematic asymmetry in stop perception. Korean-speaking learners of Mandarin Chinese and naive listeners labeled voiceless unaspirated stops preceded or followed by low or high extrinsic tonal context (e.g., ma(LO).pa vs. ma(HI).pa) either as lenis (associated with a low F0 at the vowel onset) or as fortis stops (with a high F0). Further, the target tone itself varied between level and rising (e.g., ma(LO).pa(LEV) vs. ma(LO).pa(RIS)). Both groups of listeners showed significant contrastive effects of extrinsic context. Specifically, more lenis responses were elicited in a high tone context than in a low one, and vice versa. This indicates that the onset F0 of a stop is perceived lower in a high tone context, which, in turn, provides positive evidence for lenis stops. This effect was more clearly pronounced for the level than for the contour tone target and also for the preceding than for the following context irrespective of linguistic experience. Despite qualitative similarities, learners showed larger effects for all F0 variables, indicating that the degree of context effects may be enhanced by one's phonetic knowledge, namely sensitivity to F0 cues along with the processing of consecutive tones acquired through learning a tone language.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0918,Context Matters for Tone and Intonation Processing in Mandarin,"In tonal languages such as Mandarin, both lexical tone and sentence intonation are primarily signaled by F0. Their F0 encodings are sometimes in conflict and sometimes in congruency. The present study investigated how tone and intonation, with F0 encodings in conflict or in congruency, are processed and how semantic context may affect their processing. To this end, tone and intonation identification experiments were conducted in both semantically neutral and constraining contexts. Results showed that the overall performance of tone identification was better than that of intonation. Specifically, tone identification was seldom affected by intonation information irrespective of semantic contexts. However, intonation identification, particularly question intonation, was susceptible to the final lexical tone identity and affected by the semantic context. In the semantically neutral context, questions ending with a rising tone and a falling tone were equally difficult to identify. In the semantically constraining context, questions ending with a falling tone were much better identified than those ending with a rising tone. This perceptual asymmetry suggests that top-down information provided by the semantically constraining context can play a facilitating role for listeners to disentangle intonational information from tonal information, but mainly in sentences with the lexical falling tone in the final position.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0919,Evidence For Selective Adaptation and Recalibration in the Perception of Lexical Stress,"Individuals vary in how they produce speech. This variability affects both the segments (vowels and consonants) and the suprasegmental properties of their speech (prosody). Previous literature has demonstrated that listeners can adapt to variability in how different talkers pronounce the segments of speech. This study shows that listeners can also adapt to variability in how talkers produce lexical stress. Experiment 1 demonstrates a selective adaptation effect in lexical stress perception: repeatedly hearing Dutch trochaic words biased perception of a subsequent lexical stress continuum towards more iamb responses. Experiment 2 demonstrates a recalibration effect in lexical stress perception: when ambiguous suprasegmental cues to lexical stress were disambiguated by lexical orthographic context as signaling a trochaic word in an exposure phase, Dutch participants categorized a subsequent test continuum as more trochee-like. Moreover, the selective adaptation and recalibration effects generalized to novel words, not encountered during exposure. Together, the experiments demonstrate that listeners also flexibly adapt to variability in the suprasegmental properties of speech, thus expanding our understanding of the utility of listener adaptation in speech perception. Moreover, the combined outcomes speak for an architecture of spoken word recognition involving abstract prosodic representations at a prelexical level of analysis.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0920,Phonetic and Lexical Encoding of Tone in Cantonese Heritage Speakers,"Heritage speakers contend with at least two languages: the less dominant first language (L1), that is, the heritage language, and the more dominant second language (L2). In some cases, their L1 and L2 bear striking phonological differences. In the current study, we investigate Toronto-born Cantonese heritage speakers and their maintenance of Cantonese lexical tone, a linguistic feature that is absent from English, the more dominant L2. Across two experiments, Cantonese heritage speakers were tested on their phonetic/phonological and lexical encoding of tone in Cantonese. Experiment 1 was an AX discrimination task with varying inter-stimulus intervals (ISIs), which revealed that heritage speakers discriminated tone pairs with disparate pitch contours better than those with shared pitch contours. Experiment 2 was a medium-term repetition priming experiment, designed to extend the findings of Experiment 1 by examining tone representations at the lexical level. We observed a positive correlation between English dominance and priming in tone minimal pairs that shared contours. Thus, while increased English dominance does not affect heritage speakers' phonological-level representations, tasks that require lexical access suggest that heritage Cantonese speakers may not robustly and fully distinctively encode Cantonese tone in lexical memory.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0921,Phonetic and Phono-Lexical Accuracy of Non-Native Tone Production by English-L1 and Mandarin-L1 Speakers,"Lexical tones are known to be a challenging aspect of speech to acquire in a second language, but several factors are known to affect tone learning facility, such as L1 tonal status (whether a learner's L1 is tonal or not), tone type (the shape of the tones to be acquired), and individual extralinguistic factors (such as musicianship, pitch aptitude, and working memory). Crucially, most of our knowledge of the effect of these factors is based on evidence from perception. The production side of tone learning and the origins of individual variability in learning facility remain relatively understudied. To this end, this study investigated non-native tone production-both in terms of phonetic accuracy in a pseudoword imitation task and in terms of phono-lexical accuracy in a picture-naming task-by English-L1 and Mandarin-L1 speakers. Results show that L1 tonal status and tone type dynamically affected both imitation and picture-naming accuracy, as there were specific accuracy patterns for the English and Mandarin groups. Production accuracy was further facilitated by individual musical experience, working memory, and pitch aptitude. This study's findings add to the currently limited literature on how both language-specific and individual extralinguistic factors modulate non-native tone processing in the speaking modality.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0922,Kinect-ing the Dots: Using Motion-Capture Technology to Distinguish Sign Language Linguistic From Gestural Expressions,"Just as vocalization proceeds in a continuous stream in speech, so too do movements of the hands, face, and body in sign languages. Here, we use motion-capture technology to distinguish lexical signs in sign language from other common types of expression in the signing stream. One type of expression is constructed action, the enactment of (aspects of) referents and events by (parts of) the body. Another is classifier constructions, the manual representation of analogue and gradient motions and locations simultaneously with specified referent morphemes. The term signing is commonly used for all of these, but we show that not all visual signals in sign languages are of the same type. In this study of Israeli Sign Language, we use motion capture to show that the motion of lexical signs differs significantly along several kinematic parameters from that of the two other modes of expression: constructed action and the classifier forms. In so doing, we show how motion-capture technology can help to define the universal linguistic category ""word,"" and to distinguish it from the expressive gestural elements that are commonly found across sign languages.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0923,The Syntactic Pasts of Nouns Shape Their Prosodic Future: Lexico-Syntactic Effects on Position and Duration,"Phrasal prosody is often viewed as a level of linguistic representation at which the phonetic profile of an utterance varies independently of the lexical items it contains. For example, the same word, when produced at the edges of prosodic phrases, will take longer to produce than when it is produced within the edges of a phrase. Lengthening effects have also been found for words when placed in different syntactic or lexical contexts. Recent evidence suggests that lexico-syntactic information-for example, the global syntactic distributions of words-affects phonetic duration in production, irrespective of other factors. The present study asks whether these lexico-syntactic effects on duration interact with prosodic position within the phrase. Specifically, we ask whether (a) the lexico-syntactic information of a word determines its prosodic position, and (b) whether, beyond any categorical effects on positioning, lexico-syntactic factors affect duration within prosodic positions. We address these questions using the Santa Barbara Corpus of Spoken American English. We operationalize syntactic information as the diversity and the typicality of the syntactic distributions of nouns based on a dependency parse of the British National Corpus. We find that earlier positions in the prosodic phrase generally prefer words with higher syntactic diversity. In addition, diversity and typicality modulate duration more reliably in nonfinal positions. Together, our results point to an early influence of lexico-syntactic considerations on prosodic planning.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0924,Modeling Lexical Tones for Speaker Discrimination,"Fundamental frequency (F0) has been widely studied and used in the context of speaker discrimination and forensic voice comparison casework, but most previous studies focused on long-term F0 statistics. Lexical tone, the linguistically structured and dynamic aspects of F0, has received much less research attention. A main methodological issue lies on how tonal F0 should be parameterized for the best speaker discrimination performance. This paper compares the speaker discriminatory performance of three approaches with lexical tone modeling: discrete cosine transform (DCT), polynomial curve fitting, and quantitative target approximation (qTA). Results show that using parameters based on DCT and polynomials led to similarly promising performance, whereas those based on qTA generally yielded relatively poor performance. Implications modeling surface tonal F0 and the underlying articulatory processes for speaker discrimination are discussed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0925,Flexibility and Stability in Lexical Tone Recalibration: Evidence from Tone Perceptual Learning,"Listeners adjust their perception of sound categories when confronted with variations in speech. Previous research on speech recalibration has primarily focused on segmental variation, demonstrating that recalibration tends to be specific to individual speakers and situations and often persists over time. In this study, we present findings on the perceptual learning of lexical tone in Standard Chinese, a suprasegmental feature signaled primarily through pitch variations to distinguish morpheme/word meanings. Native speakers of Standard Chinese showed a recalibration of tone category boundaries immediately following exposure to ambiguous tonal pitch contours. However, this recalibration effect significantly weakened after 12 hours. Furthermore, participants trained at night did not exhibit delayed stabilization, a phenomenon commonly observed during sleep-induced consolidation. Our results replicate previous findings and provide new evidence suggesting that while our perceptual system can flexibly adapt to real-time sensory inputs, subsequent consolidation processes, such as those occurring during sleep, may exhibit selectivity and, under certain conditions, may be ineffective.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0926,The Perception of Lexical Pitch Accent in South Kyungsang Korean: The Relevance of Accent Shape,"In this study, we tested whether the perception of pitch contours within a lexical pitch accent can be better understood through tonal targets in the Autosegmental-Metrical (AM) model or as an entire tonal configuration identification. Specifically, a categorization experiment was conducted to see how South Kyungsang Korean (SKK) listeners perceive their high (H) and rising (LH) lexical pitch accents. Auditory stimuli were manipulated depending on H peak alignment (earlier vs. later), rise shape (domed or ""convex"" vs. scooped or ""concave""), or segmental duration (shorter vs. longer). Results showed that F0 rise shape and segmental duration influenced SKK listeners' categorization, while no effect of peak alignment was observed. Specifically, they responded to more scooped shapes as an LH, while more domed shapes were mainly assigned to H responses. Moreover, shorter duration induced a H categorization, while longer duration was related to an LH. Results suggest that SKK listeners use both F0 shape and segmental duration as important cues for tonal contrast, though F0 shape shows stronger categorical effect than duration. Thus, F0 shape information is important to determine phonological representation of lexical pitch accents, as opposed to strict tonal alignment defined in Autosegmental-Metrical theory.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0927,THE INTERACTION OF COARTICULATION AND PROSODY IN SOUND CHANGE,"Ohala (1974, 1981a) has proposed that sound changes can originate in hearers' misinterpretations of synchronic phonetic patterns. This paper applies this idea to sound changes that are conditioned by the prosodic environment, such as the voicing of voiceless fricatives in unstressed syllables in Proto-Germanic. Browman and Goldstein's (1989, 1990) ""gestural score"" suggests a representation of synchronic patterns in which extreme overlap between gestures of neighboring phoneme segments in casual speech can produce the appearance of a feature change or a segment deletion. Many of the sound changes that are conditioned by prosodic environment can be viewed as a diachronic reinterpretation of just such synchronic fast-speech processes. For example, vowel reduction in unstressed syllables can be viewed as a reinterpretation of undershoot that occurs when the vowel is overlapped to a great extent by the oral gestures for neighboring consonants. Phonetic data are reviewed that support analogous accounts of stop spirantization, voiceless obstruent voicing, and even the insertion of an intrusive stop in clusters such as /ns/ in some prosodic environments.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0928,"Auditory Perception, Suprasegmental Speech Processing, and Vocabulary Development in Chinese Preschoolers","The current study examined the associations between basic auditory perception, speech prosodic processing, and vocabulary development in Chinese kindergartners, specifically, whether early basic auditory perception may be related to linguistic prosodic processing in Chinese Mandarin vocabulary acquisition. A series of language, auditory, and linguistic prosodic tests were given to 100 preschool children who had not yet learned how to read Chinese characters. The results suggested that lexical tone sensitivity and intonation production were significantly correlated with children's general vocabulary abilities. In particular, tone awareness was associated with comprehensive language development, whereas intonation production was associated with both comprehensive and expressive language development. Regression analyses revealed that tone sensitivity accounted for 36% of the unique variance in vocabulary development, whereas intonation production accounted for 6% of the variance in vocabulary development. Moreover, auditory frequency discrimination was significantly correlated with lexical tone sensitivity, syllable duration discrimination, and intonation production in Mandarin Chinese. Also it provided significant contributions to tone sensitivity and intonation production. Auditory frequency discrimination may indirectly affect early vocabulary development through Chinese speech prosody.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0929,Separating the Novel Speech Sound Perception of Lexical Tone Chimeras From Their Auditory Signal Manipulations: Behavioral and Electroencephalographic Evidence,"Previous research has shown the novelty of lexical-tone chimeras (artificially constructed speech sounds created by combining normal speech sounds of a given language) to native speakers of the language from which the chimera components were drawn. However, the source of such novelty remains unclear. Our goal in this study was to separate the effects of chimeric tonal novelty in Mandarin speech from the effects of auditory signal manipulations. We recruited 20 native speakers of Mandarin and constructed two sets of lexical-tone chimeras by interchanging the envelopes and fine structures of both a falling/yi(4)/and a rising/yi(2)/Mandarin tone through 1, 2, 3, 4, 6, 8, 16, 32, and 64 auditory filter banks. We conducted pitch-perception ability tasks via a two-alternative, forced-choice paradigm to produce behavioral (versus physiological) pitch perception data. We also obtained electroencephalographic measurements through the scalp-recorded frequency-following response (FFR). Analyses of variances and post hoc Greenhouse-Geisser procedures revealed that the differences observed in the participants' reaction times and FFR measurements were attributable primarily to chimeric novelty rather than signal manipulation effects. These findings can be useful in assessing neuroplasticity and developing speech-processing strategies.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0930,A Comparison of Stimulus Variability in Lexical Tone and Melody Perception,"Music and language share perceptual resources, and both map sound to invariant categories-invariant over and within speakers for language and over instruments and keys for music. The effects of stimulus variability on lexical tone and musical interval tasks among non-tone language speakers were compared using a matching (XAB) task under varying levels of stimulus variability. Listeners perceived Mandarin words better with single rather than multiple speakers and showed similar advantages in melodic interval perception for low (single instrument) versus high (multiple instruments) variability sets. Lexical tone and musical interval perception were affected similarly by increasing stimulus variability, on average. However, the magnitude of variability effects within subjects was not well correlated between the tasks, providing no evidence for shared category-mapping mechanism for the two domains. Instead, it suggests that crossover between tone and melody processing is driven by shared encoding of acoustic-phonetic features, and that differences in performance and learning by tone language speakers and musicians in the other domain represent progress along a phonetic-phonological-lexical continuum.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0931,The effect of tone hyperarticulation in Cantonese infant-directed speech on toddlers' word recognition in the second year of life,"The acoustic properties of infant-directed speech (IDS) have been widely studied, but whether and how young learners' language development benefits from individual properties remains to be confirmed. This study investigated whether toddlers' word processing was affected by tone hyperarticulation in the IDS of a tone language. Nineteen- and 23-month-old Cantonese-learning toddlers completed a familiar word recognition task and were tested (a) in the hyperarticulated-tone (HT) condition in which the tonal distances were exaggerated, and (b) in the non-hyperarticulated-tone (NT) condition with smaller tonal distances that resembled those in adult-directed speech. The 19-month-old toddlers performed significantly better in the HT condition than in the NT condition, while the 23-month-olds performed comparably well in both conditions. These findings suggest that tone language learners' word recognition can be facilitated by tone hyperarticulation in IDS, in the middle of the second year of life; as their language development proceeds, this facilitatory effect appears to largely diminish by the end of the second year of life.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0932,Efficacy of an integrated intervention with vocabulary and phonetic training for Mandarin-speaking children with developmental language disorders,"Background: Children with developmental language disorder (DLD) demonstrate deficits in vocabulary development and novel word learning processes, which have been proposed to stem from their speech perception deficits. Aims: This study had two aims. The first was to evaluate the efficacy of an intervention incorporating a computer-based phonetic training programme with rich and explicit vocabulary instruction. The second aim was to investigate the adjuvant treatment effect of phonetic training on word learning performance. Methods and procedures: The experimental group comprised 34 children with DLD aged 5-7 years, and the control group comprised another 15 children with DLD. All participants in the experimental group attended 1-h direct vocabulary instruction classes weekly for 9 weeks. They were also asked to play computer-based phonetic training games individually at home or school. Pre-post differences between groups in word definition and speech perception tasks were examined. The relationship between the experimental group's vocabulary learning performance and scores on the phonetic training games was examined using correlation analysis. Results: Between-group comparisons revealed that children in the experimental group exhibited significantly greater pre-post gains in word definition task than did the control group. Children in the experimental group performed better in the lexical tone posttest No improvement in the speech discrimination tasks was detected in the control group. Finally, correlation analyses indicated a positive relationship between the total number of phonetic games played and the pre-post gain in the word definition production task. The children's response accuracy in the lexical tone discrimination games was significantly correlated with the performance of the posttest tone discrimination task and the pre-post gain in the word definition task. Conclusions: The results of this study suggest that vocabulary intervention programmes incorporating speech perception training promote children's vocabulary and speech perception abilities. These findings can serve as evidence to support future school-based studies.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0933,Ultimate attainment of second language articles: A case study of an endstate second language Turkish-English speaker,"An area of considerable interest in second language (L2) acquisition is the difficulties learners face with the acquisition of articles. This article examines the role of prosody in the acquisition of articles by an endstate L2 English speaker focusing on the free morphemes the and a. In order to analyse the articles produced by a Turkish speaker named SD, we used the Praat (Boersma and Weenink, 2006) phonetic analysis software to determine the prosodic shape of each article in article + noun configurations and article + adjective + noun configurations. The aim of the analysis is to see whether a more detailed analysis of the data would be fully consistent with the strong or weak interpretation of the Prosodic Transfer Hypothesis. The findings of our analysis show that SD produces a large percentage of stressed articles, which are non target-like. We discuss the implications of our analysis for the interlanguage representation of articles by SD as well as the Prosodic Transfer Hypothesis.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0934,Unfamiliar orthographic information and second language word learning: A novel lexicon study,"Recent research indicates that knowledge of words' spellings can influence knowledge of the phonological forms of second language (L2) words when the first and second languages use the same orthographic symbols. It is yet unknown whether learners can make similar use of unfamiliar orthographic symbols. In this study we investigate whether native English speakers use orthographic tone marks to help them associate lexical tone with new L2 words? Native English speakers with no knowledge of Mandarin were assigned to 'Tone Marks' or 'No Tone Marks' word learning groups. During a word learning phase, they learned to associate Mandarin nonwords varying in lexical tone with orthographic forms (written in pinyin with/without tone marks) and pictured 'meanings'. In Experiment 1, participants were asked whether a picture associated with, for example, tone 1 matched an auditory form containing tone 2. Tone Marks participants outperformed No Tone Marks participants, suggesting that the availability of unfamiliar orthographic symbols helped them associate lexical tone with the new words. In Experiment 2, the test involved matching an orthographic representation and an auditory word. Tone Marks participants performed above chance, while No Tone Marks participants did not, indicating that Tone Marks participants learned the correspondences between auditory tones and tone marks to some extent. We conclude that the presence of a novel orthographic feature (in this case, tone marks) can support native English speakers' ability to associate a novel phonological feature (in this case, lexical tone) with newly-learned lexical items.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0935,"Lexical encoding of L2 tones: The role of L1 stress, pitch accent and intonation","Native language prosodic structure is known to modulate the processing of non-native suprasegmental information. It has been shown that native speakers of French, a language without lexical stress, have difficulties storing non-native stress contrasts. We investigated whether the ability to store lexical tone (as in Mandarin Chinese) also depends on the first language (L I) prosodic structure and, if so, how. We tested participants from a stress language (German), a language without word stress (French), a language with restricted lexical tonal contrasts (Japanese), and Mandarin Chinese controls. Furthermore, German has a rich intonational structure, while French and Japanese dispose of fewer utterance-level pitch contrasts. The participants learnt associations between disyllabic non-words (4 tonal contrasts) and objects and indicated whether picture word pairs matched with what they had learnt (complete match, segmental or tonal mismatch conditions). In the tonal mismatch condition, the Mandarin Chinese controls had the highest sensitivity, followed by the German participants. The French and Japanese participants showed no sensitivity towards these tonal contrasts. Utterance-level prosody is hence better able to predict success in second language (L2) tone learning than word prosody.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0936,The perception of Mandarin lexical tones by native Korean speakers differing in their experience with Mandarin,"While it is well established that non-native speakers differ from native speakers in their perception and/or production of Mandarin lexical tones, empirical studies focusing on non-native learners are still limited. The objective of this study is to add to the current understanding of lexical tone perception by comparing native speakers of standard Korean from the Seoul/Kyunggi area differing in Mandarin experience (NK1, NK2) with native speakers of Mandarin. NK1 (n = 10) had no experience with Mandarin whereas NK2 (n = 10) consisted of highly advanced learners of Mandarin. A group of 10 native Mandarin (NM) speakers was included as controls. Accuracy of perception of six tone pairs (T1-T2, T1-T3, T1-T4, T2-T3, T2-T4, T3-T4) was assessed in a four-alternative forced-choice discrimination test. As expected, the NK2 group with extensive Mandarin learning experience resembled the NM group to a greater extent than did the NK1 group. T2-T3 was the hardest pair for both NK groups, but NK2 had the largest advantage over NK1 for this pair. Apart from T2-T3 which is generally considered difficult, tone pairs involving T1 caused some misperception by the NK groups. This may be related to the difficulty with perceiving a level tone which shows the least fundamental frequency (F0) movement and possibly has limited perceptual salience.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0937,From sounds to words: The relation between phonological and lexical processing of tone in L2 Mandarin,"Successful listening in a second language (L2) involves learning to identify the relevant acoustic-phonetic dimensions that differentiate between words in the L2, and then use these cues to access lexical representations during real-time comprehension. This is a particularly challenging goal to achieve when the relevant acoustic-phonetic dimensions in the L2 differ from those in the L1, as is the case for the L2 acquisition of Mandarin, a tonal language, by speakers of non-tonal languages like English. Previous work shows tone in L2 is perceived less categorically (Shen and Froud, 2019) and weighted less in word recognition (Pelzl et al., 2019) than in L1. However, little is known about the link between categorical perception of tone and use of tone in real time L2 word recognition at the level of the individual learner. This study presents evidence from 30 native and 29 L1-English speakers of Mandarin who completed a real-time spoken word recognition and a tone identification task. Results show that L2 learners differed from native speakers in both the extent to which they perceived tone categorically as well as in their ability to use tonal cues to distinguish between words in real-time comprehension. Critically, learners who reliably distinguished between words differing by tone alone in the word recognition task also showed more categorical perception of tone on the identification task. Moreover, within this group, performance on the two tasks was strongly correlated. This provides the first direct evidence showing that the ability to perceive tone categorically is related to the weighting of tonal cues during spoken word recognition, thus contributing to a better understanding of the link between phonemic and lexical processing, which has been argued to be a key component in the L2 acquisition of tone (Wong and Perrachione, 2007).",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0938,The combined effects of L1-specific and extralinguistic factors on individual performance in a tone categorization and word identification task by English-L1 and Mandarin-L1 speakers,"Adult second language learners often show considerable individual variability in the ease with which lexical tones are learned. It is known that factors pertaining to a learner's first language (L1; such as L1 tonal status or L1 tone type) as well as extralinguistic factors (such as musical experience and working memory) modulate tone learning facility. However, how such L1-specific and extralinguistic factors affect performance together in dynamic ways is less well understood. Therefore, to unpack the potential interactions between these factors for individual learners, we assessed the combined effects of L1 tonal status, L1 tone type, and musical experience and working memory on second language (L2) tone perception and word learning in a tonal pseudolanguage by English-L1 and Mandarin-L1 adult learners, by using a pre-lexical tone categorization task and a lexical word identification task. We found that L2 tone perception and word learning were primarily facilitated by extralinguistic factors, but that the degree to which learners rely on these factors is modulated by their L1 tonal status, as for instance musical experience facilitated perception and word learning for English, but not for Mandarin participants. We also found clear effects of L1 tone type, as Mandarin participants tended to struggle with categorizing and lexically processing level tone contrasts, which do not occur in Mandarin.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0939,The effect of second-language learning experience on Korean listeners' use of pitch cues in the perception of Cantonese tones,"Past studies have found that the linguistic experience of previously-acquired languages, such as one's native-language (L1) and second-language (L2) learning experience, modulates the perception of novel sounds from a third language (L3). Lexical tone in L3 is a good case for testing the influence of L1 or L2, as listeners with varying language backgrounds may use different pitch cues (pitch contour or height) in tone perception. The present study focuses on L2 learners of Mandarin whose L1 variety is either Seoul Korean (SK), a non-tonal stressless language, or Gyeongsang Korean (GK), a tonal pitch-accent language. Intermediate-to-advanced SK-speaking and GK-speaking L2 learners of Mandarin were recruited as target groups, and naive listeners of respective L1 varieties were recruited as control groups. The participants completed an AX forced-choice tone discrimination task. Four Cantonese tones, one rising tone and three level tones, were used. Contour-level and level-level tonal contrasts were target tone pairs, allowing for testing the primary use of pitch contour and pitch height, respectively. The results showed that the two groups of naive listeners had greater accuracy in discriminating level-level than contour-level tonal contrasts. In contrast, L2 learners, independent of their L1 varieties, showed higher accuracy in discriminating contour-level than level-level tonal contrasts. The L2 learners' perceptual pattern is consistent with Mandarin listeners, as reported in previous work. Taken together, the findings provide evidence for a possible developmental change in which Korean-speaking L2 learners might have a perceptual cue shift from pitch height to pitch contour through their L2 experience in Mandarin. The findings about the role of L2 proficiency in Mandarin further supported the effect of L2 experience on learners' increased use of pitch contour.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0940,The effects of absolute pitch ability and musical training on lexical tone perception,"The relationship between processing of speech and music was explored here via the linguistic vehicle of lexical tone. People with amusia have been found to be impaired on linguistic tasks; we examined whether absolute pitch (AP) possessors have an advantage on linguistic tasks. Participants were 3 groups of monolingual Australian-English speakers: non-AP musicians (musically-trained individuals who did not possess AP), AP musicians (musically-trained individuals who were AP possessors), and non-musicians (no musical training). Perceptual discrimination was tested in an AX same-different task for lexical tones presented in three contexts: normal Thai speech, low-pass filtered speech tones, and violin, with processing level manipulated via variation of the interstimulus interval (ISI). Non-musicians showed attenuated pitch discrimination of tones in speech, suggesting speech specialisation. On the other hand, all musicians showed greater accuracy, faster reaction times and less variation in accuracy across stimulus types than non-musicians. Importantly, AP musicians showed greater accuracy than non-AP musicians in the speech context, implying a domain-general advantage due to AP. However, speech-violin accuracy correlations for AP musicians were almost zero at the longer ISI, suggesting less commonality of mechanisms during more extensive processing. Results are discussed in terms of the role of AP in tone language perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0941,The Effect of Lexical Tone and Sex on Nasalance in Cantonese-Speakers,"Objectives: This study aims to investigate the effect of lexical tone and sex on nasalance in Cantonese, a lexical tone language, and to identify both statistical and clinical implications. Design: Forty Cantonese-speaking adults were recruited; 72 stimuli words based on 12 syllables across 3 syllable types (CV, CVV, CVC) and 6 lexical tones were constructed. Each stimulus was repeated 6 times, resulting in 1296 repetitions randomized for each participant. Acoustic data were annotated and mean nasalance extracted from the Nasometer (TM) software. Results: A 3-way mixed analysis of variance with sex as the between-subjects factor and syllable type and tones as the within-subjects factors showed a significant 3-way interaction effect, F(5.491,647.953) = 6.759, P < .001, partial eta(2) = .054, a significant 2-way interaction effect of syllable type x tone, F(5.491,647.953) = 57.524, P < .001, partial eta(2) = .328, and significant main effects, for example, sex, F(1118) = 12.078, P = .001, partial eta(2) = .093. Females had higher nasalance than males across all syllable types. Tone 1 words and CVC syllable type yielded the highest nasalance values. Conclusions: Study findings show an effect of lexical tone and syllable type on nasalance for males and females. The theory of transpalatal acoustic transmission is used to explain and discuss study findings. Clinical implications pertain to the potential need for separate normative databases for males and females and control of tone 1 words and syllable type (CVC) with final syllable nasal, in nasalance speech sampling material and in cross-linguistic comparisons.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0942,Harnessing the musician advantage: Short-term musical training affects non-native cue weighting of linguistic pitch,"Lexical tone languages like Mandarin Chinese require listeners to discriminate among different pitch patterns. A syllable spoken with a rising pitch (e.g. bi 'nose') carries a different meaning than the same syllable spoken with a falling pitch (e.g. b 'arm'). For native speakers (L1) of a non-tonal language, accurate perception of tones in a second language (L2) is notoriously difficult. Musicians, however, have typically shown an aptitude for lexical tone learning due to the unique perceptual demands of music. This study tested whether musical effects can be exploited to improve linguistic abilities in the general population. A pre-test, 8-week training, post-test design was used to measure L1 English participants' sensitivity to tone. Individual Differences Scaling was used to measure participants' weighting of pitch height and movement cues. Participants took part in classroom Mandarin learning only (+L2), musical ear training only (+Music), or classroom learning combined with musical training (+L2+Music). An L1 Mandarin group served as a baseline. At pre-test, mean sensitivity to tone and multidimensional scaling results were similar across all three L1 English groups. After training, all three L1 English groups improved in mean sensitivity, though only the +L2+Music group did so at a significant rate. Multidimensional scaling revealed that all groups increased their weighting of the more informative pitch movement cue at roughly equal rates. Short-term musical training thus affected change in cue weighting of linguistic pitch in a manner comparable to that occurring after a semester of L2 classroom learning. When combined with classroom learning, short-term musical training resulted in even greater sensitivity to pitch movement cues. These results contribute to models of music-language interaction and suggest that focused application of non-linguistic acoustic training can improve phonetic perception in ways that are relevant to language learning.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0943,Mental representations of speech and musical pitch contours reveal a diversity of profiles in autism spectrum disorder,"As an information-bearing auditory attribute of sound, pitch plays a crucial role in the perception of speech and music. Studies examining pitch processing in autism spectrum disorder have produced equivocal results. To understand this discrepancy from a mechanistic perspective, we used a novel data-driven method, the reverse-correlation paradigm, to explore whether the equivocal findings in autism spectrum disorder have high-level origins in top-down comparisons of internal mental representations of pitch contours. Thirty-two Mandarin-speaking autistic individuals and 32 non-autistic individuals undertook three subtasks testing mental representations of pitch contours in speech, complex tone and melody, respectively. The results indicate that while the two groups exhibited similar representations of pitch contours across the three conditions, the autistic group showed a significantly higher intra-group variability than the non-autistic group. In addition, the two groups did not differ significantly in internal noise, a measure of the robustness of participant responses to external variability, suggesting that the present findings translate genuinely qualitative differences and similarities between groups in pitch processing. These findings uncover for the first time that pitch patterns in speech and music are mentally represented in a similar manner in autistic and non-autistic individuals, through domain-general top-down mechanisms. Lay abstract As a key auditory attribute of sounds, pitch is ubiquitous in our everyday listening experience involving language, music and environmental sounds. Given its critical role in auditory processing related to communication, numerous studies have investigated pitch processing in autism spectrum disorder. However, the findings have been mixed, reporting either enhanced, typical or impaired performance among autistic individuals. By investigating top-down comparisons of internal mental representations of pitch contours in speech and music, this study shows for the first time that, while autistic individuals exhibit diverse profiles of pitch processing compared to non-autistic individuals, their mental representations of pitch contours are typical across domains. These findings suggest that pitch-processing mechanisms are shared across domains in autism spectrum disorder and provide theoretical implications for using music to improve speech for those autistic individuals who have language problems.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0944,Categorical perception of Mandarin lexical tones in language-delayed autistic children,"Enhanced pitch perception has been identified in autistic individuals, but it remains understudied whether such enhancement can be observed in the lexical tone perception of language-delayed autistic children. This study examined the categorical perception of Mandarin lexical tones in 23 language-delayed autistic children and two groups of non-autistic children, with one matched on chronological age (n = 23) and the other on developmental age in language ability (n = 23). The participants were required to identify and discriminate lexical tones. A wider identification boundary width and a lower between-category discrimination accuracy were found in autistic children than their chronological-age-matched non-autistic peers, but the autistic group exhibited seemingly comparable performance to the group of developmental-age-matched non-autistic children. While both non-autistic groups displayed a typical categorical perception pattern with enhanced sensitivity to between-category tone pairs relative to within-category ones, such a categorical perception pattern was not observed in the autistic group. These findings suggest among language-delayed autistic children with a developmental age around 4, categorical perception is still developing. Finally, we found categorical perception performance correlated with language ability, indicating autistic children's language disability might be predictive of their poor categorical perception of speech sounds. Lay abstract Some theories suggested that autistic people have better pitch perception skills than non-autistic people. However, in a context where pitch patterns are used to differentiate word meanings (i.e. lexical tones), autistic people may encounter difficulties, especially those with less language experience. We tested this by asking language-delayed autistic children to identify and discriminate two Mandarin lexical tones (/yi/ with Tone 1, meaning 'clothes'; /yi/ with Tone 2, meaning 'aunt'; /yi/: the standard romanization of Mandarin Chinese). On average, these autistic children were 7.35 years old, but their developmental age in language ability was 4.20, lagging behind 7-year-old non-autistic children in terms of language ability. Autistic children's performance in identifying and discriminating lexical tones was compared with two groups of non-autistic children: one group was matched with the autistic group on age, and the other was matched based on language ability. Autistic children performed differently from the non-autistic children matched on age, while autistic and non-autistic children matched on language ability exhibited seemingly similar performance. However, both the non-autistic groups have developed the perceptual ability to process lexical tones as different categories, but this ability was still developing in autistic children. Finally, we found autistic children who performed worse in identifying lexical tones had poorer language ability. The results suggest that language disability might have adverse influence on the development of skills of speech sound processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0945,The production of Norwegian tones by multilingual non-native speakers,"Aim and objectives/purpose/research questions: The aim of this study is to examine the extent to which multilingual second language (L2) speakers of Norwegian manage to produce lexical pitch accents (L*H- or H*LH<(-)) as expected in natural spontaneous speech. Using native speech as a reference, we analyse realizations of multilingual speakers whose respective dominant languages are Lingala, a lexical tone language, and Swahili, a non-tonal language with fixed stress, and hypothesize that this difference might be reflected in the speakers' competence in the East Norwegian tone system. Design/methodology/approach: We examined a corpus of spontaneous speech produced by eight L2 speakers and two native speakers of East Norwegian. Acoustic analysis was performed to collect fundamental frequency (f0) contours of 60 accentual phrases per speaker. Data and analysis: For LH and HLH tonal patterns, measuring points were defined for quantitative evaluation of f0 values. Relevant aspects investigated were (a) pattern consistency, (b) f0 dynamic range and (c) rate of f0 change. Pattern consistency data were statistically evaluated using chi-square testing. The dynamic range and rate of f0 change data were explored through to linear mixed effects models. Findings/conclusions: We found no really substantial differences between the speaker groups in the parameters we examined, neither between the L2 speakers and the Norwegian natives nor between the Lingala and Swahili speakers. Originality and significance/implications: This study is a contribution to the scarcely explored area of L2 acquisition of tones. It is concerned with languages that have received little or no attention in the field: Norwegian, Lingala and Swahili. Participants are multilinguals who have extensive language learning experience. Further, the study is based on a corpus of spontaneous speech.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0946,The development of consonant and lexical-tone discrimination between 3 and 6 years: Effect of language exposure,"Aims and objectives: The present study explored children's discrimination capacities for lexical tones and consonants between 3 and 6 years of age and the effect of native language on this ability. Recent studies in infants have shown a perceptual rebound for non-native listeners during the second year of life, but only for lexical tones. However, the later stages of development, and particularly when children start pre-school, are yet not clear. Design: Discrimination abilities of 134 children were measured in three age groups between 3 and 6 years using a behavioural task where children detected a change in lexical tones or consonants. Children were either French monolinguals, French bilinguals exposed to an Asian tone language or French bilinguals exposed to a second non-tone language at home. Data and analysis: Overall, results indicated that higher detection scores for consonants were observed from 4 to 5 years, while for lexical tones the highest scores were observed only at 5-6 years. Moreover, bilingual children exposed to an Asian tone language had higher scores for tones compared to monolingual French children. Interestingly, both bilingual groups, whether exposed to an Asian tone language or to a non-tone language, had better scores for tones than for French consonants, while monolinguals performed equally with both. Conclusions: Language exposure from an early age influences phonological development and bilingualism seems to enhance the perception of prosodic information. Originality: This study is the first to show a different developmental trajectory for consonant and lexical-tone discrimination between 3 and 6 years according to the native language.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0947,Cantonese tone production in pre-school Urdu-Cantonese bilingual minority children,"Aim: In this study, we examine the production of Cantonese tones by preschool Urdu-Cantonese children living in Hong Kong. Methodology: 21 first language Urdu second language Cantonese children (ages 4-6) and 20 age-matched first language Cantonese children participated in a picture-naming experiment with 86 words (109 syllables in total). Data and Analysis: Acoustic analysis was carried out for perceptually correct and incorrect tone productions of each tone. Comparisons were also made across speaker groups regarding accuracy rates and error patterns. Findings: Overall, first-language Urdu participants had lower accuracy and greater tone confusion than first language Cantonese participants. The pattern is attributable to influence from Urdu prosody, ongoing Cantonese tone mergers, and general sensitivity to phonetic information. Originality: This is the first empirical study on the acquisition of Cantonese tones by children who are heritage speakers of a non-tone language.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0948,The influence of L2 experience on L1 speech perception: Evidence from Mandarin-English bilinguals,"Aims: Previous studies have documented that consonants contribute more to lexically related processes than vowels in English, whereas an opposite pattern occurs in Mandarin Chinese processing. The current study aims to examine how the long-term learning of English (L2) influences Mandarin (L1) consonant, vowel, and tone perception in bilingual adults. It also attempts to examine the multi-competence theory at the level of speech perception. Methodology: A total of 73 Mandarin-English bilinguals with different L2 proficiency levels performed the L1 syllable perception task in an active oddball paradigm. The deviant syllables differed from the standard in consonant, vowel, or tone. Participants were asked to press a button whenever they heard a different syllable. Data and analysis: The reaction time and accuracy of each deviant syllable were recorded by E-prime 2.0. These data were analyzed using mixed-effects models. Findings/conclusions: Results showed that the relative role of vowel was the strongest, followed by consonant, and then by tone in L1 speech perception. The pattern of their relative roles/phonological bias was not influenced by L2 proficiency, indicating that the phonological bias in L1 speech perception is not affected by a transfer from non-native to native processing, but determined by the acoustic/phonological cues and lexical information carried by consonant, vowel, and tone. However, the perceptual sensitivity to L1 speech increased with L2 proficiency, demonstrating a positive L2 effect on L1 at the level of speech perception. Originality: The current study first investigated the influence of L2 experience on L1 phonological bias in bilinguals whose first and second languages were typologically distant. Significance/implications: Our findings lend support to the multi-competence theory and extend it to the level of speech perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0949,"Lexical tonal effects in code-switching: A comparative study of Cantonese, Mandarin, and Vietnamese switching with English","Aims and objectives: Previous research has revealed much about the syntactic and social variables conditioning code-switching (i.e., the alternation between two or more languages in a discourse or utterance); however, little is known about the phonological effects. Our work explores this area by asking two main questions: (1) Does lexical tone affect code-switching between a tonal language and a non-tonal language? and (2) Is this effect (or lack thereof) observable cross-linguistically? Methodology: We examine natural code-switching production between Cantonese and English, Mandarin and English, and Vietnamese and English. We use a semi-automatic natural-language processing method to process and extract relevant variables, including tonal categories at switch points. Data and analysis: Data include transcribed natural speech from three bilingual corpora: the HLVC corpus (Cantonese/English, 25 speakers), the SEAME corpus (Mandarin/English, 20 speakers), and the CanVEC corpus (Vietnamese/English, 45 speakers). We use logistic mixed-effects models to examine tonal effects, taking into account other factors such as frequency and grammatical category. Findings/conclusion: We found a robust tonal effect in Cantonese/English, a less robust effect in Mandarin/English, and no effect in Vietnamese/English. This indicates there is a tonal effect in code-switching between a tonal and a non-tonal language, but this effect is language-dependent. We also found a specific T3 'step-up' pattern at Cantonese-English switch points and offered some possible phonological explanations. Originality: This is the first study that systematically investigates tonal effects in code-switching across different language pairs, using comparable data and methods. Our finding of a Cantonese-English T3 'step-up' pattern is also a novel discovery that hitherto has not been documented. Significance/implications: Theoretically, our findings support Clyne's 'facilitation theory' in code-switching at a prosodic level. Empirically, we nevertheless emphasised the complexity of different prosodic features and social variables in play, thereby rejecting the idea of 'predicting' code-switching solely based on linguistic factors.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0950,To believe or not to believe? How voice and accent information in speech alter listener impressions of trust,"Our decision to believe what another person says can be influenced by vocally expressed confidence in speech and by whether the speaker-listener are members of the same social group. The dynamic effects of these two information sources on neurocognitive processes that promote believability impressions from vocal cues are unclear. Here, English Canadian listeners were presented personal statements (She has access to the building) produced in a confident or doubtful voice by speakers of their own dialect (in-group) or speakers from two different ""out-groups"" (regional or foreign-accented English). Participants rated how believable the speaker is for each statement and event-related potentials (ERPs) were analysed from utterance onset. Believability decisions were modulated by both the speaker's vocal confidence level and their perceived in-group status. For in-group speakers, ERP effects revealed an early differentiation of vocally expressed confidence (i.e., N100, P200), highlighting the motivational significance of doubtful voices for drawing believability inferences. These early effects on vocal confidence perception were qualitatively different or absent when speakers had an accent; evaluating out-group voices was associated with increased demands on contextual integration and re-analysis of a non-native representation of believability (i.e., increased N400, late negativity response). Accent intelligibility and experience with particular out-group accents each influenced how vocal confidence was processed for out-group speakers. The N100 amplitude was sensitive to out-group attitudes and predicted actual believability decisions for certain out-group speakers. We propose a neurocognitive model in which vocal identity information (social categorization) dynamically influences how vocal expressions are decoded and used to derive social inferences during person perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0951,"Influence of emotional prosody, content, and repetition on memory recognition of speaker identity","Recognising individuals through their voice requires listeners to form an invariant representation of the speaker's identity, immune to episodic changes that may occur between encounters. We conducted two experiments to investigate to what extent within-speaker stimulus variability influences different behavioural indices of implicit and explicit identity recognition memory, using short sentences with semantically neutral content. In Experiment 1, we assessed how speaker recognition was affected by changes in prosody (fearful to neutral, and vice versa in a between-group design) and speech content. Results revealed that, regardless of encoding prosody, changes in prosody, independent of content, or changes in content, when prosody was kept unchanged, led to a reduced accuracy in explicit voice recognition. In contrast, both groups exhibited the same pattern of response times (RTs) for correctly recognised speakers: faster responses to fearful than neutral stimuli, and a facilitating effect for same-content stimuli only for neutral sentences. In Experiment 2, we investigated whether an invariant representation of a speaker's identity benefitted from exposure to different exemplars varying in emotional prosody (fearful and happy) and content (Multi condition), compared to repeated presentations of a single sentence (Uni condition). We found a significant repetition priming effect (i.e., reduced RTs over repetitions of the same voice identity) only for speakers in the Uni condition during encoding, but faster RTs when correctly recognising old speakers from the Multi, compared to the Uni, condition. Overall, our findings confirm that changes in emotional prosody and/or speech content can affect listeners' implicit and explicit recognition of newly familiarised speakers.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0952,Representational level matters for tone-word recognition: Evidence from form priming,"In a form priming experiment with a lexical decision task, we investigated whether the representational structure of lexical tone in lexical memory impacts spoken-word recognition in Mandarin. Target monosyllabic words were preceded by five types of primes: (1) the same real words (/lun4/-/lun4/), (2) real words with only tone contrasts (/lun2/-/lun4/), (3) unrelated real words (/pie3/-/lun4/), (4) pseudowords with only tone contrasts (*/lun3/-/lun4/), and (5) unrelated pseudowords (*/tai3/-/lun4/). We found a facilitation effect in target words with pseudoword primes that share the segmental syllable but contrast in tones (*/lun3/-/lun4/). Moreover, no evident form priming effect was observed in target words primed by real words with only tone contrasts (/lun2/-/lun4/). These results suggest that the recognition of a tone word is influenced by the representational level of tone accessed by the prime word. The distinctive priming patterns between real-word and pseudoword primes are best explained by the connectionist models of tone-word recognition, which assume a hierarchical representation of lexical tone.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0953,Lexical tone perception and learning in older adults: A review and future directions,"While the literature is well represented in accounting for how aging influences segmental properties of speech, less is known about its influences on suprasegmental properties such as lexical tones. In addition, foreign language learning is increasingly endorsed as being a potential intervention to boost cognitive reserve and overall well-being in older adults. Empirical studies on young learners learning lexical tones are aplenty in comparison with older learners. Challenges in this domain for older learners might be different due to aging and other learner-internal factors. This review consolidates behavioural and neuroscientific research related to lexical tone, speech perception, factors characterising learner groups, and other variables that would influence lexical tone perception and learning in older adults. Factors commonly identified to influence tone learning in younger adult populations, such as musical experience, language background, and motivation in learning a new language, are discussed in relation to older learner groups and recommendations to boost lexical tone learning in older age are provided based on existing studies.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0954,Discourses about independence: A corpus-based analysis of discourse prosodies in Spanish and Catalan newspapers,"The present study is a corpus-assisted analysis of discourses on the Catalan movement of independence in the Catalan and Spanish press. A referendum for Catalan independence was held on 1 October 2017 but was not approved by the central Spanish government and was declared illegal. Spanish and Catalan newspapers differed considerably in their treatment and representation of the conflict, which also drew the attention of the international press to Catalonia. My analysis interconnects theoretical perspectives from linguistic critical discourse studies, cognitive linguistics, and media studies. The methodology is based on the analysis of two comparable corpora of journalistic texts (in Catalan, Spanish). My aim is to identify and compare different features of the semantic prosody of words describing the event of geopolitical separation and the different narratives on this sociopolitical conflict that these prosodies contribute to producing. More specifically, I focus on a selection of keywords, like independence, secession, and sovereignty, that seem to be indicative of different discursive strategies consciously, and possibly unconsciously, chosen by a selection of national Catalan and Spanish newspapers. By carrying out a contrastive analysis of the two corpora, I am able to describe certain similarities, like the similar frequent use of the word independence, and differences, like the more frequent use of the word secession in the Spanish corpus, that indicate different framings of the same event and can be related to different ideological positionings against or in favor of Catalonia's independence from Spain.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0955,More than just an immigrant: The semantic patterns of (im)migrant/predicate-pairings in news stories about Mexican and Central American (im)migrants to the USA. A corpus-assisted discourse study,"In this paper we explore how some of the largest US-newspapers linguistically frame immigrants to the USA in articles about Mexican and Central American immigrants. Specifically, it is a corpus-assisted discourse study which examines the frequency of different semantic predicate-types with (im)migrant subjects and (im)migrant by-agents in the quest for underlying positive or negative biases. We wish to ascertain what activities (im)migrants are presented as taking part in, principally as agents. The analysis shows that more than half of the (im)migrant/predicate-pairings reflect the dictionary definitions of (im)migrant. However, immigrants are described as illegal 66% of the times that their location is mentioned with an immigrant/predicate-pairing. The non-definition-confirming pairings also show evidence of a negative framing of immigrants, but not of migrants. Furthermore, immigrants are more often than migrants cast as agents of activities that do not simply reiterate their status as (im)migrants. Finally, we found evidence of the Negation Bias in the immigrant/predicate-pairings.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0956,Pitch Perception in Tone Language-Speaking Adults With and Without Autism Spectrum Disorders,"Enhanced low-level pitch perception has been universally reported in autism spectrum disorders (ASD). This study examined whether tone language speakers with ASD exhibit this advantage. The pitch perception skill of 20 Cantonese-speaking adults with ASD was compared with that of 20 neurotypical individuals. Participants discriminated pairs of real syllable, pseudo-syllable (syllables that do not conform the phonotactic rules or are accidental gaps), and non-speech (syllables with attenuated high-frequency segmental content) stimuli contrasting pitch levels. The results revealed significantly higher discrimination ability in both groups for the non-speech stimuli than for the pseudo-syllables with one semitone difference. No significant group differences were noted. Different from previous findings, post hoc analysis found that enhanced pitch perception was observed in a subgroup of participants with ASD showing no history of delayed speech onset. The tone language experience may have modulated the pitch processing mechanism in the speakers in both ASD and non-ASD groups.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0957,Categorical Perception of Lexical Tones and Stops in Mandarin-Speaking Musicians and Nonmusicians,"This study investigates the perception of Mandarin lexical tones and stops to examine the degree of overlap between music and language. Eighteen musicians and 21 nonmusicians participated in a typical categorical perception task. Results showed that musicians and nonmusicians had comparable degree of categorical perception of tones and stops. Compared to nonmusicians, musicians exhibited enhanced sensitivities to within-category lexical tone stimuli. However, this improved ability was not observed in the perception of stops. These findings imply that musical experience strengthens the acuity of subtle low-level acoustic variations between within-category lexical tone stimuli without interfering with the high-level phonological representations of lexical tones, and this facilitatory effect is selective and could not readily extend to stop consonants in native language.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0958,Music Training Can Improve Music and Speech Perception in Pediatric Mandarin-Speaking Cochlear Implant Users,"Due to limited spectral resolution, cochlear implants (CIs) do not convey pitch information very well. Pitch cues are important for perception of music and tonal language; it is possible that music training may improve performance in both listening tasks. In this study, we investigated music training outcomes in terms of perception of music, lexical tones, and sentences in 22 young (4.8 to 9.3 years old), prelingually deaf Mandarin-speaking CI users. Music perception was measured using a melodic contour identification (MCI) task. Speech perception was measured for lexical tones and sentences presented in quiet. Subjects received 8 weeks of MCI training using pitch ranges not used for testing. Music and speech perception were measured at 2, 4, and 8 weeks after training was begun; follow-up measures were made 4 weeks after training was stopped. Mean baseline performance was 33.2%, 76.9%, and 45.8% correct for MCI, lexical tone recognition, and sentence recognition, respectively. After 8 weeks of MCI training, mean performance significantly improved by 22.9, 14.4, and 14.5 percentage points for MCI, lexical tone recognition, and sentence recognition, respectively (p < .05 in all cases). Four weeks after training was stopped, there was no significant change in posttraining music and speech performance. The results suggest that music training can significantly improve pediatric Mandarin-speaking CI users' music and speech perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0959,Masking Effects in the Perception of Multiple Simultaneous Talkers in Normal-Hearing and Cochlear Implant Listeners,"For normal-hearing (NH) listeners, monaural factors, such as voice pitch cues, may play an important role in the segregation of speech signals in multitalker environments. However, cochlear implant (CI) users experience difficulties in segregating speech signals in multitalker environments in part due to the coarse spectral resolution. The present study examined how the vocal characteristics of the target and masking talkers influence listeners' ability to extract information from a target phrase in a multitalker environment. Speech recognition thresholds (SRTs) were measured with one, two, or four masker talkers for different combinations of target-masker vocal characteristics in 10 adult Mandarin-speaking NH listeners and 12 adult Mandarin-speaking CI users. The results showed that CI users performed significantly poorer than NH listeners in the presence of competing talkers. As the number of masker talkers increased, the mean SRTs significantly worsened from -22.0 dB to -5.2 dB for NH listeners but significantly improved from 5.9 dB to 2.8 dB for CI users. The results suggest that the flattened peaks and valleys with increased numbers of competing talkers may reduce NH listeners' ability to use dips in the spectral and temporal envelopes that allow for ""glimpses"" of the target speech. However, the flattened temporal envelope of the resultant masker signals may be less disruptive to the amplitude contour of the target speech, which is important for Mandarin-speaking CI users' lexical tone recognition. The amount of masking release was further estimated by comparing SRTs between the same-sex maskers and the different-sex maskers. There was a large amount of masking release in NH adults (12 dB) and a small but significant amount of masking release in CI adults (2 dB). These results suggest that adult CI users may significantly benefit from voice pitch differences between target and masker speech.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0960,"Thai lexical tone perception in native speakers of Thai, English and Mandarin Chinese: An event-related potentials training study","Background: Tone languages such as Thai and Mandarin Chinese use differences in fundamental frequency (F-0, pitch) to distinguish lexical meaning. Previous behavioral studies have shown that native speakers of a non-tone language have difficulty discriminating among tone contrasts and are sensitive to different F-0 dimensions than speakers of a tone language. The aim of the present ERP study was to investigate the effect of language background and training on the non-attentive processing of lexical tones. EEG was recorded from 12 adult native speakers of Mandarin Chinese, 12 native speakers of American English, and 11 Thai speakers while they were watching a movie and were presented with multiple tokens of low-falling, mid-level and high-rising Thai lexical tones. High-rising or low-falling tokens were presented as deviants among mid-level standard tokens, and vice versa. EEG data and data from a behavioral discrimination task were collected before and after a two-day perceptual categorization training task. Results: Behavioral discrimination improved after training in both the Chinese and the English groups. Low-falling tone deviants versus standards elicited a mismatch negativity (MMN) in all language groups. Before, but not after training, the English speakers showed a larger MMN compared to the Chinese, even though English speakers performed worst in the behavioral tasks. The MMN was followed by a late negativity, which became smaller with improved discrimination. The High-rising deviants versus standards elicited a late negativity, which was left-lateralized only in the English and Chinese groups. Conclusion: Results showed that native speakers of English, Chinese and Thai recruited largely similar mechanisms when non-attentively processing Thai lexical tones. However, native Thai speakers differed from the Chinese and English speakers with respect to the processing of late F-0 contour differences (high-rising versus mid-level tones). In addition, native speakers of a non-tone language (English) were initially more sensitive to F-0 onset differences (low-falling versus mid-level contrast), which was suppressed as a result of training. This result converges with results from previous behavioral studies and supports the view that attentive as well as non-attentive processing of F-0 contrasts is affected by language background, but is malleable even in adult learners.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0961,Differential weighting of temporal envelope cues from the low-frequency region for Mandarin sentence recognition in noise,"Background Temporal envelope cues are conveyed by cochlear implants (CIs) to hearing loss patients to restore hearing. Although CIs could enable users to communicate in clear listening environments, noisy environments still pose a problem. To improve speech-processing strategies used in Chinese CIs, we explored the relative contributions made by the temporal envelope in various frequency regions, as relevant to Mandarin sentence recognition in noise. Methods Original speech material from the Mandarin version of the Hearing in Noise Test (MHINT) was mixed with speech-shaped noise (SSN), sinusoidally amplitude-modulated speech-shaped noise (SAM SSN), and sinusoidally amplitude-modulated (SAM) white noise (4 Hz) at a + 5 dB signal-to-noise ratio, respectively. Envelope information of the noise-corrupted speech material was extracted from 30 contiguous bands that were allocated to five frequency regions. The intelligibility of the noise-corrupted speech material (temporal cues from one or two regions were removed) was measured to estimate the relative weights of temporal envelope cues from the five frequency regions. Results In SSN, the mean weights of Regions 1-5 were 0.34, 0.19, 0.20, 0.16, and 0.11, respectively; in SAM SSN, the mean weights of Regions 1-5 were 0.34, 0.17, 0.24, 0.14, and 0.11, respectively; and in SAM white noise, the mean weights of Regions 1-5 were 0.46, 0.24, 0.22, 0.06, and 0.02, respectively. Conclusions The results suggest that the temporal envelope in the low-frequency region transmits the greatest amount of information in terms of Mandarin sentence recognition for three types of noise, which differed from the perception strategy employed in clear listening environments.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0962,Emotional voice conversion using neural networks with arbitrary scales F0 based on wavelet transform,"An artificial neural network is an important model for training features of voice conversion (VC) tasks. Typically, neural networks (NNs) are very effective in processing nonlinear features, such as Mel Cepstral Coefficients (MCC), which represent the spectrum features. However, a simple representation of fundamental frequency (F0) is not enough for NNs to deal with emotional voice VC. This is because the time sequence of F0 for an emotional voice changes drastically. Therefore, in our previous method, we used the continuous wavelet transform (CWT) to decompose F0 into 30 discrete scales, each separated by one third of an octave, which can be trained by NNs for prosody modeling in emotional VC. In this study, we propose the arbitrary scales CWT (AS-CWT) method to systematically capture F0 features of different temporal scales, which can represent different prosodic levels ranging from micro-prosody to sentence levels. Meanwhile, the proposed method uses deep belief networks (DBNs) to pre-train the NNs that then convert spectral features. By utilizing these approaches, the proposed method can change the spectrum and the F0 for an emotional voice simultaneously as well as outperform other state-of-the-art methods in terms of emotional VC.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0963,A parametric prosody coding approach for Mandarin speech using a hierarchical prosodic model,"In this paper, a novel parametric prosody coding approach for Mandarin speech is proposed. It employs a hierarchical prosodic model (HPM) as a prosody-generating model in the encoder to analyze the speech prosody of the input utterance to obtain a parametric representation of four prosodic-acoustic features of syllable pitch contour, syllable duration, syllable energy level, and syllable-juncture pause duration for encoding. In the decoder, the four prosodic-acoustic features are reconstructed by a synthesis operation using the decoded HPM parameters. The reconstructed prosodic features are lastly used in an HMM-based speech synthesizer to generate the reconstructed speech. Objective and subjective evaluations showed that the proposed prosody coding approach encoded speech with better quality and lower data rate than the conventional segment-based coding scheme with vector or scalar quantization approach did. The reconstructed speech encoded by the proposed approach has good quality at low data rates of 81.4 and 72.7 bps for speaker-dependent and speaker-independent tasks, respectively. An application of the proposed prosody coding approach to speaking rate conversion by directly changing the HPM parameters to those of a different speaking rate is also illustrated. An informal listening test confirmed that both converted speeches of high and low speaking rate sounded very smooth.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0964,Preattentive processing of lexical tone perception by the human brain as indexed by the mismatch negativity paradigm,"Mismatch negativity (MMN) was used to investigate the processing of the discrimination between native and non-native CV syllables in tonal languages. MMN elicited by the native word was greater than that elicited by the non-native word. Hearing a native-language deviant significantly altered the elicited MMN in both amplitude and scalp voltage field distribution, reflecting the presence of a long-term memory trace for spoken words in tonal languages.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0965,Prosodic and melodic processing in adults and children -: Behavioral and electrophysiologic approaches,"The results of a series of experiments aimed at directly comparing the prosodic level of processing in language with the melodic level of processing in music are reported. The first series of experiments was conducted on adults, musicians and nonmusicians, and the second one on 7- to 9-year-old musician and nonmusician children. However, as this last study is still in progress, only preliminary results will be presented. The theoretic framework within which these experiments are taking place is described. The first problem concerns the specificity of the perceptive and cognitive computations necessary to perceive and understand language. We argue that comparing language with music can provide interesting insights into this complex issue. The second problem is linked to the relationship between different types of learning. Does early musical training influence the way in which musicians process some aspects of language as prosody? These two problems are considered and the results of the experiments are described.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0966,A Multimodal Sentiment Analysis Model Enhanced with Non-verbal Information and Contrastive Learning,"Deep learning methods have gained popularity in multimodal sentiment analysis due to their impressive representation and fusion capabilities in recent years. Existing studies often analyze the emotions of individuals using multimodal information such as text, facial expressions, and speech intonation, primarily employing complex fusion methods. However, existing models inadequately consider the dynamic changes in emotions over long time sequences, resulting in suboptimal performance in sentiment analysis. In response to this issue, a Multimodal Sentiment Analysis Model Enhanced with Non-verbal Information and Contrastive Learning is proposed in this paper. Firstly, the paper employs long-term textual information to enable the model to learn dynamic changes in audio and video across extended time sequences. Subsequently, a gating mechanism is employed to eliminate redundant information and semantic ambiguity between modalities. Finally, contrastive learning is applied to strengthen the interaction between modalities, enhancing the model's generalization. Experimental results demonstrate that on the CMU-MOSI dataset, the model improves the Pearson Correlation coefficient (Corr) and F1 score by 3.7% and 2.1%, respectively. On the CMU-MOSEI dataset, the model increases ""Corr"" and ""F1 score"" by 1.4% and 1.1%, respectively. Therefore, the proposed model effectively utilizes intermodal interaction information while eliminating information redundancy.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0967,Chinese and English infants' tone perception: Evidence for perceptual reorganization,"Over half the world's population speaks a tone language, yet infant speech perception research has typically focused on consonants and vowels. Very young infants can discriminate a wide range of native and normative consonants and vowels, and then in a process of perceptual reorganization over the 1st year, discrimination of most normative speech sounds deteriorates. We investigated perceptual reorganization for tones by testing 6- and 9-month-old infants from tone (Chinese) and nontone (English) language environments for speech (lexical tone) and nonspeech (violin sound) tone discrimination in both cross-sectional and longitudinal studies. Overall, Chinese infants performed equally well at 6 and 9 months for both speech and nonspeech tone discrimination. Conversely, English infants' discrimination of lexical tone declined between 6 and 9 months of age, whereas their nonspeech tone discrimination remained constant. These results indicate that the reorganization of tone perception is a function of the native language environment, and that this reorganization is linguistically based.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0968,Tashkent in the Imperial Project of Russian Colonization of the Turkestan Region in the second half of the 19th-early 20th centuries: Discourse and Practices,"The article, based on the materials of notes, essays, memoirs and reminiscences of Russian officials, publicists, writers, social and political figures, united by the 'frame' of imperial experts, reveals the content of discourse and ideas about the city of Tashkent as an outpost of Russian colonization of the Turkestan region in the second half of the 19th-early 20th centuries. The method of discourse analysis allowed us to identify key patterns in experts' perceptions of the representation of Tashkent as a model of internal colonization of the eastern periphery of the empire. In the course of the study, it was established that the construction of the image of Tashkent as the imperial centre of the Turkestan region implied the fixation of the city's special military and administrative status in the personal texts of imperial experts, which was expressed in the massive 'occupation' narrative prevailing on the pages of published materials. In the publications of the second half of the 19th century, the intonation of the empire's militaristic presence and dominance in the region was clearly traced, and Tashkent was labelled as a Russian military facility with a predominance of the military population over the civilian population, which provided relatively comfortable conditions for the administration and broadcasting of imperial power as a transformative and culturalising force. In the early 20th century, the content of Tashkent's discourse and the understanding of the empire's practical plans in Central Asia underwent significant changes, which, according to experts, was due to the final 'pacification' of the periphery and gave rise to cultural initiatives of a Russification nature, and with them the promotion of the idea of Russian national conservatives about the need to promote such a variant of the development and ""appropriation"" of the east, in which cultural differences would be gradually erased and a 'large Russian nation' would be formed. In this regard, Tashkent was presented to imperial experts as a 'platform' for the processing, taking into account local conditions, of Russian culture by means of education and dissemination of domestic experience of farming, designed to sedentarise the nomads and establish sedentarisation of the remaining groups of the autochthonous population.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0969,"Decision making on ambiguous stimuli such as prosody by subjects suffering from paranoid schizophrenia, alcohol dependence, and without psychiatric diagnosis","The aim of this study is the empirical verification of the Bayesian approach applied to the description of the decision-making process with regard to prosodic stimuli in different psychopathological states. Using the Bayesian formalism, the interpretation of a disturbance in internal representation of the contextual information in schizophrenia was given. The results obtained satisfied the formula derived from Bayes' theorem in all tested except a schizophrenic group. Results were interpreted as reflecting cognitive flexibility, and discussed in the context of social adaptation. Although the investigation was based on psychopathological grounds, the results may be applied to the functioning of working memory in general.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0970,Effects of Music Perception on Language Development in Children With Cochlear Implants: A Systematic Review,"Children with cochlear implants (CIs) have difficulties developing overall language skills due to a lack of neurological adjustment for multimodality inputs. Fortunately, music perception might play a role in strengthening the connection between auditory and motor perception, as language and music have shared neural mechanisms in the left hemisphere of the brain.This review synthesized the recently peer-reviewed studies on the role of music perception in language development for children with CIs by containing the search on pertinent keywords in eight databases following systematic inclusion rubrics. A total of 17 most relevant studies published between 2014 and 2025 were identified. We analyzed the studies focusing on the special role of music perception linked to improved speech perception in children with CIs and the types of musical training that have been found to be effective in improving speech perception and/or spoken language in children with CIs. We analyzed them by the demographics of the participants, music training methods, and language assessment tools, etc. for language development in children with CIs. We concluded the following two major findings: (1) A strong correlation exists between music perception and language development, especially in children with CIs, and (2) music training can potentially bridge the gap in language performance between children with CIs and those with normal hearing by facilitating crossmodal neuroplasticity. The findings of this synthesis suggest that children with CIs may benefit from music perception in early childhood to strengthen their neuroplasticity through visual-motor connectivity through music.The findings of our systematic review highlight the unique role of music perception in bridging language gaps for children with CIs.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0971,Prosody and emergence of the senses: propositions for a cognitive study of intonation,"This paper aims at giving English intonation a driving role in the building and the emergence of meaning. It presents four propositions, going from the perception of intonation to its role in the representation of meaning. First, the concept of intonational form, based on the gestalt model of good form, is introduced. Second, the fundamental characteristic of intonational form is its dynamic nature. Third, intonation is positioned in the semantic layer, and is on par with the other linguistic components (syntax, lexicon, grammar). Finally, it is the evolution of the verbal scene that gives intonation its fundamental role.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0972,Acoustic correlates of prominence in Kala Lizu (Tibeto-Burman),"This study investigates acoustic correlates of prominence in Lizu (Tibeto-Burman). Lizu has been argued to have a hybrid prosodic system combining lexical tone on monosyllabic words and prominence patterns with stress-like and tonal characteristics on polysyllabic words, although empirical evidence is lacking. This study presents an acoustic investigation of the pitch patterns on disyllabic words in the Kala variety of Lizu (HL, HH, LH). Using the parameters of duration, intensity, and f0, it attempts to sort out different forms of prominence, and to explore their interaction with lexical tone. The measurements are taken from experimental data with eight Lizu speakers (4 male and 4 female). The acoustic results and statistical analyses suggest that the first syllable in Kala Lizu is the position of prominence in all three pitch patterns. Our results are consistent with interpreting the pitch pattern HL as more stress-like (with intensity as an acoustic correlate of stress) and the pitch patterns HH and LH as more tone -like (with stress cued by full realization of lexical tone). This study contributes to a better understanding of the prosodic organization of Kala Lizu and it also suggests methodology for further exploration of other Lizu varieties.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0973,Polar responses in Russian across modalities and across interfaces,"This paper investigates gestures and prosody in polar responses in Russian as part of a larger research program of studying meaning as it is expressed through various channels and constrained at various levels of representation and their interfaces. Based on the data on head nods and a gestural-intonational duster used to question the rationale behind the antecedent speech act in Russian responses, it argues that gestures and intonational contours should be treated on a par with spoken words and their parts when it comes to fitting them into typologies of meaning-encoding expressions in spoken language. It also shows, based on the data on linear placement of gestural and spoken polarity markers in Russian as well as prosodic grouping in Russian (and English) polar responses, that studying gestural content and prosodic properties of utterances can help us reveal various interface constraints in natural language.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0974,Accounting for lexical tones when modeling phonological distance,"Methods of quantifying distance between sound sequences are known as phonological distance measures. Despite the wide application across subfields, phonological distance has been calculated mainly with features related to consonants and vowels. This research report establishes new measurements of phonological distance that incorporate lexical tone through experimental approaches and modeling, using Hong Kong Cantonese as a case study. Results show correspondences between the experimental data and predictions from information-theoretic measures, including entropy measures and functional load, suggesting that lexical components which play a more crucial role in phonological distance judgments are lexically less predictable as well. Implications for phonological distance measures are discussed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0975,Lexical Tone in Metnyo Ambel,"In this paper, I outline the tone system of the Metnyo dialect of Ambel, an endangered Austronesian language spoken in the Raja Ampat archipelago in east Indonesia. The tone system of Metnyo Ambel is binary and privative: /H/ syllables contrast with toneless syllables. I will present data from monosyllabic and polysyllabic words, as well as words of varying morphological complexity, to show that tone is culminative, both at the level of the morpheme and in the output, but not obligatory. The realization of tone will also be described. In particular, I focus on the pitch patterns found on intonation phrase-final syllables, which depend on a complex interaction between lexical tone, intonational boundary tones (whether the word is lexical or grammatical), and the moraic weight of the syllable. Finally, the tone system of Metnyo Ambel will be compared with the prosodic systems of other nearby languages, including the closely related Maya and Matbat.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0976,Musical Melody and Speech Intonation: Singing a Different Tune?,"Music and speech are often cited as characteristically human forms of communication. Both share the features of hierarchical structure, complex sound systems, and sensorimotor sequencing demands, and both are used to convey and influence emotions, among other functions [1]. Both music and speech also prominently use acoustical frequency modulations, perceived as variations in pitch, as part of their communicative repertoire. Given these similarities, and the fact that pitch perception and production involve the same peripheral transduction system (cochlea) and the same production mechanism (vocal tract), it might be natural to assume that pitch processing in speech and music would also depend on the same underlying cognitive and neural mechanisms. In this essay we argue that the processing of pitch information differs significantly for speech and music; specifically, we suggest that there are two pitch-related processing systems, one for more coarse-grained, approximate analysis and one for more fine-grained accurate representation, and that the latter is unique to music. More broadly, this dissociation offers clues about the interface between sensory and motor systems, and highlights the idea that multiple processing streams are a ubiquitous feature of neuro-cognitive architectures.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0977,Learning Pitch with STDP: A Computational Model of Place and Temporal Pitch Perception Using Spiking Neural Networks,"Pitch perception is important for understanding speech prosody, music perception, recognizing tones in tonal languages, and perceiving speech in noisy environments. The two principal pitch perception theories consider the place of maximum neural excitation along the auditory nerve and the temporal pattern of the auditory neurons' action potentials (spikes) as pitch cues. This paper describes a biophysical mechanism by which fine-structure temporal information can be extracted from the spikes generated at the auditory periphery. Deriving meaningful pitch-related information from spike times requires neural structures specialized in capturing synchronous or correlated activity from amongst neural events. The emergence of such pitch-processing neural mechanisms is described through a computational model of auditory processing. Simulation results show that a correlation-based, unsupervised, spike-based form of Hebbian learning can explain the development of neural structures required for recognizing the pitch of simple and complex tones, with or without the fundamental frequency. The temporal code is robust to variations in the spectral shape of the signal and thus can explain the phenomenon of pitch constancy.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0978,A unitary model of auditory frequency change perception,"Author summaryAs we speak or play music, the sounds we produce change their frequency content over time. Perceiving such frequency changes is crucial to e.g. differentiating vowels and understanding prosody in speech, or following melodies in music. Traditionally, such frequency changes have been described as perceptual changes in ""pitch"" or ""timbre"", with some contradictory experimental findings as to the independence between such dimensions. Here, we study frequency change perception by generalizing a classic auditory stimulus, the so-called Shepard tones, in order to concurrently manipulate acoustic cues to pitch and timbre in fully symmetric and parametric manner. Our main conclusion is that listeners use a compound perceptual dimension to achieve the ecological task of tracking frequency-change across sounds. Interestingly, this dimension can be modeled as an adaptive combination of acoustic cues, weighted according to task demands and listener-specific biases. In addition to suggesting a resolution for discrepancies in the pitch literature, our model reveals basic analogies between the seemingly mysterious concepts of ""pitch"" and ""timbre"" and other perceptual dimensions, such as auditory space, which have long been known to be derived from cue combination. Changes in the frequency content of sounds over time are arguably the most basic form of information about the behavior of sound-emitting objects. In perceptual studies, such changes have mostly been investigated separately, as aspects of either pitch or timbre. Here, we propose a unitary account of ""up"" and ""down"" subjective judgments of frequency change, based on a model combining auditory correlates of acoustic cues in a sound-specific and listener-specific manner. To do so, we introduce a generalized version of so-called Shepard tones, allowing symmetric manipulations of spectral information on a fine scale, usually associated to pitch (spectral fine structure, SFS), and on a coarse scale, usually associated timbre (spectral envelope, SE). In a series of behavioral experiments, listeners reported ""up"" or ""down"" shifts across pairs of generalized Shepard tones that differed in SFS, in SE, or in both. We observed the classic properties of Shepard tones for either SFS or SE shifts: subjective judgements followed the smallest log-frequency change direction, with cases of ambiguity and circularity. Interestingly, when both SFS and SE changes were applied concurrently (synergistically or antagonistically), we observed a trade-off between cues. Listeners were encouraged to report when they perceived ""both"" directions of change concurrently, but this rarely happened, suggesting a unitary percept. A computational model could accurately fit the behavioral data by combining different cues reflecting frequency changes after auditory filtering. The model revealed that cue weighting depended on the nature of the sound. When presented with harmonic sounds, listeners put more weight on SFS-related cues, whereas inharmonic sounds led to more weight on SE-related cues. Moreover, these stimulus-based factors were modulated by inter-individual differences, revealing variability across listeners in the detailed recipe for ""up"" and ""down"" judgments. We argue that frequency changes are tracked perceptually via the adaptive combination of a diverse set of cues, in a manner that is in fact similar to the derivation of other basic auditory dimensions such as spatial location.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0979,Sleep deprivation detected by voice analysis,"Sleep deprivation has an ever-increasing impact on individuals and societies. Yet, to date, there is no quick and objective test for sleep deprivation. Here, we used automated acoustic analyses of the voice to detect sleep deprivation. Building on current machine-learning approaches, we focused on interpretability by introducing two novel ideas: the use of a fully generic auditory representation as input feature space, combined with an interpretation technique based on reverse correlation. The auditory representation consisted of a spectro-temporal modulation analysis derived from neurophysiology. The interpretation method aimed to reveal the regions of the auditory representation that supported the classifiers' decisions. Results showed that generic auditory features could be used to detect sleep deprivation successfully, with an accuracy comparable to state-of-the-art speech features. Furthermore, the interpretation revealed two distinct effects of sleep deprivation on the voice: changes in slow temporal modulations related to prosody and changes in spectral features related to voice quality. Importantly, the relative balance of the two effects varied widely across individuals, even though the amount of sleep deprivation was controlled, thus confirming the need to characterize sleep deprivation at the individual level. Moreover, while the prosody factor correlated with subjective sleepiness reports, the voice quality factor did not, consistent with the presence of both explicit and implicit consequences of sleep deprivation. Overall, the findings show that individual effects of sleep deprivation may be observed in vocal biomarkers. Future investigations correlating such markers with objective physiological measures of sleep deprivation could enable ""sleep stethoscopes"" for the cost-effective diagnosis of the individual effects of sleep deprivation. Sleep deprivation has an ever-increasing impact on individuals and societies, from accidents to chronic conditions costing billions to health systems. Yet, to date, there is no quick and objective test for sleep deprivation. We show that sleep deprivation can be detected at the individual level with voice recordings. Importantly, we focused on interpretability, which allowed us to identify two independent effects of sleep deprivation on the voice: changes in prosody and changes in voice quality or timbre. The results also revealed a striking variability in individual reactions to the same deprivation, further confirming the need to consider the effects of sleep deprivation at the individual level. Vocal markers could be correlated to specific underlying physiological factors in future studies, outlining possible cost-effective and non-invasive ""sleep stethoscopes"".",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0980,Deep-learning models reveal how context and listener attention shape electrophysiological correlates of speech-to-language transformation,"To transform continuous speech into words, the human brain must resolve variability across utterances in intonation, speech rate, volume, accents and so on. A promising approach to explaining this process has been to model electroencephalogram (EEG) recordings of brain responses to speech. Contemporary models typically invoke context invariant speech categories (e.g. phonemes) as an intermediary representational stage between sounds and words. However, such models may not capture the complete picture because they do not model the brain mechanism that categorizes sounds and consequently may overlook associated neural representations. By providing end-to-end accounts of speech-to-text transformation, new deep-learning systems could enable more complete brain models. We model EEG recordings of audiobook comprehension with the deep-learning speech recognition system Whisper. We find that (1) Whisper provides a self-contained EEG model of an intermediary representational stage that reflects elements of prelexical and lexical representation and prediction; (2) EEG modeling is more accurate when informed by 5-10s of speech context, which traditional context invariant categorical models do not encode; (3) Deep Whisper layers encoding linguistic structure were more accurate EEG models of selectively attended speech in two-speaker ""cocktail party"" listening conditions than early layers encoding acoustics. No such layer depth advantage was observed for unattended speech, consistent with a more superficial level of linguistic processing in the brain.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0981,Time Course of the Involvement of the Right Anterior Superior Temporal Gyrus and the Right Fronto-Parietal Operculum in Emotional Prosody Perception,"In verbal communication, not only the meaning of the words convey information, but also the tone of voice (prosody) conveys crucial information about the emotional state and intentions of others. In various studies right frontal and right temporal regions have been found to play a role in emotional prosody perception. Here, we used triple-pulse repetitive transcranial magnetic stimulation (rTMS) to shed light on the precise time course of involvement of the right anterior superior temporal gyrus and the right fronto-parietal operculum. We hypothesized that information would be processed in the right anterior superior temporal gyrus before being processed in the right fronto-parietal operculum. Right-handed healthy subjects performed an emotional prosody task. During listening to each sentence a triplet of TMS pulses was applied to one of the regions at one of six time points (400-1900 ms). Results showed a significant main effect of Time for right anterior superior temporal gyrus and right fronto-parietal operculum. The largest interference was observed half-way through the sentence. This effect was stronger for withdrawal emotions than for the approach emotion. A further experiment with the inclusion of an active control condition, TMS over the EEG site POz (midline parietal-occipital junction), revealed stronger effects at the fronto-parietal operculum and anterior superior temporal gyrus relative to the active control condition. No evidence was found for sequential processing of emotional prosodic information from right anterior superior temporal gyrus to the right fronto-parietal operculum, but the results revealed more parallel processing. Our results suggest that both right fronto-parietal operculum and right anterior superior temporal gyrus are critical for emotional prosody perception at a relatively late time period after sentence onset. This may reflect that emotional cues can still be ambiguous at the beginning of sentences, but become more apparent half-way through the sentence.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0982,Mark My Words: Tone of Voice Changes Affective Word Representations in Memory,"The present study explored the effect of speaker prosody on the representation of words in memory. To this end, participants were presented with a series of words and asked to remember the words for a subsequent recognition test. During study, words were presented auditorily with an emotional or neutral prosody, whereas during test, words were presented visually. Recognition performance was comparable for words studied with emotional and neutral prosody. However, subsequent valence ratings indicated that study prosody changed the affective representation of words in memory. Compared to words with neutral prosody, words with sad prosody were later rated as more negative and words with happy prosody were later rated as more positive. Interestingly, the participants' ability to remember study prosody failed to predict this effect, suggesting that changes in word valence were implicit and associated with initial word processing rather than word retrieval. Taken together these results identify a mechanism by which speakers can have sustained effects on listener attitudes towards word referents.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0983,Auditory-Motor Mapping Training as an Intervention to Facilitate Speech Output in Non-Verbal Children with Autism: A Proof of Concept Study,"Although up to 25% of children with autism are non-verbal, there are very few interventions that can reliably produce significant improvements in speech output. Recently, a novel intervention called Auditory-Motor Mapping Training (AMMT) has been developed, which aims to promote speech production directly by training he association between sounds and articulatory actions using intonation and bimanual motor activities. AMMT capitalizes on the inherent musical strengths of children with autism, and offers activities that they intrinsically enjoy. It also engages and potentially stimulates a network of brain regions that may be dysfunctional in autism. Here we report an initial efficacy study to provide 'proof of concept' for AMMT. Six non-verbal children with autism participated. Prior to treatment, the children had no intelligible words. They each received 40 individual sessions of AMMT 5 times per week, over an 8-week period. Probe assessments were conducted periodically during baseline, therapy, and follow-up sessions. After therapy, all children showed significant improvements in their ability to articulate words and phrases, with generalization to items that were not practiced during therapy sessions. Because these children had no or minimal vocal output prior to treatment, the acquisition of speech sounds and word approximations through AMMT represents a critical step in expressive language development in children with autism.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0984,Preattentive Extraction of Abstract Auditory Rules in Speech Sound Stream: A Mismatch Negativity Study Using Lexical Tones,"Background: Extraction of linguistically relevant auditory features is critical for speech comprehension in complex auditory environments, in which the relationships between acoustic stimuli are often abstract and constant while the stimuli per se are varying. These relationships are referred to as the abstract auditory rule in speech and have been investigated for their underlying neural mechanisms at an attentive stage. However, the issue of whether or not there is a sensory intelligence that enables one to automatically encode abstract auditory rules in speech at a preattentive stage has not yet been thoroughly addressed. Methodology/ Principal Findings: We chose Chinese lexical tones for the current study because they help to define word meaning and hence facilitate the fabrication of an abstract auditory rule in a speech sound stream. We continuously presented native Chinese speakers with Chinese vowels differing in formant, intensity, and level of pitch to construct a complex and varying auditory stream. In this stream, most of the sounds shared flat lexical tones to form an embedded abstract auditory rule. Occasionally the rule was randomly violated by those with a rising or falling lexical tone. The results showed that the violation of the abstract auditory rule of lexical tones evoked a robust preattentive auditory response, as revealed by whole-head electrical recordings of the mismatch negativity (MMN), though none of the subjects acquired explicit knowledge of the rule or became aware of the violation. Conclusions/Significance: Our results demonstrate that there is an auditory sensory intelligence in the perception of Chinese lexical tones. The existence of this intelligence suggests that the humans can automatically extract abstract auditory rules in speech at a preattentive stage to ensure speech communication in complex and noisy auditory environments without drawing on conscious resources.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0985,The Mechanism of Speech Processing in Congenital Amusia: Evidence from Mandarin Speakers,"Congenital amusia is a neuro-developmental disorder of pitch perception that causes severe problems with music processing but only subtle difficulties in speech processing. This study investigated speech processing in a group of Mandarin speakers with congenital amusia. Thirteen Mandarin amusics and thirteen matched controls participated in a set of tone and intonation perception tasks and two pitch threshold tasks. Compared with controls, amusics showed impaired performance on word discrimination in natural speech and their gliding tone analogs. They also performed worse than controls on discriminating gliding tone sequences derived from statements and questions, and showed elevated thresholds for pitch change detection and pitch direction discrimination. However, they performed as well as controls on word identification, and on statement-question identification and discrimination in natural speech. Overall, tasks that involved multiple acoustic cues to communicative meaning were not impacted by amusia. Only when the tasks relied mainly on pitch sensitivity did amusics show impaired performance compared to controls. These findings help explain why amusia only affects speech processing in subtle ways. Further studies on a larger sample of Mandarin amusics and on amusics of other language backgrounds are needed to consolidate these results.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0986,The Derived Allele of ASPM Is Associated with Lexical Tone Perception,"The ASPM and MCPH1 genes have been implicated in the adaptive evolution of the human brain [Mekel-Bobrov N. et al., 2005. Ongoing adaptive evolution of ASPM, a brain size determinant in homo sapiens. Science 309; Evans P. D. et al., 2005. Microcephalin, a gene regulating brain size, continues to evolve adaptively in humans. Science 309]. Curiously, experimental attempts have failed to connect the implicated SNPs in these genes with higher-level brain functions. These results stand in contrast with a population-level study linking the population frequency of their alleles with the tendency to use lexical tones in a language [Dediu D., Ladd D. R., 2007. Linguistic tone is related to the population frequency of the adaptive haplogroups of two brain size genes, ASPM and microcephalin. Proc. Natl. Acad. Sci. U. S. A. 104]. In the present study, we found a significant correlation between the load of the derived alleles of ASPM and tone perception in a group of European Americans who did not speak a tone language. Moreover, preliminary results showed a significant correlation between ASPM load and hemodynamic responses to lexical tones in the auditory cortex, and such correlation remained after phonemic awareness, auditory working memory, and non-verbal IQ were controlled. As in previous studies, no significant correlation between ASPM and cognitive measures were found. MCPH1 did not correlate with any measures. These results suggest that the association between the recently derived allele of ASPM is likely to be specific and is tied to higher level brain functions in the temporal cortex related to human communication.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0987,Amusia Results in Abnormal Brain Activity following Inappropriate Intonation during Speech Comprehension,"Pitch processing is a critical ability on which humans' tonal musical experience depends, and which is also of paramount importance for decoding prosody in speech. Congenital amusia refers to deficits in the ability to properly process musical pitch, and recent evidence has suggested that this musical pitch disorder may impact upon the processing of speech sounds. Here we present the first electrophysiological evidence demonstrating that individuals with amusia who speak Mandarin Chinese are impaired in classifying prosody as appropriate or inappropriate during a speech comprehension task. When presented with inappropriate prosody stimuli, control participants elicited a larger P600 and smaller N100 relative to the appropriate condition. In contrast, amusics did not show significant difference between the appropriate and inappropriate conditions in either the N100 or the P600 component. This provides further evidence that the pitch perception deficits associated with amusia may also affect intonation processing during speech comprehension in those who speak a tonal language such as Mandrian, and suggests music and language share some cognitive and neural resources.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P0988,Rapid Extraction of Lexical Tone Phonology in Chinese Characters: A Visual Mismatch Negativity Study,"Background: In alphabetic languages, emerging evidence from behavioral and neuroimaging studies shows the rapid and automatic activation of phonological information in visual word recognition. In the mapping from orthography to phonology, unlike most alphabetic languages in which there is a natural correspondence between the visual and phonological forms, in logographic Chinese, the mapping between visual and phonological forms is rather arbitrary and depends on learning and experience. The issue of whether the phonological information is rapidly and automatically extracted in Chinese characters by the brain has not yet been thoroughly addressed. Methodology/Principal Findings: We continuously presented Chinese characters differing in orthography and meaning to adult native Mandarin Chinese speakers to construct a constant varying visual stream. In the stream, most stimuli were homophones of Chinese characters: The phonological features embedded in these visual characters were the same, including consonants, vowels and the lexical tone. Occasionally, the rule of phonology was randomly violated by characters whose phonological features differed in the lexical tone. Conclusions/Significance: We showed that the violation of the lexical tone phonology evoked an early, robust visual response, as revealed by whole-head electrical recordings of the visual mismatch negativity (vMMN), indicating the rapid extraction of phonological information embedded in Chinese characters. Source analysis revealed that the vMMN was involved in neural activations of the visual cortex, suggesting that the visual sensory memory is sensitive to phonological information embedded in visual words at an early processing stage.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0989,Conveying Movement in Music and Prosody,"We investigated whether acoustic variation of musical properties can analogically convey descriptive information about an object. Specifically, we tested whether information from the temporal structure in music interacts with perception of a visual image to form an analog perceptual representation as a natural part of music perception. In Experiment 1, listeners heard music with an accelerating or decelerating temporal pattern, and then saw a picture of a still or moving object and decided whether it was animate or inanimate - a task unrelated to the patterning of the music. Object classification was faster when musical motion matched visually depicted motion. In Experiment 2, participants heard spoken sentences that were accompanied by accelerating or decelerating music, and then were presented with a picture of a still or moving object. When motion information in the music matched motion information in the picture, participants were similarly faster to respond. Fast and slow temporal patterns without acceleration and deceleration, however, did not make participants faster when they saw a picture depicting congruent motion information (Experiment 3), suggesting that understanding temporal structure information in music may depend on specific metaphors about motion in music. Taken together, these results suggest that visuo-spatial referential information can be analogically conveyed and represented by music and can be integrated with speech or influence the understanding of speech.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0990,A Word by Any Other Intonation: FMRI Evidence for Implicit Memory Traces for Pitch Contours of Spoken Words in Adult Brains,"Objectives: Intonation may serve as a cue for facilitated recognition and processing of spoken words and it has been suggested that the pitch contour of spoken words is implicitly remembered. Thus, using the repetition suppression (RS) effect of BOLD-fMRI signals, we tested whether the same spoken words are differentially processed in language and auditory brain areas depending on whether or not they retain an arbitrary intonation pattern. Experimental design: Words were presented repeatedly in three blocks for passive and active listening tasks. There were three prosodic conditions in each of which a different set of words was used and specific task-irrelevant intonation changes were applied: (i) All words presented in a set flat monotonous pitch contour (ii) Each word had an arbitrary pitch contour that was set throughout the three repetitions. (iii) Each word had a different arbitrary pitch contour in each of its repetition. Principal findings: The repeated presentations of words with a set pitch contour, resulted in robust behavioral priming effects as well as in significant RS of the BOLD signals in primary auditory cortex (BA 41), temporal areas (BA 21 22) bilaterally and in Broca's area. However, changing the intonation of the same words on each successive repetition resulted in reduced behavioral priming and the abolition of RS effects. Conclusions: Intonation patterns are retained in memory even when the intonation is task-irrelevant. Implicit memory traces for the pitch contour of spoken words were reflected in facilitated neuronal processing in auditory and language associated areas. Thus, the results lend support for the notion that prosody and specifically pitch contour is strongly associated with the memory representation of spoken words.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0991,Implicit Target Substitution and Sequencing for Lexical Tone Production in Chinese: An fMRI Study,"In this study, we examine the neural substrates underlying Tone 3 sandhi and tone sequencing in Mandarin Chinese using fMRI. Tone 3 sandhi is traditionally described as the substitution of Tone 3 with Tone 2 when followed by another Tone 3 (i.e., 33 -> 23). According to current speech production models, target substitution is expected to engage the posterior inferior frontal gyrus. Since Tone 3 sandhi is, to some extent, independent of segments, which makes it more similar to singing, right-lateralized activation in this region was predicted. As for tone sequencing, based on studies in sequencing, we expected the involvement of the supplementary motor area. In the experiments, participants were asked to produce twelve four-syllable sequences with the same tone assignment (the repeated sequences) or a different tone assignment (the mixed sequences). We found right-lateralized posterior inferior frontal gyrus activation for the sequence 3333 (Tone 3 sandhi) and left-lateralized activation in the supplementary motor area for the mixed sequences (tone sequencing). We proposed that tones and segments could be processed in parallel in the left and right hemispheres, but their integration, or the product of their integration, is hosted in the left hemisphere.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0992,Developmental Changes in Mismatch Responses to Mandarin Consonants and Lexical Tones from Early to Middle Childhood,"The purpose of this study was to use mismatch responses (MMRs) to explore the dynamic changes of Mandarin speech perception abilities from early to middle childhood. Twenty preschoolers, 18 school-aged children, and 26 adults participated in this study. Two sets of synthesized speech stimuli varying in Mandarin consonant (alveolo-palatal affricate vs. fricative) and lexical tone features (rising vs. contour tone) were used to examine the developmental course of speech perception abilities. The results indicated that only the adult group demonstrated typical early mismatch negativity (MMN) responses, suggesting that the ability to discriminate specific speech cues in Mandarin consonant and lexical tone is a continuing process in preschool-and school-aged children. Additionally, distinct MMR patterns provided evidence indicating diverse developmental courses to different speech characteristics. By incorporating data from the two speech conditions, we propose using MMR profiles consisting of mismatch negativity (MMN), positive mismatch response (p-MMR), and late discriminative negativity (LDN) as possible brain indices to investigate speech perception development.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0993,Contribution of Auditory Working Memory to Speech Understanding in Mandarin-Speaking Cochlear Implant Users,"Purpose: To investigate how auditory working memory relates to speech perception performance by Mandarin-speaking cochlear implant (CI) users. Method: Auditory working memory and speech perception was measured in Mandarin-speaking CI and normal-hearing (NH) participants. Working memory capacity was measured using forward digit span and backward digit span; working memory efficiency was measured using articulation rate. Speech perception was assessed with: (a) word-in-sentence recognition in quiet, (b) word-in-sentence recognition in speech-shaped steady noise at +5 dB signal-to-noise ratio, (c) Chinese disyllable recognition in quiet, (d) Chinese lexical tone recognition in quiet. Self-reported school rank was also collected regarding performance in schoolwork. Results: There was large inter-subject variability in auditory working memory and speech performance for CI participants. Working memory and speech performance were significantly poorer for CI than for NH participants. All three working memory measures were strongly correlated with each other for both CI and NH participants. Partial correlation analyses were performed on the CI data while controlling for demographic variables. Working memory efficiency was significantly correlated only with sentence recognition in quiet when working memory capacity was partialled out. Working memory capacity was correlated with disyllable recognition and school rank when efficiency was partialled out. There was no correlation between working memory and lexical tone recognition in the present CI participants. Conclusions: Mandarin-speaking CI users experience significant deficits in auditory working memory and speech performance compared with NH listeners. The present data suggest that auditory working memory may contribute to CI users' difficulties in speech understanding. The present pattern of results with Mandarin-speaking CI users is consistent with previous auditory working memory studies with English-speaking CI users, suggesting that the lexical importance of voice pitch cues (albeit poorly coded by the CI) did not influence the relationship between working memory and speech perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0994,Motherese by Eye and Ear: Infants Perceive Visual Prosody in Point-Line Displays of Talking Heads,"Infant-directed (ID) speech provides exaggerated auditory and visual prosodic cues. Here we investigated if infants were sensitive to the match between the auditory and visual correlates of ID speech prosody. We presented 8-month-old infants with two silent line-joined point-light displays of faces speaking different ID sentences, and a single vocal-only sentence matched to one of the displays. Infants looked longer to the matched than mismatched visual signal when full-spectrum speech was presented; and when the vocal signals contained speech low-pass filtered at 400 Hz. When the visual display was separated into rigid (head only) and non-rigid (face only) motion, the infants looked longer to the visual match in the rigid condition; and to the visual mismatch in the non-rigid condition. Overall, the results suggest 8-month-olds can extract information about the prosodic structure of speech from voice and head kinematics, and are sensitive to their match; and that they are less sensitive to the match between lip and voice information in connected speech.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0995,The Role of Temporal Envelope and Fine Structure in Mandarin Lexical Tone Perception in Auditory Neuropathy Spectrum Disorder,"Temporal information in a signal can be partitioned into temporal envelope (E) and fine structure (FS). Fine structure is important for lexical tone perception for normal-hearing (NH) listeners, and listeners with sensorineural hearing loss (SNHL) have an impaired ability to use FS in lexical tone perception due to the reduced frequency resolution. The present study was aimed to assess which of the acoustic aspects (E or FS) played a more important role in lexical tone perception in subjects with auditory neuropathy spectrum disorder (ANSD) and to determine whether it was the deficit in temporal resolution or frequency resolution that might lead to more detrimental effects on FS processing in pitch perception. Fifty-eight native Mandarin Chinese-speaking subjects (27 with ANSD, 16 with SNHL, and 15 with NH) were assessed for (1) their ability to recognize lexical tones using acoustic E or FS cues with the ""auditory chimera"" technique, (2) temporal resolution as measured with temporal gap detection (TGD) threshold, and (3) frequency resolution as measured with the Q(10dB) values of the psychophysical tuning curves. Overall, 26.5%, 60.2%, and 92.1% of lexical tone responses were consistent with FS cues for tone perception for listeners with ANSD, SNHL, and NH, respectively. The mean TGD threshold was significantly higher for listeners with ANSD (11.9 ms) than for SNHL (4.0 ms; p < 0.001) and NH (3.9 ms; p < 0.001) listeners, with no significant difference between SNHL and NH listeners. In contrast, the mean Q10dB for listeners with SNHL (1.8 +/- 0.4) was significantly lower than that for ANSD (3.5 +/- 1.0; p < 0.001) and NH (3.4 +/- 0.9; p < 0.001) listeners, with no significant difference between ANSD and NH listeners. These results suggest that reduced temporal resolution, as opposed to reduced frequency selectivity, in ANSD subjects leads to greater degradation of FS processing for pitch perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0996,Distributional Learning of Lexical Tones: A Comparison of Attended vs. Unattended Listening,"This study examines whether non-tone language listeners can acquire lexical tone categories distributionally and whether attention in the training phase modulates the effect of distributional learning. Native Australian English listeners were trained on a Thai lexical tone minimal pair and their performance was assessed using a discrimination task before and after training. During Training, participants either heard a Unimodal distribution that would induce a single central category, which should hinder their discrimination of that minimal pair, or a Bimodal distribution that would induce two separate categories that should facilitate their discrimination. The participants either heard the distribution passively (Experiments 1A and 1B) or performed a cover task during training designed to encourage auditory attention to the entire distribution (Experiment 2). In passive listening (Experiments 1A and 1B), results indicated no effect of distributional learning: the Bimodal group did not outperform the Unimodal group in discriminating the Thai tone minimal pairs. Moreover, both Unimodal and Bimodal groups improved above chance on most test aspects from Pretest to Posttest. However, when participants' auditory attention was encouraged using the cover task (Experiment 2), distributional learning was found: the Bimodal group outperformed the Unimodal group on a novel test syllable minimal pair at Posttest relative to at Pretest. Furthermore, the Bimodal group showed above-chance improvement from Pretest to Posttest on three test aspects, while the Unimodal group only showed above-chance improvement on one test aspect. These results suggest that non-tone language listeners are able to learn lexical tones distributionally but only when auditory attention is encouraged in the acquisition phase. This implies that distributional learning of lexical tones is more readily induced when participants attend carefully during training, presumably because they are better able to compute the relevant statistics of the distribution.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0997,Impact of Acute Sleep Deprivation on Sarcasm Detection,"There is growing evidence that sleep plays a pivotal role on health, cognition and emotional regulation. However, the interplay between sleep and social cognition remains an uncharted research area. In particular, little is known about the impact of sleep deprivation on sarcasm detection, an ability which, once altered, may hamper everyday social interactions. The aim of this study is to determine whether sleep-deprived participants are as able as sleep-rested participants to adopt another perspective in gauging sarcastic statements. At 9am, after a whole night of sleep (n = 15) or a sleep deprivation night (n = 15), participants had to read the description of an event happening to a group of friends. An ambiguous voicemail message left by one of the friends on another's phone was then presented, and participants had to decide whether the recipient would perceive the message as sincere or as sarcastic. Messages were uttered with a neutral intonation and were either: (1) sarcastic from both the participant's and the addressee's perspectives (i.e. both had access to the relevant background knowledge to gauge the message as sarcastic), (2) sarcastic from the participant's but not from the addressee's perspective (i.e. the addressee lacked context knowledge to detect sarcasm) or (3) sincere. A fourth category consisted in messages sarcastic from both the participant's and from the addressee's perspective, uttered with a sarcastic tone. Although sleep-deprived participants were as accurate as sleep-rested participants in interpreting the voice message, they were also slower. Blunted reaction time was not fully explained by generalized cognitive slowing after sleep deprivation; rather, it could reflect a compensatory mechanism supporting normative accuracy level in sarcasm understanding. Introducing prosodic cues compensated for increased processing difficulties in sarcasm detection after sleep deprivation. Our findings support the hypothesis that sleep deprivation might damage the flow of social interactions by slowing perspective-taking processes.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P0998,A Tale of Two Features: Perception of Cantonese Lexical Tone and English Lexical Stress in Cantonese-English Bilinguals,"This study investigated the similarities and differences in perception of Cantonese tones and English stress patterns by Cantonese-English bilingual children, adults, and English monolingual adults. All three groups were asked to discriminate pairs of syllables that minimally differed in either Cantonese tone or in English stress. Bilingual children's performance on tone perception was comparable to their performance on stress perception. By contrast, bilingual adults' performance on tone perception was lower than their performance on stress perception, and there was a similar pattern in English monolingual adults. Bilingual adults tended to perform better than English monolingual adults on both the tone and stress perception tests. A significant correlation between tone perception and stress perception performance was found in bilingual children but not in bilingual adults. All three groups showed lower accuracy in the high rising-low rising contrast than any of the other 14 Cantonese tone contrasts. The acoustic analyses revealed that average FO, FO onset, and FO major slope were the critical acoustic correlates of Cantonese tones, whereas multiple acoustic correlates were salient in English stress, including average FO, spectral balance, duration and intensity. We argue that participants' difficulty in perceiving high rising-low rising contrasts originated from the contrasts' similarities in FO onset and average FO; indeed the difference between their major slopes was the only cue with which to distinguish them. Acoustic-perceptual correlation analyses showed that although the average FO and FO onset were associated with tone perception performance in all three groups, FO major slope was only associated with tone perception in the bilingual adult group. These results support a dynamic interactive account of suprasegmental speech perception by emphasizing the positive prosodic transfer between Cantonese tone and English stress, and the role that level of bilingual language experience and age play in shaping suprasegmental speech perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P0999,Representation and Processing of Lexical Tone and Tonal Variants: Evidence from the Mismatch Negativity,"Pronunciation variation is ubiquitous in the speech signal. Different models of lexical representation have been put forward to deal with speech variability, which differ in the level as well as the nature of mental representation. We present the first mismatch negativity (MMN) study investigating the effect of allophonic variation on the mental representation and neural processing of lexical tones. Native speakers of Standard Chinese (SC) participated in an oddball electroencephalography (EEG) experiment. All stimuli have the same segments (ma) but different lexical tones: level [T1], rising [T2], and dipping [T3]. In connected speech with a T3T3 sequence, the first T3 may undergo allophonic change and is produced with a rising pitch contour (T3V), similar to the lexical T2 pitch contour. Four oddball conditions were constructed (T1/T3, T3/T1, T2/T3, T3/T2; standard/deviant). All four conditions elicited MMN effects, with the T1-T3 pair eliciting comparable MMNs, but the T2-T3 pair asymmetrical MMN effects. There were significantly greater and earlier MMN effects in the T2/T3 condition than that in the reversed T3/T2 condition. Furthermore, the T3/T2 condition showed more rightward MMN effects than the T2/T3 condition and the T1-T3 pair. Such asymmetries suggest co-activation of long-term memory representations of both T3 and T3V when T3 serves as the standard. The acoustic similarity between the activated T3V (by the standard T3) and the incoming deviant stimulus T2 induces acoustic processing of the tonal contrast in the T3/T2 condition, similar to that of within-category lexical tone processing, which is in contrast to the processing of between-category lexical tones observed in the T2/T3, T1/T3, and T3/T1 conditions.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1000,Auditory-Motor Mapping Training: Comparing the Effects of a Novel Speech Treatment to a Control Treatment for Minimally Verbal Children with Autism,"This study compared Auditory-Motor Mapping Training (AMMT), an intonation-based treatment for facilitating spoken language in minimally verbal children with autism spectrum disorder (ASD), to a matched control treatment, Speech Repetition Therapy (SRT). 23 minimally verbal children with ASD (20 male, mean age 6; 5) received at least 25 sessions of AMMT. Seven (all male) were matched on age and verbal ability to seven participants (five male) who received SRT. Outcome measures were Percent Syllables Approximated, Percent Consonants Correct (of 86), and Percent Vowels Correct (of 61) produced on two sets of 15 bisyllabic stimuli. All subjects were assessed on these measures several times at baseline and after 10, 15, 20, and 25 sessions. The post-25 session assessment timepoint, common to all participants, was compared to Best Baseline performance. Overall, after 25 sessions, AMMT participants increased by 19.4% Syllables Approximated, 13.8% Consonants Correct, and19.1% Vowels Correct, compared to Best Baseline. In the matched AMMT-SRT group, after 25 sessions, AMMT participants produced 29.0% more Syllables Approximated (SRT 3.6%); 17.9% more Consonants Correct (SRT 0.5); and 17.6% more Vowels Correct (SRT 0.8%). Chi-square tests showed that significantly more AMMT than SRT participants in both the overall and matched groups improved significantly in number of Syllables Approximated per stimulus and number of Consonants Correct per stimulus. Pretreatment ability to imitate phonemes, but not chronological age or baseline performance on outcome measures, was significantly correlated with amount of improvement after 25 sessions. Intonation-based therapy may offer a promising new interventional approach for teaching spoken language to minimally verbal children with ASD.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1001,Mandarin-English Bilinguals Process Lexical Tones in Newly Learned Words in Accordance with the Language Context,"Previous research has mainly considered the impact of tone-language experience on ability to discriminate linguistic pitch, but proficient bilingual listening requires differential processing of sound variation in each language context. Here, we ask whether Mandarin-English bilinguals, for whom pitch indicates word distinctions in one language but not the other, can process pitch differently in a Mandarin context vs. an English context. Across three eye tracked word-learning experiments, results indicated that tone-intonation bilinguals process tone in accordance with the language context. In Experiment 1, 51 Mandarin-English bilinguals and 26 English speakers without tone experience were taught Mandarin-compatible novel words with tones. Mandarin-English bilinguals out-performed English speakers, and, for bilinguals, overall accuracy was correlated with Mandarin dominance. Experiment 2 taught 24 Mandarin-English bilinguals and 25 English speakers novel words with Mandarin like tones, but English-like phonemes and phonotactics. The Mandarin-dominance advantages observed in Experiment 1 disappeared when words were English-like. Experiment 3 contrasted Mandarin-like vs. English-like words in a within-subjects design, providing even stronger evidence that bilinguals can process tone language-specifically. Bilinguals (N = 58), regardless of language dominance, attended more to tone than English speakers without Mandarin experience (N = 28), but only when words were Mandarin-like not when they were English-like. Mandarin-English bilinguals thus tailor tone processing to the within-word language context.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1002,Pitch contour impairment in congenital amusia: New insights from the Self-paced Audio-visual Contour Task (SACT),"Individuals with congenital amusia usually exhibit impairments in melodic contour processing when asked to compare pairs of melodies that may or may not be identical to one another. However, it is unclear whether the impairment observed in contour processing is caused by an impairment of pitch discrimination, or is a consequence of poor pitch memory. To help resolve this ambiguity, we designed a novel Self-paced Audio-visual Contour Task (SACT) that evaluates sensitivity to contour while placing minimal burden on memory. In this task, participants control the pace of an auditory contour that is simultaneously accompanied by a visual contour, and they are asked to judge whether the two contours are congruent or incongruent. In Experiment 1, melodic contours varying in pitch were presented with a series of dots that varied in spatial height. Amusics exhibited reduced sensitivity to audio-visual congruency in comparison to control participants. To exclude the possibility that the impairment arises from a general deficit in cross-modal mapping, Experiment 2 examined sensitivity to cross-modal mapping for two other auditory dimensions: timbral brightness and loudness. Amusics and controls were significantly more sensitive to large than small contour changes, and to changes in loudness than changes in timbre. However, there were no group differences in cross-modal mapping, suggesting that individuals with congenital amusia can comprehend spatial representations of acoustic information. Taken together, the findings indicate that pitch contour processing in congenital amusia remains impaired even when pitch memory is relatively unburdened.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1003,Deficits of congenital amusia beyond pitch: Evidence from impaired categorical perception of vowels in Cantonese-speaking congenital amusics,"Congenital amusia is a lifelong disorder of fine-grained pitch processing in music and speech. However, it remains unclear whether amusia is a pitch-specific deficit, or whether it affects frequency/spectral processing more broadly, such as the perception of formant frequency in vowels, apart from pitch. In this study, in order to illuminate the scope of the deficits, we compared the performance of 15 Cantonese-speaking amusics and 15 matched controls on the categorical perception of sound continua in four stimulus contexts: lexical tone, pure tone, vowel, and voice onset time (VOT). Whereas lexical tone, pure tone and vowel continua rely on frequency/spectral processing, the VOT continuum depends on duration/temporal processing. We found that the amusic participants performed similarly to controls in all stimulus contexts in the identification, in terms of the across-category boundary location and boundary width. However, the amusic participants performed systematically worse than controls in discriminating stimuli in those three contexts that depended on frequency/spectral processing (lexical tone, pure tone and vowel), whereas they performed normally when discriminating duration differences (VOT). These findings suggest that the deficit of amusia is probably not pitch specific, but affects frequency/spectral processing more broadly. Furthermore, there appeared to be differences in the impairment of frequency/spectral discrimination in speech and nonspeech contexts. The amusic participants exhibited less benefit in between-category discriminations than controls in speech contexts (lexical tone and vowel), suggesting reduced categorical perception; on the other hand, they performed inferiorly compared to controls across the board regardless of between-and within-category discriminations in nonspeech contexts (pure tone), suggesting impaired general auditory processing. These differences imply that the frequency/spectral-processing deficit might be manifested differentially in speech and nonspeech contexts in amusics D it is manifested as a deficit of higher-level phonological processing in speech sounds, and as a deficit of lower-level auditory processing in nonspeech sounds.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1004,Tone perception in Mandarin-speaking school age children with otitis media with effusion,"Objectives The present study explored tone perception ability in school age Mandarin-speaking children with otitis media with effusion (OME) in noisy listening environments. The study investigated the interaction effects of noise, tone type, age, and hearing status on monaural tone perception, and assessed the application of a hierarchical clustering algorithm for profiling hearing impairment in children with OME. Methods Forty-one children with normal hearing and normal middle ear status and 84 children with OME with or without hearing loss participated in this study. The children with OME were further divided into two subgroups based on their severity and pattern of hearing loss using a hierarchical clustering algorithm. Monaural tone recognition was measured using a picture-identification test format incorporating six sets of monosyllabic words conveying four lexical tones under speech spectrum noise, with the signal-to-noise ratio (SNR) conditions ranging from -9 to -21 dB. Results Linear correlation indicated tone recognition thresholds of children with OME were significantly correlated with age and pure tone hearing thresholds at every frequency tested. Children with hearing thresholds less affected by OME performed similarly to their peers with normal hearing. Tone recognition thresholds of children with auditory status more affected by OME were significantly inferior to those of children with normal hearing or with minor hearing loss. Younger children demonstrated poorer tone recognition performance than older children with OME. A mixed design repeated-measure ANCOVA showed significant main effects of listening condition, hearing status, and tone type on tone recognition. Contrast comparisons revealed that tone recognition scores were significantly better under -12 dB SNR than under -15 dB SNR conditions and tone recognition scores were significantly worse under -18 dB SNR than those obtained under -15 dB SNR conditions. Tone 1 was the easiest tone to identify and Tone 3 was the most difficult tone to identify for all participants, when considering -12, -15, and -18 dB SNR as within-subject variables. The interaction effect between hearing status and tone type indicated that children with greater levels of OME-related hearing loss had more impaired tone perception of Tone 1 and Tone 2 compared to their peers with lesser levels of OME-related hearing loss. However, tone perception of Tone 3 and Tone 4 remained similar among all three groups. Tone 2 and Tone 3 were the most perceptually difficult tones for children with or without OME-related hearing loss in all listening conditions. Conclusions The hierarchical clustering algorithm demonstrated usefulness in risk stratification for tone perception deficiency in children with OME- related hearing loss. There was marked impairment in tone perception in noise for children with greater levels of OME- related hearing loss. Monaural lexical tone perception in younger children was more vulnerable to noise and OME- related hearing loss than that in older children.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1005,CLEESE: An open-source audio-transformation toolbox for data-driven experiments in speech and music cognition,"Over the past few years, the field of visual social cognition and face processing has been dramatically impacted by a series of data-driven studies employing computer-graphics tools to synthesize arbitrary meaningful facial expressions. In the auditory modality, reverse correlation is traditionally used to characterize sensory processing at the level of spectral or spectro-temporal stimulus properties, but not higher-level cognitive processing of e.g. words, sentences or music, by lack of tools able to manipulate the stimulus dimensions that are relevant for these processes. Here, we present an open-source audio-transformation toolbox, called CLEESE, able to systematically randomize the prosody/melody of existing speech and music recordings. CLEESE works by cutting recordings in small successive time segments (e.g. every successive 100 milliseconds in a spoken utterance), and applying a random parametric transformation of each segment's pitch, duration or amplitude, using a new Python-language implementation of the phase-vocoder digital audio technique. We present here two applications of the tool to generate stimuli for studying intonation processing of interrogative vs declarative speech, and rhythm processing of sung melodies.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1006,A preliminary study of auditory mismatch response on the day of cochlear implant activation in children with hearing aids prior implantation,"Objective The study aimed to explore the characteristics of auditory mismatch response (MMR) in hearing-impaired children on the day when the cochlear implant (CI) was started (power-up) and the speech processor was programmed, and to investigate the effects of wearing hearing aids (HAs) before cochlear implantation on the early stage of postoperative auditory cortex plasticity, providing some demonstrative data for the objective evaluation of postoperative early auditory ability in children who underwent cochlear implantation. Methods The participants were 34 children with profound sensorineural hearing loss, who underwent cochlear implantation. The classical passive Oddball paradigm was adopted, using a pair of vowels which only have different lexical tones. The standard stimulus was /a2/ and the devious stimulus was /a4/. Results 1) On the day of CI activation, the auditory MMR has been elicited in 30 children; the MMR incidence was 88%. 2) We observed both positive and negative auditory MMR waveforms. And logistic regression analysis showed that it was influenced by the age at cochlear implantation. 3) The duration with HA before surgery significantly influenced the MMR latency. The children with longer duration of HA use have much earlier latency of MMR. 4) There was a significant positive correlation between the age at HA use initiation and MMR amplitude. Earlier initial HA use was associated with smaller amplitude. Conclusions MMR in response to Mandarin lexical tone can be recorded in most pediatric patients who had experience with HA on the day of CI power up. MMR is closely associated with the age at cochlear implantation, duration of HA use, and the age at HA use initiation. Hearing impaired children should wear HA as early as possible and ensure consistent usage.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1007,The effect of overnight consolidation in the perceptual learning of non-native tonal contrasts,"Sleep-mediated overnight consolidation has been found to facilitate perceptual learning by promoting learners' generalization across talkers in their perception of novel segmental categories. Lexical tone is characterized by high variability across talkers, and displays dynamic change over time. For this reason, it remains unclear whether a similar effect of overnight consolidation would be found for perceptual learning of novel tonal contrasts. Thus, this study aims to examine whether overnight consolidation facilitates talker-independent learning of lexical tones in the identification and discrimination of novel Cantonese level tones by Mandarin listeners. Two groups of Mandarin listeners were perceptually trained either in the morning or in the evening. Listeners were trained in a tone identification (ID) task with feedback using stimuli produced by a trained talker. Their post-training changes and generalization to a novel talker were then tested in the ID and AX discrimination tasks using stimuli produced by trained and untrained talkers in three posttests following training: immediately after training, 12-hour delay, and 24-hour delay. While the evening group slept between the first and second posttests, the morning group did not. The accuracy rates in the ID task showed that the evening group showed an improved trend, predicted by their individual sleep time, in identifying the level tones produced by both the trained and untrained talkers; in contrast, the morning group showed a declining trend. The d-prime scores in the AX discrimination task did not show different patterns between the two groups. The finding of sleep-related identification changes over time suggests that overnight consolidation might have facilitated tone learning of stimuli produced by the novel talker and eventually facilitated the formation of a more talker-independent representation of novel tone categories in long-term memory. The results are discussed in light of the features of lexical tones to shed light on the mechanism of phonetic learning.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1008,Parallel pitch processing in speech and melody: A study of the interference of musical melody on lexical pitch perception in speakers of Mandarin,"Music and language have long been considered two distinct cognitive faculties governed by domain-specific cognitive and neural mechanisms. Recent work into the domain-specificity of pitch processing in both domains appears to suggest pitch processing to be governed by shared neural mechanisms. The current study aimed to explore the domain-specificity of pitch processing by simultaneously presenting pitch contours in speech and music to speakers of a tonal language, and measuring behavioral response and event-related potentials (ERPs). Native speakers of Mandarin were exposed to concurrent pitch contours in melody and speech. Contours in melody emulated those in speech were either congruent or incongruent with the pitch contour of the lexical tone (i.e., rising or falling). Component magnitudes of the N2b and N400 were used as indices of lexical processing. We found that the N2b was modulated by melodic pitch; incongruent item evoked significantly stronger amplitude. There was a trend of N400 to be modulated in the same way. Interestingly, these effects were present only on rising tones. Amplitude and time-course of the N2b and N400 may suggest an interference of melodic pitch contours with both early and late stages of phonological and semantic processing.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1009,Invisible or high-risk: Computer-assisted discourse analysis of references to Aboriginal and Torres Strait Islander people(s) and issues in a newspaper corpus about diabetes,"This article employs computer-assisted methods to analyse references to Aboriginal and Torres Strait Islander people(s) and issues in a newspaper corpus about diabetes. The objectives are to identify both the frequency and quality of social representation. The dataset consisted of 694 items from 12 Australian newspapers in a five-year period (2013-2017). The quantitative analysis focused on frequency (raw/normalised) and range (number/percentage of texts). The qualitative analysis focused on the identification of semantic prosody (co-occurrence with negative/positive words and phrases) and on selective social actor analysis. The qualitative analysis also compared choices made by the press to language practices recommended in relevant reporting guidelines. Key results include that references to Aboriginal and Torres Strait Islander people(s) or matters appear to be extremely rare. In addition, newspapers' language choices only partially align with guidelines. References that do occur can be classified into four categories: a) references to [groups of] people and other references to identity; b) names of services, institutions, professions, roles etc; c) non-human nouns related to health; d) non-human nouns related to culture. Qualitative analysis of the word COMMUNITY suggests that newspapers for the most part do recognise the existence of different communities at a national level. However, analysis of all references to [groups of] people shows that the vast majority occur in contexts to do with negativity, therefore having a negative semantic prosody. More specifically, there is a strong association with mentions of a higher risk, likelihood, or incidence of having or developing diabetes (or complications/effects). In sum, Aboriginal and Torres Strait Islander people(s) and issues lack in visibility in Australian diabetes coverage, and are associated with deficit framing, which can be disempowering. To change the discourse would require both an increased visibility as well as changing the deficit lens.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1010,"Different stages of emotional prosody processing in healthy ageing-evidence from behavioural responses, ERPs, tDCS, and tRNS","Past research suggests that the ability to recognise the emotional intent of a speaker decreases as a function of age. Yet, few studies have looked at the underlying cause for this effect in a systematic way. This paper builds on the view that emotional prosody perception is a multi-stage process and explores which step of the recognition processing line is impaired in healthy ageing using time-sensitive event-related brain potentials (ERPs). Results suggest that early processes linked to salience detection as reflected in the P200 component and initial build-up of emotional representation as linked to a subsequent negative ERP component are largely unaffected in healthy ageing. The two groups show, however, emotional prosody recognition differences: older participants recognise emotional intentions of speakers less well than younger participants do. These findings were followed up by two neuro-stimulation studies specifically targeting the inferior frontal cortex to test if recognition improves during active stimulation relative to sham. Overall, results suggests that neither tDCS nor high-frequency tRNS stimulation at 2mA for 30 minutes facilitates emotional prosody recognition rates in healthy older adults.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1011,Is Cantonese lexical tone information important for sentence recognition accuracy in quiet and in noise?,"In Chinese languages, tones are used to express the lexical meaning of words. It is therefore important to analyze the role of lexical tone in Chinese sentence recognition accuracy. There is a lack of research on the role of Cantonese lexical tones in sentence recognition accuracy. Therefore, this study examined the contribution of lexical tone information to Cantonese sentence recognition accuracy and its cognitive correlates in adults with normal hearing (NH). A text-to-speech synthesis engine was used to synthesize Cantonese daily-use sentences with each word carrying an original or a flat lexical tone, which were then presented to 97 participants in quiet, in speech-shaped noise (SSN), and in two-talker babble (TTB) noise conditions. Both target sentences and noises were presented at 65 dB binaurally via insert headphones. It was found that listeners with NH can almost perfectly recognize a daily-use Cantonese sentence with mismatched lexical tone information in quiet, while their sentence recognition decreases substantially in noise. The same finding was reported for Mandarin, which has a relatively simple tonal system, suggesting that the current results may be applicable to other tonal languages. In addition, working memory (WM) was significantly related to decline in sentence recognition score in the TTB but not in the SSN, when the lexical tones were mismatched. This finding can be explained using the Ease of Language Understanding model and suggests that those with higher WM are less likely to be affected by the degraded lexical information for perceiving daily-use sentences in the TTB.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1012,Emotional tones of voice affect the acoustics and perception of Mandarin tones,"Lexical tones and emotions are conveyed by a similar set of acoustic parameters; therefore, listeners of tonal languages face the challenge of processing lexical tones and emotions in the acoustic signal concurrently. This study examined how emotions affect the acoustics and perception of Mandarin tones. In Experiment 1, Mandarin tones were produced by professional actors with angry, fear, happy, sad, and neutral tones of voice. Acoustic analyses on mean F0, F0 range, mean amplitude, and duration were conducted on syllables excised from a carrier phrase. The results showed that emotions affect Mandarin tone acoustics to different degrees depending on specific Mandarin tones and specific emotions. In Experiment 2, selected syllables from Experiment 1 were presented in isolation or in context. Listeners were asked to identify the Mandarin tones and emotions of the syllables. The results showed that emotions affect Mandarin tone identification to a greater extent than Mandarin tones affect emotion recognition. Both Mandarin tones and emotions were identified more accurately in syllables presented with the carrier phrase, but the carrier phrase affected Mandarin tone identification and emotion recognition to different degrees. These findings suggest that lexical tones and emotions interact in complex but systematic ways.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1013,Cantus: Construction and evaluation of a software solution for real-time vocal music training and musical intonation assessment,"The development of the ability to sing or play in tune is one of the most critical tasks in music training. In music education, melodic patterns are usually learned by imitative processes (modelling). Once modelled, pitch sounds are then associated to a representation according to a syllabic system such as the Guidonian system - or an arbitrary single syllable - or western graphic notation system symbols. From a didactic standpoint, few advances have been made in this area besides the use of audio-supported guides and existing software, which use a microphone to analyse the input and estimate the pitch or fundamental frequency of the given tone. However, these programmes lack the necessary analytical algorithm to provide the student with precise feedback on their execution; and also they do not provide adequate noise-robust solutions to minimize the student assessment error rate. The ongoing research discussed in this article focuses on Cantus, a new software solution expressly designed as an assessment and diagnosis tool for online training and assessment of vocal musical intonation at the initial stages of music education. Cantus software embodies the latest research on real-time analysis of audio stream, which permits the teacher to customize music training by means of recording patterns and embedding them into the programme. The study presented in this article includes the design, implementation and assessment of Cantus by music teachers. The pilot study for the software assessment includes a sample of 21 music teachers working at thirteen music schools in Valencia, Spain. These teachers worked with the software at their own pace for a week in order to evaluate it. Subsequently, a two-part questionnaire was filled in with (1) questions related to demographics, professional experience and the use of ITC; and (2) questions related to the software's technical and didactic aspects. The questionnaire also included three open questions related to Cantus, namely advantages, issues and suggestions. The results show an excellent reception by teachers, who consider this software as a highly adequate music training tool at the initial stages of music education.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1014,DOES PHONOLOGICAL PROMINENCE EXIST?,"There is no motivation for collapsing the prosodic structure of words and the prosodic structure of sentences of English into a single representation that expresses degrees of stress or prominence. Word stress is represented as foot structure, while sentence prosody is represented in terms of phonological phrasing and the distribution of pitch accents. The reason why native speakers can perform rating tasks for prominence on syllables is that their grammar dictates that focus-marking pitch accents be located in stressed syllables and that emphatic speech implies a hyper-articulation of pitch accents in the emphasized text. As a result, any factor, phonetic or otherwise, that affects the listener's impression of the emphasis with which the speaker expresses his message will affect the prominence rating of accentable syllables in the text. Speakers of languages with different prosodic grammars may not be able to perform such tasks.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1015,Ecolinguistics in Bangladeshi Secondary-Level English Textbooks: A Corpus Analysis,"Ecolinguistics has developed over the past five decades in response to ecological crises and the growing demand for an ecological perspective within linguistics (Zhou, 2021). This study examines the integration of environmental content in 'English for Today Classes Nine-Ten', a secondary-level textbook used in Bangladesh, a country facing a range of environmental challenges. Employing both qualitative and quantitative methods, including the use of AntConc and semantic prosody analysis, the study assessed the frequency, context, and presentation of ecological themes such as sustainability, conservation, and environmental awareness. The findings revealed that ecological terms are included in the textbook inconsistently and with limited diversity, which may hinder students' development of environmental awareness. The research aimed to influence curriculum development and educational policies, underlining the need to incorporate varied and recent environmental issues into educational materials. Therefore, this study emphasizes the need for a more comprehensive inclusion of ecological elements to promote ecolinguistic awareness among students. It contributes to ecolinguistics by recommending improvements in environmental representation in English curricula. Additionally, it offers valuable insights for educators and policymakers to enhance students' environmental literacy and promote sustainable development.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1016,Refugees' dehumanization in the Spanish media: A corpus-assisted study within the semantic preference framework,"The dehumanization of migrants and refugees in the media has been the object of numerous critical discourse analyses and metaphor-based studies which have primarily dealt with English written news articles. This paper, however, addresses the dehumanizing language which is used to refer to refugees in a 1.8-million-word corpus of Spanish news articles collected from the digital libraries of El Mundo and El Pais, the two most widely read Spanish newspapers. Our research particularly aims to explore how the dehumanization of the lemma refugiado is constructed through the identification of semantic preferences. It is concerned with synchronic and diachronic aspects, offering results on the evolution of refugees' dehumanization from 2010 to 2016. The dehumanizing collocates are determined via a corpus-based analysis, followed by a detailed manual analysis conducted in order to label the different collocates of refugiado semantically and classify them into more specific semantic subsets. The results show that the lemma refugiado usually collocates with dehumanizing words that express, by frequency order, quantification, out-of-control phenomenon, objectification, and economic burden. The analysis also demonstrates that the collocates corresponding to these four semantic subsets are unusually frequent in the 2015-16 period, giving rise to seasonal collocates strongly related to the Syrian civil war and other Middle-East armed conflicts.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1017,Lexical Tone Perception in Mandarin Chinese Speakers With Aphasia,"The brain localization debate of lexical tone processing concerns functional hypothesis that lexical tone, owing to its strong linguistic features, is dominant in the left hemisphere, and acoustic hypothesis that all pitch patterns, including lexical tone, are dominant in the right hemisphere due to their acoustic features. Lexical tone as a complex signal contains acoustic components that carry linguistic, paralinguistic, and nonlinguistic information. To examine these two hypotheses, the current study adopted triplet stimuli including Chinese characters, their corresponding pinyin with a diacritic, and the four diacritics representing Chinese lexical tones. The stimuli represent the variation of lexical tone for its linguistic and acoustic features. The results of a listening task by Mandarin Chinese speakers with and without aphasia support the functional hypothesis that pitch patterns are lateralized to different hemispheres of the brain depending on their functions, with lexical tone to the left hemisphere as a function of linguistic features.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1018,A Contrastive Analysis of English and Chinese Intonation Systems: An Auto-Segmental Metrical Framework,"Intonation refers to the use of supra-segmental features to convey pragmatic meanings at the sentence level in a linguistically structured way. The difference in intonation between the native language and a foreign language may influence second language learners' acquisition of intonation. The purpose of this study is to explore the similarities and differences at the level of phonological representation between English and Chinese intonation systems. This study investigated English and Chinese intonation systems, respectively, from both form and meaning under the Auto-Segmental Metrical framework by referring to previous studies and illustrating examples. The results showed that in terms of form, there were notable differences in the structural elements and their inventories between the intonation systems of English and Chinese. In terms of meaning, assertions were represented by different structural elements in English and Chinese intonation systems; the types of structural elements in English intonation possessed the capability to convey complex and subtle meanings, contrasting with the comparatively simpler nature of Chinese intonation.The results reveal that Chinese EFL learners demonstrate considerable difficulties in the production of the structural elements of English intonation and their combinations due to L1 intonation interference.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1019,The multimodal marking of aspect: The case of five periphrastic auxiliary constructions in North American English,"Cognitive linguistics (CL) has, in recent years, seen an increase in appeals to include multiple modalities in language analyses. While individual studies have incorporated gesture, gaze, facial expression, and prosody, among other modalities, CL has yet to completely embrace the systematic analysis of face-to-face interaction. Here, I present an investigation of five aspect-marking periphrastic constructions in North American English. Using naturalistic inter-actional data (n = 250) from the Red Hen archive, this study establishes a multimodal profile for auxiliary constructions headed by one of five highly aspectualized verbs: CONTINUE, KEEP, START, STOP, and QUIT, as in The jackpot continued to grow and He quit smoking. Results show that gesture timing, the structure of the gesture stroke, and gesture movement type, are variables that iconically and differentially represent distinctive aspectual conceptualizations. This study enhances our understanding of aspectual representation in co-speech gesture and informs the ongoing debate within CL and construction grammar circles of what constitutes conventionalization, or what constitutes a construction (mono- or multimodal).",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1020,Silent imperatives: A multimodal approach to warning expressions,"This article argues in favor of a formal model that views language as an integrated multimodal system where syntax, prosody, and gestures are linked. It demonstrates that specific syntactic structures trigger sensorimotor realizations, which include intonation, phonology, and gestures. We argue that gestures, in particular non-lexical co-speech gestures, are not merely an additional channel but a fundamental part of grammar, integrated into the same system that governs word order and phonological realization. We suggest a syntactic representation of warning expressions that combines syntactic and pragmatic aspects, based on Italian data and experimental evidence. Specifically, we argue that warnings always include a warning call, which can be either lexical in content or expressed solely through special prosody and gestures.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1021,Lyric Poetry and the Disorientation of Empathy,"In the present essay, I argue that empathy constitutes the mode in which lyric poetry registers in the readers. However, unlike in prose, where the reader is allowed to empathize with the characters via the mediation of the narrator, in poetry, as Jonathan Culler and a number of other theoreticians of the lyric have indicated, the reader assumes the position of the speaker, thus becoming a reperformer of the text. This positioning, in turn, creates a situation in which the text, rather than representing a mental state, embodies it and in the process of being enacted impels the reader to internalize this state. I then move on to complement this distinction between poetry and prose by noting the fact that critics who explore how empathy is employed in reading fiction appear to depart from assumptions of comprehensibility and stability of the representations of characters' mental states. This is shown in the analysis of the work of such critics as Suzanne Keen and Liza Zunshine. By contrast, in lyric poetry, empathy is both necessitated and simultaneously disoriented through the discontinuous, open-ended nature of the poetic text. As a result, the reader is perpetually made to feel into the speaker's evocations of mental states but his or her empathic efforts are thwarted by the operations of the text in which a given affect is being evoked and disarticulated at the same time. This dialectic of empathy and disorientation is a dynamic process that can take various forms. In the last section of the present essay, I analyze three poems, ""Punishment"" by Seamus Heaney, ""The Loaf"" by Paul Muldoon and ""Geis"" by Caitriona O'Reilly, in order to show how the empathic impulse is both triggered and disoriented by the tensions between the poems' denotative meanings and their formal features, mainly prosody and rhyme scheme. Thus, a tentative conclusion is that lyric poetry's formal complexity and its non-mimetic nature enter into a dynamic relationship with the propositional content - a dynamic which contributes to the continual disorientation of our empathic capacity that is the essential form of our performance of the poetic text. This tension may manifest itself in how form and content challenge each other or how they cooperate, which in either case leaves us with a rather uncomfortable feeling of having witnessed not a representation of but an embodied, real-time moment of intimate and essentially aporetic experiential performance.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1022,Two-dimensional semantic analysis of Japanese mimetics,"This paper argues that two-dimensional semantic representation is necessary to account for the semantics of Japanese mimetics (giongo/gitaigo), following the insight of Diffloth (1972). One dimension is called the analytic dimension, the dimension of ''ordinary semantics,'' where meaning is represented as a hierarchical structure of decontextualized semantic primitives. The other is called the affecto-imagistic dimension, where meaning is represented in terms of affect and various kinds of imagery (auditory, visual, tactile, motoric, etc). It subsumes what is traditionally called the expressive function of language due to its affective character, but it has far greater referential capability. I will argue that the semantics of mimetics crucially involves the affecto-imagistic dimension. The evidence includes seeming redundancy of a mimetic in a clause, impossibility of logical negation, high association with expressive intonation and spontaneous iconic gestures, and iconism in the morphology of mimetics. Positing the two dimensions leads to an alternative to Jackendoff's (1983) conceptual structure hypothesis, which states that the analytic dimension is the only level of representation where language and other kinds of cognitive information are compatible.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1023,Tone and intonation in the dialect of Hasselt,"The West-Limburgian dialect of Hasselt has a word accent distinction comparable to the distinction between accent 1 and accent 2 in Swedish and Norwegian. To understand how, Hasselt speakers interpret the accentual contrast, a reading task was carried out in which words differing by accent class only were embedded in different prosodic contexts. The results suggest that Hasselt speakers, like speakers of the more eastern dialects of Venlo, Roermond, and Cologne, mark accent 2 by lexical tone, while accent 1 remains lexically toneless. Hasselt speakers differ, however, both in the realization and the distribution of the accentual contrast. These differences are attributed to variation in tonal association. While the eastern speakers associate tones to sonorant moras, Hasselt speakers associate tones to syllables.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1024,In defense of prosodic typology: A response to Beckman & Venditti,"In two recent handbook articles, Beckman & Venditti (2010, 2011) present overviews of tone and intonation which take issue with both traditional typology and recent attempts to bring clarity to the study of prosodic typology. In the course of their coverage Beckman & Venditti question the ""usefulness"" of distinguishing prosodic systems by ""tonemic function alone"" (e.g., lexical tone, stress, intonation) and raise the question ""Is typology needed?"" Within this context I once again argue for a ""property-driven"" approach to prosodic typology whose goal is not to classify languages into prosodic types, rather to accurately characterize the same vs. different ways in which prosodic properties are exploited. We thus ask (i) whether a given language has word-level contrastive pitch (""tone""), word-level metrical structure (""stress""), both, or neither; (ii) if yes, what does the prosodic system do with the tones and/or stress, both at the word level and postlexically? Given the level-ordered nature of phonological systems, only after the first two questions are dealt with can we move on to the the question with which Beckman & Venditti are most concerned: (iii) how are the surface or output word-prosodic properties integrated with phrase-and utterance-level intonation? While Beckman & Venditti question the usefulness of ""broad-stroke typologies"" which have traditionally distinguished tone, stress, and intonation, their disposition to minimize systemic differences in favor of surface comparisons of phonetic realizations raises important questions concerning levels of representation and the nature of phonological typology itself.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1025,Lexical function of pitch in the first language shapes cross-linguistic perception of Thai tones,"Determining the factors involved in the non-native perception of the pitch patterns of tones is complicated by the fact that all languages use pitch to various extents, whether linguistic (e.g., intonation) or non-linguistic (e.g., singing). Moreover, many languages use pitch to distinguish lexical items with varying degrees of functional load and differences in inventory of such pitch patterns. The current study attempts to understand what factors determine accurate naive (= non-learner) perception of non-native tones, in order to establish the baseline for acquisition of a tonal L2. We examine the perception of Thai tones (i.e., three level tones, two contour tones) by speakers of languages on a spectrum of lexically contrastive pitch usage: Mandarin (lexical tone), Japanese (lexical pitch accent), English (lexical stress), and Korean (no lexically contrastive pitch). Results suggest that the importance of lexically contrastive pitch in the L1 influences non-native tone perception so that not all non-tonal language speakers possess the same level of tonal sensitivity, resulting in a hierarchy of perceptual accuracy. Referencing the Feature Hypothesis (McAllister et al. 2002), we propose the Functional Pitch Hypothesis to model our findings: the degree to which linguistic pitch differentiates lexical items in the L1 shapes the naive perception of a non-native lexically contrastive pitch system, e.g., tones.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1026,Features of low functional load in mono- and bilinguals' lexical access: evidence from Swedish tonal accent,"Swedish makes use of tonal accents (Accents 1 and 2) to contrast words, but the functional load is very low, with some regional dialects not even exhibiting the contrast. In particular given the low number of minimal pairs, the question is whether tonal word accent is used in lexical access. Here we present two cross-modal fragment semantic priming studies in order to address this question. Both experiments use first syllable fragments in order to prime semantically related targets. Experiment 1 utilises words whose first syllable occurs with both accent patterns, creating a situation in which there is lexical competition between words that differ solely in terms of accent. Experiment 2 removes this competition by using words that have no such accent competitors. Our results show that native speakers of Swedish use tonal word accent in lexical access: Accent mispronunciations failed to prime semantically related targets, regardless of whether primes had accent competitors or not. Results for a group of early bilingual speakers (who grew up with one Swedish-speaking parent and one other non-tonal language) showed no differences in processing compared to the monolinguals. This indicates that the extraction of accent features during acquisition and their use in lexical access is robust, even in a scenario where multiple input languages lead to tonal word accent being a useful feature for only some of the lexical items that are being acquired. There is no doubt that the accent system is well entrenched into the bilinguals' phonological system.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1027,Investigating the role of musical experience in lexical tone perception: non-musicians and amateur musicians' perception of Mandarin tones,"Previous studies have found that musicians typically discriminate Mandarin tones better than non-musicians. However, the relationship between musical experience and tone perception is unclear. In the current study, 39 monolingual native English speakers with no previous experience of tone languages and a range of musical backgrounds (non-musicians and amateur musicians) completed 6 tasks, including lexical tone identification, working memory, test of first language (L1) and second language (L2) segmental perception and the Goldsmiths Musical Sophistication Index which measures musical ability and experience. Results indicated that tone identification was significantly correlated with music training, musical ability, and pitch discrimination. However, a path analysis showed that pitch discrimination and musical ability, but not music training, directly influenced tone identification. Music training had a positive direct influence on pitch discrimination and musical ability, and indirectly influences tone identification via these mediators. Follow-up multivariate multiple regression showed that different tones are affected differently: pitch discrimination ability mainly influenced identification of Tones 3 and 4, while musical ability significantly influenced Tones 1 and 4. Overall, na & iuml;ve, non-tone language speakers do not need music training or a musical background to be able to identify Mandarin tones with a high degree of accuracy.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1028,Initial peaks and final falls in the intonation of Manchego Spanish wh-questions,"This paper investigates the phonetics and phonology of initial peaks and final falls in wh-questions produced by speakers of the variety of Spanish spoken in the Castile-La Mancha (Manchego) region of Spain. The acoustic analysis is based on speech data for nine speakers, and the goal is to identify how utterance-initial and utterance-final F-0 gestures relate to broader issues in intonational phonology and the prosodic signaling of wh-questions. The findings for left periphery constituents provide evidence for a H tone at the utterance boundary for all speakers, although the exact autosegmental representation cannot be provided due to variability in peak alignment patterns. The findings for right periphery constituents indicate two distinct speaker groups based on nuclear syllable and posttonic gestures. Specifically, the continuum of final falls is motivated by contrasting bitonal nuclear pitch accent configurations: H + L-star vs. jL + H-star. The boundary L% specification is argued for all speakers in spite of seemingly divergent posttonic gestures. The experimental findings speak to cross-linguistic issues such as prominence marking in wh-question intonation, the syntax-prosody interface in wh-questions, and the internal structure of pitch accent configurations.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1029,Children's processing of morphosyntactic and prosodic cues in overriding context-based hypotheses: an eye tracking study,"This research explores children's ability to integrate contextual and linguistic cues. Prior work has shown that children are not able to weigh contextual information in an adult-like way and that between the age of 4 and 6 they show difficulties in revising a hypothesis they have made based on early-arriving linguistic information in sentence processing. Therefore we considered children's ability to confirm or override a context-based hypothesis based on linguistic information. Our objective in this study was to test (1) children's (ages 4-6) ability to form a hypothesis based on contextual information, (2) their ability to override such a hypothesis based on linguistic information and (3) how children are able to use different types of linguistic cues (morphosyntactic versus prosodic) to confirm or override the initial hypothesis. Results from both offline (pointing) and online (eye tracking) tasks suggest that children in this age group indeed form hypotheses based on contextual information. Age effects were found regarding children's ability to override these hypotheses. Overall, 4-year-olds were not shown to be able to override their hypotheses using linguistic information of interest. For 5- and 6-year-olds, it depended on the types of linguistic cues that were available to them. Children were better at using morphosyntactic cues to override an initial hypothesis than they were at using prosodic cues to do so. Our results suggest that children slowly develop the ability to override hypotheses based on early-arriving information, even when that information is extralinguistic and contextual. Children must learn to weight different types of cues in an adult-like way. This developmental period of learning to prioritize different cues in an adult-like way is consistent with a constraint-based model of learning.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1030,The marketization of discourse about education in UK general election manifestos,"After 1945, a broad 'post-war consensus' developed in the West. There was general agreement that the state had an important role to play in such areas as macroeconomic management, environmental protection, and social provision for health, education, and welfare. But since the late 1970s, as part of the neo-liberal project to extend the market into every aspect of social life, there has been a backlash against 'inefficient, 'bureaucratic', 'unwieldy', and 'inflexible' state provision. In this article, I examine the discursive dimension of one facet of the 'new capitalism': the marketization of education in the UK. Using frameworks derived from critical discourse analysis, I analyze texts from three election manifestos: the Labour and Conservative manifestos from the 1987 election (a turning point in UK education policy), and the Labour 1997 manifesto. I show how aspects of textual organization, such as patterns of transitivity, the representation of social actors, semantic prosody, and coherence, have a central role to play in the construction of 'comprehensive' and 'market' conceptualizations of the domain.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1031,Information structure and the status of NP in Russian,"This paper investigates the role of the information structure in the interpretation of nominal elements in Russian. More specifically, it concentrates on the correlations between the position an NP occupies within a sentence and its reading with respect to (in)definiteness. To this end, different ways of indicating (in)definiteness in this language are discussed. Russian, as a language lacking articles, has an option of denoting this feature overtly (lexically or morphosyntactically); alternatively, an NP may remain unspecified. It is proposed that, in the latter case, the interpretation depends on the role the NP has in the discourse (or information) structure. The paper further examines the information packaging of Russian clauses. It is concluded that the appropriate representation of a sentence in this language consists of three parts: optional Topic (s) and Neutral Information and obligatory New Information Focus in a neutral intonation sentence of Contrastive Focus (1992) and King (1995), is contrasted the traditional two-way division into Topic and Focus or Theme and Rheme. Finally, the paper looks at the possible types of NPs representing various functions of information structure. It is shown that neither definiteness nor the related feature of specificity may provide a one-to-one correspondence. It is then demonstrated that the feature underlying the dependence between the interpretation of NP and its role in the information structure is D(iscourse)-linking (Pesetsky 1787).",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1032,"Focus and contrastive topic in questions and answers, with particular reference to Turkish","Much recent research has recognized the importance of focus and contrastive topic in assertions for discourse coherence. However, with few exceptions, it has been neglected that focus and contrastive topic also occur in questions, and have a similar role in establishing coherence. We propose a framework of dynamic interpretation based on the notion of Commitment Spaces that show that a uniform interpretation of focus and contrastive topic is possible. The algebraic representation format is rich enough so that a separate introduction of discourse trees is not necessary. The paper discusses these phenomena for Turkish, a language with an explicit focus marker for polar and alternative questions, which distinguishes focus from contrastive topic.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1033,Intonation in Northern Vietnamese,"Languages employ a range of different means for realizing communicative functions, including word choice, morpho-syntax and prosody. The relative weight of each of these means varies cross-linguistically. In this paper, we look at the role of intonation in the realization of communicative functions in Northern Vietnamese, a language with a complex lexical tone system. Our results, which are based on systematically controlled data, show that there are a number of acoustic strategies for realizing communicative functions, predominantly based on global f0 and intensity and local sentence-final f0. These strategies are subject to a great degree of speaker variation, although this variation appears to be consistent with intonational universals as reflected in biological codes.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1034,Contrasting the high rise and the low rise intonations in a dialect with the Central Franconian tone,"The dialect of Helden (Netherlands) maintains intonational contrasts between the fall, the fall-rise, the low rise and the high rise, while also having a binary lexical tone contrast in (accented or unaccented) final syllables in the IP as well as in accented IP-internal syllables. The lexical tone contrast, which is an instance of the Central Franconian tone contrast known as Accent 1 vs Accent 2, appears in the stressed syllable of words, and so competes with the contour differences that are due to the intonational contrasts. In general, the Helden lexical contrasts fall within the phonetic variation that is available for the Standard Dutch intonation contours. The dialect maintains these multiple contrasts (a) by different timings of the falling movement in the fall and the fall-rise, which is later for Accent 2; (b) by different timings of the rising movement in the low rise, which is later for Accent 2; (c) by a lowered realization of the first H-tone after the lexical tone in the post-focal contour of the low rise and the high rise, and (d) by making IP-final rimes with Accent 2 longer than those with Accent 1.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1035,Prosodic focus with and without post-focus compression: A typological divide within the same language family?,"There is increasing evidence that many languages encode prosodic focus not only with phonetic variations in the focused component itself, but also with a reduction of pitch range and intensity of the post-focus components, a strategy known as post-focus compression (PFC). However, evidence is also emerging that in many other languages prosodic encoding of focus is markedly different, suggesting that PFC might be related to factors such as the presence of lexical tone, stress or the availability of morphosyntactic means of signaling focus. The current study investigated the production and perception of focus in Taiwanese, Taiwan Mandarin and Beijing Mandarin, three languages/dialects that are all tonal and that have similar morphosyntactic means for indicating focus. Results showed clear evidence of PFC in Beijing Mandarin but lack of it in Taiwanese and Taiwan Mandarin, suggesting that PFC is independent of the factors mentioned above. Most interestingly, Taiwan Mandarin seems to have lost PFC due to close contact with Taiwanese despite its effectiveness as demonstrated by the perceptual experiment. The new findings, taken together with other recent finding about prosodic focus, seem to suggest that PFC is a ""hard-to-evolve"" prosodic feature that may have a single historical origin. Thus there is a need for large scale experimental research to explore the cross-linguistic distribution of PFC, so as to broaden our understanding of not only prosodic typology, but also language contact, bilingualism and language evolution in general.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1036,A new approach to prosodic grouping,"This paper reports on two experiments concerning the prosodic realization and perception of various sentences with three or four coordinated names in German. The expression of prosodic boundaries, as evidenced by pitch and duration, is shown to signal the depth of syntactic embedding of the conjuncts and also the branching direction of the co-ordination structure. The results of the production experiment inspire a model of syntax-prosody mapping, which assumes that the strength of a prosodic boundary after a given constituent is a function of a) the syntactic relation to the following constituent and b) the depth of its syntactic embedding. Comparison reveals that the proposed model provides better predictions than other current approaches to prosodic boundary strength. The perception experiment indicates that listeners recognize recursively embedded coordination structures on the basis of the prosodic form of the sentence. We argue for a recursive representation of prosodic constituent structure at the level of the phonological phrase and above.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1037,Rhythmic parsing,"A controlled reading experiment reveals that stress-based linguistic rhythm impinges on syntactic ambiguity resolution in silent and oral reading. The results suggest that, at points of syntactic underspecification, the accruing prosodic representation may affect even the earliest stages of structure building, viz. the analysis of syntactic features of an ambiguous word. Such an effect remains inexplicable in the context of (psycho-) linguistic theories that assume a strictly unidirectional relationship between syntactic and phonological processes, the latter merely interpreting the conditions the syntactic component imposes on it. Here, a performance compatible grammar in the framework of Optimal Parsing is presented that is capable of capturing the reading data. The model integrates syntactic parsing and prosodification in reading and predicts that, at points of syntactic indetermination, weak prosodic constraints alone may guide syntactic structure assignment. This suggests a bidirectional relationship between syntax and phonology in grammar and processing while, at the same time, confirming a tight coupling of language production and comprehension.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1038,Suppressing Sensorimotor Activity Modulates the Discrimination of Auditory Emotions But Not Speaker Identity,"Our ability to recognize the emotions of others is a crucial feature of human social cognition. Functional neuroimaging studies indicate that activity in sensorimotor cortices is evoked during the perception of emotion. In the visual domain, right somatosensory cortex activity has been shown to be critical for facial emotion recognition. However, the importance of sensorimotor representations in modalities outside of vision remains unknown. Here we use continuous theta-burst transcranial magnetic stimulation (cTBS) to investigate whether neural activity in the right postcentral gyrus (rPoG) and right lateral premotor cortex (rPM) is involved in nonverbal auditory emotion recognition. Three groups of participants completed same-different tasks on auditory stimuli, discriminating between the emotion expressed and the speakers' identities, before and following cTBS targeted at rPoG, rPM, or the vertex (control site). A task-selective deficit in auditory emotion discrimination was observed. Stimulation to rPoG and rPM resulted in a disruption of participants' abilities to discriminate emotion, but not identity, from vocal signals. These findings suggest that sensorimotor activity may be a modality-independent mechanism which aids emotion discrimination.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1039,Supramodal Representation of Emotions,"Supramodal representation of emotion and its neural substrates have recently attracted attention as a marker of social cognition. However, the question whether perceptual integration of facial and vocal emotions takes place in primary sensory areas, multimodal cortices, or in affective structures remains unanswered yet. Using novel computer-generated stimuli, we combined emotional faces and voices in congruent and incongruent ways and assessed functional brain data (fMRI) during an emotional classification task. Both congruent and incongruent audiovisual stimuli evoked larger responses in thalamus and superior temporal regions compared with unimodal conditions. Congruent emotions were characterized by activation in amygdala, insula, ventral posterior cingulate (vPCC), temporo-occipital, and auditory cortices; incongruent emotions activated a frontoparietal network and bilateral caudate nucleus, indicating a greater processing load in working memory and emotion-encoding areas. The vPCC alone exhibited differential reactions to congruency and incongruency for all emotion categories and can thus be considered a central structure for supramodal representation of complex emotional information. Moreover, the left amygdala reflected supramodal representation of happy stimuli. These findings document that emotional information does not merge at the perceptual audiovisual integration level in unimodal or multimodal areas, but in vPCC and amygdala.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1040,Adaptation to Vocal Expressions Reveals Multistep Perception of Auditory Emotion,"The human voice carries speech as well as important nonlinguistic signals that influence our social interactions. Among these cues that impact our behavior and communication with other people is the perceived emotional state of the speaker. A theoretical framework for the neural processing stages of emotional prosody has suggested that auditory emotion is perceived in multiple steps (Schirmer and Kotz, 2006) involving low-level auditory analysis and integration of the acoustic information followed by higher-level cognition. Empirical evidence for this multistep processing chain, however, is still sparse. We examined this question using functional magnetic resonance imaging and a continuous carry-over design (Aguirre, 2007) to measure brain activity while volunteers listened to non-speech-affective vocalizations morphed on a continuum between anger and fear. Analyses dissociated neuronal adaptation effects induced by similarity in perceived emotional content between consecutive stimuli from those induced by their acoustic similarity. We found that bilateral voice-sensitive auditory regions as well as right amygdala coded the physical difference between consecutive stimuli. In contrast, activity in bilateral anterior insulae, medial superior frontal cortex, precuneus, and subcortical regions such as bilateral hippocampi depended predominantly on the perceptual difference between morphs. Our results suggest that the processing of vocal affect recognition is a multistep process involving largely distinct neural networks. Amygdala and auditory areas predominantly code emotion-related acoustic information while more anterior insular and prefrontal regions respond to the abstract, cognitive representation of vocal affect.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1041,Affirmation as a type of modality: evidence from Serbian and English,"The aim of the paper is to present modes of grammatical representation of affirmation in Serbian and English. I will explore the hypothesis that affirmation is a type of modality and its expression involves syntactic and morphological markers. The formal expression of strong types of affirmation modality is characterized by the preference for long, full and stressed variants of modal auxiliaries Alternatively, affirmative verb forms may be intensified by additional suprasegmental elements, such as intonation, or even paralinguistic tools, gestures and non-verbal ways of reinforcing a statement. This seems to confirm the general function-form isomorphism requiring the more complex mental events to be represented by more complex structures. I will argue that this requirement may initiate the process of auxiliation, perceived as a search to find a more adequate means for expressing affirmation modality.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1042,"Absolute pitch, speech, and tone language: Some experiments and a proposed framework","Absolute pitch is generally considered to reflect a rare musical endowment; however, its characteristics are puzzling and its genesis is unclear. We describe two experiments in which native speakers of tone languages-Mandarin and Vietnamese-were found to display a remarkably precise and stable form of absolute pitch in enunciating words. We further describe a third experiment in which speakers of English displayed less stability on an analogous task. Based on these findings, and considering the related literatures on critical periods in speech development, and the neurological underpinnings of lexical tone, we propose a framework for the genesis of absolute pitch. The framework assumes that absolute pitch originally evolved as a feature of speech, analogous to other features such as vowel quality, and that speakers of tone language naturally acquire this feature during the critical period for speech acquisition. We further propose that the acquisition of absolute pitch by rare individuals who speak an intonation language may be associated with a critical period of unusually long duration, so that it encompasses the age at which the child can take music lessons. We conclude that the potential to acquire absolute pitch is universally present at birth, and that it can be realized by enabling the infant to associate pitches with verbal labels during the critical period for speech acquisition.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1043,Preserved singing in aphasia: A case study of the efficacy of melodic intonation therapy,"THIS STUDY EXAMINED. THE EFFICACY of Melodic Intonation Therapy (MIT) in a male singer (KL) with severe Broca's aphasia. Thirty novel phrases were allocated to one of three experimental conditions: unrehearsed, rehearsed verbal production (repetition), and rehearsed verbal production with melody (MIT). The results showed superior production of MIT phrases during therapy. Comparison of performance at baseline, 1 week, and 5 weeks after therapy revealed an initial beneficial effect of both types of rehearsal; however, MIT was more durable, facilitating longer-term phrase production. Our findings suggest that MIT facilitated KL's speech praxis, and that combining melody and speech through rehearsal promoted separate storage and/or access to the phrase representation.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1044,Speech intonation perception deficits in musical tone deafness (congenital amusia),"TO WHAT EXTENT DO MUSIC and language share neural mechanisms for processing pitch patterns? Musical tone-deafness (amusia) provides important evidence on this question. Amusics have problems with musical melody perception, yet early work suggested that they had no problems with the perception of speech intonation (Ayotte, Peretz, & Hyde, 2002). However, here we show that about 30% of amusics from independent studies (British and French-Canadian) have difficulty discriminating a statement from a question on the basis of a final pitch fall or rise. This suggests that pitch direction perception deficits in amusia (known from previous psychophysical work) can extend to speech. For British amusics, the direction deficit is related to the rate of change of the final pitch glide in statements/questions, with increased discrimination difficulty when rates are relatively slow. These findings suggest that amusia provides a useful window on the neural relations between melodic processing in language and music.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1045,TONAL PRIMING BEYOND TONICS,"THE MUSICAL PRIMING PARADIGM ALLOWS FOR INVESTIGATION of listeners' expectations based on their implicit knowledge of tonal stability. To date, priming data are limited to reports of facilitated processing for tonic over nontonic events. The special status of the tonic as a cognitive reference point brings into question the subtlety of listeners' tonal knowledge: Is the facilitated processing observed in priming studies limited to tonic events, or is tone processing influenced by subtler tonal contrasts? The present study investigated tonal priming for mediants (the third scale degree) over leading tones (the seventh scale degree) presented in melodic contexts. Experiment 1 used a timbre discrimination task and Experiment 2 an intonation task. Facilitated processing was observed for the more tonally stable mediants over the less stable leading tones, thus showing that priming effects are not limited to pairs of tonal degrees including the tonic. This finding emphasizes the subtlety of nonexpert listeners' tonal knowledge.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1046,IMPAIRED EXPLICIT PROCESSING OF MUSICAL SYNTAX AND TONALITY IN A GROUP OF MANDARIN-SPEAKING CONGENITAL AMUSICS,"pitch discrimination impairments were associated with syntax and tonality processing. In Experiment 1, we assessed whether congenital amusia is associated with impaired explicit processing of musical syntax. Congruity ratings were examined for syntactically regular or irregular endings in harmonic and melodic contexts. Unlike controls, amusic participants failed to explicitly distinguish regular from irregular endings in both contexts. Surprisingly, however, a concurrent manipulation of pitch distance did not affect the processing of musical syntax for amusics, and their impaired music-syntactic processing was uncorrelated with their pitch discrimination thresholds. In Experiment 2, we assessed tonality perception using a probe-tone paradigm. Recovery of the tonal hierarchy was less evident for the amusic group than for the control group, and this reduced sensitivity to tonality in amusia was also unrelated to poor pitch discrimination. These findings support the view that music structure is processed by cognitive and neural resources that operate independently of pitch discrimination, and that these resources are impaired in explicit judgments for individuals with congenital amusia.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1047,PERCEPTUAL LEARNING OF PITCH DIRECTION IN CONGENITAL AMUSIA: EVIDENCE FROM CHINESE SPEAKERS,"CONGENITAL AMUSIA IS A LIFELONG DISORDER OF musical processing for which no effective treatments have been found. The present study aimed to treat amusics' impairments in pitch direction identification through auditory training. Prior to training, twenty Chinese-speaking amusics and 20 matched controls were tested on the Montreal Battery of Evaluation of Amusia (MBEA) and two psychophysical pitch threshold tasks for identification of pitch direction in speech and music. Subsequently, ten of the twenty amusics undertook 10 sessions of adaptive-tracking pitch direction training, while the remaining 10 received no training. Post training, all amusics were retested on the pitch threshold tasks and on the three pitch-based MBEA subtests. Trained amusics demonstrated significantly improved thresholds for pitch direction identification in both speech and music, to the level of non-amusic control participants, although no significant difference was observed between trained and untrained amusics in the MBEA subtests. This provides the first clear positive evidence for improvement in pitch direction processing through auditory training in amusia. Further training studies are required to target different deficit areas in congenital amusia, so as to reveal which aspects of improvement will be most beneficial to the normal functioning of musical processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1048,THE SELECTIVITY OF MUSICAL ADVANTAGE: MUSICIANS EXHIBIT PERCEPTUAL ADVANTAGE FOR SOME BUT NOT ALL CANTONESE TONES,"THE OPERAHYPOTHESIS THEORIZES HOW MUSICAL experience heightens perceptual acuity to lexical tones. One missing element in the hypothesis is whether musical advantage is general to all or specific to some lexical tones. To further extend the hypothesis, this study investigated whether English musicians consistently outperformed English nonmusicians in perceiving a variety of Cantonese tones. In an AXB discrimination task, the musicians exhibited superior discriminatory performance over the nonmusicians only in the high level, high rising, and mid-level tone contexts. Similarly, in a Cantonese tone sequence recall task, the musicians significantly outperformed the nonmusicians only in the contour tone context but not in the level tone context. Collectively, the results reflect the selectivity of musical advantage-musical experience is only advantageous to the perception of some but not all Cantonese tones, and elements of selectivity can be introduced to the OPERA hypothesis. Methodologically, the findings highlight the need to include a wide variety of lexical tone contrasts when studying music-to-language transfer.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1049,CAN THE INTENDED MESSAGES OF MISMATCHED LEXICAL TONE IN IGBO MUSIC BE UNDERSTOOD? A TEST FOR LISTENERS' PERCEPTION OF THE MATCHED VERSUS MISMATCHED COMPOSITIONS,"IN TONE LANGUAGES, ALTERATION OF LEXICAL TONE changes the intended meaning. This implies that composers should equally match lexical tone in their music for intelligible communication of the intended textual messages, a compositional approach termed Lexical Tone Determinants (LTD) in this study. Yet, in the Igbo language setting, some composers creatively disregard/mismatch lexical tone, which is branded as Musical/Creative Determinants (M/CD). It is believed that mismatched lexical tone in Igbo music alters listeners' comprehension of the intended messages; on the other hand, it is argued that thorough match of lexical tone constrains musical creativity. Listeners' perception of textual messages in LTD and M/CD music has not been empirically tested (side-by-side) to verify whether comprehension is lost or not, at least, in the Igbo language context. This empirical void gap is verified in this particular study to substantiate the propositions/findings using comparative measures to collect data through listeners' perception in live-performance of newly composed LTD and M/CD pieces. Specifically, it examines whether mismatched lexical tone in Igbo music alters message comprehension or not. The data were collated, presented, and analyzed statistically with chisquare deployed to evaluate their difference in message comprehension.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1050,"MUSICAL ADVANTAGE IN LEXICAL TONE PERCEPTION HINGES ON MUSICAL INSTRUMENT: A COMPARISON BETWEEN PITCHED MUSICIANS, UNPITCHED MUSICIANS, AND NONMUSICIANS","DIFFERENT MUSICAL INSTRUMENTS HAVE DIFFERENT pitch processing demands. However, correlational studies have seldom considered the role of musical instruments in music-to-language transfer. Addressing this research gap could contribute to a nuanced understanding of music-to-language transfer. To this end, we investigated whether pitched musicians had a unique musical advantage in lexical tone perception relative to unpitched musicians and nonmusicians. Specifically, we compared Cantonese pitched musicians, unpitched musicians, and nonmusicians on Thai tone discrimination and sequence recall. In the Thai tone discrimination task, the pitched musicians outperformed the unpitched musicians and the nonmusicians. Moreover, the unpitched musicians and the nonmusicians performed similarly. In the Thai tone sequence recall task, both pitched and unpitched musicians recalled level tone sequences more accurately than the nonmusicians, but the pitched musicians showed the largest musical advantage. However, the three groups recalled contour tone sequences with similar accuracy. Collectively, the pitched musicians had a unique musical advantage in lexical tone discrimination and the largest musical advantage in level tone sequence recall. From a theoretical perspective, this study offers correlational evidence for the Precision element of the OPERA hypothesis. The choice of musical instrumental may matter for musicto-language transfer in lexical tone discrimination and level tone sequence recall.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1051,The Image of a Schoolboy in the B. Okudzhava's War Prose,"The article analyzes the history and poetics of Okudzhava's war prose and its role in modern literature. Particular attention is paid to the image of the schoolboy character, which structures the text. The author reflects on his orientation towards classical literature and the creators of ""lieutenant prose,"" but treats the past with an emphatically apprentice attitude. This trusting intonation is inherited by his autobiographical protagonist, whose image emphasises the features of a private individual, an amateur. The figure of the schoolboy combines contradictory features: courage and naivety, belief in myths and fairy tales, as well as inner weariness and self-irony dictated by the author's position. A special place is held by the story ""Be Well, Schoolboy,"" which is structured according to the soliloquy principle, when the hero's self-discovery takes place, the war becomes an initiation and a test of the book truths ""about feats, about glory,"" which are alienated when confronted with the chaos of reality. The author demonstrates the gap between the perception of the schoolboy by those around him and his self-perception, which opens up an ambiguous and ironic perspective. The motifs of the ""naked man,"" the Other, Mercurianism and the labyrinth, which converge in the image of the schoolboy hero, have become the key concepts in these texts. The traditional paradigm of the ""little man,"" whose home is an overcoat, is apparent behind the character of Okudzhava's prose. A peculiar result of the author's reflections on the war is the rejection of the epic tradition of the classics. Instead, the schoolboy inherits the old spoon of the ""eternal soldier"" Shongin, becoming a part of his destiny, so commonplace in its tragic nature. The most important role in the representation of the battle theme is played by the cultural centrism paradigm, which helps to include the traumatic, horrible experience of war in the memory and post-memory space.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1052,THE INTONATION OF TRANSACTIONAL INTERROGATIVE SENTENCES IN THE SPANISH SPOKEN IN COLOMBIA,"The article presents the description and analysis of the intonation of different transactional interrogative sentences, that is, those aimed at the exchange of information. The data analyzed corresponds to the Spanish spoken in Colombia, specifically in four cities: Bogota, Cali, Medellin and Cartagena. The sp-tobi transcription system was used for the prosodic representation, within the theoretical-methodological framework of the autosegmental metric model (MA). Overall, two prosodic configurations were found to be associated to dialectal groups: L+H* L% in Medellin and H+L* HH% in Bogota and Cali; Cartagena presents both configurations in the nuclear tone.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1053,Piano Works of Transcarpathian Composers of the 20th and Early 21st Centuries: The Phenomenon Research Algorithms,"The purpose of this article is to study the continuity of musical thinking professionalization on ex-amples of piano works by Transcarpathian composers, which are very significant for the formation of the Transcarpathian compositional tradition as a historical phenomenon of mastery development by Transcarpathian composers (Zsigmond Lengyel, Dezso Zador, Istvan Marton, Emil Kobulei, Mykola Popenko, Volodymir Volontyr, Anatoly Zatin, Viktor Telychko) regarding their compliance with the academic norms of musical thinking and with historically composed stylistic invariants. The approach to the research phenomenon is monadological, which means the intention to diagnose a mentally peculiar discourse of the stylistic design, combining the assimilation of historically relevant thought forms and the intonational stock of a multiethnic folklore of the Transcarpathian region. We come to the conclusion that the piano works of Transcarpathian composers reflect a historically determined manoeuvre of ""catching up"" with the stylistic initiatives of the whole twentieth century with its idea of a global cultural synthesis and reinterpretation/neo-restoration of traditions. It has been found that the starting point for the professionalization of music composition in Transcarpathia was the modern modality of style - a position that is usually characterized as a ""post-Romantic reaction"" to all the traditional and total renewal of musical thinking in order to innovate. At the same time, for the style -forming initiatives of Transcarpathian composers the discourse of stylization became most relevant - a special type of musical thinking that created the newest representation of the ""intonation image of the world"" and found its rather original embodiment in the postmodern phase under the guise of ""in-tellectual performance.""",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1054,"CONTRASTIVE FOCUS, GIVENNESS AND THE UNMARKED STATUS OF ""DISCOURSE-NEW""","New evidence is provided for a grammatical principle that singles out contrastive focus (Rooth 1996; Truckenbrodt 1995) and distinguishes it from discourse-new ""informational"" focus. Since the prosody of discourse-given constituents may also be distinguished from discourse-new, a three-way distinction in representation is motivated. It is assumed that an F-feature marks just contrastive focus ( Jackendoff 1972, Rooth 1992), and that a G-feature marks discourse-given constituents (Fery-Samek-Lodovici 2006), while discourse-new is unmarked. A crucial argument for G-marking comes from second occurrence focus (SOF) prosody, which arguably derives from a syntactic representation where SOF is both F-marked and G-marked. This analysis relies on a new G-Marking Condition specifying that a contrastive focus may be G-marked only if the focus semantic value of its scope is discourse-given, i.e., only if the contrast itself is given.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1055,Speaker variability in the realisation of lexical tones,"While previous studies on the speaker-discriminatory power of static f0 parameters abound, few have focused on the dynamic and linguistically structured aspects of f0. Lexical tone offers a case in point for this endeavour. This article reports an exploratory study on the speaker-discriminatory power of individual lexical tones and of the height relationship of level tone pairs in Cantonese, and the effects of voice level and linguistic condition on their realisation. Twenty native Cantonese speakers produced systematically controlled words either in isolation or in a carrier sentence under two voice levels (normal and loud). Results show that f0 height and f0 dynamics are separate dimensions of a tone and are affected by voice level and linguistic condition in different ways. Moreover, discriminant analyses reveal that the contours of individual tones and the height differences of level tone pairs are useful parameters for characterising speakers.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1056,Primacy of mouth over eyes to perceive audiovisual Mandarin lexical tones,"The visual cues of lexical tones are more implicit and much less investigated than consonants and vowels, and it is still unclear what facial areas contribute to facial tones identification. This study investigated Chinese and English speakers' eye movements when they were asked to identify audiovisual Mandarin lexical tones. The Chinese and English speakers were presented with an audiovisual clip of Mandarin monosyllables (for instance, /a/, /a/, /i/, /i/) and were asked to identify whether the syllables were a dipping tone (/a/, / i/) or a falling tone (/ a/, /i/). These audiovisual syllables were presented in clear, noisy and silent (absence of audio signal) conditions. An eye-tracker recorded the participants' eye movements. Results showed that the participants gazed more at the mouth than the eyes. In addition, when acoustic conditions became adverse, both the Chinese and English speakers increased their gaze duration at the mouth rather than at the eyes. The findings suggested that the mouth is the primary area that listeners utilise in their perception of audiovisual lexical tones. The similar eye movements between the Chinese and English speakers imply that the mouth acts as a perceptual cue that provides articulatory information, as opposed to social and pragmatic information.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1057,Variable f0 of toneless moras in Suzhou Chinese: a computational analysis,"Toneless Tone Bearing Units (TBUs) as an analytical device have been widely used in tone and intonation studies, but there is a lack of consensus on how toneless TBUs realize in f0. This study investigates the variable f0 realization of phonologically toneless moras in Suzhou Chinese (Northern Wu) using Naive Bayes classification models. By embedding the toneless moras in different contexts (e.g., between two H tones, preceded by H and followed by L), I have found both intra- and inter-speaker variation across the tonal contexts. Speakers of Suzhou Chinese realized toneless moras in three main ways: insertion of a 'Default L' tone, linear pitch interpolation between the left and right tonal contexts, and spreading of the left tonal context. In addition, I found insufficient evidence for the proposal that toneless TBUs as weak elements in speech realize as a stable target in the mid pitch range (Y. Chen & Xu 2006). The categorical pitch variation of toneless moras is indicative of a phonological model of variable/optional processes, where a single phonological representation (here, the absence of tone) can be mapped to distinctive surface forms (Coetzee & Pater 2011; Coetzee & Kawahara 2013).",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1058,An apparent case of outwardly-sensitive allomorphy in the Armenian definite,"Cross-linguistically, it is rare to find cases of phonologically-conditioned allomorphy where the trigger morpheme lies external or outside the target morpheme. At first sight, the Armenian definite suffix seems to be such a case. The definite suffix uses various surface forms. The choice of surface form is conditioned by the preceding segment, the following clitic, and/or the following word. However, we argue that this outward sensitivity is epiphenomenal and not actual allomorphy. We derive the surface forms by using an abstract underlying representation that uses floating segments or ghost segments. These segments go through rigid cycles of spell-out and phonological strata. Constraint re-rankings of autosegmental docking, phrasal resyllabification, and cluster avoidance explain a range of dialectal variation. In sum, the Armenian definite suffix is one apparent case of outwardly-sensitive allomorphy that is reducible to latent segments.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1059,Multimodal cues to intonational categories: Gesture apex coordination with tonal events,"This study argues for a multimodal view of the identification, representation, and implementation of intonational structure, with evidence from gesture apex-tone coordination in Turkish. Many studies have reported consistent synchronisation of atomic prominence markers across modalities (i.e., pitch accents and gesture apexes). This is prima facie evidence that gesture and prosody are implemented together, and therefore the former can play a role in the identification and perception of the latter through apex-tone synchronisation. However, only few studies considered the full intonational context when investigating synchronisation (e.g., potential alignment of apexes with boundary tones). This is particularly relevant for Turkish as there is disagreement in the literature about whether all words in Turkish bear a pitch accent. In this study, we test the synchronisation of apexes with all intonational events in Turkish natural speech data annotated for gesture and prosody, resulting in 820 gesture apex and 3697 tonal event annotations. The study uses syllable duration (160ms) to determine synchronisation between these anchors via equivalence tests while also integrating gestural and prosodic context as factors that can affect the temporal distance between these units through mixed-effects linear regression. The findings showed that apexes were chiefly synchronised with pitch accents (71%), indicating that prominence was the primary constraint for synchronisation. However, analysis of cases with no prosodic prominence provides the first evidence for a hierarchical constraint on synchronisation, since apexes were preferentially synchronised with the tones marking prosodic words (76%) and not with the markers of prosodic constituents higher in the hierarchy. This finding supports the claim that there may be accentless words in Turkish since the absence of prominence caused a systematic shift in the synchronisation behaviour of apexes. More generally, the study shows how multimodal evidence from gesture can be used in the identification of phonological categories, and that prosodic structure is likely to be expressed through multimodal cues as a composite signal.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1060,Expanding the gestural model of lexical tone: Evidence from two dialects of Serbian,"There is mounting evidence suggesting that temporal information is necessary in representations of lexical tone. Gestural models of tone provide a natural entry point to linking abstract association with physical realization, but remain underdeveloped. We present the results of two acoustic production studies on two dialects of Serbian, a lexical pitch accent language. In the Belgrade dialect, pitch accents are aligned relatively late in the tone-bearing unit, while in the Valjevo dialect, pitch accents are phonetically retracted, sometimes into the preceding syllable. We varied the phonetic duration of syllable onsets of candidate tone-bearing units in falling (experiment 1) and rising (experiment 2) pitch accents, and measured the effects on the timing of FO excursions. Consistent interactions between FO excursions and the segmental content indicate that the phonological system of abstract tone association is the same in both dialects, despite differences in temporal alignment. We argue that this apparent mismatch between association and alignment can be expressed straightforwardly in the Articulatory Phonology framework by allowing tone gestures to coordinate with other gestures in all the ways that segmental gestures can, rather than restricting tone to c-center coordination.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1061,"Lexical competition, sound change, and language dominance in Cantonese tone category recognition","Speech perception can be shaped by factors such as lexical competition, synchronic variation , language dominance. Listeners can use lexical information to categorize sounds , recognize words, but systematic variation may act to neutralize lexical contrasts. Additionally, the detail in the phonological-lexical representations may vary along with linguistic experience. We examine the confluence of these factors in the current study, examining how lexical competition structures phonetic variation of merging vs. non-merging tone categories in Cantonese-English bilinguals varying in relative Cantonese vs. English dominance. Listeners categorized tokens from lexical tone continua generated from minimal pairs (Experiment 1: Word identification) and from word-nonword pairs (Experiment 2: Lexical decision). When there are lexical competitors at both endpoints of the continua, listeners maintained more discrete categorization functions for non-merging tones than merging tones. In the absence of lexical competition, there was no difference in response functions for merging and non-merging tones at the group level. English -dominant early Cantonese-English bilinguals consistently showed less sigmoidal response functions compared to Cantonese-dominant early bilinguals in both experiments suggesting that lexical representations may be stored with less precise tonal detail by English-dominant Cantonese speakers. Overall, these data suggest that the presence of a lexical competitor constrains the range of acceptable phonetic variation for non-merging tones, while listeners are more accepting of phonetic variation in the merging tone categories.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1062,Identifying generalizable knowledge from the distribution of tonotactic accidental gaps in Mandarin,"This study investigates tonotactic accidental gaps (unattested syllable-tone combinations) in Mandarin Chinese. In a corpus study, we found that, independent of syllable type, T2 (rising) and T3 (falling-rising) gaps were over-represented, whereas T1 (high level) and T4 (falling) gaps were under-represented. We also observed fewer T1 gaps with voiceless onsets and more T2 and T3 gaps with voiceless onsets, a pattern that is consistent with cross-linguistic observations. While these trends were generally reflected in a wordlikeness rating experiment by Mandarin listeners, their judgements of these gaps, similar to those of real words, were also guided by neighborhood density. Furthermore, T2 gaps with real-word T3 counterparts were rated as more wordlike, a result attributed to the T3 sandhi in Mandarin Chinese. Finally, we used harmonic scores generated from the UCLA Phonotactic Learner to explicitly test the role of lexical knowledge and markedness constraints in modeling speakers' tonotactic knowledge reflected in the wordlikeness ratings. We found that grammars induced from lexical data were the most successful at predicting wordlikeness ratings of gaps and lexical syllables combined. However, when focused on the ratings of tonotactic gaps, grammars with markedness constraints informed by cross-linguistic observations were more successful even without the constraints being weighted on lexical data. The results show how lexical knowledge and universal markedness, which is not entirely learnable from the lexicon, may account for some tonotactic generalizations.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1063,Perception of ATR in Dagaare,"This paper reports on two related perception studies about the property Advanced Tongue Root (ATR) in Dagaare (Mabia; Ghana). We examine how well native speakers are able to distinguish ATR contrasts as well as the effects of harmony and disharmony on perception, thereby testing hypotheses that have been made in the literature about the perceptual motivations of harmony systems. We find that, as expected, ATR mid vowels and Retracted Tongue Root (RTR) high vowels are the hardest to distinguish in Dagaare, but contrary to expectations, harmony does not improve accuracy in discriminating ATR contrasts. Nonetheless, we find the accuracy on disharmonic disyllabic forms is significantly worse than the accuracy in monosyllabic forms, which may indicate that disharmony hurts perception. We examine the implications for our understanding of the motivations of harmony systems and discuss how this paper contributes to the very minimal existing literature on perception in African languages.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1064,An orthographic bridge between Japanese and Afrikaans - the choice of a roman transliteration system,"In this article the focus is on the linguistic, and more particularly the orthographic, aspect of a translating dictionary between Afrikaans and Japanese to be published shortly. The orthographic aspect specifically relates to the transliteration of the indigenous Japanese script in the roman alphabet and the linguistic considerations which influenced the choice of particular forms. The fact that the two languages are typologically, genealogically and geographically as far apart as is possible on this planet, brings about certain challenges to ensure optimal accessibility to the other language, and in particular to Japanese. These challenges led to decisions regarding the representation of Japanese words written in the roman alphabet this forms the basis of the article. Japanese utilises various systems of writing, but the most general and default form is the logographic writing system, which is based on Chinese. This fact makes it extremely difficult for the average Western learner of Japanese to master simultaneously the orthography, the pronunciation and grammar of the language. Unlike Chinese, Japanese utilises, in addition to the logographic system (kanji), also a phonetic system of 46 basic syllabic symbols (hiragana), mainly to represent grammatical words and affixes, but also as pronunciation guidance for words written in kanji. A parallel phonetic system (also 46 symbols), to wit katakana, which is a mirror image of hiragana, is used for loan words, to express emphasis, for onomatopoeic words, terminology and some names of Japanese companies and products. In addition to the abovementioned writing systems, Japanese is also written in the roman alphabet. Three comparable writing systems exist, namely the so-called Hepburn-system (or Hebon-shiki), Kunrei-shiki, and Nihon-shiki. In Japanese, the systems are collectively known as roomaji [ro:mad3i 1, and all Japanese already learn at primary school level to use one of the systems, in particular Kunrei-shiki. (In translating dictionaries a choice for a specific system is normally done, mostly for Hebon-shiki, or adapted versions, for reasons to be discussed shortly) In short, romanised Japanese is used predominantly to make texts accessible to non-Japanese foreigners, while the logographic-cum-syllabic system is used by Japanese among themselves. As an introduction, an outline is given of the historical development of roomaji from the first Japanese-Portuguese dictionary in 1603 to the present-day situation. One of the functionally most important reasons for the use of roomaji in handbooks and dictionaries for foreigners is that it enables the learner to concentrate from the outset on the grammar and pronunciation of the language a much simpler task than first having to learn to read and write logographically. Unlike in the case of inflecting languages, the grammar ofJapanese is also relatively uncomplicated One would hence be able to regard the use of a roomaji orthography as a bridgingfacility, until such time as the learner ofJapanese has become conversant with the indigenous writing system(s). A further contributing factor is the fact that logographic-syllabic writing does not utilise spaces between words, so that syntactic and morphological categories in Japanese operate invisibly at the level of orthography. In Afrikaans, spacing of words is naturally of importance, because it is one of the ways to distinguish between affixes and independent grammatical words. By way of example: Ek gebruik 'n Powerpoint-aanbieding vir my voordrag. (I use a PowerPoint presentation for my address) In this sentence, all kanji symbols are black, katakana (for loanwords from Western languages) blue, and hiragana (for affixes and particles) red. If the Japanese sentence is romanised, the orthographic parsing becomes visible: [GRAPHICS] Although orthographical differences form the most important category, the spectrum of dfferences can be divided as follows: (a) Typological (Japanese as agglutinating language, as against the semi-inflecting, analytical nature of Afrikaans) (b) Grammatical (a corollary of typological differences) (c) Phonological (opposite complexities as regards the vocalism compared to the consonantism) (d) Orthographical (an alphabetic -phonological system in Afrikaans, as against a combination of an ideo-or logogrammatical and a syllabic-moraic system in the case of Japanese) As regards typological differences, the combination with indigenous agglutinating languages is not unusual for Afrikaans a facilitating factor being that (especially as regards the Nguni languages) a mutual orthography is used, which simplifies access to both languages considerably. In the case of Japanese, however, it is a crucial factor as is demonstrated in the article. Grammatical differences of importance are found in particular at the level of morphology, such as the processes of word formation and derivation. As far as phonological differences are concerned, the most important difference regarding the roman systems of writing is the fact that the length of both Japanese vowels and consonants is reflected in the spelling in terms of the number of morae. In Afrikaans, vowel length and the doubling of consonants (at least as they are reflected in the orthography) have different functions, as will be indicated below Regarding Japanese, the doubling of a vowel mora (or lengthening of a vowel) could be indicated in roomaji by: (a) doubling the vowel letter kankyoo (environment) seesan (production) shoyuu (posession) (b) adding an i or u to the relevant syllabic single vowel, as is done also in the hiragana syllabarium (also in Nihon-shiki and Kunrei-shiki) Or (c) by placing a macron on the relevant vowel letter (as in Hebon-shiki), particularly o and As regards consonants, the roomaji character is always doubled, and the pronunciation of the consonant also lasts two counts, as in the sentence: Vowel length in Afrikaans is only semantically distinctive in the case of one vowel, namely /a/ as against /a:/ (for instance mat [mat] versus maat [ma:t]), and a long /a:/ is also written as one letter in open syllables, for example in mate. The doubling of consonants, on the other hand, unlike Japanese, determines the nature of the vowel in the preceding closed syllable (as in wette/wete [veta], bosse/bose). In view of these differences (as well as some grammatical considerations explicated in the article), some of the choices made at the various levels of description will now be highlighted. Firstly, to express a double vowel mora, the doubling of vowel letters was preferred above the addition of u or i, because the combination of o+u and e+i in both cases represent diphtongs in Afrikaans, and could hence be misleading to learners attempting to interpret the pronunciation. Naturally, when u and i introduce a new syllable, as in ou above and omou, the sequence is written as such, since the second vowel in the sequence representsthe sound which occurs in the diphthong. An example from the text: In some dictionaries attempts are made to reflect the prosody (for instance tone accent) in the roomaji orthography to assist the learner. An investigation of such attempts led to the conclusion that the result would render the text more cumbersome and have restricted utility only. Furthermore, the kana orthography, which does not contain such prosodic indicators, is more transparent, without the encumbrance of diacritic symbols, in reflecting individual speech sounds from a segmental perspective. For this reason it was decided not to use prosodic markers, but to supplement the orthographic representation of all Japanese lemmas, as in the case of Afrikaans, by means of a sound bite accessible via a hyperlink in the text, in which the lemma is pronounced by a mother tongue speaker. As far as the morphology is concerned: Because Japanese verb forms which are derived by means of the verb sum (meaning do or make) from nouns, and sometimes adjectives, are mostly translated as a single word in Afrikaans, the Japanese verb in roomaji is also written as a single compound verb (by means of a hyphen). If adjectival or adverbial derivations are formed in Japanese by means of the addition of the suffixes na or ni, such derivations are also written as hyphenated words, as in: In the case of compounds, the principle is likewise applied that such items, when not linked by means of the genitive particle no, represent single words, mostly written with a hyphen, especially when word length could represent an obstacle: A last morphological consideration is the use of honorific prefixes before nouns, which are an unusual morpheme forAfrikaans learners of Japanese, and would thus require the use of a hyphen, as in the case of o- (prefix to tanjoobi 'birthday): The list of directives is obviously not exhaustive. By scrutinising some selected categories, an element of academic precision (and accountability) could be applied in the process of compiling a dictionary in which a common orthography is used with a view to bridge a gap and facilitate mutual access to Afrikaans and Japanese for speakers of the two languages.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1065,THE PARTICLE DA IN ANSWERS TO QUESTIONS (IN THE CONTEXT OF LEXICOGRAPHICAL PARAMETERIZATION),"Particles are very difficult for foreign students who study Russian, especially modal particles that are still poorly researched both theoretically and methodologically. A dictionary of particles for foreigners could meet this need. The special purpose of the dictionary requires new parameters for particle description. The author has analyzed the representation of particle DA in different dictionaries and concluded that the existing description cannot meet the goals. This article focuses on the description of the modal particle DA as functioning in speech acts, in particular, in answering a question. The author found out that DA is not used in initiating phrases. The meaning of modal particles derives from its pragmatic essence, so the author characterizes them in terms of pragmatic linguistics. Based on J. R. Searle's classification of speech acts (SA) and later researchers of SA (N.A. Trofimova and so on), the author has worked out her own classification of SA, in which the particle DA functions: a) representatives; b) directives; c) commisives; g) interrogatives; d) expressives. The author also has mixed SA, such as expressive directives and expressive interrogatives as a peculiarity for particle DA. Material for the analysis was the dialogical speech from fiction. The author notes that the difference between the natural dialogue and the fiction one is irrelevant. The analysis showed that the particle functions naturally in response phrases because the most common meaning for DA is a reference to the presupposition: ""I say P, because you asked"". DA is always used at the beginning of SA and connects the previous phrase (the question) to the next one (the answer), in other words, the role of DA is always to serve as a connector in a dialogue. In answering questions DA functions mainly in representatives, and is used with particles UZH, VOT, VED', as well as in interrogatives, together with particles RAZVE, ZH. All these particles make speech acts more expressive. The author has differentiated the general meaning of reference into more specific meanings depending on the tone of the phrases in which DA functions, and got seven meanings: 1) perplexity/bewilderment DA; 2) DA of reflections; 3) DA of reference to what was mentioned; 4) DA of reference to the experience of interlocutors and their knowledge of situation; 5) DA of reference to some components of the situation noticed only by the speaker; 6) DA of strengthening of emotions, mostly negative ones; 7) categorical/confident DA. In the first two groups DA is uttered in a little bit stretched manner, while in others it is uttered briefly, it emphasizes the proposition, thus, slow response contrasts with fast response. In all cases DA is unstressed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1066,Cultural and Educational Radio Discourse: On Basic Features and Communicative-Pragmatic Specifics,"The article aims to consider cultural and educational radio discourse as a specific phenomenon, to describe its basic features and communicative and pragmatic specifics. The sources of the empirical material were the cultural and educational programs of the Echo of Moscow radio station arranged in the genre of conversation: Book Casino, Culture Shock, Non-Past Time released in 20112019, and listeners' comments. The archive of the contexts includes 1,290 statements of the hosts and guests of the studio, 297 comments. The material was analyzed in three stages: 1) listening to the on-air recording: the communicative intentions of the program participants were determined, taking into account the intonation features of the interlocutors' speech; 2) describing the linguistic means of implementing communicative tactics, taking into account the contextual use; 3) analyzing listeners' comments on the basis of comprehending the communicative situation of the program the comment was on, the intentions expressed by the author, and the contextual use of linguistic units. The study employed a communicative-pragmatic approach, implemented by a number of methods. The method of discourse analysis was used to determine the structure of radio discourse, to identify cultural and educational radio discourse as its segment, to argue for the inclusion of listeners' comments in the studied discourse field. The method of intent analysis was used to identify communicative tactics. The linguistic representation of strategies and tactics was examined using linguistic analysis, including the contextual analysis of linguistic units. The basic features of cultural and educational radio discourse were described according to the following parameters: communicative goal, subject matter, distribution channel, type of authorship, forms of existence, functional-genre type, way of perception, participants. It has been substantiated that the communicative goal of education in the sphere of culture is specific; it shapes the studied variety of discourse. The hosts determine the organization of the discourse. Guests play the dominant role in providing the content of the program. Listeners participate in communication as equal participants. Based on the basic features, cultural and educational radio discourse is defined as a type of radio discourse, which is distinguished by the communicative goal of education in the field of culture, which determines the topics of the programs, the choice of communication strategies and tactics, the nature of the speech behavior of the participants in communication. The communicative and pragmatic specifics of cultural and educational radio discourse have been analyzed through the study of the strategies of: (1) presentation of the guest of the program, (2) discussion of the cultural event, (3) evaluation of the presenters of the program in listeners' (addressees') comments. It has been revealed that the statements of program participants and listeners are replete with linguistic units with pragmatic potential that develops the cognitive sphere of the addressee.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1067,SPEECH CULTURE IN THE CONTEXT OF SOCIALIST REALISM,"The part of the Soviet speech culture which corresponds to the positions of socialist realism is studied. Value orientations and technological canons of speech culture caused by the method of socialist realism are discussed. The differentiated criteria intended for the development of the models of speech culture corresponding to the positions of socialist realism are formulated. A sample of the Soviet speech art is analyzed. The empirical material of the study is stage speech art of the official branch of Soviet culture. The author explains the urgency of the comprehension of the stage speech art of the Soviet period by the absence of a scientific reflection on this issue and by the timeliness of its reconsideration. The author' previously developed typology of public speech culture is used as the methodological support of this research. This typology proves that stage speech art of the official branch of Soviet culture is of the rhetorical type. The purpose of the article is to formulate the logic-stylistic and intonation-melodic canons of stage speech art of the official branch of Soviet culture. The principles of socialist realism and the value orientations of stage speech art are analyzed. The possibility of applying the value orientations of stage speech art as criteria for the identification of rhetorical type models is argued. The correspondence between the general principles of socialist realism, the value orientations of stage speech art and its technological canons is established in the publication. The author determines the meaning of the term ""canon"" in connection with speech culture and stage speech art, establishes and justifies the technological canons connected with the logic and style of stage speech art, argues the action of four canons: ""the priority of text"", ""illustrative quality"", ""intonation-melodic simplicity"", ""intonation-melodic solemnity"". The results of linguistic studies are used for the substantiation of the logic-stylistic and intonation-melodic canons of stage speech art. The connection of vocabulary and syntax of stage speech is emphasized. The article testifies to the early formation of the ""priority of text"" canon, explains the principle of its implementation in speech culture and in stage speech art. The author pays special attention to the argumentation of the ""illustrative quality"" canon. The technology of the illustrative approach use in the stage representation of the author's text is described in the article. The author proves the connection betwen using simple speech tasks in the work with the text and the illustrative style in stage speech art.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1068,COMPOSER-PERFORMING PRACTICE OF A PIANIST AS A WAY TO COMPREHEND THE FIGURATIVE-EMOTIONAL RANGE OF A MUSICAL WORK,"The article discusses one of the methods for comprehending the content of a musical work through the interaction of the pianist's composing and performing practice. No one can deny that the listener ""gets"" a work only ""from the hands"" of the performerpianist, and if the composer is not a pianist, then the audience in the hall will inevitably receive the product of joint creativity. That is why it seems appropriate to talk about the fact of a single composing and performing practice, without special emphasis on the composing or performing component of the creative process. When such an intratextual structure as the figurative-emotional series of a work is brought to the fore, the line between the composer and the performer becomes even more blurred. Of course, one cannot speak of replacing one with another - each has its own field of activity but it seems to be quite adequate to assert the presence of some creative conglomerate. Of course, in every piece of music there are many components that must be observed by the performer unconditionally: size, key signs, etc. This article will focus specifically on the figurative and emotional content of the work. As you know, each performer expresses the vision of a musical work in his own way. From the side of performing practice, a detailed analysis of the Second Piano Concerto by Ekaterina Prikhodovskaya was carried out. It is interesting that the composition in question was created practically ""in a dialogue"" between the authors of the article - the composer who wrote the concerto for piano and orchestra, and the pianist for whom this concerto was written and dedicated. The compositions of Anna Okisheva are considered as a composer's activity. On the example of her own works, Anna Okisheva gives an associative analysis, using works of art and literature as an addition to the figurative-emotional range. The uniqueness of this method lies in the fact that the performer, being a composer, already has an idea of the figurative and emotional content. In this example, we clearly see the process of ""encryption"" and ""decryption"" of a musical composition. Together and separately, these works demonstrate a fairly extensive representation of the figurative and emotional range. Thanks to the appeal to related arts (in particular, literature and painting), a more accurate and versatile formulation of the fan-shaped model of the creative process occurs (it was specified earlier). So, there is an inseparable, perhaps invisible at first glance, interconnection between composing and performing practice. Comprehension of the figurative-emotional range of a musical work in fact, its content - is indeed the right way to create an incentive for cooperation between the composer, performer and listener (even taking into account time distances).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1069,ON THE CONCEPT OF A SYSTEMIC DICTIONARY OF META-INDICATORS,"The multidimensional illustrative dictionary of meta-indicators includes lexemes, phraseological units and word combinations that regularly explicate metatext in A. Wierzbicka's interpretation. The aim of the article is to substantiate the expediency of the composition of the dictionary entry and the depth of the description of its components (zones, lexicographic parameters) and to show the dependence of the lexicographic type of meta-indicators on the number of lexicographic parameters and their specifics. Each lexicographic type of metatextual indicators is a group of dictionary units selected on the basis of a set of interconnected integral and differential heterogeneous lexicographic parameters, description zones. Variants of dictionary entries represent three lexicographic types of meta-indicators: connectors (takim obrazom 'this way', prezhde vsego 'first of all', slovom 'in a word', itak 'so', nakonets 'finally', vo-pervykh 'firstly'), quasiperformatives (povtoryu 'I'll repeat', dobavlyu I'll add') and semasiological metaelements (v pryamom smysle 'literally', v polnom smysle slova 'in the full sense of the word', v plokhom smysle slova 'in a bad sense of the word', obrazno govorya 'figuratively speaking'). The dictionary entry is considered as a text consisting of several zones. A zone is understood as a compositional part of a dictionary entry that has a length from a word to a complex syntactic unit, represents areas of the description of a dictionary unit in a certain aspect, is characterised by the integrity of the representation of this information and contains one (or more) lexicographic parameter. Dictionary entries have the same set of mandatory lexicographic parameters and differ in the number of zones and optional lexicographic parameters. Obligatory parameters include the following zones: the title with mandatory (spelling) and optional (information on the variation of the meta-indicator expression and on the grammatical paradigm of some vocabulary units) parameters; the scope of operation with mandatory parameters (distribution characteristic of the dictionary unit, the scope of its operation, the range of the meta-indicator); the status in the language system with mandatory (grammatical qualification of the dictionary unit) and optional (prosody, punctuation and functional homonymy) parameters; semantics with mandatory (interpretation, pragmatic program, functional-stylistic) and optional (synonyms, negative material) parameters. ""Formal functions"" is an optional zone. In the dictionary of meta-indicators a dictionary entry includes up to seven zones and 14 lexicographic parameters that reflect the word-and text-centered, systemic-structural approaches to the study of metatext and naturally connect information of different sections of linguistics and semiotics. Lexicographic units of the same type have minor variations of the set of lexicographic parameters quantitatively (1-2), e.g., dictionary entries of connector meta-indicators may vary in the presence / absence of the functional homonymy zone in lexical units. Different types of lexicographic meta-indicators are identified based on (1) the number of zones and lexicographic parameters; (2) differential lexicographic parameters.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1070,Semantic dimensions of populism in English (dictionary and corpus data analysis),"The aim of the article is to reveal the semantic content of the concept '' populism '' in modern English. The need to address this topic is driven by the fact that a significant part of the research is dedicated to the analysis of specific forms of populism or populist parties in the aspect of political science, discourse theory, political rhetoric, and ideology. From the standpoint of linguistics, the content of this concept was practically not considered. The study focuses on various structural components of the definitions and illustrative contexts of the word '' populism '' in British and American explanatory dictionaries and the texts of the Corpus of Contemporary American English (COCA). Electronic versions of explanatory English dictionaries were used for the analysis: Merriam-Webster Dictionary, Oxford English Dictionary (LEXICO), Cambridge English Dictionary, Collins English Dictionary, and Longman Dictionary of Contemporary English. Definitions, dictionary labels, and illustrative contexts of the words '' populism '' and '' populist '' were analyzed. The corpus analysis was carried out on the basis of the Corpus of Contemporary American English (COCA). The semantic content of the concept of populism was revealed by analyzing the collocates and clusters of this lexeme in accordance with the peculiarities of its syntactic representation. The selection of the collocates was carried out on the basis of the MI (mutual information) metric. The results were compared with sort by frequency; matching collocates were selected for analysis. Clusters were allocated within four words to the left or to the right of the node word, the communication distance was medium. The methods of semantic and corpus-oriented analysis, as well as the method of discourse analysis, were used to identify the evaluative tone (semantic prosody) of language units that characterize the ideologeme '' populism ''. The analysis of dictionary definitions in all considered lexicographical sources showed that the policy of populism is based on antagonism between the elite and the people, the protection of the interests of the people is emphasized as well. In the given examples of the corpus, as well as in the illustrative contexts of the dictionaries, various '' types '' of populism are mentioned: '' traditional '' vs. '' new '', '' good '' vs. '' bad '', '' progressive '', '' economic '', '' market '', '' cultural ''. The analysis of the examples showed that both in the function of the semantic object and in the function of the semantic subject, the lexeme '' populism '' is used mainly with predicates with negative connotations. Attributive collocations also indicate the negative-evaluative connotation. Based on the conducted research, it can be concluded that populism is an extremely heterogeneous phenomenon that manifests itself both in the political and socio-cultural spheres. Populism is based on the system of binary oppositions built on the confrontation of the people and the elite, with attitudes towards nationalism, protectionism, conservatism, authoritarianism, nativism (opposition to immigration), and racism.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1071,Exploring lexical stress processing in L2 English: A comparative eye-tracking study of native English listeners and Japanese listeners,"Shin, Jeonghwa. 2023. Exploring lexical stress processing in L2 English: A comparative eye-tracking study of native English listeners and Japanese listeners. Linguistic Research 40(3): 587-606. This study aims to explore how individuals with a native language characterized by a lexical pitch accent approach lexical stress in a stress-timed L2 during spoken word recognition. To this end, native English listeners and Japanese listeners of English participated in two phases of experiment: a three-day training and a subsequent eye-tracking experiment. The eye-tracking results revealed distinct processing patterns. Native English listeners predominantly recognized trochaic words by relying on the initial stressed syllable. In contrast, for iambic words, they utilized both the initial unstressed and the second stressed syllables for recognition. Japanese listeners of English demonstrated a different pattern of processing. They initiated lexical access within the first syllable of trochaic stress patterns and slightly later, still relying on first-syllable information, for iambic words. This finding implies that a single initial syllable is enough for Japanese listeners of English to utilize word stress information during L2 English spoken word recognition unlike native English listeners. The equal efficiency in employing two lexical stress patterns in L2 English suggests that lexical processing strategies transferred from the L2 listeners' native language could facilitate word recognition in the target language. While this study underscores the advantages of L1 prosodic structures in L2 English word recognition, it does not imply that Japanese listeners of English process English word stress in the same manner as native English listeners do during overall English word recognition. (Korea Military Academy)",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1072,From Positive to Death: A Corpus-Based Semantic Analysis of COVID-19 Representation in Malaysian English News Reports,"More than a year after being declared a pandemic, Covid-19 has not shown any sure signs of dissipating even as the battle to curb it continues. In Malaysia, the Movement Control Order (MCO), or lockdown seems to be the most effective way to curb the spread of the disease. Unfortunately, studies show that lockdowns affect people 's livelihoods and lifestyle, as well as their emotional and mental state. This situation, in many countries, is exacerbated by the onslaught of negative news on Covid-19 and heightened news consumption via various media platforms. Given this, the objective of the current study is to analyse the representation of Covid-19 in Malaysian media based on a corpus of news reports during Malaysia's first lockdown, i.e. MCO 1.0. This was a period of uncertainty lasting six weeks beginning from 18 March 2020 which saw increased reports of mental health cases and domestic violence cases in the country. News reports published in two Malaysian English online newspapers, The Star Online and Malaysiakini during MCO 1.0 formed the corpus of study. Using collocational analysis, the study examined the semantic prosody of Covid-19 and how it is represented in Malaysian new reports. The findings show that 'Covid-19' generally occurs in the company of unfavourable associations, causing it to acquire a negative prosody and in turn negatively represented in the news reports. The unfavourable portrayal of Covid-19, coupled with the increase in news consumption may adversely affect readers' emotions and anxiety levels, which in turn, may contribute to crisis fatigue.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1073,Semantic Preference and Semantic Prosody of the Collocations of Sustainable in NOW Corpus,"Sustainability has dominated the conversation about climate change since the early 2000s. The presence of Sustainable Development Goals strengthened the connection between the two (SDGs) set up by the United Nations General Assembly in 2015, with Goal Number 13 being climate action concerning the ever-worsening climate change. Ever since the word 'sustainable' has been heavily circulated in the media, it has been associated with various words from fashion to finance. Utilising the News on the Web (NOW) Corpus, the study explores the representation of the word 'sustainable' in media concerning climate change discussion under semantic prosody and semantic preference analysis. Using collocations of the node word, semantic preference determines the semantic set related to the node word, while semantic prosody interprets the environment in which the node word pertains. The collocations are semantically labelled with the help of an automatic semantic tagger UCREL Semantic Analysis System (USAS) to find the semantic preference. At the same time, the concordance lines in NOW Corpus are examined to determine the semantic prosody. The study finds that the word 'sustainable' tends to be associated with semantic sets related to the environment, Sustainable Development Goals, social issues and humanity, as well as money. It also indicates that the node word has positive prosody. The representation of the word 'sustainable' in the media is seen as favourable, not only as a way of living but also as a way of behaving in many aspects encompassing our lives.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1074,A Comparative Literary Study of the Prosodic Systems of English and Arabic Poetry,"This study investigates the similarities and differences between the prosodic systems of English and Arabic poetry. It is qualitative research on the two systems, where the methods of representation are explained, compared, and contrasted, with examples of words and lines from both languages' poetry. It supposes that music, as a common ground for the poetry of both languages, has given them the essential elements of having to do with the beat and rhythm, though these terms are slightly different in music. This is added to the fact that both are rhythmic languages, affected by the factor of rhythm as a common feature between them. Furthermore, the study attempts to prove that it is possible to have some examples of Arabic poetry represented through the English prosodic system and vice versa. This showed that there is much similarity between the two systems in the practical sense of depending on the vowels as fundamental to the existence and representation of syllables. The differences between the two prosodic systems were found in the significance of the level of the beats and the representation of consonants and long vowels.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1075,A Corpus Driven Analysis of Representations Around the Word 'ekonomi' in Malaysian Hansard Corpus,"Politicians constantly talk about wealth, power and education, which is often justified on the grounds that it will aid economic growth, which in turn will raise living standard. Thus, many economic issues are seen through the eyes of political beliefs. This paper reports on a corpus-driven analysis around the word ekonomi (economy) in the Malaysian Hansard Corpus. The objectives are to analyse the trend concerning the word ekonomi and to find out the representations around the word ekonom. The analysis showed that the word ekonomi was at its peak in Parliament 6 and Parliament 9, but declined in Parliament 8. The analysis also involved examining the collocational meaning of the word ekonomi and its collocates to determine the categories in which the word ekonomi (economy) was referenced. Positive noun collocates such as pertumbuhan (growth) and kekukuhan (stable) are mainly categorised into the government's policy, government's plan and economic activities in the country. Whilst negative noun collocates such as kemerosotan'(decline) and kelembapan (sluggish) are mostly categorised into economic situation in the country and globally and the government's effort to handle sluggish economy in Malaysia. The findings also revealed binary conceptualisations of 'us' vs. 'them', which is common in political discourse. The binary conceptualizations demonstrated that the government's efforts are commended. On the other hand, the unfavourable economic situation which happened in the country was due to factors which are beyond the government's control. The paper concludes that the representations around the word ekonomi involved justifying or legitimizing government's course of action.The study suggests a comparison of Hansard data from other Asean or Asian countries, focusing on representation of the word 'ekonomi'.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1076,Learning English Intonation Through Exposure to Resynthesized Self-produced Stimuli,"EFL learners are prone to having problems in pronunciation, while their problems in intonation are more salient. The Chinese EFL pronunciation classroom has long been criticized for teacher-centered, ""one-size-fits-all"" teaching, which is inefficient and ineffective for solving individual student's specific pronunciation problems. This study conducted an experiment to examine the effectiveness of exposure to resynthesized self-produced stimuli for intonation learning. The participants were 66 first year English majors studying at a university in China. The treatment was a form of English intonation training wherein the students in the experimental group used their resynthesized self-produced stimuli (their own voices) as the pronunciation model for learning while the control group used a model produced by a native speaker. After the training, the results of the intonation production test showed that the experimental group outperformed the control group in eight intonation patterns. The students' problems in intonation support Mennen's (2007) claim that intonation learning involves a first stage of acquiring the phonological representations of intonation patterns and a second stage of acquiring the phonetic realizations of those patterns. The results of this study revealed that exposure to resynthesized self-produced stimuli for intonation learning was as effective as the native speaker model for helping the students form the phonological representations of intonation patterns, while it was more effective than the native speaker model for facilitating the students to produce more accurate phonetic realizations of those patterns.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1077,"Collocations of Pria, Lelaki, and Jantan as Representations of Masculinity in Indonesia","Language is one way to understand a society and its culture, including masculine norms. Exploring evolutionary masculinity through language is an intriguing concept to revisit. The research examines words synonymous with ""men"" in Indonesia and reviews their usage to depict current masculinity in the country. This research applied discourse analysis to corpora sourced from the Leipzig Corpora and CQPWeb. The data were analyzed using semantic preference to find meanings and semantic prosody to find connotations of pria, lelaki, and jantan. The findings reveal differences in the meanings and usage of the pria, lelaki, and jantan words. The difference in meaning is that pria is an adult male, whereas lelaki is a representation of men who are not limited in age, and jantan is interpreted as the genitals of animals or plants and men in the context of masculinity. According to usage, the word pria is frequently used in the public sphere, such as in the context of work and news discourse. Lelaki tends to be used more in the personal sphere, such as family, rather than in public settings. Jantan tends to be used in public discourse. The connotations of pria, lelaki, and jantan is neutral. This study successfully demonstrated the shift in Indonesian masculinity from traditional to new forms, indicating the impact of language studies on the analysis of masculinization in Indonesia.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1078,"A Vocal Cycle for Mezzo-Soprano, Bassoon and Piano ""The Fragments"" by Elisavieta Panchecko Set to Poems by Anna Akhmatova","Elisavieta Panchenko's vocal cycle ""The Fragments"" set to poetry by Anna Akhmatova presents an example of a genuinely innovative contemporary manifestation of a specific kind of intonation of speech with its minutest emotional tints, as well as a non-standard type of interpretation of the solo instrumental timbre. Written for a vocal-instrumental trio (mezzo-soprano, bassoon, piano), this is, nonetheless, a vocal cycle one in the essence and nature of its musical thematicism. The features of through development in the cycle, the continuous renewal of musical material, the freedom of thematic development peculiar for the most part to instrumental compositions - all of these are directed for the most part towards heightened attention to micro-syntactic units of the poetic and musical texts. The musical content of the cycle is abundant with fine achievements in the domains of mode, harmony, texture and intonation, characteristic to the music of the present and previous centuries. The dialogue of the two equitable soloists - the two dramatic characters of the poem - is already being formed during the process of the representation. For the first time in vocal music the bassoon is interpreted as being endowed with a vocal timbre, albeit not possessing verbal characteristics of an unperceived character. The composer implements a counter-tendency to the picturesque content of the source hidden in the implication of the narrative: not only does the mother mourn the loss of her unjustly arrested son, but also the son when still alive bids his mother farewell, without hoping to see her again.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1079,The National-Historical Ballad in the Musical Art of Romanticism: Towards Posing the Problem,"As the result of systematic analysis of the romantic of the two branches ballad genre, the taboo and the national-historical ones, their affinity (their narrative qualities, dramatic substance and incorporation of the fantastic element) and the individual signs of the national-historical genre model are distinguished. Examination is made of its genre-related phenomenological markers: the genre code ""The Human Being and the Historical Tragedy of the People""; the general intonation of the Nordic sublime genre of; the protagonists are - the hero, his beloved and the narrator. Carrying out the search for the archetypal regular laws in the storylines, the motive complexes the genre-related semantic lines of character of the heroes in the archetypal construction of the genre on the level of composition and dramaturgy, the authors arrive at the conclusion that the national-historical ballad, while preserving its logical-structural content, converts the ""general intonation"" of the genre, supplanting the category of irrational fear. The heroes of the ballad receive a ""reversing"": the infernal newcomer is replaced by the warrior, the narrator - by the singer-prophet, and the self-sacrificing beloved - by the heroic maiden. On the basis of analysis of works by John Blackwood McEwen, Arnold Bax, Horatio Parker, Gerard Bunk and Franz Poenitz the methods of musical representation of the mentioned heroes are examined. It is noted that in the mature formation of the genre each hero becomes attributed with a stable lexical-intonational sphere. The battle, panegyric and heroic-dramatic style present the distinctive features of the warrior, the female lamentation and Nordic landscape comprise the musical portrait of the maid of the North, whereas the musical ""emblem"" of the bard is taken on by the odic and song-narrative style.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1080,Long-Term Fundamental Frequency Modeling Based on Wavelet Packet Transform for Voice Conversion,"Prosody conversion is an important part in voice conversion, where fundamental frequency (F0), which carries important speaker individuality information (e.g., tone, intonation, etc.), is regarded as one of the key prosodic features in the excitation model for speech synthesis. In a conventional approach based on continuous wavelet transform for modeling F0, analysis is carried out on a frame level and is prone to losing high-frequency information in the process of decomposition and reconstruction. In order to address this problem, the paper shows a representation of long-term fundamental frequency based on Wavelet Packet Transform (WPT). Specifically, the long-term F0 is decomposed using WPT, and a joint vector is formed by combining the resulted average power spectrum. Furthermore, the method is applied in a voice conversion system. Voice conversion experiments are conducted on Chinese and English speech data to evaluate the performance of the proposed method. The results show that the proposed method is obviously better than the method based on wavelet transform in all conversion scenarios but performs a little worse than the method based on mean and variance in same-gender conversion scenario.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1081,Refugees in the Spanish press: A corpus-assisted study of the semantic prosody of the term refugiado from a diachronic perspective,"This paper explores the semantic prosody of the lemma refugiado in the Spanish press over a 7-year period (2010-16) with the aim of examining the discursive representation of refugees in this textual genre. The study is concerned with diachronic and quantitative aspects, while focusing on the negative lexical units collocating with refugiado. The research is based on the analysis of a 1.8-million-word corpus of Spanish news articles about refugees that were extracted from the digital libraries of El Pais and El Mundo. The results show that the frequency of refugiado increases over the 2010-16 period, and so does its negative semantic prosody. This negativity -which is much higher in the last two years under study- is constructed through certain consistent collocates, seasonal collocates and patterns of language associated with refugees' massive occupation and victimization.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1082,Categorization and its linguistic representation,"The cognitive science approached categorization from merely linguistic perspectives - the researchers focus on categorization, linguistic means of its representation (articles, pronouns, verbs, word order, intonation), while the process and the results of categorization are verbalized and language means employed in this cognitive operation are still not well studied. We aim to build the categorization frame, reveal the principles of profiling and language means of verbalization employed while describing the process. The research starts with gathering the lexical units from the Russian-Russian Dictionary by Sergey Ozhegov that describe the process of categorization. Then the authors build a categorization frame - first, the objects are classified, second, the speaker rationalizes his/her decision, then, the categorizing per segoes - the objects are referred to some class, the speaker expresses the extent of his confidence of the decision, the extent to which the object is assigned to the class, or correlates the object with the members of the class without 'establishing the membership'. The research resulted in the typology of items that describe categorization, profiling operations with the focus on the way of acquiring knowledge (oboznachat', vyglyadet'), circumstances, aspects of the decision making process - hesitating (vsyo-taki), doubting the decision (vrode, kak budto), estimating the degree to which the object meets the requirements to the members of the category (identichnyj, pohozhij, nastoyashchij), while other language units establish similarities between objects of different classes (pohozhij), emphasize the result of such a cognitive operation as likening (mnimyj, lozhnyj, shozhij), or a weak correspondence between the object and those properties that allow it to be assigned to a particular class (the ""worst"" representative of the class: zauryadnyj, zahudalyj, nikudyshnij).",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1083,Translation as Strategic Foreignization: A Study of the Politics of Translation in Mother Forest: An Unfinished Autobiography,"The study draws upon Lawrence Venuti's concept of foreignization as a strategic tool employed in the translation of CK Janu's Mother Forest: An Unfinished Autobiography. The translation works to mould an ethnic autobiography and represent a subaltern subject through explicit signifiers of subalternity, masqueraded as an attempt to ""retain the flavour of Janu's intonation and the sing-song nature of her speech in translation"". As a mode of representation, this study identifies the text as catering to a transnational publishing industry and the global academic marketplace, transforming the cultural value of an ethnic subaltern text into what Graham Huggan describes as ""tawdry ethnic goods"" in the late capitalist supermarket.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1084,Introspective Study of Emotion Icon in Public Chat as a Gesture of Texting,"An emotion icon, better known as emoticon is a metacommunicative pictorial representation of a facial expression that, in the absence of body language and prosody, serves to draw a receiver's attention to the tenor or temper of a sender's nominal verbal communication, changing and improving its interpretation. The present study investigates the use of these nonverbal cues in whatsapp public chat. The analysis focuses on the multifunctionality of emoticons, their role in online relational work, and possible connections between emoticon use and language proficiency and thus contributes to a more complete understanding of emotive communication online. Its ultimate goal is to help clarify the role of emoticons within a larger conceptual framework of emotive and relational meaning. To this end, this study takes a micro-analytic approach to show how English as foreign language learners use emoticons in text chat. The analysis shows that emoticons are highly context-sensitive and can display affect or serve as contextual cues to signal illocutionary force or humor.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1085,THE EMOTIONAL PROSODY OF U.S. FATAL AIR-ACCIDENT DOCKETS ONLINE: RISKING RISK COMMUNICATION?,"Risk communication is grounded in both rationality and emotion (Fischhoff & Kadvany 2011, Boholm & Corvellec 2014). Recent investigations have proved that emotions do affect risk and danger perceptions by functioning as 'mediators' (Xie et al. 2011) and become important in decision-making. My study explores how emotion is induced by the National Transportation Safety Board of the United States of America (NTSB for short) to influence the mentalities and behaviours of its broad mixed audience and thus increase risk prevention. With that research purpose in mind, I examine an electronic corpus of over 500 online samples of fatal aviation dockets issued yearly online by the NTSB between the time span 20102015 and contained in its website databases. The emotional engagement deployed to mediate the perceptions of risk and danger by the general public constitutes a unique genre among all other world transportation agencies, since through informative vividness it pursues to activate the processes of memory, inference (i.e. judgement) and decision-making. I take Stubbs' (2001) concept of 'discursive prosody' as point of departure and resort to a blended theoretical framework that combines Narratology, Corpus Linguistics, Critical Discourse Analysis, and Proximisation (Cap 2013) and Positioning (Harre & van Langenhove 1999) Theories. I will show that the NTSB's emotional prosody is more rhetorical than lexical and that the narrative strategies of focalisation and speech representation play a salient role. To conclude I will reflect on some of the possible consequences of over-exploiting emotional engagement in risk communication.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1086,AUTOMATIC PROSODY GENERATION IN A TEXT-TO-SPEECH SYSTEM FOR HEBREW,"The paper presents the module for automatic prosody generation within a system for automatic synthesis of high-quality speech based on arbitrary text in Hebrew. The high quality of synthesis is due to the high accuracy of automatic prosody generation, enabling the introduction of elements of natural sentence prosody of Hebrew. Automatic morphological annotation of text is based on the application of an expert algorithm relying on transformational rules. Syntactic-prosodic parsing is also rule based, while the generation of the acoustic representation of prosodic features is based on classification and regression trees. A tree structure generated during the training phase enables accurate prediction of the acoustic representatives of prosody, namely, durations of phonetic segments as well as temporal evolution of fundamental frequency and energy. Such an approach to automatic prosody generation has lead to an improvement in the quality of synthesized speech, as confirmed by listening tests.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1087,Intelligibility and the listener: The role of lexical stress,"For some 30 years, intelligibility has been recognized as an appropriate goal for pronunciation instruction, yet remarkably little is known about the factors that make a language learner's speech intelligible. Studies have traced correlations between features of normative speech and native speakers' intelligibility judgements. They have tended to regard prosody as a global phenomenon and to view intelligibility as primarily a quality of the speaker. The present article focuses on a single prosodic element, lexical stress, and shifts the focus of study to the listener. It draws on findings in psycholinguistics that have rarely been applied to second language (L2) contexts. Groups of listeners were asked to transcribe recorded material in which the variables of lexical stress and vowel quality were manipulated. Recognizing the extent to which English is employed in international contexts, the study contrasted the effect of the variables on native listeners (NLs) with their effect on normative listeners (NNLs). NLs and NNLs were found to respond in remarkably similar ways to the problems posed by stress misallocation. For both groups, the extent to which intelligibility was compromised depended greatly on the direction in which stress was shifted and whether changes in vowel quality were involved.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1088,ETHNOGRAPHIC CINEMA: TO THE PROBLEM OF GENRE BOUNDARIES,"We analyze ""artistic and ethnographic"" (as the authors position it) of the film ""Yakut Wedding. XIX century"" (2016), in which creation the leading ethnographers of the republic took an active part. The film is an example of anthropological staged movie. Visualization of the folklore text (wedding ceremony), as a parade, displays the cultural codes of the ethnos, a semiotic analysis of which is proposed in order to determine the genre specificity of ethnographic cinema. The film as a visual anthropological study of folklore text offers an interpretation of the chromatic code of the Yakut culture (analysis of the semantics of white in the culture of herders). The analysis of the musical and acoustic code is carried out: the bride's long song - reconstruction of the archaic meloform; analysis of intonation of the participants of the ritual as a speech characteristic. In addition, an analysis of the kinetic, culinary, and numerical codes of culture visualized in the film is presented. Based on the materials of festival cinema and the works of modern researchers in the field of visual anthropology, the characteristics of ethnographic cinema are generalized. Criteria are proposed, that distinguish ethnographic from other genres of documentary films. In our opinion, any representation, including a visual one made by a person about a person, is anthropological in nature. It is revealed that the genre of ethnographic cinema involves a detailed accented consideration of everyday life; search for similarities and differences of cultural texts; scientific commentary (interpretation); direction of collective perception (manipulation); creating the illusion of being here and now; the desire to convey the aura of culture as much as possible; implantation or indigenous films; ""embed"" the camera; ""included"" cinema, the achievement of naturalness; iconophobia ""avoiding direct gaze ""; the inclusion of the viewer in reflective activity (empathy); obligatory consonance of film aesthetics with aesthetics of culture; preference for the look of a ""stranger ""; ""the illusion of a neutral observer"". The features of the stylistics of cinematic ethnography of the Yakuts, the problems of mutual framing in multilevel and multivalent indigenous cinema are considered. It is proved that ethnographic cinema is not only a commentary on culture, and self-portrait is not an end in itself. The film is not limited to the experience of reconstruction of the rite, but it represents a mentality, is an act of self-awareness of culture.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1089,"INTERPRETATIVE-ANALYTICAL HYPOSTASES IN ""SEPT FRAGMENTS DE TRISTAN TZARA"" 1 FOR VOICE AND PIANO BY ADRIAN POP","The present paper provides a descriptive analysis of the Sept fragments de Tristan Tzara, a unitary song cycle, organized on the basis of an internal dramaturgy, in which the poet's verses (used in French in original) outline the thematic framework pervaded by the central idea of love. The unity of the interpretation of the cycle of the Seven fragments from Tristan Tzara can be achieved only after a deep knowledge of the expressive and symbolic springs of the musical-poetic discourse. The study of the musical segments closely leads to the comprehension of the musical language in the process of decoding the encrypted meanings in the score, an image of the whole being configured only when the cycle is complete. The elements of assimilation, memorization and interpretation will be tracked and chiselled into the vocal-instrumental duo throughout each song. The vocal techniques used vary according to both the particularities of the language elements and the way of their psycho-affective representation in interpretation. The vocality, adapted to the rhythmic and metrical writing of the pieces, requires a perfect mastery of the interpretive technique. The vocal part, as a constitutive part of a musical discourse with a modal language of synthesis, is emphasized by the writing in the piano accompaniment, in which the chordic structures are either gravitational or geometric. Analysing the form of the works, we concluded that, in each case, they are entirely subordinate to the needs of a dramatic sense and closely related to the poetic and musical images. Nevertheless, our paper is embedded within a personal interpretative vision on the ""fragments"" of the cycle, bearing the imprint of the subjectivity that resides in the personal reception of the meanings of music.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1090,LINGUISTIC MEANS IN IMAGE OF COVID-19 (GERMAN POLITICAL DISCOURSE IN MEDIA),"The article is focused on the role of lexical and prosodic means of expression in the coverage of the COVID-19 pandemic in German political media discourse. As a research task, the authors identified an attempt to assess the dynamics of the image of the coronavirus phenomenon in the media and to identify the conditionality of the use of linguistic units at certain stages of the pandemic. The material for the study was transcripts and audio recordings of plots from leading German news channels in the first half of 2020. Particular attention is paid to the analysis of the frequency of the use of lexical units and the correlation of lexical and prosodic means of expression. In the course of linguo-stylistic analysis, the main stylistic means are established, depicting the course of the pandemic and the fight against the virus. The prosodic features of the informants' speech, which are determined in the course of the electroacoustic analysis of statements, are brought into consideration. The impact of COVID-19 on all spheres of public life in Germany and its maximum representation in the media determine the relevance of this study. The novelty of the study is seen in the application of a comprehensive methodology for analyzing expressive means in the speech of informants in the period from January to May 2020.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1091,CATEGORIZING MANDARIN TONES INTO LISTENERS' NATIVE PROSODIC CATEGORIES: THE ROLE OF PHONETIC PROPERTIES,"This study examined whether native speakers of non-tone languages (Australian English, and French) were able to perceive foreign Mandarin tones in a sentence environment according to their native prosodic categories. Results found that both English and French speakers were able to perceptually categorize foreign tones into their intonational categories (i-Categories), and that categorizations were based on the contextual phonetic similarities of the pitch contours they perceived between Mandarin tones and their native i-Categories. Results also showed that French speakers, but not English speakers, were able to detect the fine-detailed phonetic feature differences between Tone 3 and Tone 4 (low/falling tone vs. high-falling tone). The findings support a new extension of the Perceptual Assimilation Model (PAM; Best 1995) to suprasegmental phonology (So and Best 2008): that non-native prosodic categories (e. g. lexical tones) will be assimilated to the categories of listeners' native prosodic system (e. g. intonation). In addition, rhythmic differences among languages may also contribute to perception of non-native tones.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1092,PROCESSING CLITIC PRONOUNS IN BULGARIAN - EVIDENCE FROM NORMAL AND AGRAMMATIC COMPREHENSION,"Clitic clusters display a complicated interaction of prosodic and syntactic properties which determines their word order and stress patterns. In Bulgarian, short pronouns appear as unstressed verbal enclitics in positive utterances. Proclitic negation attracts the pronouns and forms with them a prosodic unit stressed on the second syllable, the pronoun. Theoretical linguistics characterizes the behaviour of object clitics in terms of ""non-trivial chains"" (Boskovic 2001) containing copies. The overt realisation of a higher or lower copy depends on phonological constraints like enclitisation requirements. In line with the slow-syntax-hypothesis (Burkhardt et al. 2008) and with the assumption that prosody-related processes may also compete for the same limited processing resources of Broca's aphasics (Avrutin et al. 1999), we test sensitivity to the phono-syntactic constraints negation imposes on the word order of personal and reflexive clitics. Results suggest that the pattern of agrammatic processing of clitic clusters resembles normal comprehension but proceeds in a protracted manner. Employing a self-paced reading task and an experimental design which reduces discourse-related interpretation processes, we also show that the syntactic functions of personal object clitics as syntactic object agreement markers in Bulgarian are relatively preserved in the aphasic group.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1093,Effect of linguistic experience on the discrimination of Shona lexical tone,"This article examines how experience with a tone language (Thai) and with pitch variations at the sentential level (German) influences the perception of a typologically different tone language (Shona). Tone perception in adults is influenced by experience with the phonological inventory of one's native language. However, the extant data demonstrates that good to ceiling performance on tone perception is not restricted to tone language listeners. To this effect tone perceptual discrimination was investigated in experiment 1 using minimal pairs of Shona words and their filtered homologues, whereas experiment 2 tested the effect of increasing the number of phonetic contrasts using minimal and non-minimal pairs in Shona words and low-pass filtered stimuli. The results revealed that the Shona and Thai listeners discriminated both the Shona words and low-pass filtered stimuli significantly better than the non-tone listeners in both experiments. In experiment 2 it was also observed that although the accuracy performance of the tone language listeners is comparable, the types and pattern of errors committed are quite different.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1094,Mental representation of tonal spreading in Bemba: Evidence from elicited production and perception,"Previous research has shown that listeners from tonal languages are better at processing tone compared to speakers from non-tonal languages. However, most of this research has tested Asian tone languages, particularly those which have many tonal contrasts and a dense tone-to-syllable association. In this paper we investigate the mental representation of derived tones in Bemba, a Bantu language that has a two-way tone contrast but which shows robust tone spreading patterns. Specifically, we test ternary high-tone spreading, a process that is unique from a phonological perspective. In a production task we test whether ternary spread can be extended to non-words. We complement this with an AX discrimination task comparing binary vs ternary spread, which are phonologically contrastive, on the one hand, with a tonally similarly salient but non-phonologically relevant contrast, on the other. We show that in both the production and percep- tion of non-words, ternary spread is distinct from binary spread, suggesting that derived tone is equally mentally represented as lexical tone is in Asian tone languages.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1095,Development and applications of alternative methods of segmentation for Mandarin Hearing in Noise Test in normal-hearing listeners and cochlear implant users,"Conclusion: The study demonstrates that two methods of segmentation, i.e. word and character segmentations, produce equivalent results in the Mandarin Hearing in Noise Test (MHINT). Potentially, both methods of segmentation can be used clinically. A majority of the Mandarin-speaking cochlear implant subjects could complete MHINT - with the more relaxed adaptive rules. The results make it possible to compare the performance of cochlear implant users across languages. Objectives: The primary purpose of the present study was to evaluate the modified adaptive scoring rules and to develop alternative methods of segmentations in MHINT that are suitable for the Chinese language. Methods: Thirty Mandarin-speaking normal-hearing adults were tested with MHINT using three adaptive rules based on character and/or word segmentation of the sentences. Twenty-three Mandarin-speaking post-lingually deafened cochlear implant patients were also recruited to participate in the testing. Results: There were no significant differences in the reception threshold for sentences and speech recognition scores obtained with either method of segmentation (p > 0.05). Fifteen of the 23 cochlear implant subjects (65%) could be tested with the modified adaptive scoring rules. The performance-intensity functions of the cochlear implant subjects were shifted at least 8-10 dB to higher signal-to-noise ratios as compared with those of the normal-hearing subjects.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1096,Tone perception and production in pediatric cochlear implants users,"Conclusions: In prelingually deaf children with cochlear implants, tone perception and production performance are highly correlated. This result is consistent with the hypothesis that tone perception is the prerequisite for good tone production. Objectives: Previous research has shown remarkable deficits in tone perception and production in native tone language-speaking, prelingually deafened children with cochlear implants. The purpose of the present study was to investigate the relationship between tone perception and production in those children. Methods: Twenty-five prelingually deaf children with cochlear implants participated in the study. All subjects were Advanced Bionics CII/90K users with various lengths of implant use. To evaluate tone perception performance, subjects completed a computerized tone contrast test. For tone production performance, an artificial neural network was used to evaluate the accuracy of tones recorded from each of the 25 subjects. Results: Large individual differences in tone perception and production performance were observed in these subjects. Tone perception accuracy ranged from 50.0 to 96.9% correct (chance performance == 50% correct; mean == 71.0% correct). Tone production performance ranged from 19.4 to 97.2% correct (mean == 52.0% correct). A strong correlation was found between tone perception and production performance in this group of subjects (r == 0.805).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1097,Mandarin lexical tone recognition in sensorineural hearing-impaired listeners and cochlear implant users,"Conclusions: As the hearing loss becomes more severe, the tone recognition performance of hearing-impaired listeners gradually but slowly reduces. The tone recognition performance of cochlear implant listeners is below or close to the performance of severely hearing-impaired listeners. Objectives: The present study aimed to investigate the Mandarin lexical tone recognition performance of sensorineural hearing-impaired listeners and post-lingually deafened cochlear implant users. Methods: Tone recognition performance was measured for 30 normal-hearing subjects, 41sensorineural hearing-impaired listeners, and 12 cochlear implant users using 128 monosyllables recorded by a male and a female adult native Mandarin speaker. Results: The results indicated that the accuracy of tone recognition was 99.3%, 96.4%, 93.7%, 83.9%, and 81.0% for the normal-hearing, moderate, moderate to severe, severely hearing-impaired, and cochlear implant subjects, respectively. For the hearing-impaired subjects, a significantly negative correlation was observed between tone recognition performance and the audiometric hearing thresholds. For cochlear implant subjects, Tone 3 was the easiest one to perceive and Tone 2 was the hardest one to perceive. They tended to misperceive Tone 1 as Tone 2, and misperceive Tone 2 as Tones 1 and 3.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1098,Word stress processing in specific language impairment: Auditory or representational deficits?,"Word stress processing has repeatedly been reported to be affected in specific language impairment (SLI) with potential consequences for various aspects of language development. However, it still remains unresolved whether word stress impairments in SLI are due to deficits in basic auditory processing or to a degraded phonological representation or both. We addressed this question examining an unselected sample of 10 children with SLI and 11 typically developing (TD) children, aged about 8 years, with respect to their basic auditory processing (duration and skewness discrimination) and phonological representation of prosodic (word stress) and segmental (consonant) contrasts. Our results show lower performance of the SLI group compared to the TD group in all tasks. Crucially, two subgroups of children with SLI emerged from our analyses: While one group was impaired in basic auditory perception, particularly affecting duration discrimination, the other showed no significant auditory processing deficits but a representational impairment.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1099,Lexical tone and stuttering loci in Mandarin: Evidence from preschool children who stutter,"The purpose of this study was to examine the relationship between stuttering loci and lexical tone in Mandarin-speaking preschoolers. Conversational samples from 20 Taiwanese children who stutter (CWS; M = 4:9; range = 3:2-6:4) were analysed for frequency and type of speech disfluency and lexical tone associated with stuttering-like disfluencies (SLDs). Results indicated that SLDs were significantly more likely to be produced on Mandarin syllables carrying Tone 3 and Tone 4 syllables compared to syllables carrying either Tone 1 or Tone 2. Post-hoc analyses revealed: (1) no significant differences in the stuttering frequencies between Tone 1 and Tone 2, or between Tone 3 and Tone 4, and (2) a higher incidence of stuttering on syllables carrying Tone 3 and Tone 4 embedded in conflicting (as opposed to compatible) tonal contexts. Results suggest that the higher incidence of stuttering on Mandarin syllables carrying either Tone 3 or 4 may be attributed to the increased level of speech motor demand underlying rapid F0 change both within and across syllables.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1100,LEXICAL TONE DISRUPTION IN CANTONESE APHASIC SPEAKERS,"This study reports on the nature of tonal disruption in brain-damaged subjects. The language selected for investigation was Cantonese, a Chinese dialect spoken in southern China and Hong Kong with six lexical tones. Brain-damaged subjects were asked to identify and produce Cantonese words in isolation. It was found that lexical tone disruption is a generalized sign in aphasia. There was no evidence that there exists a particular pattern of tonal disruption in any specific type of aphasia. Results also indicated tonal disruption in both production and perception tasks. This impairment was often more severe with the perceptual ability than the production. Indeed, the data suggested that tonal disruption can be a disorder at either the phonological or phonetic level.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1101,Musical pitch and lexical tone perception with cochlear implants,"Objective: The purpose of the present study was to test the hypothesis that cochlear implant (CI) users' music perception is correlated with their lexical tone perception, and the two types of perception share similar mechanisms in electric hearing. Design: A lexical tone perception test and a pitch interval discrimination test were administered to a group of CI users and a group of normal-hearing (NH) listeners. Sample study: Nineteen adult CI users and 10 NH listeners who are native-Mandarin-Chinese speakers participated in the study. Result: Tone-perception performance of the CI group was, on average, 58.3% correct (+/- 19.78% correct), and performance of the NH group was near perfect. The CI group had a mean threshold of 5.66 semitones (+/- 5.57 semitones) in pitch discrimination as compared to the threshold of 0.44 semitone from the NH group. There was a strong correlation between the CI users' tone-perception performance and their pitch discrimination threshold (r = -0.75, p < 0.001). Conclution: Musical and lexical pitch perceptions are strongly correlated with each other and they might share similar mechanisms in electric hearing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1102,Pitch perception and frequency-following responses elicited by lexical-tone chimeras,"Objective: Previous research has shown the usefulness of utilizing auditory chimeras in assessing a listener's perception of the envelope and fine structure for an acoustic stimulus. However, research comparing and contrasting behavioral and electrophysiological responses to this stimulus type is scarce. Design: Two sets of chimeric stimuli were constructed by interchanging the envelopes and fine-structures of the rising/yi(2)/and falling/yi(4)/Mandarin pitch contours that were filtered through 1, 2, 4, 8, 16, 32, and 64 frequency banks. Behavioral pitch-perception tasks were administered through a two-alternative, forced-choice paradigm. Electrophysiological responses were measured through scalp-recorded frequency-following responses (FFRs) to the lexical-tone chimeras. Study sample: Twenty American and twenty Chinese adults were recruited. Results: A two-way analysis of variance showed significance (p<0.05) within and across the filter bank and language background factors for the behavioral measurements, while the frequency-following response demonstrated a significance only across the filter banks. Conclusions: Perceptual importance of envelope cues increases starting from 16 filter banks, while the FFR accuracy and magnitude decreases with increasing number of filter banks. These results can be useful in assessing experience-dependent neuroplasticity and in designing speech processing strategies for cochlear-implant users who speak tonal or non-tonal languages around the globe.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1103,Assessing prosodic skills in five European languages: Cross-linguistic differences in typical and atypical populations,"Following demand for a prosody assessment procedure, the test Profiling Elements of Prosody in Speech-Communication (PEPS-C), has been translated from English into Spanish, French, Flemish and Norwegian. This provides scope to examine receptive and expressive prosodic ability in Romance (Spanish and French) as well as Germanic (English and Flemish) languages, and includes the possibility of assessing these skills with regard to lexical tone (Norwegian). Cross-linguistic similarities and differences relevant to the translation are considered. Preliminary findings concerning 8-year-old neurotypical children speaking the five languages are reported. The appropriateness of investigating contrastive stress in Romance as well as Germanic languages is considered: results are reported for assessing this skill in Spanish and English speakers and suggest that in Spanish it is acquired much later than in English. We also examine the feasibility of assessing and comparing prosodic disorder in the five languages, using assessments of prosody in Spanish and English speakers with Williams syndrome as an example. We conclude that, with caveats, the original design of the UK test may indicate comparable stages of prosodic development in neurotypical children and is appropriate for the evaluation of prosodic skills for adults and children, both neurotypical and with impairment, in all five languages.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1104,Lexical tone perception in native speakers of Cantonese,"Purpose. This study aimed at investigating (1) tone perception development among typically-developing Cantonese speakers and (2) the hierarchy of tone perception difficulty among the 15 tone contrasts. Method. Two-hundred typically-developing children aged 3-10 and a group of 25 normal hearing adults were recruited. They were tested on a pool of 75-item calibrated recorded speech signals. Participants responded to each stimulus by pointing at the corresponding picture displayed on a computer screen from a choice of four. Result. There was a gradual increase in tone perception accuracy from children aged 3-6. After age 6, tone perception accuracy was similar to adults with an average error rate of 3-8%. The two tone contrasts that listeners consistently found difficult to distinguish were T2T5 (high-rising vs low-rising) and T3T6 (mid-level vs low-level). In addition, all children groups also showed difficulty in T4T6 identification (low-falling vs low-level). Conclusion. Tone perception is not error-free even among native Cantonese-speaking adults. Overall tone identification performance improved steadily from age 3 to age 6. Based on the participants' performance, a three-tier set of tone groups, with an increasing level of difficulty for identification, is proposed for rehabilitation purposes. These tone groups are (1) Easy: T1T2, T1T3, T1T4, T1T5, T1T6, and T2T3, (2) Medium: T2T4, T2T6, T3T4, and T4T5, and (3) Hard: T2T5, T3T5, T3T6, T4T6, and T5T6.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1105,Speech of Six-year-old Children with Logopathology: Features and State of Development,"Purpose. The purpose of the article is to describe the state of speech development of six-year-old children with logopathology.Research methods and techniques. Research methods and techniques. During the experimental research, theoretical methods aimed at analysing the research results and forming conclusions were used. Empirical methods included analysis, comparison, data processing, as well as observation and interviews with children during the use of various types of tasks aimed at studying the state of formation of phonemic processes (phonemic perception, phonemic analysis, phonemic representation), lexical (passive, active dictionaries), grammatical (composing stories on various topics, using pronouns, agreement of words in gender, number, case), prosodic levels (loudness, tempo, intonation, diction) competences. Results. The results of the conducted research give a clear idea that there are significant differences between the groups of studied children with logopathology and those with normotypical psychophysical development regarding the formation of their speech and language competences. Children of older preschool age (six years old) who had low indicators had persistent violations of phonemic competence (perception, analysis and representation); insufficiently formed lexical competence (misunderstanding of words, difficulties in composing a story, problems with classification of concepts and definition of words with the opposite meaning); the grammatical competence is not formed (agrammatism, distortion of the sound structure of words, perseveration, paraphasia, inability to use pronouns, agree words and number, gender, case); undeveloped prosodic skills (unregulated voice strength, pace of speech, vague utterances, diction abilities are limited due to persistent phonological disorders).Conclusions. Six-year-old children with logopathology have insufficiently developed phonetic, lexical, grammatical and prosodic competences, as compared to the results of their peers with normotypical psychophysical development. The lack of speech and language competences will affect children's mastery of writing and reading skills during learning the curriculum at school.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1106,Personality Profiling Through Language: Assessment Techniques for Big Five Traits in Psycholinguistic Perspective,"Purpose. The present paper aims to systematically examine contemporary approaches to personality assessment based on linguistic and paralinguistic indicators, with a specific focus on the Big Five trait model. The study addresses a growing interdisciplinary demand for theoretically grounded, empirically validated, and practically applicable methodologies in the domain of indirect personality profiling. The central objective is to map the modalities, feature types, computational methods, and personality traits targeted in empirical research published between 2017 and 2025. Methods. A systematic search was conducted using the Scopus database, yielding an initial set of publications screened according to defined inclusion criteria: empirical design, and direct relevance to Big Five trait recognition using linguistic or paralinguistic data. The final sample comprised 20 studies. Each entry was coded along four analytic dimensions: (1) modality (text, speech, or multimodal); (2) indicator type (linguistic, paralinguistic, or both); (3) trait representation (explicit Big Five or inferred); and (4) computational methodology (psychometrics, statistical modeling, machine learning, deep learning). Descriptive and thematic synthesis procedures were applied. Results. The analysis revealed a dominance of linguistic indicators, particularly lexical and syntactic features extracted from text-based data such as social media posts or self-descriptions. Paralinguistic indicators-such as prosody, intonation, and speech rhythm-were significantly underrepresented, despite evidence of their predictive value. Only a small subset of studies employed multimodal models. Deep learning architectures were used in a limited number of cases but showed promising accuracy and interpretability when combined with trait-specific embeddings. Extraversion and neuroticism were the most frequently assessed traits, with agreeableness and conscientiousness comparatively neglected. A trend toward integrating AI-generated linguistic items into psychometric scale construction was also observed. Conclusions. Current research on psycholinguistic personality profiling reflects significant methodological innovation but also conceptual and technical fragmentation. The field would benefit from standardized assessment protocols, shared benchmarking datasets, and increased attention to underrepresented traits and modalities. In particular, future studies should explore multimodal modeling, trait-specific feature calibration, and ethically robust implementation frameworks. The review highlights substantial translational potential for these methods in forensic psychology, human-computer interaction, organizational diagnostics, and digital mental health.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1107,THE PROSODIC SISTEM OF PRAZNICA,"This paper examines the prosodic system of the Cakavian dialect of Praznica. Previous studies of Cakavian dialects spoken on the island of Brac have revealed the existence of a classic Cakavian three-accent system, consisting of a short-falling accent, a long-falling accent, and an acute, as well as pre-accentual length. This paper does not intend to question previous studies of the Brac local dialects, because these have been based mainly on listeners' perceptions, but it intends to demonstrate and describe the complex prosodic system of the dialect of Praznica through acoustic analysis. Furthermore, the paper aims to find out if older speakers have preserved a more archaic prosodic system in comparison with groups of younger and middle-aged speakers. For this purpose, 15 speakers were recorded and divided into groups according to age and gender. The criteria for selecting the speakers were that they had to be originally from Praznica, that they had lived there for the last 10 years, and that both of their parents were from Praznica. The material for recording consisted of an open list of topics for spontaneous spoken utterance. Words with the least influence of sentence intonation on the lexical tone were chosen for acoustic analysis in Praat. Frequency direction, frequency range, and duration as well as the tone level of the post-accented syllable were calculated and examined for vowels with different accents and phonological quantity. The results of the acoustic analysis show that the speakers from Praznica, apart from the expected three Cakavian accents and pre-accentual length, also have short-rising and long-rising accents and post-accentual length. No major differences were found in the prosodic system in terms of age or gender, except for greater tone differentiation among female speakers.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1108,THE FORMAL AND SEMANTIC PROPERTIES OF INTERROGATIVE SENTENCES IN ALBANIAN AND GERMAN USAGE,"This paper provides a comparison of two languages that are very distinct in grammar and pronunciation, respectively German and Albanian. Since research opportunities in comparative linguistics are infinite, this paper focuses on interrogative sentences and their sentence types. Through a detailed analysis of sentence examples of the grammar used in Albanian and German, we examine the similarities and differences related to different forms of interrogative sentences. As a framework for the classification of interrogative sentences in both languages, we have distinguished sentences into general interrogative sentences and complementary ones. In the end, the knowledge gained is summerised in a general overview. The aim of this work is to compare two languages that are very different in grammar, style and pronunciation, German and Albanian. Since the field of possible investigations in comparative linguistics is endless, the present work focuses on interrogative sentences and their respective linguistic forms. Through the detailed analysis of sentence examples, which are taken from common German and Albanian grammars, the similarities and differences in interrogative sentences are shown. The distinction between yes/no questions and questions that require a supplement serves as a regulatory framework. At the end of the work, the knowledge gained is summarized in an overview. Since these two Indo-European languages do not share major linguistic similarities, we hypothetically assume with Gerd-Dieter Nehring (2002) that on a grammatical level in the Albanian standard language (ASS) there is a basic system of marking tokens in contrast to German: ""The ASL (Albanian Standard language) agrees with German in terms of the division of parts of speech into word classes. However, there are also clear differences. In contrast to the article following the noun, the preceding article in Albanian has no determining function (e.g. topi i djalit - 'the boy's ball'), despite their common genetic origin"" (Nehring 2002, 51; our transl.). Detailed syntactical analysis demonstrates why the sentence order is flexible in German and relatively free in Albanian, a finding that could form the basis for further research on AlbanianGerman contrastive syntax. As an excerpt from a larger research project, this work provides insights into selected areas of German and Albanian grammar. The demonstration of connecting and disconnecting factors between both languages should thus be expanded and deepened. New insights are gained by combining various systematic linguistic methods with semantic and syntactic analysis. Due to the importance and the strong desire for good language skills, this research can be valuable for teachers as well as scholars of German. The aim of the study is to give an overview of and an insight into a still little researched and discussed part of the theory of interrogative sentences, mainly in the morphosyntax, to explain various examples in both languages and to present the variations of the interrogative sentences. An additional aim of this work is to examine the translations of literary works with regard to possible alternatives, or to clarify to what extent, e.g. the B. position and modes of verbs, intonation and melodic design, allow certain freedoms or, conversely, are bound by grammatical constraints. With regard to the analytical method used, an attempt is made to isolate the possible syntactic features of Albanian and German in order to bring them forward as relevant factors for distinguishing the interrogative sentences in the two languages. Through this analysis, we aim to show the differences between the two language systems, and as such, contribute to the language comparison between Albanian and German with regard to sentence types. By constantly referring to alternative forms of presentation and representation, knowledge about the linguistic variation of sentence types should also be increased. Because the structure of the sentences (and this includes not only the verb order in the sentence) significantly affects different types of sentences, when selecting the sentences to be examined, a large number of these factors had to be taken into account or included to a controllable extent. First, the selection of the sentences to be examined follows, as described above, the structural properties discussed in standard grammars. Second, it was also necessary to find examples that have content and linguistic relevance in both languages. And third, the semantic content of the sample sentences was also taken into account. Based on the research question of how the interrogative sentences in German and Albanian differ from each other, the following aspects and characteristics (which then guided the investigation) could be determined: (1) The sentence formation patterns in German and Albanian differ widely and require a specific focus on the respective semantic content. (2) The usual bracket formation in German does not occur in Albanian. However, what does it correspond to in this language? and (3) In both languages, the verb with its valences is the central element in the sentence. In German, the personal form of the verbal part is usually placed in the second position in the main clause (the declarative clause), but in the subordinate clause, it is usually placed at the end of the sentence. But there are also subordinate clauses with constant verb position, notably in the case of conditional clauses and indirect speech. This different rule for main and subordinate clauses does not apply in Albanian. So, while in German the position of the verbal parts changes in the subordinate clause, in Albanian this is the same as in the main clause. Accordingly, Albanian essentially has a verb in the second position. Another main goal of the project, albeit only an indirect one (through the practice of the research itself), is to show how a combination of traditional syntactic methods and experiments, which have so far only been used in other disciplines of linguistics, can also bring insights into sentence types that can be used in research. The interrogative sentence serves to express different types of interrogation. In German, a basic distinction is made between two large groups of question sentences: (1) supplementary question sentences or word question sentences and (2) decision question sentences or sentence question sentences. ""The decision question assumes that it is uncertain whether the description of the facts has or will have any relation to reality"" (Sommerfeldt/Starke 1989, 261; our transl.). On the other hand, in Albanian a distinction is made between supplementary interrogative sentences and decisive interrogative sentences, as well as direct and indirect interrogative sentences (main and subordinate clauses). According to specific modal aspects, the interrogative sentences are divided into neutral, dubitative, deliberative and polemical interrogation. Polemical interrogativity is expressed in interrogative and exclamatory sentences. Sentences with neutral interrogativity occur as decision and supplementary interrogative sentences (cf. Buchholz/Fiedler 1987, 498-9). Pitch progressions that fulfill the same linguistic functions form the realization level of the same intonation contours. There is only one indirect connection here. However, there is an agreement on the subject of interrogative sentences. Here almost all grammars agree on the classification of the following types of question sentences in both Albanian and German: decision questions, supplementary questions, alternative questions and echo questions. A further distinction is made between direct and indirect questions at the syntactic level (cf. Laposa 2003). Morphological, syntactic, and semantic criteria were created for the structure of the interrogative sentences, as they are also used as delimitation features in many other studies (cf. Buchholz/Fiedler 1987, Sommerlfeldt/Starke 1989, Duden 2009 and Helbig/Buscha 2007). A distinction is made between word questions and sentence questions, depending on whether a question word (English: ""Wh...?"") is used or not. The default answer to set questions is ""Ja"" or ""Nein"". The aim of this work was to examine the syntactic and semantic differences as well as the similarities between the interrogative sentences in Albanian and German, as there are still no corresponding results in the literature comparing the two languages. A distinction is made between two subgroups of questions: (1) the decision questions and (2) the supplementary questions. Interrogative sentences always expect a reaction from the other person, be it between speaker and listener or between writer and reader. There is usually a question mark at the end of the question sentence. Similarities and differences of the two languages in terms of the form of interrogative sentences: - Sentence questions have their own question particle ""a"" in Albanian, which is used frequently, but not exclusively. The declarative sentence can be changed in both languages by placing a question mark and changing the intonation to a decisive question. The Albanian interrogative particle "" a"" is either omitted or added in this case. - In Albanian, interrogative sentences are divided into neutral, dubitative, deliberative and polemical interrogation according to specific modal aspects. Apart from the specific intonation, the neutral decision question does not require any special elements, but the particle ""a"", which has a morphological status, can express interrogation (cf. Buchholz/Fiedler 1987, 498). - The negative word ""nicht"" can be used in both languages in the decision question sentence. The German ""nicht"" corresponds to the Albanian ""jo"". - In German, a typical form of interrogative sentences is distinguished, namely the ""W-interrogative sentences"". - In German, the conjunction ""oder"" (""apo, ose"") is then added. The word ""not"" comes after the conjunction ""oder"" at the end of the sentence. In Albanian, on the other hand, the conjunction does not appear at the end of the sentence. Kommt Martha morgen oder nicht? - A vjen neser Martha apo jo? - In the German decision question, the predicate is placed in front. In Albanian, the decision interrogative sentence differs from the declarative sentence only by the question particle ""a"" or by the use of rising intonation. In the Albanian question-and-answer sentence, the question particle ""a"" is at the beginning of the sentence. - The declarative sentence and the decision question sentence differ by the question particle ""a"", ""mos valle"", ""a mos valle"" and the question mark or the point. On the other hand, in a German question-and-answer sentence, the finite verb takes the first position, the top of the sentence (a Verberst interrogative sentence or a Verberst interrogative sentence). This syntactic question type is particularly often combined with the falling intonation contour (a question with a falling accent). - The word order in the question sentence is different in Albanian and German. In German, the verb usually comes first, followed by the subject and the other parts of the sentence. In Albanian, on the other hand, decision-making questions have a different structure. A change from the declarative sentence to decision questions is easier in German. Du isst. Ti han (Declarative sentence); Isst du? A po han ti? (decision question). - In the Albanian example sentences, the question particle ""a (po)"" stands out as the main difference. Here, a normal decision question consists of a statement and a question particle. Spielst du jeden Tag Fu ss ball? - A luan ti cdo dite futboll? The position of the clauses does not change in this case. The finite verb is found at the beginning of the sentence. The intonation is interrogative in both languages. - In Albanian there are two ways of expressing the normal decision question, but they are equivalent. Bist du in der Fabrik? - A je ti ne fabrike? / Je ne fabrike ti? In Albanian, the alternative question is marked by the order V+O+S. - In German, in addition to the answers ""ja"" and ""nein"", there is also the answer option ""doch""= ""ja"". Something similar exists in Albanian with por, vetem, megjithate. In decision questions with a negation word, the relationship between agreement and negation changes in the answer. Seid ihr mit euren Arbeitsbedingungen nicht zufrieden? - A nuk jeni ju te kenaqur me kushtet e punes? - Nein, wir sind nicht glucklich. (Bestatigung) - Jo, ne nuk jemi te lumtur. - Doch, wir sind glucklich. (Verneinung) - Megjithate, ne jemi te lumtur. - In Albanian, the answers ""ja"" und ""nein"" are also used. One answers in the affirmative in Albanian by repeating the verb of the decision question sentence. In Albanian, on the other hand, the negative word for ""not"" - ""a mos"" - is added before the verb when negating. Gehst du? - A shkon ti? / A mos po shkon ti? - [Ja], gehen wir. - Po, ne po shkojme. (Bejahung) // [Nein], gehen wir nicht. - Jo, ne nuk po shkojme (Verneinung). - Tag questions serve to ask whether something has been understood correctly or is still valid. In German, the tag question always requires the particle ""doch"". In Albanian, a particle (""apo"", ""po si"") is also used and placed at the end of the sentence. However, the intonation for the tag question is different. Du sprichst doch Deutsch? - Ti po flet gjermanisht, apo? Not least because of migration and cultural exchange, German and Albanian have a fruitful relationship with each other. The present analysis of the different forms of interrogative sentences in both languages can be used by language learners as well as linguists for further research. Without claiming to be exhaustive, the analyzes available so far form a first insight into and an overview of the various interrogative sentences and their respective constructions in general, as well as with regard to their roles in the Albanian and German languages in particular. In the best case, the work should serve as a basis for teaching and learning materials and close a gap in the research field of contrastive linguistics.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1109,Sketching women: a corpus-based approach to representations of women's agency in political Internet corpora in Arabic and English,"In this paper, I use methods from corpus linguistics to examine patterns pertaining to the representation of women in online Arabic- and English-language political corpora. I highlight the discursive differences and similarities that characterise the two corpora. Using word sketches, I identify representational categories in each corpus that are indexed by patterns of collocation. Analysis of semantic preference and prosody in each corpus reveals the ways in which women are represented. An exploration of the representations of women and gendered agency in both corpora reveals incongruities between the message of women's empowerment that the outlets promote and the implicit discursive representations of gender and gendered agency.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1110,The representation of migrants in Spanish judicial decisions: using corpus data to refute hate speech,"The phenomenon of immigration and its depiction in media texts have been examined profusely within the field of corpus-based discourse analysis (Gabrielatos and Baker, 2008; Baker et al., 2013; and Blinder and Allen, 2016). This research seeks to present it as reflected in a corpus of 600 judicial decisions issued by Spanish courts in the years 2016 and 2017. This analysis was motivated by the rise of extreme right-wing parties in Europe in recent years. Such parties dehumanise immigrants and portray them as a threat to the welfare state. On first examination, the results appear to dissociate immigration and crime since a considerable percentage of the keywords obtained (about 20 percent) revolves around three major topoi (namely, `family', `territory/access' and `legal punishment') and there is no evidence of any major offences or crimes amongst the top-ranking lexicon. The study of the collocate networks of the keywords within the category `legal punishment' confirms our initial perception; in fact, out of twentyone collocates, only the word delito (`crime') itself collocates with terms referring to typified crimes such as violencia (`violence'). In parallel, the data were triangulated using the text-classification software UMUTextStats (Garcia-Diaz et al., 2018). The results of this second analysis also confirm our initial observations.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1111,Exploring the Complexity of the L2 Intonation System: An Acoustic and Eye-Tracking Study,"Phonological research has demonstrated that English intonation, variably referred to as prosody, is a multidimensional and multilayered system situated at the interface of information structure, morphosyntactic structure, phonological phenomena, and pragmatic functions. The structural and functional complexity of the intonational system, however, is largely under-addressed in L2 pronunciation teaching, leading to a lack of spontaneous use of intonation despite successful imitation in classrooms. Focusing on contrastive and implicational sentence stress, this study explored the complexity of the English intonation system by investigating how L1 English and Mandarin-English L2 speakers use multiple acoustic features (i.e., pitch range, pitch level, duration, and intensity) in signaling contrastive and implicational information and how one acoustic feature (maximum pitch level) is affected by information structure (contrast), morphosyntactic structure (phrasal boundary), and a phonological phenomenon (declination) in L1 English and Mandarin-English L2 speakers' speech. Using eye-tracking technology, we also investigated (1) L1 English and Mandarin-English L2 speakers' real-time processing of lexical items that carry information structure (i.e., contrast) and typically receive stress in L1 speakers' speech; (2) the influence of visual enhancement (italics and bold) on L1 English and Mandarin-English L2 speakers' processing of contrastive information; and (3) L1 English and Mandarin-English L2 speakers' processing of pictures with contrastive information. Statistical analysis using linear mixed-effects models showed that L1 English speakers and Mandarin-English L2 speakers differed in their use of acoustic cues in signaling contrastive and implicational information. They also differed in the use of maximum pitch level in signaling sentence stress influenced by contrast, phrasal boundary, and declination. We did not find differences in L1 English and Mandarin-English L2 speakers' processing of contrastive and implicational information at the sentence level, but the two groups of participants differ in their processing of contrastive information in passages and pictures. These results suggest that processing limitations may be the reason why L2 speakers did not use English intonation spontaneously. The findings of this study also suggest that Complexity Theory (CT), which emphasizes the complex and dynamic nature of intonation, is a theoretical framework that has the potential of bridging the gap between L2 phonology and L2 pronunciation teaching.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1112,Advanced Second Language Learners of Mandarin Show Persistent Deficits for Lexical Tone Encoding in Picture-to-Word Form Matching,"People who grow up speaking a language without lexical tones typically find it difficult to master tonal languages after childhood. Accumulating research suggests that much of the challenge for these second language (L2) speakers has to do not with identification of the tones themselves, but with the bindings between tones and lexical units. The question that remains open is how much of these lexical binding problems are problems of encoding (incomplete knowledge of the tone-to-word relations) vs. retrieval (failure to access those relations in online processing). While recent work using lexical decision tasks suggests that both may play a role, one issue is that failure on a lexical decision task may reflect a lack of learner confidence about what is not a word, rather than non-native representation or processing of known words. Here we provide complementary evidence using a picture-phonology matching paradigm in Mandarin in which participants decide whether or not a spoken target matches a specific image, with concurrent event-related potential (ERP) recording to provide potential insight into differences in L1 and L2 tone processing strategies. As in the lexical decision case, we find that advanced L2 learners show a clear disadvantage in accurately identifying tone mismatched targets relative to vowel mismatched targets. We explore the contribution of incomplete/uncertain lexical knowledge to this performance disadvantage by examining individual data from an explicit tone knowledge post-test. Results suggest that explicit tone word knowledge and confidence explains some but not all of the errors in picture-phonology matching. Analysis of ERPs from correct trials shows some differences in the strength of L1 and L2 responses, but does not provide clear evidence toward differences in processing that could explain the L2 disadvantage for tones. In sum, these results converge with previous evidence from lexical decision tasks in showing that advanced L2 listeners continue to have difficulties with lexical tone recognition, and in suggesting that these difficulties reflect problems both in encoding lexical tone knowledge and in retrieving that knowledge in real time.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1113,Perception of Intonation on Neutral Tone in Mandarin,"In Mandarin, lexical tone has been found to interact with intonational tone to influence intonation perception, with the falling T4 facilitating the perception of the statement/question contrast the most, and the rising T2 the least. However, in addition to the four citation tones T1-T4, Mandarin has ""neutral tone"" which marks weak, non-initial syllables that do not carry a citation tone. The prevailing view is that neutral tone is, in fact, phonologically toneless. It is unknown whether neutral tone can also affect intonation perception. However, it is reasonable to hypothesize that if neutral tone is indeed toneless, it cannot interact with intonational tone in the same way as citation tones do. We investigated this novel hypothesis with a perception experiment in which 22 Mandarin speakers had to determine whether disyllabic citation tone and neutral tone words were a question or statement. Results show that the identification of intonation contours is more accurate for neutral tone than for T2, and similarly accurate for neutral tone and T4, regardless of whether the neutral tone is intrinsic or derived. Furthermore, both T4 and neutral tone are realized with a reduced pitch range at a higher pitch level in questions, unlike T2, which is characterized by a slightly expanded pitch range and a higher pitch level. It is possible that intonation perception in Mandarin is facilitated by changes in the phonetic shapes of lexical tones brought by intonation rather than the phonological interaction between lexical tones and intonation. The importance of pitch changes to the intonation perception in Mandarin was further tested in a second perception experiment with the same 22 participants and disyllabic stimuli with manipulated pitch level and range. Results indicate that the use of pitch cues in intonation perception shows tone-specific differences, namely, pitch range is more important in signaling the question/statement contrast in utterances ending with T4 or neutral tone, while pitch level is the only perceptual cue to interrogativity for utterances ending in T2.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1114,The dual role of post-stop fundamental frequency in the production and perception of stops in Mandarin-English bilinguals,"In non-tonal languages with a two-way laryngeal contrast, post-stop fundamental frequency (F0) tends to vary as a function of phonological voicing in stops, and listeners use it as a cue for stop voicing. In tonal languages, F0 is the most important acoustic correlate for tone, and listeners likewise rely heavily on F0 to differentiate tones. Given this ambiguity of F0 in its ability to signal phonological voicing and tone, how do speakers of a tonal language weight it in production and perception? Relatedly, do bilingual speakers of tonal and non-tonal languages use the same weights across different language contexts? To address these questions, the cross-linguistic performances from L1 (first language) Mandarin-L2 (second language) English bilinguals dominant in Mandarin in online production and perception experiments are compared. In the production experiment, the participant read aloud Mandarin and English monosyllabic words, the onsets of which typified their two-way laryngeal contrast. For the perception experiment, which utilized a forced-choice identification paradigm, both the English and Mandarin versions shared the same target audio stimuli, comprising monosyllables whose F0 contours were modeled after Mandarin Tone 1 and Tone 4, and whose onset was always a bilabial stop. The voice onset time of the bilabial stop and the onset F0 of the nucleus were manipulated orthogonally. The production results suggest that post-stop F0 following aspirated/voiceless stops was higher than that following unaspirated/voiced stops in both Mandarin and English production. However, the F0 difference in English was larger as compared to Mandarin, indicating that participants assigned more production weight to post-stop F0 in English than in Mandarin. On the perception side, participants used post-stop F0 as a cue in perceiving stops in both English and Mandarin, with higher post-stop F0 leading to more aspirated/voiceless responses, but they allocated more weight to post-stop F0 when interpreting audio stimuli as English words than as Mandarin words. Overall, these results argue for a dual function of F0 in cueing phonological voicing in stops and lexical tone across production and perception in Mandarin. Furthermore, they suggest that bilinguals are able to dynamically adjust even a secondary cue according to different language contexts.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1115,Written representation of spoken interaction in the official parliamentary transcripts of the Finnish Parliament,"In this article, I will analyze the written representation of spoken interaction in the official plenary session transcripts of the Finnish Parliament. The official parliamentary transcripts are not-and cannot be-identical copies of the original speech event. Instead, they are linguistically and textually edited in many ways. I will examine the different types of editorial changes that are made in the official Finnish parliamentary transcripts. These include phonological, morphological, and syntactic alterations, editing out of self-repairs, planning expressions, stuttering and slips-of-tongue, and finding written ways of expression for phenomena such as pauses, prosody, gestures, and non-verbal events. I will also discuss how the editorial changes affect the written representation of plenary session interaction.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1116,Different facial cues for different speech styles in Mandarin tone articulation,"Visual facial information, particularly hyperarticulated lip movements in clear speech, has been shown to benefit segmental speech perception. Little research has focused on prosody, such as lexical tone, presumably because production of prosody primarily involves laryngeal activities not necessarily distinguishable through visible articulatory movements. However, there is evidence that head, eyebrow, and lip movements correlate with production of pitch-related variations. One subsequent question is whether such visual cues are linguistically meaningful. In this study, we compare movements of the head, eyebrows and lips associated with plain (conversational) vs. clear speech styles of Mandarin tone articulation to examine the extent to which clear-speech modifications involve signal-based overall exaggerated facial movements or code-based enhancement of linguistically relevant articulatory movements. Applying computer-vision techniques to recorded speech, visible movements of the frontal face were tracked and measured for 20 native Mandarin speakers speaking in two speech styles: plain and clear. Thirty-three head, eyebrow and lip movement features based on distance, time, and kinematics were extracted from each individual tone word. A random forest classifier was used to identify the important features that differentiate the two styles across tones and for each tone. Mixed-effects models were then performed to determine the features that were significantly different between the two styles. Overall, for all the four Mandarin tones, we found longer duration and greater movements of the head, eyebrows, and lips in clear speech than in plain speech. Additionally, across tones, the maximum movement happened relatively earlier in clear than plain speech. Although limited evidence of tone-specific modifications was also observed, the cues involved overlap with signal-based changes. These findings suggest that visual facial tonal modifications for clear speech primarily adopt signal-based general emphatic cues that strengthen signal saliency.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1117,Revered and reviled: a sentiment analysis of female and male referents in three languages,"Our study contributes to the less explored domain of lexical typology, focusing on semantic prosody and connotation. Semantic derogation, or pejoration of nouns referring to women, whereby such words acquire connotations and further denotations of social pejoration, immorality and/or loose sexuality, has been a very prominent question in studies on gender and language (change). It has been argued that pejoration emerges due to the general derogatory attitudes toward female referents. However, the evidence for systematic differences in connotations of female- vs. male-related words is fragmentary and often fairly impressionistic; moreover, many researchers argue that expressed sentiments toward women (as well as men) often are ambivalent. One should also expect gender differences in connotations to have decreased in the recent years, thanks to the advances of feminism and social progress. We test these ideas in a study of positive and negative connotations of feminine and masculine term pairs such as woman - man, girl - boy, wife - husband, etc. Sentences containing these words were sampled from diachronic corpora of English, Chinese and Russian, and sentiment scores for every word were obtained using two systems for Aspect-Based Sentiment Analysis: PyABSA, and OpenAI's large language model GPT-3.5. The Generalized Linear Mixed Models of our data provide no indications of significantly more negative sentiment toward female referents in comparison with their male counterparts. However, some of the models suggest that female referents are more infrequently associated with neutral sentiment than male ones. Neither do our data support the hypothesis of the diachronic convergence between the genders. In sum, results suggest that pejoration is unlikely to be explained simply by negative attitudes to female referents in general.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1118,Multimodal cues in L2 lexical tone acquisition: current research and future directions,"This review discusses the effectiveness of visual and haptic cues for second language (L2) lexical tone acquisition, with a special focus on observation and production of hand gestures. It explains how these cues can facilitate initial acquisition of L2 lexical tones via multimodal depictions of pitch. In doing so, it provides recommendations for incorporation of multimodal cues into L2 lexical tone pedagogy.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1119,Investigating the variation of intonation contours in Northern Vietnamese tones,"Intonation is an instrument for structuring discourse and emphasizing different types of information. In German, for example, pitch is used to highlight focus, while in Vietnamese, different pitch contours distinguish lexical tones. As of yet, the interplay between intonation and lexical tone in relation to information structure has not been sufficiently investigated across languages. Vietnamese has six lexical tones and is particularly interesting for investigating the influence of different intonation strategies on the realization of tones. Here, we present a production study with 70 Northern Vietnamese speakers. The participants read six sentences under two conditions. In each sentence, a word occurring in the final position of the sentence and carrying one of the six tones was pronounced in two different discourse contexts. Acoustic analyses of the intonation contours showed that Vietnamese speakers realized the words with significant differences in pitch at the onset. Yet, the strategies for raising or lowering the pitch varied depending on the tone. Our results show the use of prosodic cues in a complex tone system across a large number of speakers. In addition, the study can serve as a starting point for educational programs that include training on intonation patterns in specific contexts.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1120,Tone superimposition technique in Speech Sciences: a tutorial,"In the literature, we encounter papers reporting manipulating pitch contours in speech tokens for a specific problem to be addressed in experiments (e.g., learning pitch patterns superimposed onto a pseudo-syllable), usually in the field of Speech Perception and Spoken Word Recognition. This type of research often tests listeners' perceptual and processing skills in tonal languages (e.g., Mandarin, Thai, etc.), and requires superimposing a pitch contour onto a spoken syllable. However, very few studies reported in detail how this critical manipulation was done to meet specific experimental needs. In addition, there was neither specific guideline or description of the techniques being used, nor how 'natural' these manipulated tokens sounded in a particular language upon speech synthesis. Because this technique is crucial in establishing the conclusions in various studies, here, we will demonstrate our method of establishing this technique of tone superimposition (i.e., lexical tones in Mandarin) onto English syllables. In line with the open science model, we will also show our stimuli and procedures via OSF for readers to evaluate the validity of this technique. Manipulating the pitch contour in a spoken syllable can be complicated and change the perception of the spoken syllable in a significant way. Thus, we will also show the important factors to be considered in this process for doing research in Speech Sciences.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1121,"Unreal words, real competition: Mandarin recognition slows for syllable-matched tonal gaps","Word recognition in tone languages like Mandarin is influenced not only by phonological structure but also by lexical tone. Prior research using auditory lexical decision tasks has shown that real monosyllables are generally processed more quickly and accurately than tonal gaps i.e., impossible syllable-tone combinations. However, these studies often did not control for syllable overlap between tonal gaps and real monosyllables, potentially underestimating lexical competition effects. The present study addressed this gap by contrasting real monosyllables, syllable-matched tonal gaps, and syllable-unmatched tonal gaps in a controlled auditory lexical decision task with 54 native Mandarin speakers. Results revealed that reaction times were significantly faster and accuracy higher for real monosyllables compared to both syllable-matched tonal gaps and syllable-unmatched tonal gaps. More importantly, syllable-matched tonal gaps elicited slower reaction times than syllable-unmatched tonal gaps, indicating increased lexical competition when tonal gaps share the same syllable as real monosyllables. These findings emphasize the critical role of phonological similarity and lexical competition in Mandarin word recognition. By controlling syllabic overlap, this study improves upon previous methodologies and offers a clearer assessment of auditory word processing in tone languages.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1122,Voice Pitch Elicited Frequency Following Response in Chinese Elderlies,"Background: Perceptual and electrophysiological studies have found reduced speech discrimination in quiet and noisy environment, delayed neural timing, decreased neural synchrony, and decreased temporal processing ability in elderlies, even those with normal hearing. However, recent studies have also demonstrated that language experience and auditory training enhance the temporal dynamics of sound encoding in the auditory brainstem response (ABR). The purpose of this study was to explore the pitch processing ability at the brainstem level in an aging population that has a tonal language background. Method: Mandarin speaking younger (n = 12) and older (n = 12) adults were recruited for this study. All participants had normal audiometric test results and normal suprathreshold click-evoked ABR. To record frequency following responses (FFRs) elicited by Mandarin lexical tones, two Mandarin Chinese syllables with different fundamental frequency pitch contours (Flat Tone and Falling Tone) were presented at 70 dB SPL. Fundamental frequencies (f0) of both the stimulus and the responses were extracted and compared to individual brainstem responses. Two indices were used to examine different aspects of pitch processing ability at the brainstem level: Pitch Strength and Pitch Correlation. Results: Lexical tone elicited FFR were overall weaker in the older adult group compared to their younger adult counterpart. Measured by Pitch Strength and Pitch Correlation, statistically significant group differences were only found when the tone with a falling f0 (Falling Tone) were used as the stimulus. Conclusion: Results of this study demonstrated that in a tonal language speaking population, pitch processing ability at the brainstem level of older adults are not as strong and robust as their younger counterparts. Findings of this study are consistent with previous reports on brainstem responses of older adults whose native language is English. On the other hand, lexical tone elicited FFRs have been shown to correlate with the length of language exposure. Older adults' degraded responses in our study may also be due to that, the Mandarin speaking older adults' long term exposure somewhat counteracted the negative impact on aging and helped maintain, or at least reduced, the degradation rate in their temporal processing capacity at the brainstem level.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1123,Effects of Inter-Stimulus Interval on Speech-Evoked Frequency-Following Response in Elderly Adults,"Background: The speech-evoked frequency following response (FFR) has shown to be useful in assessing complex auditory processing abilities and in different age groups. While many aspects of FFR have been studied extensively, the effect of timing, as measured by inter-stimulus-interval (ISI), especially in the older adult population, has yet to be thoroughly investigated. Objective: The purpose of this study was to examine the effects of different ISIs on speech evoked FFR in older and younger adults who speak a tonal language, and to investigate whether the older adults' FFR were more susceptible to the change in ISI. Materials and Methods: Twenty-two normal hearing participants were recruited in our study, including 11 young adult participants and 11 elderly participants. An Intelligent Hearing Systems Smart EP evoke potential system was used to record the FFR in four ISI conditions (40, 80, 120 and 160 ms). A recorded natural speech token with a falling tone /yi/ was used as the stimulus. Two indices, stimulus-to-response correlation coefficient and pitch strength, were used to quantify the FFR responses. Two-way analysis of variance (ANOVA) was used to analyze the differences in different age groups and different ISI conditions. Results: There was no significant difference in stimulus-to-response correlation coefficient and pitch strength among the different ISI conditions, in either age groups. Older adults appeared to have weaker FFR for all ISI conditions when compared to their younger adult counterparts. Conclusion: Shorter ISIs did not result in worse FFRs from older adults or younger adults. For speech-evoked FFR using a recorded natural speech token that is 250 ms in length, an ISI of as short as 40 ms appeared to be sufficient and effective to record FFR for elderly adults.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1124,Temporal relation between top-down and bottom-up processing in lexical tone perception,"Speech perception entails both top-down processing that relies primarily on language experience and bottom-up processing that depends mainly on instant auditory input. Previous models of speech perception often claim that bottom-up processing occurs in an early time window, whereas top-down processing takes place in a late time window after stimulus onset. In this paper, we evaluated the temporal relation of both types of processing in lexical tone perception. We conducted a series of event-related potential (ERR) experiments that recruited Mandarin participants and adopted three experimental paradigms, namely dichotic listening, lexical decision with phonological priming, and semantic violation. By systematically analyzing the lateralization patterns of the early and late ERR components that are observed in these experiments, we discovered that: auditory processing of pitch variations in tones, as a bottom-up effect, elicited greater right hemisphere activation; in contrast, linguistic processing of lexical tones, as a top-down effect, elicited greater left hemisphere activation. We also found that both types of processing co-occurred in both the early (around 200 ms) and late (around 300-500 ms) time windows, which supported a parallel model of lexical tone perception. Unlike the previous view that language processing is special and performed by dedicated neural circuitry, our study have elucidated that language processing can be decomposed into general cognitive functions (e.g., sensory and memory) and share neural resources with these functions.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1125,Spectro-Temporal Processing in a Two-Stream Computational Model of Auditory Cortex,"Neural processing of sounds in the dorsal and ventral streams of the (human) auditory cortex is optimized for analyzing fine-grained temporal and spectral information, respectively. Here we use a Wilson and Cowan firing-rate modeling framework to simulate spectro-temporal processing of sounds in these auditory streams and to investigate the link between neural population activity and behavioral results of psychoacoustic experiments. The proposed model consisted of two core (A1 and R, representing primary areas) and two belt (Slow and Fast, representing rostral and caudal processing respectively) areas, differing in terms of their spectral and temporal response properties. First, we simulated the responses to amplitude modulated (AM) noise and tones. In agreement with electrophysiological results, we observed an area-dependent transition from a temporal (synchronization) to a rate code when moving from low to high modulation rates. Simulated neural responses in a task of amplitude modulation detection suggested that thresholds derived from population responses in core areas closely resembled those of psychoacoustic experiments in human listeners. For tones, simulated modulation threshold functions were found to be dependent on the carrier frequency. Second, we simulated the responses to complex tones with missing fundamental stimuli and found that synchronization of responses in the Fast area accurately encoded pitch, with the strength of synchronization depending on number and order of harmonic components. Finally, using speech stimuli, we showed that the spectral and temporal structure of the speech was reflected in parallel by the modeled areas. The analyses highlighted that the Slow stream coded with high spectral precision the aspects of the speech signal characterized by slow temporal changes (e.g., prosody), while the Fast stream encoded primarily the faster changes (e.g., phonemes, consonants, temporal pitch). Interestingly, the pitch of a speaker was encoded both spatially (i.e., tonotopically) in Slow area and temporally in Fast area. Overall, performed simulations showed that the model is valuable for generating hypotheses on how the different cortical areas/streams may contribute toward behaviorally relevant aspects of auditory processing. The model can be used in combination with physiological models of neurovascular coupling to generate predictions for human functional MRI experiments.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1126,Hippocampal temporal-parietal junction interaction in the production of psychotic symptoms: a framework for understanding the schizophrenic syndrome,"A framework is described for understanding the schizophrenic syndrome at the brain systems level. It is hypothesized that over-activation of dynamic gesture and social perceptual processes in the temporal-parietal occipital junction (TPJ), posterior superior temporal sulcus (PSTS) and surrounding regions produce the syndrome (including positive and negative symptoms, their prevalence, prodromal signs, and cognitive deficits). Hippocampal system hyper-activity and atrophy have been consistently found in schizophrenia. Hippocampal activity is highly correlated with activity in the TPJ and may be a source of over-excitation of the TPJ and surrounding regions. Strong evidence for this comes from in-vivo recordings in humans during psychotic episodes. Many positive symptoms of schizophrenia can be reframed as the erroneous sense of a presence or other who is observing, acting, speaking, or controlling; these qualia are similar to those evoked during abnormal activation of the TPJ. The TPJ and PSTS play a key role in the perception (and production) of dynamic social, emotional, and attentional gestures for the self and others (e.g., body/face/eye gestures, audiovisual speech and prosody, and social attentional gestures such as eye gaze). The single cell representation of dynamic gestures is multimodal (auditory, visual, tactile), matching the predominant hallucinatory categories in schizophrenia. Inherent in the single cell perceptual signal of dynamic gesture representations is a computation of intention, agency, and anticipation or expectancy (for the self and others). Stimulation of the TPJ resulting in activation of the self representation has been shown to result a feeling of a presence or multiple presences (due to heautoscopy) and also bizarre tactile experiences. Neurons in the TPJ are also tuned, or biased to detect threat related emotions. Abnormal over-activation in this system could produce the conscious hallucination of a voice (audiovisual speech), a person or a touch. Over-activation could interfere with attentional/emotional gesture perception and production (negative symptoms). It could produce the unconscious feeling of being watched, followed, or of a social situation unfolding along with accompanying abnormal perception of intent and agency (delusions). Abnormal activity in the TPJ would also be predicted to create several cognitive disturbances that are characteristic of schizophrenia, including abnormalities in attention, predictive social processing, working memory, and a bias to erroneously perceive threat.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1127,Voices to reckon with: perceptions of voice identity in clinical and non-clinical voice hearers,"The current review focuses on the perception of voice identity in clinical and non-clinical voice hearers. Identity perception in auditory verbal hallucinations (AVH) is grounded in the mechanisms of human (i.e., real, external) voice perception, and shapes the emotional (distress) and behavioral (help-seeking) response to the experience. Yet, the phenomenological assessment of voice identity is often limited, for example to the gender of the voice, and has failed to take advantage of recent models and evidence on human voice perception. In this paper we aim to synthesize the literature on identity in real and hallucinated voices and begin by providing a comprehensive overview of the features used to judge voice identity in healthy individuals and in people with schizophrenia. The findings suggest some subtle, but possibly systematic biases across different levels of voice identity in clinical hallucinators that are associated with higher levels of distress. Next we provide a critical evaluation of voice processing abilities in clinical and non-clinical voice hearers, including recent data collected in our laboratory. Our studies used diverse methods, assessing recognition and binding of words and voices in memory as well as multidimensional scaling of voice dissimilarity judgments. The findings overall point to significant difficulties recognizing familiar speakers and discriminating between unfamiliar speakers in people with schizophrenia, both with and without AVH. In contrast, these voice processing abilities appear to be generally intact in non-clinical hallucinators. The review highlights some important avenues for future research and treatment of AVH associated with a need for care, and suggests some novel insights into other symptoms of psychosis.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1128,Brainstem encoding of speech and musical stimuli in congenital amusia: evidence from Cantonese speakers,"Congenital amusia is a neurodevelopmental disorder of musical processing that also impacts subtle aspects of speech processing. It remains debated at what stage(s) of auditory processing deficits in amusia arise. In this study, we investigated whether amusia originates from impaired subcortical encoding of speech On quiet and noise) and musical sounds in the brainstem. Fourteen Cantonese-speaking amusics and 14 matched controls passively listened to six Cantonese lexical tones in quiet, two Cantonese tones in noise (signal-to-noise ratios at 0 and 20 dB), and two cello tones in quiet while their frequency-following responses (FFRs) to these tones were recorded. All participants also completed a behavioral lexical tone identification task. The results indicated normal brainstem encoding of pitch in speech On quiet and noise) and musical stimuli in amusics relative to controls, as measured by FFR pitch strength, pitch error, and stimulus-to-response correlation. There was also no group difference in neural conduction time or FFR amplitudes. Both groups demonstrated better FFRs to speech On quiet and noise) than to musical stimuli. However, a significant group difference was observed for tone identification, with amusics showing significantly lower accuracy than controls. Analysis of the tone confusion matrices suggested that amusics were more likely than controls to confuse between tones that shared similar acoustic features. Interestingly, this deficit in lexical tone identification was not coupled with brainstem abnormality for either speech or musical stimuli. Together, our results suggest that the amusic brainstem is not functioning abnormally, although higher-order linguistic pitch processing is impaired in amusia. This finding has significant implications for theories of central auditory processing, requiring further investigations into how different stages of auditory processing interact in the human brain.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1129,A Meta-Analytic Study of the Neural Systems for Auditory Processing of Lexical Tones,"The neural systems of lexical tone processing have been studied for many years. However, previous findings have been mixed with regard to the hemispheric specialization for the perception of linguistic pitch patterns in native speakers of tonal language. In this study, we performed two activation likelihood estimation (ALE) metaanalyses, one on neuroimaging studies of auditory processing of lexical tones in tonal languages (17 studies), and the other on auditory processing of lexical information in non-tonal languages as a control analysis for comparison (15 studies). The lexical tone ALE analysis showed significant brain activations in bilateral inferior prefrontal regions, bilateral superior temporal regions and the right caudate, while the control ALE analysis showed significant cortical activity in the left inferior frontal gyrus and left temporoparietal regions. However, we failed to obtain significant differences from the contrast analysis between two auditory conditions, which might be caused by the limited number of studies available for comparison. Although the current study lacks evidence to argue for a lexical tone specific activation pattern, our results provide clues and directions for future investigations on this topic, more sophisticated methods are needed to explore this question in more depth as well.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1130,Dynamic Facial Expressions Prime the Processing of Emotional Prosody,"Evidence suggests that emotion is represented supramodally in the human brain. Emotional facial expressions, which often precede vocally expressed emotion in real life, can modulate event-related potentials (N100 and P200) during emotional prosody processing. To investigate these cross-modal emotional interactions, two lines of research have been put forward: cross-modal integration and cross-modal priming. In cross-modal integration studies, visual and auditory channels are temporally aligned, while in priming studies they are presented consecutively. Here we used cross-modal emotional priming to study the interaction of dynamic visual and auditory emotional information. Specifically, we presented dynamic facial expressions (angry, happy, neutral) as primes and emotionally-intoned pseudo-speech sentences (angry, happy) as targets. We were interested in how prime-target congruency would affect early auditory event-related potentials, i.e., N100 and P200, in order to shed more light on how dynamic facial information is used in cross-modal emotional prediction. Results showed enhanced N100 amplitudes for incongruently primed compared to congruently and neutrally primed emotional prosody, while the latter two conditions did not significantly differ. However, N100 peak latency was significantly delayed in the neutral condition compared to the other two conditions. Source reconstruction revealed that the right parahippocampal gyrus was activated in incongruent compared to congruent trials in the N100 time window. No significant ERP effects were observed in the P200 range. Our results indicate that dynamic facial expressions influence vocal emotion processing at an early point in time, and that an emotional mismatch between a facial expression and its ensuing vocal emotional signal induces additional processing costs in the brain, potentially because the cross-modal emotional prediction mechanism is violated in case of emotional prime-target incongruency.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1131,On the representation of hierarchical structure: Revisiting Darwin's musical protolanguage,"In this article, we address the tenability of Darwin's musical protolanguage, arguing that a more compelling evolutionary scenario is one where a prosodic protolanguage is taken to be the preliminary step to represent the hierarchy involved in linguistic structures within a linear auditory signal. We hypothesize that the establishment of a prosodic protolanguage results from an enhancement of a rhythmic system that transformed linear signals into speech prosody, which in turn can mark syntactic hierarchical relations. To develop this claim, we explore the role of prosodic cues on the parsing of syntactic structures, as well as neuroscientific evidence connecting the evolutionary development of music and linguistic capacities. Finally, we entertain the assumption that the capacity to generate hierarchical structure might have developed as part of tool-making in human prehistory, and hence was established prior to the enhancement of a prosodic protolinguistic system.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1132,Language experience during the sensitive period narrows infants' sensory encoding of lexical tones-Music intervention reverses it,"The sensitive period for phonetic learning (6 similar to 12 months), evidenced by improved native speech processing and declined non-native speech processing, represents an early milestone in language acquisition. We examined the extent that sensory encoding of speech is altered by experience during this period by testing two hypotheses: (1) early sensory encoding of non-native speech declines as infants gain native-language experience, and (2) music intervention reverses this decline. We longitudinally measured the frequency-following response (FFR), a robust indicator of early sensory encoding along the auditory pathway, to a Mandarin lexical tone in 7- and 11-months-old monolingual English-learning infants. Infants received either no intervention (language-experience group) or music intervention (music-intervention group) randomly between FFR recordings. The language-experience group exhibited the expected decline in FFR pitch-tracking accuracy to the Mandarin tone, while the music-intervention group did not. Our results support both hypotheses and demonstrate that both language and music experiences alter infants' speech encoding.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1133,Multi-modal cross-linguistic perception of Mandarin tones in clear speech,"Clearly enunciated speech (relative to conversational, plain speech) involves articulatory and acoustic modifications that enhance auditory-visual (AV) segmental intelligibility. However, little research has explored clear-speech effects on the perception of suprasegmental properties such as lexical tone, particularly involving visual (facial) perception. Since tone production does not primarily rely on vocal tract configurations, tones may be less visually distinctive. Questions thus arise as to whether clear speech can enhance visual tone intelligibility, and if so, whether any intelligibility gain can be attributable to tone-specific category-enhancing (code-based) clear-speech cues or tone-general saliency-enhancing (signal-based) cues. The present study addresses these questions by examining the identification of clear and plain Mandarin tones with visual-only, auditory-only, and AV input modalities by native (Mandarin) and nonnative (English) perceivers. Results show that code-based visual and acoustic clear tone modifications, although limited, affect both native and nonnative intelligibility, with category-enhancing cues increasing intelligibility and category-blurring cues decreasing intelligibility. In contrast, signal-based cues, which are extensively available, do not benefit native intelligibility, although they contribute to nonnative intelligibility gain. These findings demonstrate that linguistically relevant visual tonal cues are existent. In clear speech, such tone category-enhancing cues are incorporated with saliency-enhancing cues across AV modalities for intelligibility improvements.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1134,Intonation processing of interrogative words in Mandarin: an event-related potential study,"Intonation is the variation in pitch used in speech, which forms the premise of tonal and non-tonal languages. Interrogative words are words that introduce questions. Previous research lacks clarity regarding the specific cues used in the processing of word intonation. To address this gap, this study used the event-related potential electroencephalogram (EEG) research method to explore the intonation processing of tone two (mid-rising) interrogative words in Mandarin. For this, the word ""shui,"" meaning ""who,"" was selected as the experimental material. To avoid the influence of the environment, gender, and semantics, the Hum version, corresponding to the stimulus material, was also adopted for the experiment. This study used a passive oddball paradigm to examine the clues of intonation information processing in automatic cognitive processing through amplitude, latency, time window, and evoked location potential mismatch negativity. The standard stimulus was the declarative intonation with a high probability of occurrence (90%), and the deviant stimulus was the interrogative intonation with a low probability of occurrence (10%). In the time window of 370-450 ms, the mismatch negativity was found at the F3, F4, C3, Cz, and C4 channels. The findings show that, in the passive oddball paradigm, lexical semantics are essential for intonation processing at the pre-attentive level, which is dominated by the frontal and central areas of the brain. The results support the functional and comprehensive hypotheses that the processing of intonation is based on the function of language and that bilateral regions are involved in this processing. This study makes an important contribution by providing event-related potential evidence that lexical semantics plays a key role in the pre-attentive processing of intonation, as shown by the significant differences between semantic and non-semantic conditions.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1135,An acoustic model of speech dysprosody in patients with Parkinson's disease,"Purpose: This study aimed to determine the acoustic properties most indicative of dysprosody severity in patients with Parkinson's disease using an automated acoustic assessment procedure. Method: A total of 108 read speech recordings of 68 speakers with PD (45 male, 23 female, aged 65.0 +/- 9.8 years) were made with active levodopa treatment. A total of 40 of the patients were additionally recorded without levodopa treatment to increase the range of dysprosody severity in the sample. Four human clinical experts independently assessed the patients' recordings in terms of dysprosody severity. Separately, a speech processing pipeline extracted the acoustic properties of prosodic relevance from automatically identified portions of speech used as utterance proxies. Five machine learning models were trained on 75% of speech portions and the perceptual evaluations of the speaker's dysprosody severity in a 10-fold cross-validation procedure. They were evaluated regarding their ability to predict the perceptual assessments of recordings excluded during training. The models' performances were assessed by their ability to accurately predict clinical experts' dysprosody severity assessments. Results: The acoustic predictors of importance spanned several acoustic domains of prosodic relevance, with the variability in f(o) change between intonational turning points and the average first Mel-frequency cepstral coefficient at these points being the two top predictors. While predominant in the literature, variability in utterance-wide f(o) was found to be only the fifth strongest predictor. Conclusion: Human expert raters' assessments of dysprosody can be approximated by the automated procedure, affording application in clinical settings where an experienced expert is unavailable. Variability in pitch does not adequately describe the level of dysprosody due to Parkinson's disease.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1136,Deficits in the Sensitivity to Pitch Sweeps by School-Aged Children Wearing Cochlear Implants,"Sensitivity to static changes in pitch has been shown to be poorer in school-aged children wearing cochlear implants (Cis) than children with normal hearing (NH), but it is unclear whether this is also the case for dynamic changes in pitch. Yet, dynamically changing pitch has considerable ecological relevance in terms of natural speech, particularly aspects such as intonation, emotion, or lexical tone information. Twenty one children with NH and 23 children wearing a CI participated in this study, along with 18 NH adults and 6 CI adults for comparison. Listeners with Cls used their clinically assigned settings with envelope-based coding strategies. Percent correct was measured in one- or three-interval two-alternative forced choice tasks, for the direction or discrimination of harmonic complexes based on a linearly rising or falling fundamental frequency. Sweep rates were adjusted per subject, in a logarithmic scale, so as to cover the full extent of the psychometric function. Data for up- and down-sweeps were fitted separately, using a maximum-likelihood technique. Fits were similar for up- and down-sweeps in the discrimination task, but diverged in the direction task because psychometric functions for down-sweeps were very shallow. Hits and false alarms were then converted into d' and beta values, from which a threshold was extracted at a d' of 0.77. Thresholds were very consistent between the two tasks and considerably higher (worse) for CI listeners than for their NH peers. Thresholds were also higher for children than adults. Factors such as age at implantation, age at profound hearing loss, and duration of CI experience did not play any major role in this sensitivity. Thresholds of dynamic pitch sensitivity (in either task) also correlated with thresholds for static pitch sensitivity and with performance in tasks related to speech prosody.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1137,Neurophysiological and Behavioral Responses of Mandarin Lexical Tone Processing,"Language experience enhances discrimination of speech contrasts at a behavioral- perceptual level, as well as at a pre-attentive level, as indexed by event-related potential (ERP) mismatch negativity (MMN) responses. The enhanced sensitivity could be the result of changes in acoustic resolution and/or long-term memory representations of the relevant information in the auditory cortex. To examine these possibilities, we used a short (ca. 600 ms) vs. long (ca. 2,600 ms) interstimulus interval (ISI) in a passive, oddball discrimination task while obtaining ERPs. These ISI differences were used to test whether cross-linguistic differences in processing Mandarin lexical tone are a function of differences in acoustic resolution and/or differences in long-term memory representations. Bisyllabic nonword tokens that differed in lexical tone categories were presented using a passive listening multiple oddball paradigm. Behavioral discrimination and identification data were also collected. The ERP results revealed robust MMNs to both easy and difficult lexical tone differences for both groups at short ISIs. At long ISIs, there was either no change or an enhanced MMN amplitude for the Mandarin group, but reduced MMN amplitude for the English group. In addition, the Mandarin listeners showed a larger late negativity (LN) discriminative response than the English listeners for lexical tone contrasts in the long ISI condition. Mandarin speakers outperformed English speakers in the behavioral tasks, especially under the long ISI conditions with the more similar lexical tone pair. These results suggest that the acoustic correlates of lexical tone are fairly robust and easily discriminated at short ISIs, when the auditory sensory memory trace is strong. At longer ISIs beyond 2.5 s language-specific experience is necessary for robust discrimination.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1138,The Functional Neuroanatomy of Lexical Tone Perception: An Activation Likelihood Estimation Meta-Analysis,"In tonal language such as Chinese, lexical tone serves as a phonemic feature in determining word meaning. Meanwhile, it is close to prosody in terms of suprasegmental pitch variations and larynx-based articulation. The important yet mixed nature of lexical tone has evoked considerable studies, but no consensus has been reached on its functional neuroanatomy. This meta-analysis aimed at uncovering the neural network of lexical tone perception in comparison with that of phoneme and prosody in a unified framework. Independent Activation Likelihood Estimation meta-analyses were conducted for different linguistic elements: lexical tone by native tonal language speakers, lexical tone by non-tonal language speakers, phoneme, word-level prosody, and sentence-level prosody. Results showed that lexical tone and prosody studies demonstrated more extensive activations in the right than the left auditory cortex, whereas the opposite pattern was found for phoneme studies. Only tonal language speakers consistently recruited the left anterior superior temporal gyrus (STG) for processing lexical tone, an area implicated in phoneme processing and word-form recognition. Moreover, an anterior-lateral to posterior-medial gradient of activation as a function of element timescale was revealed in the right STG, in which the activation for lexical tone lied between that for phoneme and that for prosody. Another topological pattern was shown on the left precentral gyrus (preCG), with the activation for lexical tone overlapped with that for prosody but ventral to that for phoneme. These findings provide evidence that the neural network for lexical tone perception is hybrid with those for phoneme and prosody. That is, resembling prosody, lexical tone perception, regardless of language experience, involved right auditory cortex, with activation localized between sites engaged by phonemic and prosodic processing, suggesting a hierarchical organization of representations in the right auditory cortex. For tonal language speakers, lexical tone additionally engaged the left STG lexical mapping network, consistent with the phonemic representation. Similarly, when processing lexical tone, only tonal language speakers engaged the left preCG site implicated in prosody perception, consistent with tonal language speakers having stronger articulatory representations for lexical tone in the laryngeal sensorimotor network. A dynamic dual-stream model for lexical tone perception was proposed and discussed.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1139,Processing of Acoustic Information in Lexical Tone Production and Perception by Pediatric Cochlear Implant Recipients,"Purpose: This study examined the utilization of multiple types of acoustic information in lexical tone production and perception by pediatric cochlear implant (CI) recipients who are native speakers of Mandarin Chinese. Methods: Lexical tones were recorded from CI recipients and their peers with normal hearing (NH). Each participant was asked to produce a disyllabic word, yan jing, with which the first syllable was pronounced as Tone 3 (a low dipping tone) while the second syllable was pronounced as Tone 1 (a high level tone, meaning ""eyes"") or as Tone 4 (a high falling tone, meaning ""eyeglasses""). In addition, a parametric manipulation in fundamental frequency (F0) and duration of Tones 1 and 4 used in a lexical tone recognition task in Peng et al. (2017) was adopted to evaluate the perceptual reliance on each dimension. Results: Mixed-effect analyses of duration, intensity, and F0 cues revealed that NH children focused exclusively on marking distinct F0 contours, while CI participants shortened Tone 4 or prolonged Tone 1 to enhance their contrast. In line with these production strategies, NH children relied primarily on F0 cues to identify the two tones, whereas CI children showed greater reliance on duration cues. Moreover, CI participants who placed greater perceptual weight on duration cues also tended to exhibit smaller changes in their F0 production. Conclusion: Pediatric CI recipients appear to contrast the secondary acoustic dimension (duration) in addition to F0 contours for both lexical tone production and perception. These findings suggest that perception and production strategies of lexical tones are well coupled in this pediatric CI population.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1140,Acoustic Assessment of Tone Production of Prelingually-Deafened Mandarin-Speaking Children With Cochlear Implants,"Objective The purpose of the present study was to investigate Mandarin tone production performance of prelingually deafened children with cochlear implants (CIs) using modified acoustic analyses and to evaluate the relationship between demographic factors of those CI children and their tone production ability. Methods Two hundred seventy-eight prelingually deafened children with CIs and 173 age-matched normal-hearing (NH) children participated in the study. Thirty-six monosyllabic Mandarin Chinese words were recorded from each subject. The fundamental frequencies (F0) were extracted from the tone tokens. Two acoustic measures (i.e., differentiability and hit rate) were computed based on the F0 onset and offset values (i.e., the tone ellipses of the two-dimensional [2D] method) or the F0 onset, midpoint, and offset values (i.e., the tone ellipsoids of the 3D method). The correlations between the acoustic measures as well as between the methods were performed. The relationship between demographic factors and acoustic measures were also explored. Results The children with CIs showed significantly poorer performance in tone differentiability and hit rate than the NH children. For both CI and NH groups, performance on the two acoustic measures was highly correlated with each other (r values: 0.895-0.961). The performance between the two methods (i.e., 2D and 3D methods) was also highly correlated (r values: 0.774-0.914). Age at implantation and duration of CI use showed a weak correlation with the scores of acoustic measures under both methods. These two factors jointly accounted for 15.4-18.9% of the total variance of tone production performance. Conclusion There were significant deficits in tone production ability in most prelingually deafened children with CIs, even after prolonged use of the devices. The strong correlation between the two methods suggested that the simpler, 2D method seemed to be efficient in acoustic assessment for lexical tones in hearing-impaired children. Age at implantation and especially the duration of CI use were significant, although weak, predictors for tone development in pediatric CI users. Although a large part of tone production ability could not be attributed to these two factors, the results still encourage early implantation and continual CI use for better lexical tone development in Mandarin-speaking pediatric CI users.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1141,Increased Right Frontal Brain Activity During the Mandarin Hearing-in-Noise Test,"Purpose Previous studies have revealed increased frontal brain activation during speech comprehension in background noise. Few, however, used tonal languages. The normal pattern of brain activation during a challenging speech-in-nose task using a tonal language remains unclear. The Mandarin Hearing-in-Noise Test (HINT) is a well-established test for assessing the ability to interpret speech in background noise. The current study used Mandarin HINT (MHINT) sentences and functional magnetic resonance imaging (fMRI) to assess brain activation with MHINT sentences. Methods Thirty native Mandarin-speaking subjects with normal peripheral hearing were recruited. Functional MRI was performed while subjects were presented with either HINT ""clear"" sentences with low-level background noise [signal-to-noise ratio (SNR) = +3 dB] or ""noisy"" sentences with high-level background noise (SNR = -5 dB). Subjects were instructed to answer with a button press whether a visually presented target word was included in the sentence. Brain activation between noisy and clear sentences was compared. Activation in each condition was also compared to a resting, no sentence presentation, condition. Results Noisy sentence comprehension showed increased activity in areas associated with tone processing and working memory, including the right superior and middle frontal gyri [Brodmann Areas (BAs) 46, 10]. Reduced activity with noisy sentences was seen in auditory, language, memory and somatosensory areas, including the bilateral superior and middle temporal gyri, left Heschl's gyrus (BAs 21, 22), right temporal pole (BA 38), bilateral amygdala-hippocampus junction, and parahippocampal gyrus (BAs 28, 35), left inferior parietal lobule extending to left postcentral gyrus (BAs 2, 40), and left putamen. Conclusion Increased frontal activation in the right hemisphere occurred when comprehending noisy spoken sentences in Mandarin. Compared to studies using non-tonal languages, this activation was strongly right-sided and involved subregions not previously reported. These findings may reflect additional effort in lexical tone perception in this tonal language. Additionally, this continuous fMRI protocol may offer a time-efficient way to assess group differences in brain activation with a challenging speech-in-noise task.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1142,"Relative Weights of Temporal Envelope Cues in Different Frequency Regions for Mandarin Vowel, Consonant, and Lexical Tone Recognition","Objectives: Mandarin-speaking users of cochlear implants (CI) perform poorer than their English counterpart. This may be because present CI speech coding schemes are largely based on English. This study aims to evaluate the relative contributions of temporal envelope (E) cues to Mandarin phoneme (including vowel, and consonant) and lexical tone recognition to provide information for speech coding schemes specific to Mandarin.Design: Eleven normal hearing subjects were studied using acoustic temporal E cues that were extracted from 30 continuous frequency bands between 80 and 7,562 Hz using the Hilbert transform and divided into five frequency regions. Percent-correct recognition scores were obtained with acoustic E cues presented in three, four, and five frequency regions and their relative weights calculated using the least-square approach.Results: For stimuli with three, four, and five frequency regions, percent-correct scores for vowel recognition using E cues were 50.43-84.82%, 76.27-95.24%, and 96.58%, respectively; for consonant recognition 35.49-63.77%, 67.75-78.87%, and 87.87%; for lexical tone recognition 60.80-97.15%, 73.16-96.87%, and 96.73%. For frequency region 1 to frequency region 5, the mean weights in vowel recognition were 0.17, 0.31, 0.22, 0.18, and 0.12, respectively; in consonant recognition 0.10, 0.16, 0.18, 0.23, and 0.33; in lexical tone recognition 0.38, 0.18, 0.14, 0.16, and 0.14.Conclusion: Regions that contributed most for vowel recognition was Region 2 (502-1,022 Hz) that contains first formant (F1) information; Region 5 (3,856-7,562 Hz) contributed most to consonant recognition; Region 1 (80-502 Hz) that contains fundamental frequency (F0) information contributed most to lexical tone recognition.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1143,A Review of Speech Perception of Mandarin-Speaking Children With Cochlear Implantation,"Objective: This paper reviewed the literature on the development of and factors affecting speech perception of Mandarin-speaking children with cochlear implantation (CI). We also summarized speech outcome measures in standard Mandarin for evaluating auditory and speech perception of children with CI.Method: A comprehensive search of Google Scholar and PubMed was conducted from March to June 2021. Search terms used were speech perception/lexical tone recognition/auditory perception AND cochlear implant AND Mandarin/Chinese.Conclusion: Unilateral CI recipients demonstrated continuous improvements in auditory and speech perception for several years post-activation. Younger age at implantation and longer duration of CI use contribute to better speech perception. Having undergone a hearing aid trial before implantation and having caregivers whose educational level is higher may lead to better performance. While the findings that support the use of CI to improve speech perception continue to grow, much research is needed to validate the use of unilateral and bilateral implantation. Evidence to date, however, revealed bimodal benefits over CI-only conditions in lexical tone recognition and sentence perception in noise. Due to scarcity of research, conclusions on the benefits of bilateral CIs compared to unilateral CI or bimodal CI use cannot be drawn. Therefore, future research on bimodal and bilateral CIs is needed to guide evidence-based clinical practice.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1144,Pre-attentive fundamental frequency processing in Mandarin-speaking children with cochlear implants as revealed by the peak latency of positive mismatch response,"IntroductionFundamental frequency (F0) serves as the primary acoustic cue for Mandarin tone perception. Recent behavioral studies suggest that F0 information may be differently processed between Mandarin-speaking normal-hearing (NH) children and children with cochlear implants (CIs), which may partially explain the unsatisfactory outcome of lexical tone recognition using CIs with tonal language-oriented speech processing strategies. The aim of the current study was to provide neural evidence of F0 processing in Mandarin-speaking kindergarten-aged children with CIs compared with NH children. MethodsPositive mismatch responses (p-MMRs) to the change of the two acoustic dimensions of F0 (F0 contour and F0 level) in Mandarin-speaking kindergarten-aged children with CIs (n = 19) and their age-matched NH peers (n = 21). ResultsThe two groups of children did not show any significant difference on the mean amplitude of p-MMR to either F0 contour or F0 level change. While the CI group exhibited a significantly shorter peak latency of p-MMR to F0 contour change than to F0 level change, an opposite pattern was observed in the NH group. DiscussionThis study revealed a higher sensitivity to F0 contour change than to F0 level change in children with CIs, which was different from that in NH children. The neural evidence of discrepant F0 processing between children with CIs and NH children in this study was consistent with the previously reported behavioral findings and may serve as a reference for the development and improvement of tonal language-oriented speech processing strategies.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1145,Musicality and Age Interaction in Tone Development,"Vocal pitch, which involves not only F0 but also multiple covarying acoustic cues is central to linguistic perception and production at various levels of prosodic structure. Recent studies on language development have shown that differences in learners' musicality affect the F0 cue development in perception of sentence-level intonation or in prosodic realization of focus. This study aims to contribute toward a fuller understanding of the effect of musicality on linguistic pitch development via a close investigation of the relationship between musicality, age, and lexical tone production covering both F0 and spectral cues in children. Forty-three native Mandarin-speaking children between the ages of 4 and 6 years are recruited to participate in both a semi-spontaneous tone production task and a musicality test. For each age (4, 5, and 6 years) and musicality (below or above the median score of each age group) group, the contrastivity of the four tones is evaluated by performing automatic tone classification using three sets of acoustic cues (F0, spectral cues, and both). It has been found that higher musicality is associated with higher contrastivity of the tones produced at the age of 4 and 5 years, but not at the age of 6 years. These results suggest that musicality promotes earlier development of tone production only in earlier stages of prosodic development; by the age of 6 years, the musicality advantage in tone production subsides.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1146,Brain hemispheres with right temporal lobe damage swap dominance in early auditory processing of lexical tones,"Labor division of the two brain hemispheres refers to the dominant processing of input information on one side of the brain. At an early stage, or a preattentive stage, the right brain hemisphere is shown to dominate the auditory processing of tones, including lexical tones. However, little is known about the influence of brain damage on the labor division of the brain hemispheres for the auditory processing of linguistic tones. Here, we demonstrate swapped dominance of brain hemispheres at the preattentive stage of auditory processing of Chinese lexical tones after a stroke in the right temporal lobe (RTL). In this study, we frequently presented lexical tones to a group of patients with a stroke in the RTL and infrequently varied the tones to create an auditory contrast. The contrast evoked a mismatch negativity response, which indexes auditory processing at the preattentive stage. In the participants with a stroke in the RTL, the mismatch negativity response was lateralized to the left side, in contrast to the right lateralization pattern in the control participants. The swapped dominance of brain hemispheres indicates that the RTL is a core area for early-stage auditory tonal processing. Our study indicates the necessity of rehabilitating tonal processing functions for tonal language speakers who suffer an RTL injury.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1147,Congenital amusia (or tone-deafness) interferes with pitch processing in tone languages,"Congenital amusia is a neurogenetic disorder that affects music processing and that is ascribed to a deficit in pitch processing. We investigated whether this deficit extended to pitch processing in speech, notably the pitch changes used to contrast lexical tones in tonal languages. Congenital amusics and matched controls, all non-tonal language speakers, were tested for lexical tone discrimination in Mandarin Chinese (Experiment 1) and in Thai (Experiment 2). Tones were presented in pairs and participants were required to make same/different judgments. Experiment 2 additionally included musical analogs of Thai tones for comparison. Performance of congenital amusics was inferior to that of controls for all materials, suggesting a domain-general pitch-processing deficit. The pitch deficit of amusia is thus not limited to music, but may compromise the ability to process and learn tonal languages. Combined with acoustic analyses of the tone material, the present findings provide new insights into the nature of the pitch-processing deficit exhibited by amusics.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1148,Prosodic cues to word order: what level of representation?,"Within language, systematic correlations exist between syntactic structure and prosody. Prosodic prominence, for instance, falls on the complement and not the head of syntactic phrases, and its realization depends on the phrasal position of the prominent element. Thus, in Japanese, a functor-final language, prominence is phrase-initial, and realized as increased pitch (boolean AND Tokyo ni ""Tokyo to""), whereas in French, English, or Italian, functor-initial languages, it manifests itself as phrase-final lengthening (to Rome). Prosody is readily available in the linguistic signal even to the youngest infants. It has, therefore, been proposed that young learners might be able to exploit its correlations with syntax to bootstrap language structure. In this study, we tested this hypothesis, investigating how 8-month-old monolingual French infants processed an artificial grammar manipulating the relative position of prosodic prominence and word frequency. In Condition 1, we created a speech stream in which the two cues, prosody and frequency, were aligned, frequent words being prosodically non-prominent and infrequent ones being prominent, as is the case in natural language (functors are prosodically minimal compared to content words). In Condition 2, the two cues were misaligned, with frequent words carrying prosodic prominence, unlike in natural language. After familiarization with the aligned or the misaligned stream in a headturn preference procedure, we tested infants' preference for test items having a frequent word initial or a frequent word final word order. We found that infants' familiarized with the aligned stream showed the expected preference for the frequent word initial test items, mimicking the functor-initial word order of French. Infants in the misaligned condition showed no preference. These results suggest that infants are able to use word frequency and prosody as early cues to word order and they integrate them into a coherent representation.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1149,Preattentive processing of emotional musical tones: a multidimensional scaling and ERP study,"Musical emotion can be conveyed by subtle variations in timbre. Here, we investigated whether the brain is capable to discriminate tones differing in emotional expression by recording event-related potentials (ERPs) in an oddball paradigm under preattentive listening conditions. First, using multidimensional Fechnerian scaling, pairs of violin tones played with a happy or sad intonation were rated same or different by a group of non-musicians. Three happy and three sad tones were selected for the ERP experiment. The Fechnerian distances between tones within an emotion were in the same range as the distances between tones of different emotions. In two conditions, either 3 happy and 1 sad or 3 sad and 1 happy tone were presented in pseudo-random order. A mismatch negativity for the emotional deviant was observed, indicating that in spite of considerable perceptual differences between the three equiprobable tones of the standard emotion, a template was formed based on timbral cues against which the emotional deviant was compared. Based on Juslin's assumption of redundant code usage, we propose that tones were grouped together, because they were identified as belonging to one emotional category based on different emotion-specific cues. These results indicate that the brain forms an emotional memory trace at a preattentive level and thus, extends previous investigations in which emotional deviance was confounded with physical dissimilarity. Differences between sad and happy tones were observed which might be due to the fact that the happy emotion is mostly communicated by suprasegmental features.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1150,Perceptual pitch deficits coexist with pitch production difficulties in music but not Mandarin speech,"Congenital amusia is a musical disorder that mainly affects pitch perception. Among Mandarin speakers, some amusics also have difficulties in processing lexical tones (tone agnosics). To examine to what extent these perceptual deficits may be related to pitch production impairments in music and Mandarin speech, eight amusics, eight tone agnosics, and 12 age- and IQ-matched normal native Mandarin speakers were asked to imitate music note sequences and Mandarin words of comparable lengths. The results indicated that both the amusics and tone agnosics underperformed the controls on musical pitch production. However, tone agnosics performed no worse than the amusics, suggesting that lexical tone perception deficits may not aggravate musical pitch production difficulties. Moreover, these three groups were all able to imitate lexical tones with perfect intelligibility. Taken together, the current study shows that perceptual musical pitch and lexical tone deficits might coexist with musical pitch production difficulties. But at the same time these perceptual pitch deficits might not affect lexical tone production or the intelligibility of the speech words that were produced. The perception-production relationship for pitch among individuals with perceptual pitch deficits may be, therefore, domain-dependent.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1151,Processing word prosody - behavioral and neuroimaging evidence for heterogeneous performance in a language with variable stress,"In the present behavioral and fMRI study, we investigated for the first time interindividual variability in word stress processing in a language with variable stress position (German) in order to identify behavioral predictors and neural correlates underlying these differences. It has been argued that speakers of languages with variable stress should perform relatively well in tasks tapping into the representation and processing of word stress, given that this is a relevant feature of their language. Nevertheless, in previous studies on word stress processing large degrees of interindividual variability have been observed but were ignored or left unexplained. Twenty-five native speakers of German performed a sequence recall task using both segmental and suprasegmental stimuli. In general, the suprasegmental condition activated a subcortico-cortico-cerebellar network including, amongst others, bilateral inferior frontal gyrus, insula, precuneus, cerebellum, the basal ganglia, pre-SMA and SMA, which has been suggested to be dedicated to the processing of temporal aspects of speech. However, substantial interindividual differences were observed. In particular, main effects of group were observed in the left middle temporal gyrus (below vs. above average performance in stress processing) and in the left precuneus (above vs. below average). Moreover, condition (segmental vs. suprasegmental) and group (above vs. below average) interacted in the right hippocampus and cerebellum. At the behavioral level, differences in word stress processing could be partly explained by individual performance in basic auditory perception including duration discrimination and by working memory performance (WM). We conclude that even in a language with variable stress, interindividual differences in behavioral performance and in the neuro-cognitive foundations of stress processing can be observed which may partly be traced back to individual basic auditory processing and WM performance.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1152,Processing of syllable stress is functionally different from phoneme processing and does not profit from literacy acquisition,"Speech is characterized by phonemes and prosody. Neurocognitive evidence supports the separate processing of each type of information. Therefore, one might suggest individual development of both pathways. In this study, we examine literacy acquisition in middle childhood. Children become aware of the phonemes in speech at that time and refine phoneme processing when they acquire an alphabetic writing system. We test whether an enhanced sensitivity to phonemes in middle childhood extends to other aspects of the speech signal, such as prosody. To investigate prosodic processing, we used stress priming. Spoken stressed and unstressed syllables (primes) preceded spoken German words with stress on the first syllable (targets). We orthogonally varied stress overlap and phoneme overlap between the primes and onsets of the targets. Lexical decisions and Event-Related Potentials (ERPs) for the targets were obtained for pre-reading preschoolers, reading pupils and adults. The behavioral and ERR results were largely comparable across all groups. The fastest responses were observed when the first syllable of the target word shared stress and phonemes with the preceding prime. ERR stress priming and ERR phoneme priming started 200 ms after the target word onset. Bilateral ERR stress priming was characterized by enhanced ERR amplitudes for stress overlap. Left-lateralized ERR phoneme priming replicates previously observed reduced ERR amplitudes for phoneme overlap. Groups differed in the strength of the behavioral phoneme priming and in the late ERR phoneme priming effect. The present results show that enhanced phonological processing in middle childhood is restricted to phonemes and does not extend to prosody. These results are indicative of two parallel processing systems for phonemes and prosody that might follow different developmental trajectories in middle childhood as a function of alphabetic literacy.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1153,Individual apt tude in Mandarin lexical tone perception predicts effectiveness of high-variability training,"Although the high-variability training method can enhance learning of non-native speech categories, this can depend on individuals' aptitude. The current study asked how general the effects of perceptual aptitude are by testing whether they occur with training materials spoken by native speakers and whether they depend on the nature of the to-be-learned material. Forty-five native Dutch listeners took part in a 5-day training procedure in which they identified bisyllabic Mandarin pseudowords (e.g., asa) pronounced with different lexical tone combinations. The training materials were presented to different groups of listeners at three levels of variability: low (many repetitions of a limited set of words recorded by a single speaker), medium (fewer repetitions of a more variable set of words recorded by three speakers), and high (similar to medium but with five speakers). Overall, variability did not influence learning performance, but this was due to an interaction with individuals' perceptual aptitude: increasing variability hindered improvements in performance for low-aptitude perceivers while it helped improvements in performance for high-aptitude perceivers. These results show that the previously observed interaction between individuals' aptitude and effects of degree of variability extends to natural tokens of Mandarin speech. This interaction was not found, however, in a closely matched study in which native Dutch listeners were trained on the Japanese geminate/singleton consonant contrast. This may indicate that the effectiveness of high-variability training depends not only on individuals' aptitude in speech perception but also on the nature of the categories being acquired.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1154,Musical experience modulates categorical perception of lexical tones in native Chinese speakers,"Although musical training has been shown to facilitate both native and non-native phonetic perception, it remains unclear whether and how musical experience affects native speakers' categorical perception (CP) of speech at the suprasegmental level. Using both identification and discrimination tasks, this study compared Chinese speaking musicians and non-musicians in their CF of a lexical tone continuum (from the high level tone, Tonel to the high falling tone, Tone4). While the identification functions showed similar steepness and boundary location between the two subject groups, the discrimination results revealed superior performance in the musicians for discriminating within-category stimuli pairs but not for between-category stimuli. These findings suggest that musical training can enhance sensitivity to subtle pitch differences between within category sounds in the presence of robust mental representations in service of GP of lexical tonal contrasts.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1155,Categorical perception of lexical tones in mandarin-speaking congenital amusics,"Previous research suggests that within Mandarin-speaking congenital amusics, only a subgroup has behavioral lexical tone perception impairments (tone agnosia), whereas the rest of amusics do not. The purpose of the current study was to investigate the categorical nature of lexical tone perception in Mandarin-speaking amusics with and without behavioral lexical tone deficits. Three groups of listeners (controls, pure amusics, and amusics with tone agnosia) participated in tone identification and discrimination tasks. Indexes of the categorical perception (GP) of a physical continuum of fundamental frequencies ranging from a rising to level tone were measured. Specifically, the stimulus durations were manipulated at 100 and 200 ms. For both stimulus durations, all groups exhibited similar categorical boundaries. The pure amusics showed sharp identification slopes and significantly peaked discrimination functions similar to those of normal controls. However, such essential characteristics for the GP of lexical tones were not observed in amusics with tone agnosia. An enlarged step-size from 20 to 35 Hz was not able to produce any discrimination peaks in tone agnosics either. The current study revealed that only amusics with tone agnosia showed a lack of categorical tone perception, while the pure amusics demonstrated typical GP of lexical tones, indicating that the deficit of pitch processing in music does not necessarily result in the deficit in the GP of lexical tones. The different performance between congenital amusics with and without tone agnosia provides a new perspective on the proposition of the relationship between music and speech perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1156,The perception of speech modulation cues in lexical tones is guided by early language-specific experience,"A number of studies showed that infants reorganize their perception of speech sounds according to their native language categories during their first year of life. Still, information is lacking about the contribution of basic auditory mechanisms to this process. This study aimed to evaluate when native language experience starts to noticeably affect the perceptual processing of basic acoustic cues [i.e., frequency-modulation (FM) and amplitude-modulation information] known to be crucial for speech perception in adults. The discrimination of a lexical-tone contrast (rising versus low) was assessed in 6- and 10-month-old infants learning either French or Mandarin using a visual habituation paradigm. The lexical tones were presented in two conditions designed to either keep intact or to severely degrade the FM and fine spectral cues needed to accurately perceive voice-pitch trajectory. A third condition was designed to assess the discrimination of the same voice-pitch trajectories using click trains containing only the FM cues related to the fundamental-frequency (FO) in French- and Mandarin-learning 10-month-old infants. Results showed that the younger infants of both language groups and the Mandarin-learning 10-month-olds discriminated the intact lexical-tone contrast while French-learning 10-month-olds failed. However, only the French 10-month-olds discriminated degraded lexical tones when FM, and thus voice-pitch cues were reduced. Moreover, Mandarin-learning 10-month-olds were found to discriminate the pitch trajectories as presented in click trains better than French infants. Altogether, these results reveal that the perceptual reorganization occurring during the first year of life for lexical tones is coupled with changes in the auditory ability to use speech modulation cues.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1157,Brain readiness and the nature of language,"To identify the neural components that make a brain ready for language, it is important to have well defined linguistic phenotypes, to know precisely what language is. There are two central features to language: the capacity to form signs (words), and the capacity to combine them into complex structures. We must determine how the human brain enables these capacities. A sign is a link between a perceptual form and a conceptual meaning. Acoustic elements and content elements, are already brain-internal in non-human animals, but as categorical systems linked with brain-external elements. Being indexically tied to objects of the world, they cannot freely link to form signs. A crucial property of a language-ready brain is the capacity to process perceptual forms and contents offline, detached from any brain-external phenomena, so their ""representations"" may be linked into signs. These brain systems appear to have pleiotropic effects on a variety of phenotypic traits and not to be specifically designed for language. Syntax combines signs, so the combination of two signs operates simultaneously on their meaning and form. The operation combining the meanings long antedates its function in language: the primitive mode of predication operative in representing some information about an object. The combination of the forms is enabled by the capacity of the brain to segment vocal and visual information into discrete elements. Discrete temporal units have order and juxtaposition, and vocal units have intonation, length, and stress. These are primitive combinatorial processes. So the prior properties of the physical and conceptual elements of the sign introduce combinatoriality into the linguistic system, and from these primitive combinatorial systems derive concatenation in phonology and combination in morphosyntax. Given the nature of language, a key feature to our understanding of the language-ready brain is to be found in the mechanisms in human brains that enable the unique means of representation that allow perceptual forms and contents to be linked into signs.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1158,Evolution of tonal organization in music mirrors symbolic representation of perceptual reality. Part-1: Prehistoric,"This paper reveals the way in which musical pitch works as a peculiar form of cognition that reflects upon the organization of the surrounding world as perceived by majority of music users within a socio-cultural formation. The evidence from music theory, ethnography, archeology, organology, anthropology, psychoacoustics, and evolutionary biology is plotted against experimental evidence. Much of the methodology for this investigation comes from studies conducted within the territory of the former USSR. To date, this methodology has remained solely confined to Russian speaking scholars. A brief overview of pitch-set theory demonstrates the need to distinguish between vertical and horizontal harmony, laying out the framework for virtual music space that operates according to the perceptual laws of tonal gravity. Brought to life by bifurcation of music and speech, tonal gravity passed through eleven discrete stages of development until the onset of tonality in the seventeenth century. Each stage presents its own method of integration of separate musical tones into an auditory-cognitive unity. The theory of ""melodic intonation"" is set forth as a counterpart to harmonic theory of chords. Notions of tonality, modality, key, diatonicity, chromaticism, alteration, and modulation are defined in terms of their perception, and categorized according to the way in which they have developed historically. Tonal organization in music, and perspective organization in fine arts are explained as products of the same underlying mental process. Music seems to act as a unique medium of symbolic representation of reality through the concept of pitch. Tonal organization of pitch reflects the culture of thinking, adopted as a standard within a community of music users. Tonal organization might be a naturally formed system of optimizing individual perception of reality within a social group and its immediate environment, setting conventional standards of intellectual and emotional intelligence.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1159,"The ""Globularization Hypothesis"" of the Language-ready Brain as a Developmental Frame for Prosodic Bootstrapping Theories of Language Acquisition","In recent research (Boeckx arid Benitez-Burra( o, 2014a,b) have advanced the hypothesis that our species-specific language-ready brain should be understood as the outcome of developmental changes that occurred in our species after the split from Neanderthals-Denisovans, which resulted in a more globular braincase configuration in comparison to our closest relatives, who had elongated endocasts. According to these authors, the development of a globular brain is an essential ingredient for the language faculty and in particular, it is the centrality occupied by the thalamus in a globular brain that allows its modulatory or regulatory role, essential for syntactico-semantic computations. Their hypothesis is that the syntactico-semantic capacities arise in humans as a consequence of a process of globularization, which significantly takes place postnatally (cf. Neubauer et al., 2010). In this paper, I show that Boeckx and Benitez-Burraco's hypothesis makes an interesting developmental prediction regarding the path of language acquisition: it teases apart the onset of phonological acquisition and the onset of syntactic acquisition (the latter starting significantly later, after globularization). I argue that this hypothesis provides a developmental rationale for the prosodic bootstrapping hypothesis of language acquisition (cf. La. Gleitman arid Wanner, 1982; Mehler et al., 1988, et seq.; Gervain and Werker, 2013), which claim that prosodic cues are employed for syntactic parsing. The literature converges in the observation that a large amount of such prosodic cues (in particular, rhythmic cues) are already acquired before the completion of the globularization phase, which paves the way for the premises of the prosodic bootstrapping hypothesis, allowing babies to have a rich knowledge of the prosody of their target language before they can start parsing the primary linguistic data syntactically.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1160,Prosodic Focus Marking in Silent Reading: Effects of Discourse Context and Rhythm,"Understanding a sentence and integrating it into the discourse depends upon the identification of its focus, which, in spoken German, is marked by accentuation. In the case of written language, which lacks explicit cues to accent, readers have to draw on other kinds of information to determine the focus. We study the joint or interactive effects of two kinds of information that have no direct representation in print but have each been shown to be influential in the reader's text comprehension: (i) the (low-level) rhythmic-prosodic structure that is based on the distribution of lexically stressed syllables, and (ii) the (high-level) discourse context that is grounded in the memory of previous linguistic content. Systematically manipulating these factors, we examine the way readers resolve a syntactic ambiguity involving the scopally ambiguous focus operator auch (engl. ""too"") in both oral (Experiment 1) and silent reading (Experiment 2). The results of both experiments attest that discourse context and local linguistic rhythm conspire to guide the syntactic and, concomitantly, the focus-structural analysis of ambiguous sentences. We argue that reading comprehension requires the (implicit) assignment of accents according to the focus structure and that, by establishing a prominence profile, the implicit prosodic rhythm directly affects accent assignment.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1161,Effects of Suprasegmental Phonological Alternations on Early Word Recognition: Evidence from Tone Sandhi,"Early language acquisition is potentially complicated by the presence of many sources of variability in the speech signal. A frequent example of variability is phonological alternations, which can lead to context-driven changes in the realization of a word. The aim of the current study was to investigate effects of a highly frequent yet scarcely researched type of suprasegmental phonological alternation - tone Sandhi - on early spoken word recognition. The tone Sandhi rule investigated herein involves a tone change of the first syllable in a disyllabic unit. In accordance with third tone Sandhi, when two dipping tone syllables are juxtaposed in connected speech, the first syllable is dissimilated to a high rising tone. For example, 'flour mill' (unaltered pre-Sandhi form [fan(214) ts(h)an(214)]) undergoes tonal alternation resulting in the altered post-Sandhi form [fan(35) ts(h)an(214)]. In the current study, preschoolers' sensitivity to the effects of tone Sandhi when processing familiar words was investigated via a preferential looking paradigm. Words varied in their phonological form: one set of words was labeled with a phonological alternation due to Sandhi (Post Sandhi), one set of words was labeled with an unaltered Sandhi form (Pre Sandhi), one set consisted of non Sandhi words (Correct Pronunciation, and one set were labeled with a tonal alternation not associated with Sandhi rules (Mispronunciation). Post-Sandhi forms and correct pronunciations were associated with visual referents with comparable strength, with only a subtle processing cost observed for post-Sandhi forms in the time course of lexical selection. Likewise, pre-Sandhi forms and true mispronunciations were rejected as labels for visual references with comparable strength, with only subtle differences observed in the time course of lexical selection. Findings are discussed in terms of their impact on prevailing theories of lexical representation.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1162,Limits on Monolingualism? A Comparison of Monolingual and Bilingual Infants' Abilities to Integrate Lexical Tone in Novel Word Learning,"To construct their first lexicon, infants must determine the relationship between native phonological variation and the meanings of words. This process is arguably more complex for bilingual learners who are often confronted with phonological conflict: phonological variation that is lexically relevant in one language may be lexically irrelevant in the other. In a series of four experiments, the present study investigated English Mandarin bilingual infants' abilities to negotiate phonological conflict introduced by learning both a tone and a non-tone language. In a novel word learning task, bilingual children were tested on their sensitivity to tone variation in English and Mandarin contexts. Their abilities to interpret tone variation in a language-dependent manner were compared to those of monolingual Mandarin learning infants. Results demonstrated that at 12-13 months, bilingual infants demonstrated the ability to bind tone to word meanings in Mandarin, but to disregard tone variation when learning new words in English. In contrast, monolingual learners of Mandarin did not show evidence of integrating tones into word meanings in Mandarin at the same age even though they were learning a tone language. However, a tone discrimination paradigm confirmed that monolingual Mandarin learning infants were able to tell these tones apart at 12-13 months under a different set of conditions. Later, at 17-18 months, monolingual Mandarin learners were able to bind tone variation to word meanings when learning new words. Our findings are discussed in terms of cognitive adaptations associated with bilingualism that may ease the negotiation of phonological conflict and facilitate precocious uptake of certain properties of each language.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1163,Rhythm on Your Lips,"The Iambic-Trochaic Law (ITL) accounts for speech rhythm, grouping of sounds as either lambs-if alternating in duration-or Trochees-if alternating in pitch and/or intensity. The two different rhythms signal word order, one of the basic syntactic properties of language. We investigated the extent to which Iambic and Trochaic phrases could be auditorily and visually recognized, when visual stimuli engage lip reading. Our results show both rhythmic patterns were recognized from both, auditory and visual stimuli, suggesting that speech rhythm has a multimodal representation. We further explored whether participants could match Iambic and Trochaic phrases across the two modalities. We found that participants auditorily familiarized with Trochees, but not with lambs, were more accurate in recognizing visual targets, while participants visually familiarized with lambs, but not with Trochees, were more accurate in recognizing auditory targets. The latter results suggest an asymmetric processing of speech rhythm: in auditory domain, the changes in either pitch or intensity are better perceived and represented than changes in duration, while in the visual domain the changes in duration are better processed and represented than changes in pitch, raising important questions about domain general and specialized mechanisms for speech rhythm processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1164,Non-native Speech Learning in Older Adults,"Though there is an extensive literature investigating the ability of younger adults to learn non-native phonology, including investigations into individual differences in younger adults' lexical tone learning, very little is known about older adults' ability to learn non-native phonology, including lexical tone. There are several reasons to suspect that older adults would use different learning mechanisms when learning lexical tone than younger adults, including poorer perception of dynamic pitch, greater reliance on working memory capacity in second language learning, and poorer category learning in older adulthood. The present study examined the relationships among older adults' baseline sensitivity for pitch patterns, working memory capacity, and declarative memory capacity with their ability to learn to associate tone with lexical meaning. In older adults, baseline pitch pattern sensitivity was not associated with generalization performance. Rather, older adults' learning performance was best predicted by declarative memory capacity. These data suggest that training paradigms will need to be modified to optimize older adults' non-native speech sound learning success.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1165,"Pitch Perception in the First Year of Life, a Comparison of Lexical Tones and Musical Pitch","Pitch variation is pervasive in speech, regardless of the language to which infants are exposed. Lexical tone is influenced by general sensitivity to pitch. We examined whether the development in lexical tone perception may develop in parallel with perception of pitch in other cognitive domains namely music. Using a visual fixation paradigm, 100 and one 4- and 12-month-old Dutch infants were tested on their discrimination of Chinese rising and dipping lexical tones as well as comparable three-note musical pitch contours. The 4-month-old infants failed to show a discrimination effect in either condition, whereas the 12-month-old infants succeeded in both conditions. These results suggest that lexical tone perception may reflect and relate to general pitch perception abilities, which may serve as a basis for developing more complex language and musical skills.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1166,From Lexical Tone to Lexical Stress: A Cross-Language Mediation Model for Cantonese Children Learning English as a Second Language,"This study investigated how Cantonese lexical tone sensitivity contributed to English lexical stress sensitivity among Cantonese children who learned English as a second language (ESL). Five-hundred-and-sixteen second-to-third grade Cantonese ESL children were tested on their Cantonese lexical tone sensitivity, English lexical stress sensitivity, general auditory sensitivity, and working memory. Structural equation modeling revealed that Cantonese lexical tone sensitivity contributed to English lexical stress sensitivity both directly, and indirectly through the mediation of general auditory sensitivity, in which the direct pathway had a larger relative contribution to English lexical stress sensitivity than the indirect pathway. These results suggest that the tone-stress association might be accounted for by joint phonological and acoustic processes that underlie lexical tone and lexical stress perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1167,Perceptual Improvement of Lexical Tones in Infants: Effects of Tone Language Experience,"To learn words in a tonal language, tone-language learners should not only develop better abilities for perceiving consonants and vowels, but also for lexical tones. The divergent trend of enhancing sensitivity to native phonetic contrasts and reduced sensitivity to non-native phonetic contrast is theoretically essential to evaluate effects of listening to an ambient language on speech perception development. The loss of sensitivity in discriminating lexical tones among non-tonal language-learning infants was apparent between 6 and 12 months of age, but only few studies examined trends of differentiating native lexical tones in infancy. The sensitivity in discriminating lexical tones among 6-8 and 10-12 month-old Mandarin-learning infants (n = 120) was tested in Experiment 1 using three lexical tone contrasts of Mandarin. Facilitation of linguistic experience was shown in the tonal contrast (Tone 1 vs. 3), but both age groups performed similar in the other two tonal contrasts (Tone 2 vs. 4; Tone 2 vs. 3). In Experiment 2, 6-8 and 10-12 month-old Mandarin-learning infants (n = 90) were tested with tonal contrasts that have pitch contours either similar to or inverse from lexical tones in Mandarin, and perceptual improvement was shown only in a tonal contrast with familiar pitch contours (i.e., Tone 1 vs. 3). In Experiment 3, 6-8 and 10-12 month-old English-learning infants (n = 40) were tested with Tone 1 vs. 3 contrast of Mandarin and showed an improvement in the perception of non-native lexical tones. This study reveals that tone-language learning infants develop more accurate representations of lexical tones around their first birthday, and the results of both tone and non-tone language-learning infants imply that the rate of development depends on listening experience and the acoustical salience of specific tone contrasts.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1168,Cantonese-Speaking Children Do Not Acquire Tone Perception before Tone Production-A Perceptual and Acoustic Study of Three-Year-Olds' Monosyllabic Tones,"Models of phonological development assume that speech perception precedes speech production and that children acquire suprasegmental features earlier than segmental features. Studies of Chinese-speaking children challenge these assumptions. For example, Chinese-speaking children can produce tones before two-and-a-half years but are not able to discriminate the same tones until after 6 years of age. This study compared the perception and production of monosyllabic Cantonese tones directly in 3 -year-old children. Twenty children and their mothers identified Cantonese tones in a picture identification test and produced monosyllabic tones in a picture labeling task. To control for lexical biases on tone ratings, the mother-and child-productions were low-pass filtered to eliminate lexical information and were presented to five judges for tone classification. Detailed acoustic analysis was performed. Contrary to the view that children master lexical tones earlier than segmental phonemes, results showed that 3-year-old children could not perceive or produce any Cantonese tone with adult-like proficiency and incorrect tone productions were acoustically different from criterion. In contrast to previous findings that Cantonese-speaking children mastered tone production before tone perception, we observed more accuracy during speech perception than production. Findings from Cantonese-speaking children challenge some of the established tenets in theories of phonological development that have been tested mostly with native English speakers.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1169,How Native Prosody Affects Pitch Processing during Word Learning in Limburgian and Dutch Toddlers and Adults,"In this study, Limburgian and Dutch 2.5-to 4-year-olds and adults took part in a word learning experiment. Following the procedure employed by Quam and Swingley (2010) and Singh et al. (2014), participants learned two novel word-object mappings. After training, word recognition was tested in correct pronunciation (CP) trials and mispronunciation (MP) trials featuring a pitch change. Since Limburgian is considered a restricted tone language, we expected that the pitch change would hinder word recognition in Limburgian, but not in non-tonal Dutch listeners. Contrary to our expectations, both Limburgian and Dutch children appeared to be sensitive to pitch changes in newly learned words, indicated by a significant decrease in target fixation in MP trials compared to CP trials. Limburgian and Dutch adults showed very strong naming effects in both trial types. The results are discussed against the background of the influence of the native prosodic system.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1170,Writing System Modulates the Association between Sensitivity to Acoustic Cues in Music and Reading Ability: Evidence from Chinese-English Bilingual Children,"Music and language share many attributes and a large body of evidence shows that sensitivity to acoustic cues in music is positively related to language development and even subsequent reading acquisition. However, such association was mainly found in alphabetic languages. What remains unclear is whether sensitivity to acoustic cues in music is associated with reading in Chinese, a morphosyllabic language. The present study aimed to answer this question by measuring music (i.e., musical metric perception and pitch discrimination), language (i.e., phonological awareness, lexical tone sensitivity), and reading abilities (i.e., word recognition) among 54 third-grade Chinese-English bilingual children. After controlling for age and non-verbal intelligence, we found that both musical metric perception and pitch discrimination accounted for unique variance of Chinese phonological awareness while pitch discrimination rather than musical metric perception predicted Chinese lexical tone sensitivity. More importantly, neither musical metric perception nor pitch discrimination was associated with Chinese reading. As for English, musical metric perception and pitch discrimination were correlated with both English phonological awareness and English reading. Furthermore, sensitivity to acoustic cues in music was associated with English reading through the mediation of English phonological awareness. The current findings indicate that the association between sensitivity to acoustic cues in music and reading may be modulated by writing systems. In Chinese, the mapping between orthography and phonology is not as transparent as in alphabetic languages such as English. Thus, this opaque mapping may alter the auditory perceptual sensitivity in music to Chinese reading.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1171,Speech Perception Deficits in Mandarin-Speaking School-Aged Children with Poor Reading Comprehension,"Previous studies have shown that children learning alphabetic writing systems who have language impairment or dyslexia exhibit speech perception deficits. However, whether such deficits exist in children learning logographic writing systems who have poor reading comprehension remains uncertain. To further explore this issue, the present study examined speech perception deficits in Mandarin-speaking children with poor reading comprehension. Two self-designed tasks, consonant categorical perception task and lexical tone discrimination task were used to compare speech perception performance in children (n = 31, age range = 7; 4-10; 2) with poor reading comprehension and an age-matched typically developing group (n = 31, age range = 7; 7-9; 10). Results showed that the children with poor reading comprehension were less accurate in consonant and lexical tone discrimination tasks and perceived speech contrasts less categorically than the matched group. The correlations between speech perception skills (i.e., consonant and lexical tone discrimination sensitivities and slope of consonant identification curve) and individuals' oral language and reading comprehension were stronger than the correlations between speech perception ability and word recognition ability. In conclusion, the results revealed that Mandarin-speaking children with poor reading comprehension exhibit less-categorized speech perception, suggesting that imprecise speech perception, especially lexical tone perception, is essential to account for reading learning difficulties in Mandarin-speaking children.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1172,Constraints on Tone Sensitivity in Novel Word Learning by Monolingual and Bilingual Infants: Tone Properties Are More Influential than Tone Familiarity,"This study compared tone sensitivity in monolingual and bilingual infants in a novel word learning task. Tone language learning infants (Experiment 1, Mandarin monolingual; Experiment 2, Mandarin-English bilingual) were tested with Mandarin (native) or Thai (non-native) lexical tone pairs which contrasted static vs. dynamic (high vs. rising) tones or dynamic vs. dynamic (rising vs. falling) tones. Non-tone language, English-learning infants (Experiment 3) were tested on English intonational contrasts or the Mandarin or Thai tone contrasts. Monolingual Mandarin language infants were able to bind tones to novel words for the Mandarin High-Rising contrast, but not for the Mandarin Rising-Falling contrast; and they were insensitive to both the High-Rising and the Rising-Falling tone contrasts in Thai. Bilingual English-Mandarin infants were similar to the Mandarin monolinguals in that they were sensitive to the Mandarin High-Rising contrast and not to the Mandarin Rising-Falling contrast. However, unlike the Mandarin monolinguals, they were also sensitive to the High Rising contrast in Thai. Monolingual English learning infants were insensitive to all three types of contrasts (Mandarin, Thai, English), although they did respond differentially to tone-bearing vs. intonation-marked words. Findings suggest that infants' sensitivity to tones in word learning contexts depends heavily on tone properties, and that this influence is, in some cases, stronger than effects of language familiarity. Moreover, bilingual infants demonstrated greater phonological flexibility in tone interpretation.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1173,The Effects of Lexical Pitch Accent on Infant Word Recognition in Japanese,"Learners of lexical tone languages (e.g., Mandarin) develop sensitivity to tonal contrasts and recognize pitch-matched, but not pitch-mismatched, familiar words by 11 months. Learners of non-tone languages (e.g., English) also show a tendency to treat pitch patterns as lexically contrastive up to about 18 months. In this study, we examined if this early-developing capacity to lexically encode pitch variations enables infants to acquire a pitch accent system, in which pitch-based lexical contrasts are obscured by the interaction of lexical and non-lexical (i.e., intonational) features. Eighteen 17-month-olds learning Tokyo Japanese were tested on their recognition of familiar words with the expected pitch or the lexically opposite pitch pattern. In early trials, infants were faster in shifting their eyegaze from the distractor object to the target object than in shifting from the target to distractor in the pitch-matched condition. In later trials, however, infants showed faster distractor-to-target than target-to-distractor shifts in both the pitch-matched and pitch-mismatched conditions. We interpret these results to mean that, in a pitch-accent system, the ability to use pitch variations to recognize words is still in a nascent state at 17 months.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1174,What Can Lexical Tone Training Studies in Adults Tell Us about Tone Processing in Children?,"A growing number of studies on the acquisition of lexical tone by adult learners have revealed that factors such as language background, musical experience, cognitive abilities, and neuroanatomy all play a role in determining tone learning success. On the basis of these findings, it has been argued that the effectiveness of tone learning in adulthood depends on individual differences in these factors. However, it is not clear whether similar individual differences play an analogous role in tone learning in childhood. Indeed, relatively few studies have made comparisons between how adults and children learn lexical tones. Here, we review recent developments for tone learning in both adults and children. The review covers tone training in a range of contexts, including in naive listeners, in native speakers of other tone languages, in listeners with varying levels of musical experience, and in individuals with speech and hearing disorders. Finally, we discuss the parallels between adult and child tone learning, and provide recommendations concerning how findings in adult tone training can provide insights into tone learning for children by accommodating the needs of individual learners.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1175,Monolingual and Bilingual Infants' Ability to Use Non-native Tone for Word Learning Deteriorates by the Second Year After Birth,"Previous studies reported a non-native word learning advantage for bilingual infants at around 18 months. We investigated developmental changes in infant interpretation of sounds that aid in object mapping. Dutch monolingual and bilingual (exposed to Dutch and a second non-tone-language) infants' word learning ability was examined on two novel label-object pairings using syllables differing in Mandarin tones as labels (flat vs. falling). Infants aged 14-15 months, regardless of language backgrounds, were sensitive to violations in the label-objects pairings when lexical tones were switched compared to when they were the same as habituated. Conversely at 17-18 months, neither monolingual nor bilingual infants demonstrated learning. Linking with existing literature, infants' ability to associate non-native tones with meanings may be related to tonal acoustic properties and/or perceptual assimilation to native prosodic categories. These findings provide new insights into the relation between infant tone perception, learning, and interpretative narrowing from a developmental perspective.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1176,One Way or Another: Evidence for Perceptual Asymmetry in Pre-attentive Learning of Non-native Contrasts,"Research investigating listeners' neural sensitivity to speech sounds has largely focused on segmental features. We examined Australian English listeners' perception and learning of a supra-segmental feature, pitch direction in a non-native tonal contrast, using a passive oddball paradigm and electroencephalography. The stimuli were two contours generated from naturally produced high-level and high-falling tones in Mandarin Chinese, differing only in pitch direction (Liu and Kager, 2014). While both contours had similar pitch onsets, the pitch offset of the falling contour was lower than that of the level one. The contrast was presented in two orientations (standard and deviant reversed) and tested in two blocks with the order of block presentation counterbalanced. Mismatch negativity (MMN) responses showed that listeners discriminated the non-native tonal contrast only in the second block, reflecting indications of learning through exposure during the first block. In addition, listeners showed a later MMN peak for their second block of test relative to listeners who did the same block first, suggesting linguistic (as opposed to acoustic) processing or a misapplication of perceptual strategies from the first to the second block. The results also showed a perceptual asymmetry for change in pitch direction: listeners who encountered a falling tone deviant in the first block had larger frontal MMN amplitudes than listeners who encountered a level tone deviant in the first block. The implications of our findings for second language speech and the developmental trajectory for tone perception are discussed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1177,A Musical Approach to Speech Melody,"We present here a musical approach to speech melody, one that takes advantage of the intervallic precision made possible with musical notation. Current phonetic and phonological approaches to speech melody either assign localized pitch targets that impoverish the acoustic details of the pitch contours and/or merely highlight a few salient points of pitch change, ignoring all the rest of the syllables. We present here an alternative model using musical notation, which has the advantage of representing the pitch of all syllables in a sentence as well as permitting a specification of the intervallic excursions among syllables and the potential for group averaging of pitch use across speakers. We tested the validity of this approach by recording native speakers of Canadian English reading unfamiliar test items aloud, spanning from single words to full sentences containing multiple intonational phrases. The fundamental-frequency trajectories of the recorded items were converted from hertz into semitones, averaged across speakers, and transcribed into musical scores of relative pitch. Doing so allowed us to quantify local and global pitch-changes associated with declarative, imperative, and interrogative sentences, and to explore the melodic dynamics of these sentence types. Our basic observation is that speech is atonal. The use of a musical score ultimately has the potential to combine speech rhythm and melody into a unified representation of speech prosody, an important analytical feature that is not found in any current linguistic approach to prosody.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1178,The Duration of Auditory Sensory Memory for Vowel Processing: Neurophysiological and Behavioral Measures,"Speech perception behavioral research suggests that rates of sensory memory decay are dependent on stimulus properties at more than one level (e.g., acoustic level, phonemic level). The neurophysiology of sensory memory decay rate has rarely been examined in the context of speech processing. In a lexical tone study, we showed that long-term memory representation of lexical tone slows the decay rate of sensory memory for these tones. Here, we tested the hypothesis that long-term memory representation of vowels slows the rate of auditory sensory memory decay in a similar way to that of lexical tone. Event-related potential (ERP) responses were recorded to Mandarin non-words contrasting the vowels /i/vs. /u/ and /y/vs. /u/ from first-language (L1) Mandarin and L1 American English participants under short and long interstimulus interval (ISI) conditions (short ISI: an average of 575 ms, long ISI: an average of 2675 ms). Results revealed poorer discrimination of the vowel contrasts for English listeners than Mandarin listeners, but with different patterns for behavioral perception and neural discrimination. As predicted, English listeners showed the poorest discrimination and identification for the vowel contrast /y/ vs. /u/, and poorer performance in the long ISI condition. In contrast to Yu et al. (2017), however, we found no effect of ISI reflected in the neural responses, specifically the mismatch negativity (MMN), P3a and late negativity ERP amplitudes. We did see a language group effect, with Mandarin listeners generally showing larger MMN and English listeners showing larger P3a. The behavioral results revealed that native language experience plays a role in echoic sensory memory trace maintenance, but the failure to find an effect of ISI on the ERP results suggests that vowel and lexical tone memory traces decay at different rates. Highlights: We examined the interaction between auditory sensory memory decay and language experience. We compared MMN, P3a, LN and behavioral responses in short vs. long interstimulus intervals. We found that different from lexical tone contrast, MMN, P3a, and LN changes to vowel contrasts are not influenced by lengthening the ISI to 2.6 s. We also found that the English listeners discriminated the non-native vowel contrast with lower accuracy under the long ISI condition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1179,Lexical Tones in Mandarin Chinese Infant-Directed Speech: Age-Related Changes in the Second Year of Life,"Tonal information is essential to early word learning in tone languages. Although numerous studies have investigated the intonational and segmental properties of infant-directed speech (IDS), only a few studies have explored the properties of lexical tones in IDS. These studies mostly focused on the first year of life; thus little is known about how lexical tones in IDS change as children's vocabulary acquisition accelerates in the second year (Goldfield and Reznick, 1990; Bloom, 2001). The present study examines whether Mandarin Chinese mothers hyperarticulate lexical tones in IDS addressing 18-and 24-month-old children-at which age children are learning words at a rapid speed-vs. adult-directed speech (ADS). Thirty-nine Mandarin Chinese-speaking mothers were tested in a semi-spontaneous picture-book-reading task, in which they told the same story to their child (IDS condition) and to an adult (ADS condition). Results for the F0 measurements (minimum F0, maximum F0, and F0 range) of tone in the speech data revealed a continuum of differences among IDS addressing 18-month-olds, IDS addressing 24-month-olds, and ADS. Lexical tones in IDS addressing 18-month-old children had a higher minimum F0, higher maximum F0, and larger pitch range than lexical tones in ADS. Lexical tones in IDS addressing 24-month-old children showed more similarity to ADS tones with respect to pitch height: there were no differences in minimum F0 and maximum F0 between ADS and IDS. However, F0 range was still larger. These results suggest that lexical tones are generally hyperarticulated in Mandarin Chinese IDS addressing 18-and 24-month-old children despite the change in pitch level over time. Mandarin Chinese mothers hyperarticulate lexical tones in IDS when talking to toddlers and potentially facilitate tone acquisition and word learning.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1180,The Development of Mismatch Responses to Mandarin Lexical Tone in 12-to 24-Month-Old Infants,"This study explores the development of mismatch responses (MMRs) to Mandarin lexical tone changes in infants at 12, 18, and 24 months of age using the multi-deviant oddball paradigm with the low dipping Tone 3 (T3) as the standard, the high level Tone 1 (T1) as the large, and the high rising Tone 2 (T2) as the small deviant. The results show that the large acoustic change between T1/T3 elicited mismatch negativity (MMN) in all three age groups. The small acoustic change between T2/T3 elicited a positive mismatch response (P-MMR) at 12 and 18 months of age, but no MMR was found to the T2/T3 change at 24 months. The coexistence of MMN and P-MMR in the same age group implies that different mechanisms were used for discriminating large and small deviants. Infants were able to detect the T1/T3 change automatically and showed adult-like MMN as early as 6 months of age. However, the detection of the T2/T3 change remains effortful in infants under 24 months of age. These findings support the notion that MMN and P-MMR may be used to index the maturation of speech perception.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1181,"Adult Learning of Novel Words in a Non-native Language: Consonants, Vowels, and Tones","While words are distinguished primarily by consonants and vowels in many languages, tones are also used in the majority of the world's languages to cue lexical contrasts. However, studies on novel word learning have largely concentrated on consonants and vowels. To shed more light on the use of tonal information in novel word learning and its relationship with the development of phonological categories, the present study explored how adults' ability to learn minimal pair pseudowords in a tone language is modulated by their native phonological knowledge. Twenty-four adult speakers of three languages were tested: Cantonese, Mandarin, and French. Eye-tracking was used to record eye movements of these learners, while they were watching animated cartoons in Cantonese. On each trial, adults had to learn two new label-object associations, while the labels differed minimally by a consonant, a vowel, or a tone. Learning would therefore attest to participants' ability to use phonological information to distinguish the paired words. Results first revealed that adult learners in each language group performed better than chance in all conditions. Moreover, compared to native Cantonese adults, both Mandarin-and French-speaking adults performed worse on all three contrasts. In addition, French adults were worse on tones when compared to Mandarin adults. Lastly, no advantage for consonantal information in native lexical processing was found for Cantonese-speaking adults as predicted by the ""division of labor"" proposal, thus confirming crosslinguistic differences in consonant/vowel weight between speakers of tonal vs. non-tonal languages. These findings establish rapid novel word learning in a non-native language (long-term learning will have to be further assessed), modulated by native phonological knowledge. The implications of the findings of this adult study for further infant word learning studies are discussed.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1182,"Training Children to Perceive Non-native Lexical Tones: Tone Language Background, Bilingualism, and Auditory-Visual Information","This study investigates the role of language background and bilingual status in the perception of foreign lexical tones. Eight groups of participants, consisting of children of 6 and 8 years from one of four language background (tone or non-tone) x bilingual status (monolingual or bilingual)-Thai monolingual, English monolingual, English-Thai bilingual, and English-Arabic bilingual were trained to perceive the four Mandarin lexical tones. Half the children in each of these eight groups were given auditory-only (AO) training and half auditory-visual (AV) training. In each group Mandarin tone identification was tested before and after (pre- and post-) training with both auditory-only test (ao-test) and auditory-visual test (av test). The effect of training on Mandarin tone identification was minimal for 6-year-olds. On the other hand, 8-year-olds, particularly those with tone language experience showed greater pre- to post-training improvement, and this was best indexed by ao-test trials. Bilingual vs. monolingual background did not facilitate overall improvement due to training, but it did modulate the efficacy of the Training mode: for bilinguals both AO and AV training, and especially AO, resulted in performance gain; but for monolinguals training was most effective with AV stimuli. Again this effect was best indexed by ao-test trials. These results suggest that tone language experience, be it monolingual or bilingual, is a strong predictor of learning unfamiliar tones; that monolinguals learn best from AV training trials and bilinguals from AO training trials; and that there is no metalinguistic advantage due to bilingualism in learning to perceive lexical tones.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1183,"Cantonese Tone Identification in Three Temporal Cues in Quiet, Speech-Shaped Noise and Two-Talker Babble","Purpose: Cochlear implant processors deliver mostly temporal envelope information and limited fundamental frequency (F0) information to the users, which make pitch and lexical tone perception challenging for cochlear implantees. Different factors have been found to affect Mandarin tone perception in temporal cues but the most effective temporal cues for lexical tone identification across different backgrounds remained unclear because no study has comprehensively examined the effects and interactions of these factors, particularly, in languages that use both pitch heights and pitch shapes to differentiate lexical meanings. The present study compared identification of Cantonese tones in naturally produced stimuli, and in three temporal cues, namely the amplitude contour cue (TE50), the periodicity cue (TE500), and the temporal fine structure cue (TFS), in three different numbers of frequency bands (B04, B08, B16) in quiet and two types of noise (two male talker-babble and speech-shaped noise). Method: Naturally produced Cantonese tones and synthetic tones that combined different acoustic cues and different number of frequency bands were presented to 18 young native Cantonese speakers for tone identification in quiet and noise. Results: Among the three temporal cues, TFS was the most effective for Cantonese tone identification in quiet and noise, except for T4 (LF) identification. Its effect was even stronger when the tones were presented in 4 or 8 bands rather than 16 bands. Neither TE500 nor TE50 was effective for Cantonese tone identification in quiet or noise. In noise, most tones in TE500 and TE50 were misheard as T4 (LF), demonstrating errors in both tone shapes and tone heights. Types of noise had limited effect on tone identification. Conclusions: Findings on Mandarin tone perception in temporal cues may not be applicable to other tone languages with more complex tonal systems. TFS presented in four bands was the most effective temporal cue for Cantonese tone identification in quiet and noise. Temporal envelope cues were not effective for tone, tone shape or tone height identification in Cantonese. These findings have implications for future design of cochlear implants for tone speakers who use pitch heights or a combination of pitch heights and pitch shapes to differentiate meanings.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1184,"Phrase Position, but not Lexical Status, Affects the Prosody of Noun/Verb Homophones","Words that can occur in more than one lexical category produce regions of ambiguity that could confound language learning and processing. However, previous findings suggest that pronunciation of noun/verb homophones may, in fact, differ as a function of category of use, potentially mitigating that ambiguity. Whether these differences are part of the lexical representation of such words or mere by-products of sentence-level prosodic processes remains an open question, the answer to which is critical to resolving questions about the structure of the lexicon. In three studies, adult native speakers of English read aloud passages containing noun/verb homophones or nonce words used in both noun and verb contexts. Acoustic measurements of the target words indicated that, while sentence position influences the acoustic properties of noun/verb homophones, including duration and pitch, there are not significant effects of lexical category when other factors are controlled. Furthermore, the lexical status of a word (real or nonce) does not produce consistent prosodic effects. These findings suggest that previously reported prosodic differences in noun/verb homophones may result from the syntactic positions in which those categories tend to occur.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1185,The Effect of Speech Variability on Tonal Language Speakers' Second Language Lexical Tone Learning,"Speech variability facilitates non-tonal language speakers' lexical tone learning. However, it remains unknown whether tonal language speakers can also benefit from speech variability while learning second language (L2) lexical tones. Researchers also reported that the effectiveness of speech variability was only shown on learning new items. Considering that the first language (L1) and L2 probably share similar tonal categories, the present study hypothesizes that speech variability only promotes the tonal language speakers' acquisition of L2 tones that are different from the tones in their L1. To test this hypothesis, the present study trained native Mandarin (a tonal language) speakers to learn Cantonese tones with either high variability (HV) or low variability (LV) speech materials, and then compared their learning performance. The results partially supported this hypothesis: only Mandarin subjects' productions of Cantonese low level and mid level tones benefited from the speech variability. They probably relied on the mental representations in Li to learn the Cantonese tones that had similar Mandarin counterparts. This learning strategy limited the impact of speech variability. Furthermore, the results also revealed a discrepancy between L2 perception and production. The perception improvement may not necessarily lead to an improvement in production.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1186,How Do Infants Disaggregate Referential and Affective Pitch?,"Infants are faced with a challenge of disaggregating functions of pitch in the ambient language into affective, pragmatic or referential (the latter in tone languages only). This mini review discusses several factors that might facilitate the disaggregation of referential and affective pitch in infancy: acoustic characteristics of infant-directed speech, recognition of vocal affect, facial cues accompanying affective prosody, and lateralization of affective and referential prosody in the brain. It proposes two hypotheses concerning the role of audiovisual cues and brain lateralization",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1187,The Diversity of Tone Languages and the Roles of Pitch Variation in Non-tone Languages: Considerations for Tone Perception Research,,,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1188,Steps Toward a Universal Grammar of Dance: Local Grouping Structure in Basic Human Movement Perception,"The general goal of this paper is to investigate the structure of our unconscious mental representation of dance: we do not perceive dance as an unanalyzed flow of movement, but we unconsciously create a mental representation regulated by structural principles. Specifically, this article examines local grouping principles in dance perception inspired by Lerdahl and Jackendoff's (1983) approach to musical grouping. I spell out the basic perceptual dimensions at work in basic human movement perception, and on that basis, I propose six principles of change that determine group boundaries in dance (change of body part, orientation, level, direction, speed, quality). I experimentally test the relevance and interaction of these principles, and find that they are organized on a scale of relative strength. This experiment thus supports the hypothesis that grouping is a general cognitive capacity applying across domains and modalities, and shows how specific grouping principles are stated in relation to modality-specific and domain-specific dimensions. More generally, it takes a step toward the development of a generative theory of dance that should help extend the research avenue of comparing complex temporal cognitive activities across modalities (visual, auditory) and purposes (referential, non-referential), which so far involves spoken language, signed language and music.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1189,Protracted Development on Native Tone Interpretation: Evidence From Mandarin-Learning Infants' Novel Word Learning,"Studies have shown that infants from cultures with tone languages develop categorical perception of their native lexical tone before their first birthday, but few studies have explored whether, and when, they interpret the phonemic function of lexical tone in word learning. Two habituation-switch experiments were conducted to explore whether Mandarin-learning infants could exploit tonal cues during their word learning, and detect a change when the association of two word-object pairs was switched. In Experiment 1, two words were solely differentiated by their lexical tones (fai/ vs. /fai/), and Mandarin-learning infants failed to detect the switch of tones at 14 months, but succeeded at 18 months. In Experiment 2, two words were markedly distinct (/fai/ vs. /bou/), and infants could detect the change of words as early as 14 months. The results indicate that infants may not refer to the lexical function of tone during their novel word learning until 18 months, even though infants from birth are able to distinguish the Tone 1 vs. Tone 3 contrast. Given that lexical tone is expressed by variations of the pitch contours, which are also related to intonation, infants' increasing knowledge of both tone and intonation may contribute to their misinterpretation of pitch contours in word learning at 14 months and, further, to their development of a sophisticated use of the phonemic function of lexical tone at 18 months of age.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1190,What Makes Lexical Tone Special: A Reverse Accessing Model for Tonal Speech Perception,"Previous studies of tonal speech perception have generally suggested harder or later access to lexical tone than segmental information, but the mechanism underlying the lexical tone disadvantage is unclear. Using a speeded discrimination paradigm free of context information, we confirmed multiple lines of evidence for the lexical tone disadvantage as well as revealed a distinctive advantage of word and atonal syllable judgments over phoneme and lexical tone judgments. The results led us to propose a Reverse Accessing Model (RAM) for tonal speech perception. The RAM is an extension of the influential TRACE model, with two additional processing levels specialized for tonal speech: lexical tone and atonal syllable. Critically, information accessing is assumed to be in reverse order of information processing, and only information at the syllable level and up is maintained active for immediate use. We tested and confirmed the predictions of the RAM on discrimination of each type of phonological component under different stimulus conditions. The current results have thus demonstrated the capability of the RAM as a general framework for tonal speech perception to provide a united account for empirical observations as well as to generate testable predictions.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1191,Multi-Talker Speech Promotes Greater Knowledge-Based Spoken Mandarin Word Recognition in First and Second Language Listeners,"Spoken word recognition involves a perceptual tradeoff between the reliance on the incoming acoustic signal and knowledge about likely sound categories and their co-occurrences as words. This study examined how adult second language (L2) learners navigate between acoustic-based and knowledge-based spoken word recognition when listening to highly variable, multi-talker truncated speech, and whether this perceptual tradeoff changes as L2 listeners gradually become more proficient in their L2 after multiple months of structured classroom learning. First language (L1) Mandarin Chinese listeners and L1 English-L2 Mandarin adult listeners took part in a gating experiment. The L2 listeners were tested twice - once at the start of their intermediate/advanced L2 language class and again 2 months later. L1 listeners were only tested once. Participants were asked to identify syllable-tone words that varied in syllable token frequency (high/low according to a spoken word corpus) and syllable-conditioned tonal probability (most probable/least probable in speech given the syllable). The stimuli were recorded by 16 different talkers and presented at eight gates ranging from onset-only (gate 1) through onset +40 ms increments (gates 2 through 7) to the full word (gate 8). Mixed-effects regression modeling was used to compare performance to our previous study which used single-talker stimuli (Wiener et al., 2019). The results indicated that multi-talker speech caused both L1 and L2 listeners to rely greater on knowledge-based processing of tone. L1 listeners were able to draw on distributional knowledge of syllable-tone probabilities in early gates and switch to predominantly acoustic-based processing when more of the signal was available. In contrast, L2 listeners, with their limited experience with talker range normalization, were less able to effectively transition from probability-based to acoustic-based processing. Moreover, for the L2 listeners, the reliance on such distributional information for spoken word recognition appeared to be conditioned by the nature of the acoustic signal. Single-talker speech did not result in the same pattern of probability-based tone processing, suggesting that knowledge-based processing of L2 speech may only occur under certain acoustic conditions, such as multi-talker speech.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1192,Dichotic Perception of Lexical Tones in Cantonese-Speaking Congenital Amusics,"Congenital amusia is an inborn neurogenetic disorder of musical pitch processing, which also induces impairment in lexical tone perception. However, it has not been examined before how the brain specialization of lexical tone perception is affected in amusics. The current study adopted the dichotic listening paradigm to examine this issue, testing 18 Cantonese-speaking amusics and 18 matched controls on pitch/lexical tone identification and discrimination in three conditions: non-speech tone, low syllable variation, and high syllable variation. For typical listeners, the discrimination accuracy was higher with shorter RT in the left ear regardless of the stimulus types, suggesting a left-ear advantage in discrimination. When the demand of phonological processing increased, as in the identification task, shorter RT was still obtained in the left ear, however, the identification accuracy revealed a bilateral pattern. Taken together, the results of the identification task revealed a reduced LEA or a shift from the right hemisphere to bilateral processing in identification. Amusics exhibited overall poorer performance in both identification and discrimination tasks, indicating that pitch/lexical tone processing in dichotic listening settings was impaired, but there was no evidence that amusics showed different ear preference from controls. These findings provided temporary evidence that although amusics demonstrate deficient neural mechanisms of pitch/lexical tone processing, their ear preference patterns might not be affected. These results broadened the understanding of the nature of pitch and lexical tone processing deficiencies in amusia.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1193,Atypical Frequency Sweep Processing in Chinese Children With Reading Difficulties: Evidence From Magnetoencephalography,"Chinese lexical tones determine word meaning and are crucial in reading development. Reduced tone awareness is widely reported in children with reading difficulties (RD). Lexical-tone processing requires sensitivity to frequency-modulated sound changes. The present study investigates whether reduced tone awareness in children with RD is reflected in basic auditory processing and the level at which the breakdown occurs. Magnetoencephalographic techniques and an oddball paradigm were used to elicit auditory-related neural responses. Five frequency sweep conditions were established to mirror the frequency fluctuation in Chinese lexical tones, including one standard (level) sweep and four deviant sweeps (fast-up, fast-down, slow-up, and slow-down). A total of 14 Chinese-speaking children aged 9-12 years with RD and 13 age-matched typically developing children were recruited. The participants completed a magnetoencephalographic data acquisition session, during which they watched a silent cartoon and the auditory stimuli were presented in a pseudorandomized order. The results revealed that the significant between-group difference was caused by differences in the level of auditory sensory processing, reflected by the P1m component elicited by the slow-up frequency sweep. This finding indicated that auditory sensory processing was affected by both the duration and the direction of a frequency sweep. Sensitivity to changes in duration and frequency is crucial for the processing of suprasegmental features. Therefore, this sensory deficit might be associated with difficulties discriminating two tones with an upward frequency contour in Chinese.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1194,Reduced Sensitivity to Between-Category Information but Preserved Categorical Perception of Lexical Tones in Tone Language Speakers With Congenital Amusia,"Previous studies have shown that for congenital amusics, long-term tone language experience cannot compensate for lexical tone processing difficulties. However, it is still unknown whether such difficulties are merely caused by domain-transferred insensitivity in lower-level acoustic processing and/or by higher-level phonological processing of linguistic pitch as well. The current P300 study links and extends previous studies by uncovering the neurophysiological mechanisms underpinning lexical tone perception difficulties in Mandarin-speaking amusics. Both the behavioral index (d ') and P300 amplitude showed reduced within-category as well as between-category sensitivity among the Mandarin-speaking amusics regardless of the linguistic status of the signal. The results suggest that acoustic pitch processing difficulties in amusics are manifested profoundly and further persist into the higher-level phonological processing that involves the neural processing of different lexical tone categories. Our findings indicate that long-term tone language experience may not compensate for the reduced acoustic pitch processing in tone language speakers with amusia but rather may extend to the neural processing of the phonological information of lexical tones during the attentive stage. However, from both the behavioral and neural evidence, the peakedness scores of thed ' and P300 amplitude were comparable between amusics and controls. It seems that the basic categorical perception (CP) pattern of native lexical tones is preserved in Mandarin-speaking amusics, indicating that they may have normal or near normal long-term categorical memory.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1195,Effects of Amateur Musical Experience on Categorical Perception of Lexical Tones by Native Chinese Adults: An ERP Study,"Music impacting on speech processing is vividly evidenced in most reports involving professional musicians, while the question of whether the facilitative effects of music are limited to experts or may extend to amateurs remains to be resolved. Previous research has suggested that analogous to language experience, musicianship also modulates lexical tone perception but the influence of amateur musical experience in adulthood is poorly understood. Furthermore, little is known about how acoustic information and phonological information of lexical tones are processed by amateur musicians. This study aimed to provide neural evidence of cortical plasticity by examining categorical perception of lexical tones in Chinese adults with amateur musical experience relative to the non-musician counterparts. Fifteen adult Chinese amateur musicians and an equal number of non-musicians participated in an event-related potential (ERP) experiment. Their mismatch negativities (MMNs) to lexical tones from Mandarin Tone 2-Tone 4 continuum and non-speech tone analogs were measured. It was hypothesized that amateur musicians would exhibit different MMNs to their non-musician counterparts in processing two aspects of information in lexical tones. Results showed that the MMN mean amplitude evoked by within-category deviants was significantly larger for amateur musicians than non-musicians regardless of speech or non-speech condition. This implies the strengthened processing of acoustic information by adult amateur musicians without the need of focused attention, as the detection of subtle acoustic nuances of pitch was measurably improved. In addition, the MMN peak latency elicited by across-category deviants was significantly shorter than that by within-category deviants for both groups, indicative of the earlier processing of phonological information than acoustic information of lexical tones at the pre-attentive stage. The results mentioned above suggest that cortical plasticity can still be induced in adulthood, hence non-musicians should be defined more strictly than before. Besides, the current study enlarges the population demonstrating the beneficial effects of musical experience on perceptual and cognitive functions, namely, the effects of enhanced speech processing from music are not confined to a small group of experts but extend to a large population of amateurs.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1196,Prominence and Expectation in Speech and Music Through the Lens of Pitch Processing,"Speech and music reflect extraordinary aspects of human cognitive abilities. Pitch, as an important parameter in the auditory domain, has been the focus of previous research on the relations between speech and music. The present study continues this line of research by focusing on two aspects of pitch processing: pitch prominence and melodic expectation. Specifically, we examined the perceived boundary of prominence for focus/accent in speech and music, plus the comparison between the pitch expectation patterns of music and speech. Speech (Mandarin Chinese) and music stimuli were created with different interval steps that increased from 1 semitone to 12 semitones from the third to the fourth word/note of a sentence/melody. The results showed that ratings of both accent/focus and expectation/surprise increased with increasing semitone distance from the baseline (though this pattern was mixed with tonal stability profiles for the melodies). Nevertheless, the perceived boundary of prominence was different for music and speech, with the boundary for detecting prominence in speech higher than that in music. Expectation also showed different patterns for speech and music. The results thus favor the suggestion that speech prosody and music melody tend to require specialized pitch patterns unique to their own respective communication purposes.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1197,"Differences and Similarities in the Contributions of Phonological Awareness, Orthographic Knowledge and Semantic Competence to Reading Fluency in Chinese School-Age Children With and Without Hearing Loss","Compared with the large number of studies on reading of children with hearing loss (HL) in alphabetic languages, there are only a very limited number of studies on reading of Chinese-speaking children with HL. It remains unclear how phonological, orthographic, and semantic skills contribute to reading fluency of Chinese school-age children with HL. The present study explored this issue by examining the performances of children with HL on reading fluency and three linguistic skills compared with matched controls with normal hearing (NH). Specifically, twenty-eight children with HL and 28 chronological-age-matched children with NH were tested on word/sentence reading fluency (WRF/SRF), phonological awareness (PA) which was composed of onset/vowel/lexical tone awareness, orthographic knowledge (OK), and semantic competence (SC) which comprised animal word identification, pseudo-homophone detection, and word segmentation. Results showed that children with HL lagged behind their peers with NH in WRF/SRF and most of the phonological, orthographic, and semantic subskills except onset awareness and pseudo-homophone detection. Furthermore, the significant contributors to WRF differed between the two groups with PA being the significant contributor in the children with NH while OK being the significant contributor in the children with HL. However, the significant contributor to SRF did not differ between the two groups with SC being the only significant contributor. These results revealed not only between-group differences but also similarities in the relative contributions of PA, OK, and SC to reading fluency at both word and sentence levels, which has practical implications for developing better training programs to improve reading for children with HL.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1198,Music Does Not Facilitate Lexical Tone Normalization: A Speech-Specific Perceptual Process,"Listeners utilize the immediate contexts to efficiently normalize variable vocal streams into standard phonology units. However, researchers debated whether non-speech contexts can also serve as valid clues for speech normalization. Supporters of the two sides proposed a general-auditory hypothesis and a speech-specific hypothesis to explain the underlying mechanisms. A possible confounding factor of this inconsistency is the listeners' perceptual familiarity of the contexts, as the non-speech contexts were perceptually unfamiliar to listeners. In this study, we examined this confounding factor by recruiting a group of native Cantonese speakers with sufficient musical training experience and a control group with minimal musical training. Participants performed lexical tone judgment tasks in three contextual conditions, i.e., speech, non-speech, and music context conditions. Both groups were familiar with the speech context and not familiar with the non-speech context. The musician group was more familiar with the music context than the non-musician group. The results evidenced the lexical tone normalization process in speech context but not non-speech nor music contexts. More importantly, musicians did not outperform non-musicians on any contextual conditions even if the musicians were experienced at pitch perception, indicating that there is no noticeable transfer in pitch perception from the music domain to the linguistic domain for tonal language speakers. The findings showed that even high familiarity with a non-linguistic context cannot elicit an effective lexical tone normalization process, supporting the speech-specific basis of the perceptual normalization process.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1199,"The Effects of Lexical Tone Awareness on Early Word Recognition, Word Reading, and Spelling From Dictation of Thai Children: A Longitudinal Study","In tonal languages such as Thai, lexical tone (the pitch of a syllable) affects word meaning. This study examined the effects of lexical tone awareness (LTA) on early word recognition and the relationship between these abilities and word reading and spelling in subsequent grades. A longitudinal design was used to assess reading-related skills in 259 Thai children, first in kindergarten (130 girls, Mage=67.25months) and later in Grade 3 (Mage=102.25months). In kindergarten, the children were tested on lexical tone identification and differentiation, early literacy skills, non-verbal IQ, and early word recognition. In Grade 3, they were tested on word reading and spelling from dictation. The hierarchical regression analyses showed that the lexical tone identification skills in kindergarten accounted for 2% of the unique variance in early word recognition. However, none of the LTA skills could predict word reading and spelling from dictation after controlling for other literacy-related skills. These findings suggest that LTA skill positively associated with early word recognition at the kindergarten level, but not for word reading and spelling from dictation at a Grade 3 level.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1200,Electrophysiological Signatures of Perceiving Alternated Tone in Mandarin Chinese: Mismatch Negativity to Underlying Tone Conflict,"Although phonological alternation is prevalent in languages, the process of perceiving phonologically alternated sounds is poorly understood, especially at the neurolinguistic level. We examined the process of perceiving Mandarin 3rd tone sandhi (T3 + T3 -> T2 + T3) with a mismatch negativity (MMN) experiment. Our design has two independent variables (whether the deviant undergoes tone sandhi; whether the standard and the deviant have matched underlying tone). These two independent variables modulated ERP responses in both the first and the second syllables. Notably, despite the apparent segmental conflict between the standard and the deviant in all conditions, MMN is only observed when neither the standard nor the deviant undergoes tone sandhi, suggesting that discovering the underlying representation of an alternated sound could interfere with the generation of MMN. A tentative model with three hypothesized underlying processing mechanisms is proposed to explain the observed latency and amplitude differences across conditions. The results are also discussed in light of the potential electrophysiological signatures involved in the process of perceiving alternated sounds.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1201,The Role of Categorical Perception and Acoustic Details in the Processing of Mandarin Tonal Alternations in Contexts: An Eye-Tracking Study,"This study investigated the perception of Mandarin tonal alternations in disyllabic words. In Mandarin, a low-dipping Tone3 is converted to a high-rising Tone2 when followed by another Tone3, known as third tone sandhi. Although previous studies showed statistically significant differences in F0 between a high-rising Sandhi-Tone3 (T3) and a Tone2, native Mandarin listeners failed to correctly categorize these two tones in perception tasks. The current study utilized the visual-world paradigm in eye-tracking to further examine whether acoustic details in lexical tone aid lexical access in Mandarin. Results showed that Mandarin listeners tend to process Tone2 as Tone2 whereas they tend to first process Sandhi-T3 as both Tone3 and Tone2, then later detect the acoustic differences between the two tones revealed by the sandhi context, and finally activate the target word during lexical access. The eye-tracking results suggest that subtle acoustic details of F0 may facilitate lexical access in automatic fashion in a tone language.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1202,A Corpus Linguistics Approach to the Representation of Western Religious Beliefs in Ten Series of Chinese University English Language Teaching Textbooks,"The early Sino-Western contact was through the way in which religion and language interact to produce language contact. However, research on this contact is relatively limited to date, particularly in the realm of English language materials. In fact, there is a paucity of research on Western religions in English Language Teaching (ELT) textbooks. By applying corpus linguistics as a tool and the Critical Discourse Analysis as the theoretical framework, this manuscript critically investigates the significant semantic domains in ten English language textbook series that are officially approved and are widely used in Chinese universities. The findings suggest that various Western religious beliefs, which are the highly unusual topics in previous Chinese ELT textbooks, are represented in the textbook corpus. The results also show that when presenting the views and attitudes toward Western religious beliefs, these textbooks have adopted an eclectic approach to the material selection. Surprisingly, positive semantic prosody surrounding the concept of religion is evident and no consistent negative authorial stance toward religion is captured. Atheism has been assumed to be in the center of Chinese intellectual traditions and the essence of the Constitution of the Chinese Communist Party. Interestingly, the findings from this study provide a new understanding of Chinese foreign language textbooks in the new era, and its addition to the literature on the study of ELT textbooks, as well as its development worldwide.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1203,Atypical patterns of tone production in tone-language-speaking children with autism,"Speakers with autism spectrum disorder (ASD) are found to exhibit atypical pitch patterns in speech production. However, little is known about the production of lexical tones (T1, T2, T3, T4) as well as neutral tones (T1N, T2N, T3N, T4N) by tone-language speakers with ASD. Thus, this study investigated the height and shape of tones produced by Mandarin-speaking children with ASD and their age-matched typically developing (TD) peers. A pronunciation experiment was conducted in which the participants were asked to produce reduplicated nouns. The findings from the acoustic analyses showed that although ASD children generally produced both lexical tones and neutral tones with distinct tonal contours, there were significant differences between the ASD and TD groups for tone height and shape for T1/T1N, T3/T3N, and T4/T4N. However, we did not find any difference in T2/T2N. These data implied that the atypical acoustic pattern in the ASD group could be partially due to the suppression of the F0 range. Moreover, we found that ASD children tended to produce more errors for T2/T2N, T3/T3N than for T1/T1N, T4/T4N. The pattern of tone errors could be explained by the acquisition principle of pitch, similarities among different tones, and tone sandhi. We thus concluded that deficits in pitch processing could be responsible for the atypical tone pattern of ASD children, and speculated that the atypical tonal contours might also be due to imitation deficits. The present findings may eventually help enhance the comprehensive understanding of the representation of atypical pitch patterns in ASD across languages.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1204,Cochlear-implant Mandarin tone recognition with a disyllabic word corpus,"Despite pitch being considered the primary cue for discriminating lexical tones, there are secondary cues such as loudness contour and duration, which may allow some cochlear implant (CI) tone discrimination even with severely degraded pitch cues. To isolate pitch cues from other cues, we developed a new disyllabic word stimulus set (Di) whose primary (pitch) and secondary (loudness) cue varied independently. This Di set consists of 270 disyllabic words, each having a distinct meaning depending on the perceived tone. Thus, listeners who hear the primary pitch cue clearly may hear a different meaning from listeners who struggle with the pitch cue and must rely on the secondary loudness contour. A lexical tone recognition experiment was conducted, which compared Di with a monosyllabic set of natural recordings. Seventeen CI users and eight normal-hearing (NH) listeners took part in the experiment. Results showed that CI users had poorer pitch cues encoding and their tone recognition performance was significantly influenced by the ""missing "" or ""confusing "" secondary cues with the Di corpus. The pitch-contour-based tone recognition is still far from satisfactory for CI users compared to NH listeners, even if some appear to integrate multiple cues to achieve high scores. This disyllabic corpus could be used to examine the performance of pitch recognition of CI users and the effectiveness of pitch cue enhancement based Mandarin tone enhancement strategies. The Di corpus is freely available online: .",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1205,"The Effect of Lexicality, Frequency, and Markedness on Mandarin Tonal Categorization","While the Ganong lexicality effect has been observed for phonemic and tonal categorization, the effects of frequency and markedness are less clear, especially in terms of tonal categorization. In this study, we use Mandarin Chinese to investigate the effects of lexicality, tone frequency and markedness. We examined Mandarin speakers' tonal categorization of tokens on all possible tonal continua with one end being a word and the other being a tonotactic gap (i.e., an unattested syllable-tone combination). The results of a forced-choice identification experiment showed a general bias against the gap endpoints, with the noted exception of continua involving T4 (X-51), the most frequent lexical tone. Specifically, when T4 served as the gap endpoint, no obvious bias against it was observed regardless of its lexical status. Moreover, on the T3-T4 continua, there was an apparent bias against T3 (X-214), the tone with the most complex contour, again, regardless of lexicality, suggesting a strong markedness effect. Taken together, the results of this study show the individual effects of lexicality, tone frequency and markedness, as well as their interactions, which contribute to our understanding of tonal categorization in relation to lexical statistics (tone frequency) and phonology (markedness).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1206,The effects of lexical frequency and homophone neighborhood density on incomplete tonal neutralization,"We investigated the effects of lexical frequency and homophone neighborhood density on the acoustic realization of two neutralizing falling tones in Dalian Mandarin Chinese. Monosyllabic morphemes containing the target tones (Tone 1 and Tone 4) were produced by 60 native speakers from two generations (middle-aged vs. young). The duration of tone-bearing syllable rhymes, as well as the F0 curves and velocity profiles of the lexical tones were quantitatively analyzed via linear mixed-effects modeling and functional data analysis. Results showed no durational difference between T1 and T4. However, the F0 contours of the two falling tones were incompletely neutralized for both young and middle-aged speakers. Lexical frequency showed little effect on the incomplete tonal neutralization; there were significant differences in the turning point of the two falling tones in syllables with both high and low lexical frequency. However, homophone neighborhood density showed an effect on the incomplete neutralization between the two falling tones, reflected in significant differences in the slope and turning point of the F0 velocity profiles between the two tones carried by syllables with low density but not with high density. Moreover, homophone neighborhood density also affected the duration, the turning point of F0 curves, and velocity profiles of the T1- and T4-syllables. These results are discussed with consideration of social phonetic variations, the theory of Hypo- and Hyper-articulation (H&H), the Neighborhood Activation Model, and communication-based information-theoretic accounts. Collectively, these results broaden our understanding of the effects that lexical properties have on the acoustic details of lexical tone production and tonal sound changes.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1207,Mapping Pitch Accents to Memory Representations in Spoken Discourse Among Chinese Learners of English: Effects of L2 Proficiency and Working Memory,"We examined L2 learners' interpretation of pitch accent cues in discourse memory and how these effects vary with proficiency and working memory (WM). One hundred sixty-eight L1-Chinese participants learning L2-English listened to recorded discourses containing pairs of contrastive alternatives and then took a later recognition memory test. Their language proficiency and WM were measured through standard tests and the participants were categorized into low, medium, advanced, and high advanced language proficiency groups. We analyzed recognition memory task performance using signal detection theory to tease apart response bias (an overall tendency to affirm memory probes) from sensitivity (the ability to discern whether a specific probe statement is true). The results showed a benefit of contrastive L + H* pitch accents in rejecting probes referring to items unmentioned in a discourse, but not contrastive alternatives themselves. More proficient participants also showed more accurate memory for the discourses overall, as well as a reduced overall bias to affirm the presented statements as true. Meanwhile, that the benefit of L + H* accents in rejecting either contrast probes or unmentioned probes was modulated for people with greater working memory. Participants with higher WM were quite sure that it did not exist in the memory trace as this part of discourse wasn't mentioned. The results support a contrast-uncertainty hypothesis, in which comprehenders recall the contrast set but fail to distinguish which is the correct item. Further, these effects were influenced by proficiency and by working memory, suggesting they reflect incomplete mapping between pitch accent and discourse representation.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1208,Implicit Bias Reflects the Company That Words Keep,"In everyday language, concepts appear alongside (i.e., collocate with) related concepts. Societal biases often emerge in these collocations; e.g., female (vs. male) names collocate with art- (vs. science-) related concepts, and African American (vs. White American) names collocate with negative (vs. positive) concepts. It is unknown whether such collocations merely reflect societal biases or contribute to them. Concepts that are themselves neutral in valence but nevertheless collocate with valenced concepts provide a unique opportunity to address this question. For example, when asked, most people evaluate the concept ""cause"" as neutral, but ""cause"" is frequently followed by negative concepts (e.g., death, pain, and trouble). We use such semantically prosodic concepts to test the influence of collocation on the emergence of implicit bias: do neutral concepts that frequently collocate with valenced concepts have corresponding implicit bias? In evaluative priming tasks, participants evaluated positive/negative nouns (Study 1) or pictures (Study 2) after seeing verb primes that were (a) strongly valenced (e.g., hate and comfort), (b) neutral in valence but collocated with valenced concepts in corpora (e.g., ease and gain), or (c) neutral in valence and not collocated with valenced concepts in corpora (e.g., reply and describe). Throughout, neutral primes with positive (negative) collocates facilitated the evaluation of positive (negative) targets much like strongly valenced primes, whereas neutral primes without valenced collocates did not. That neutral concepts with valenced collocates parallel the influence of valenced concepts suggests that their collocations in natural language may be sufficient for fostering implicit bias. Societal implications of the causal embedding hypothesis are discussed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1209,"The effects of alphabetic literacy, linguistic-processing demand and tone type on the dichotic listening of lexical tones","Brain lateralization of lexical tone processing remains a matter of debate. In this study we used a dichotic listening paradigm to examine the influences of the knowledge of Jyutping (a romanization writing system which provides explicit Cantonese tone markers), linguistic-processing demand and tone type on the ear preference pattern of native tone processing in Hong Kong Cantonese speakers. While participants with little knowledge of Jyutping showed a previously reported left-ear advantage (LEA), those with a good level of Jyutping expertise exhibited either a right-ear advantage or bilateral processing during lexical tone identification and contour tone discrimination, respectively. As for the effect of linguistic-processing demand, while an LEA was found in acoustic/phonetic perception situations, this advantage disappeared and was replaced by a bilateral pattern in conditions that involved a greater extent of linguistic processing, suggesting an increased involvement of the left hemisphere. Regarding the effect of tone type, both groups showed an LEA in level tone discrimination, but only the Jyutping group demonstrated a bilateral pattern in contour tone discrimination. Overall, knowledge of written codes of tones, greater degree of linguistic processing and contour tone processing seem to influence the brain lateralization of lexical tone processing in native listeners of Cantonese by increasing the recruitment of the left-hemisphere language network.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1210,A Perceiver-Centered Approach for Representing and Annotating Prosodic Functions in Performed Music,"Musical prosody is characterized by the acoustic variations that make music expressive. However, few systematic and scalable studies exist on the function it serves or on effective tools to carry out such studies. To address this gap, we introduce a novel approach to capturing information about prosodic functions through a citizen science paradigm. In typical bottom-up approaches to studying musical prosody, acoustic properties in performed music and basic musical structures such as accents and phrases are mapped to prosodic functions, namely segmentation and prominence. In contrast, our top-down, human-centered method puts listener annotations of musical prosodic functions first, to analyze the connection between these functions, the underlying musical structures, and acoustic properties. The method is applied primarily to the exploring of segmentation and prominence in performed solo piano music. These prosodic functions are marked by means of four annotation types-boundaries, regions, note groups, and comments-in the CosmoNote web-based citizen science platform, which presents the music signal or MIDI data and related acoustic features in information layers that can be toggled on and off. Various annotation strategies are discussed and appraised: intuitive vs. analytical; real-time vs. retrospective; and, audio-based vs. visual. The end-to-end process of the data collection is described, from the providing of prosodic examples to the structuring and formatting of the annotation data for analysis, to techniques for preventing precision errors. The aim is to obtain reliable and coherent annotations that can be applied to theoretical and data-driven models of musical prosody. The outcomes include a growing library of prosodic examples with the goal of achieving an annotation convention for studying musical prosody in performed music.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1211,Identification of Minimal Pairs of Japanese Pitch Accent in Noise-Vocoded Speech,"The perception of lexical pitch accent in Japanese was assessed using noise-excited vocoder speech, which contained no fundamental frequency (f(o)) or its harmonics. While prosodic information such as in lexical stress in English and lexical tone in Mandarin Chinese is known to be encoded in multiple acoustic dimensions, such multidimensionality is less understood for lexical pitch accent in Japanese. In the present study, listeners were tested under four different conditions to investigate the contribution of non-f(o) properties to the perception of Japanese pitch accent: noise-vocoded speech stimuli consisting of 10 3-ERBN-wide bands and 15 2-ERBN-wide bands created from a male and female speaker. Results found listeners were able to identify minimal pairs of final-accented and unaccented words at a rate better than chance in all conditions, indicating the presence of secondary cues to Japanese pitch accent. Subsequent analyses were conducted to investigate if the listeners' ability to distinguish minimal pairs was correlated with duration, intensity or formant information. The results found no strong or consistent correlation, suggesting the possibility that listeners used different cues depending on the information available in the stimuli. Furthermore, the comparison of the current results with equivalent studies in English and Mandarin Chinese suggest that, although lexical prosodic information exists in multiple acoustic dimensions in Japanese, the primary cue is more salient than in other languages.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1212,Perception of Different Tone Contrasts at Sub-Lexical and Lexical Levels by Dutch Learners of Mandarin Chinese,"This study explores the difficulties in distinguishing different lexical tone contrasts at both sub-lexical and lexical levels for beginning and advanced Dutch learners of Mandarin, using a sequence-recall task and an auditory lexical decision task. In both tasks, the Tone 2-Tone 3 contrast is most prone to errors for both groups of learners. A significant improvement in the advanced group was found for this tone contrast in the sub-lexical sequence recall task, but not in the lexical decision task. This is taken as evidence that utilizing tones in on-line spoken word recognition is more complex and demanding for L2 learners than in a memory-based task. The results of the lexical decision task also revealed that advanced learners have developed a stronger sensitivity to Tone 1 compared to the other three tones, with Tone 4 showing the least sensitivity. These findings suggest different levels of robustness and distinctiveness for the representation of different lexical tones in L2 learners' lexicon and consequently different levels of proficiency in integrating tones for lexical processing. The observed patterns of difficulty are potentially related to the acoustic characteristics of different lexical tone contrasts as well as to the interference of the suprasegmental features of learner's native language (i.e., the tonal contrasts of Dutch intonation) on the acquisition of the Mandarin lexical tone contrasts.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1213,"The Sequence Recall Task and Lexicality of Tone: Exploring Tone ""Deafness""","Many perception and processing effects of the lexical status of tone have been found in behavioral, psycholinguistic, and neuroscientific research, often pitting varieties of tonal Chinese against non-tonal Germanic languages. While the linguistic and cognitive evidence for lexical tone is therefore beyond dispute, the word prosodic systems of many languages continue to escape the categorizations of typologists. One controversy concerns the existence of a typological class of ""pitch accent languages,"" another the underlying phonological nature of surface tone contrasts, which in some cases have been claimed to be metrical rather than tonal. We address the question whether the Sequence Recall Task (SRT), which has been shown to discriminate between languages with and without word stress, can distinguish languages with and without lexical tone. Using participants from non-tonal Indonesian, semi-tonal Swedish, and two varieties of tonal Mandarin, we ran SRTs with monosyllabic tonal contrasts to test the hypothesis that high performance in a tonal SRT indicates the lexical status of tone. An additional question concerned the extent to which accuracy scores depended on phonological and phonetic properties of a language's tone system, like its complexity, the existence of an experimental contrast in a language's phonology, and the phonetic salience of a contrast. The results suggest that a tonal SRT is not likely to discriminate between tonal and non-tonal languages within a typologically varied group, because of the effects of specific properties of their tone systems. Future research should therefore address the first hypothesis with participants from otherwise similar tonal and non-tonal varieties of the same language, where results from a tonal SRT may make a useful contribution to the typological debate on word prosody.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1214,How experience with tone in the native language affects the L2 acquisition of pitch accents,"This paper tested the ability of Mandarin learners of German, whose native language has lexical tone, to imitate pitch accent contrasts in German, an intonation language. In intonation languages, pitch accents do not convey lexical information; also, pitch accents are sparser than lexical tones as they only associate with prominent words in the utterance. We compared two kinds of German pitch-accent contrasts: (1) a ""non-merger"" contrast, which Mandarin listeners perceive as different and (2) a ""merger"" contrast, which sounds more similar to Mandarin listeners. Speakers of a tone language are generally very sensitive to pitch. Hypothesis 1 (H1) therefore stated that Mandarin learners produce the two kinds of contrasts similarly to native German speakers. However, the documented sensitivity to tonal contrasts, at the expense of processing phrase-level intonational contrasts, may generally hinder target-like production of intonational pitch accents in the L2 (Hypothesis 2, H2). Finally, cross-linguistic influence (CLI) predicts a difference in the realization of these two contrasts as well as improvement with higher proficiency (Hypothesis 3, H3). We used a delayed imitation paradigm, which is well-suited for assessing L2-phonetics and -phonology because it does not necessitate access to intonational meaning. We investigated the imitation of three kinds of accents, which were associated with the sentence-final noun in short wh-questions (e.g., Wer malt denn Mandalas, lit: ""Who draws PRT mandalas?"" ""Who likes drawing mandalas?""). In Experiment 1, 28 native speakers of Mandarin participated (14 low- and 14 high-proficient). The learners' productions of the two kinds of contrasts were analyzed using General Additive Mixed Models to evaluate differences in pitch accent contrasts over time, in comparison to the productions of native German participants from an earlier study in our lab. Results showed a more pronounced realization of the non-merger contrast compared to German natives and a less distinct realization of the merger contrast, with beneficial effects of proficiency, lending support to H3. Experiment 2 tested low-proficient Italian learners of German (whose L1 is an intonation language) to contextualize the Mandarin data and further investigate CLI. Italian learners realized the non-merger contrast more target-like than Mandarin learners, lending additional support to CLI (H3).",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1215,"How Tone, Intonation and Emotion Shape the Development of Infants' Fundamental Frequency Perception","Fundamental frequency (integral(0)), perceived as pitch, is the first and arguably most salient auditory component humans are exposed to since the beginning of life. It carries multiple linguistic (e.g., word meaning) and paralinguistic (e.g., speakers' emotion) functions in speech and communication. The mappings between these functions and integral(0) features vary within a language and differ cross-linguistically. For instance, a rising pitch can be perceived as a question in English but a lexical tone in Mandarin. Such variations mean that infants must learn the specific mappings based on their respective linguistic and social environments. To date, canonical theoretical frameworks and most empirical studies do not view or consider the multi-functionality of integral(0), but typically focus on individual functions. More importantly, despite the eventual mastery of integral(0) in communication, it is unclear how infants learn to decompose and recognize these overlapping functions carried by integral(0). In this paper, we review the symbioses and synergies of the lexical, intonational, and emotional functions that can be carried by integral(0) and are being acquired throughout infancy. On the basis of our review, we put forward the Learnability Hypothesis that infants decompose and acquire multiple integral(0) functions through native/environmental experiences. Under this hypothesis, we propose representative cases such as the synergy scenario, where infants use visual cues to disambiguate and decompose the different integral(0) functions. Further, viable ways to test the scenarios derived from this hypothesis are suggested across auditory and visual modalities. Discovering how infants learn to master the diverse functions carried by integral(0) can increase our understanding of linguistic systems, auditory processing and communication functions.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1216,Individual differences in nonnative lexical tone perception: Effects of tone language repertoire and musical experience,"This study sought to understand the effects of tone language repertoire and musical experience on nonnative lexical tone perception and production. Thirty-one participants completed a tone discrimination task, an imitation task, and a musical abilities task. Results showed that a larger tone language repertoire and musical experience both enhanced tone discrimination performance. However, the effects were not additive, as musical experience was associated with tone discrimination performance for single-tone language speakers, but such association was not seen for dual-tone language speakers. Furthermore, among single-tone language speakers, but not among dual-tone language speakers, musical experience and musical aptitude positively correlated with tone discrimination accuracy. It is thus concluded that individuals with varying extents of tone language experience may adopt different strategies when performing tone discrimination tasks; single-tone language speakers may draw on their musical expertise while dual-tone language speakers may rely on their extensive tone language experience instead.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1217,Incongruent visual cues affect the perception of Mandarin vowel but not tone,"Over the recent few decades, a large number of audiovisual speech studies have been focusing on the visual cues of consonants and vowels but neglecting those relating to lexical tones. In this study, we investigate whether incongruent audiovisual information interfered with the perception of lexical tones. We found that, for both Chinese and English speakers, incongruence between auditory and visemic mouth shape (i.e., visual form information) significantly interfered with reaction time and reduced the identification accuracy of vowels. However, incongruent lip movements (i.e., visual timing information) did not interfere with the perception of auditory lexical tone. We conclude that, in contrast to vowel perception, auditory tone perception seems relatively impervious to visual congruence cues, at least under these restricted laboratory conditions. The salience of visual form and timing information is discussed based on this finding.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1218,Late mismatch negativity of lexical tone at age 8 predicts Chinese children's reading ability at age 10,"BackgroundDeficits in phonological processing are commonly reported in dyslexia but longitudinal evidence that poor speech perception compromises reading is scant. This 2-year longitudinal ERP study investigates changes in pre-attentive auditory processing that underlies categorical perception of mandarin lexical tones during the years children learn to read fluently. The main purpose of the present study was to explore the development of lexical tone categorical perception to see if it can predict children's reading ability. MethodsBoth behavioral and electrophysiological measures were taken in this study. Auditory event-related potentials were collected with a passive listening oddball paradigm. Using a stimulus continuum spanning from one lexical tone category exemplar to another, we identified a between-category and a within-category tone deviant that were acoustically equidistant from a standard stimulus. The standard stimulus occurred on 80% of trials, and one of two deviants (between-category or within-category) equiprobably on the remaining trials. 8-year-old Mandarin speakers participated in both an initial ERP oddball paradigm and returned for a 2-year follow-up. ResultsThe between-category MMN and within-category MMN significantly correlate with each other at age 8 (p = 0.001) but not at age 10. The between-category MMN at age 8 can predict children's ability at age 10 (p = 0.03) but the within-category cannot. ConclusionThe categorical perception of lexical tone is still developing from age 8 to age 10. The behavioral and electrophysiological results demonstrate that categorical perception of lexical tone at age 8 predicts children's reading ability at age 10.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1219,"Chunks, pauses, and holistic processing in Mandarin spontaneous speech","Chunks are multiword sequences with independent meaning and function, or formulaic based on the intuition of native speakers, hypothesized to be holistically restored and retrieved in the mental lexicon. Previous studies suggest that pauses and intonational boundaries tend to occur at the boundaries of chunks, but less discussion was made on the influence of chunk categories over mental processing and on pause placement associated with intonational continuity. This study adopted spontaneous monologs of Mandarin natives in formal and informal settings. It examined the co-occurrence of chunks and pause-defined processing units and pause placement around chunks to explore to what extent chunks are holistically processed. The results showed that Mandarin chunks were likely to be situated within a single processing unit, indicating chunks as smaller units than processing units in spontaneous speech. Major chunk categories exhibited significantly different patterns in co-occurring with processing units, indicating the influence of chunk properties on the mental processing of chunks. In addition, chunks tended to be fluently processed in spontaneous speech production as fewer hesitations occurred before and during chunk production. Major chunk categories shared a similar threshold in encountering hesitations before chunk production and differed significantly in hesitation distribution during chunk production. Hesitations in the middle of chunks were more likely to be situated within intonation units compared to those before chunk production. Speakers' effort to maintain the intonational continuity of chunks when they encounter processing difficulties reveals the mental reality of the holistic nature of chunks. Furthermore, the co-occurrence of chunks and processing units differed significantly between the formal and informal speech genres, indicating genre influence on the mental processing of chunks. Altogether, the findings of this study have provided implications for theories on chunks and the syntactic-prosody interface and contributed to implications for the design of Mandarin instructions and teaching.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1220,Constraints on novel word learning in heritage speakers,"Introduction Recent research on word learning has found that adults can rapidly learn novel words by tracking cross-situational statistics, but learning is greatly influenced by the phonological properties of the words and by the native language of the speakers. Mandarin-native speakers could easily pick up novel words with Mandarin tones after a short exposure, but English-native speakers had specific difficulty with the tonal components. It is, however, unclear how much experience with Mandarin is needed to successfully use the tonal cue in word learning. In this study, we explored this question by focusing on the heritage language population, who typically are exposed to the target language at an early age but then develop and switch to another majority language. Specifically, we investigated whether heritage Mandarin speakers residing in an English-speaking region and speaking English as a dominant language would be able to learn novel Mandarin tonal words from statistical tracking. It helps us understand whether early exposure to the target feature is sufficient to promote the use of that feature in word learning later in life.Methods We trained 30 heritage Mandarin speakers with Mandarin pseudowords via a cross-situational statistical word learning task (CSWL).Results and discussion Heritage Mandarin speakers were able to learn the pseudowords across multiple situations, but similar-sounding words (i.e., minimal pairs) were more difficult to identify, and words that contrast only in lexical tones (i.e., Mandarin lexical tone) were distinguished at chance level throughout learning. We also collected information about the participants' heritage language (HL) experience and usage. We did not observe a relationship between HL experience/usage and performance in tonal word learning, suggesting that HL exposure does not necessarily lead to an advantage in learning the target language.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1221,Enhancing lexical tone learning for second language speakers: effects of acoustic properties in Mandarin tone perception,"Understanding the challenges faced by second language (L2) learners in lexical tone perception is crucial for effective language acquisition. This study investigates the impact of exaggerated acoustic properties on facilitating Mandarin tone learning for English speakers. Using synthesized tone stimuli, we systematically manipulated pitch contours through three key modifications: expanding the fundamental frequency (F0), increasing F0 (female voice), and extending the overall duration. Our objectives were to assess the influence of F0 expansion, higher F0, longer duration, and varied syllables on Mandarin tone learning and generalization. Participants engaged in a non-adaptive trial-by-trial tone identification task. Mixed-effects logistic regression modeling was used to analyze accuracy across learning phases, acoustic factors, and tones. Findings reveal improvements in accuracy from training to testing and generalization phases, indicating the effectiveness of perceptual training to tone perception for adult English speakers. Tone 1 emerged as the easiest to perceive, while Tone 3 posed the most challenge, consistent with established hierarchies of tonal acquisition difficulty. Analysis of acoustic factors highlighted tone-specific effects. Expanded F0 was beneficial for the identification of Tone 2 and Tone 3 but posed challenges for Tone 1 and Tone 4. Additionally, longer durations also exhibited varied effects across tones, aiding in the identification of Tone 3 and Tone 4 but hindering Tone 1 identification. The higher F0 was advantageous for Tone 2 but disadvantageous for Tone 3. Furthermore, the syllable ma facilitated the identification of Tone 1 and Tone 2 but not for Tone 3 and Tone 4. These findings enhance our understanding of the role of acoustic properties in L2 tone perception and have implications for the design of effective training programs for second language acquisition.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1222,"Comparing the phonological, musical, and general cognitive profiles of early-emerging poor, average, and good readers of Chinese","Introduction This study compared the phonological, musical, and general cognitive profiles of early-emerging poor, average, and good readers.Methods We assessed Cantonese preschool children on Chinese word reading, phonological awareness, lexical tone awareness, musical rhythm perception, musical pitch perception, working memory, and non-verbal intelligence.Results Early-emerging poor readers exhibited poorer phonological awareness than early-emerging average and good readers, whereas the latter two groups did not differ significantly. In the working memory task, early-emerging good readers outperformed both early-emerging average and poor readers, who performed similarly. No significant group differences were found in lexical tone awareness, musical rhythm perception, musical pitch perception, or non-verbal intelligence.Discussion The results reflect phonological deficits in early-emerging poor readers. Furthermore, phonological awareness and working memory were useful for identifying early-emerging poor and good readers, respectively. Clinically, these findings imply that early-emerging poor readers may benefit most from initial phonological awareness training, followed by working memory training. Moreover, working memory training may also be beneficial for early-emerging average readers seeking to improve their Chinese word reading.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1223,Social Cognition Analyzer Application-A New Method for the Analysis of Social Cognition in Patients Diagnosed With Schizophrenia,"Introduction: Because of the importance of the assessment of social cognitive impairments in schizophrenia in clinical settings, a new computer application called SCAN (Social Cognition Analyzer applicatioN) was developed. Our first aim was to examine if patients diagnosed with schizophrenia could be differentiated from healthy individuals based on the results of SCAN, taking into consideration both response rates and response times. Our second aim was to create Scanalizer, as part of SCAN, to produce social cognitive profiles of individual patients. Materials and Methods: 86 patients (SG) and 101 healthy participants (CG) were examined with SCAN. The domains were: ToM, irony, metaphor, emotion perception from prosody and social perception. SCAN displayed the tasks, recorded the answers and the response times. For the differentiation of the two groups a two-dimensional scatter plot was used. For the graphical presentation of the social cognitive profile of patients, the calculation of the distributions of CG's results was made with Kolmogorov-Smirnov Goodness-of-fit Test and with the sum of squared residuals (SSR). Results: We found that the SG's response rates were significantly lower and the SG's response times were significantly slower compared to the CG in every condition. With the two-dimensional comparison of the summary response rates and the summary response times of the participants, the SG could be differentiated from the CG and this differentiation worked irrespective of age and education. For the graphical representation of social cognitive functions of patients, distributions of the results of the CG were calculated. We found normal distributions in the response times of all conditions and in the response rates of the ToM condition. In the low-end tail of the irony condition, and in the metaphor, social perception and emotional prosody conditions, power-law distributions were found. We also found that the summary response rates of the lowest performing 10% of the CG was in the same range as the summary response rates of all examined patients. Discussion: Scanalizer enables clinicians to measure and analyse social cognitive profiles of patients diagnosed with schizophrenia. Moreover, SCAN could also be used to detect social cognitive disabilities of vulnerable individuals.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1224,Enhancing Embedded Space with Low-Level Features for Speech Emotion Recognition,"This work proposes an approach that uses a feature space by combining the representation obtained in the unsupervised learning process and manually selected features defining the prosody of the utterances. In the experiments, we used two time-frequency representations (Mel and CQT spectrograms) and EmoDB and RAVDESS databases. As the results show, the proposed system improved the classification accuracy of both representations: 1.29% for CQT and 3.75% for Mel spectrogram compared to the typical CNN architecture for the EmoDB dataset and 3.02% for CQT and 0.63% for Mel spectrogram in the case of RAVDESS. Additionally, the results present a significant increase of around 14% in classification performance in the case of happiness and disgust emotions using Mel spectrograms and around 20% in happiness and disgust emotions for CQT in the case of best models trained on EmoDB. On the other hand, in the case of models that achieved the highest result for the RAVDESS database, the most significant improvement was observed in the classification of a neutral state, around 16%, using the Mel spectrogram. For CQT representation, the most significant improvement occurred for fear and surprise, around 9%. Additionally, the average results for all prepared models showed the positive impact of the method used on the quality of classification of most emotional states. For the EmoDB database, the highest average improvement was observed for happiness-14.6%. For other emotions, it ranged from 1.2% to 8.7%. The only exception was the emotion of sadness, for which the classification quality was average decreased by 1% when using the Mel spectrogram. In turn, for the RAVDESS database, the most significant improvement also occurred for happiness-7.5%, while for other emotions ranged from 0.2% to 7.1%, except disgust and calm, the classification of which deteriorated for the Mel spectrogram and the CQT representation, respectively.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1225,Acoustic Analysis and Perceptual Evaluation of Second Language Cantonese Tones Produced by Advanced Mandarin-Speaking Learners,"The tonal system of Cantonese is very different from that of Mandarin, which creates potential challenges for Mandarin speakers when learning Cantonese. The aim of this study was to explore second language (L2) production of Cantonese tones by advanced learners whose first language (L1) is Mandarin. Forty-one informants participated in a recording experiment to provide production data of Cantonese tones. The speech data were measured acoustically using the computer software Praat (Version 6.3.10) and were evaluated perceptually by native Cantonese speakers. The relationship between the acoustic analysis and perceptual evaluation was also explored. The acoustic and perceptual evaluations confirmed that, while the tones that the Mandarin learners of Cantonese produced were non-native-like, their production of the Cantonese T1 and T2 was good in general. Furthermore, the accuracy of the perceptual evaluations could be predicted based on the acoustic features of the L2 tones. Our findings are in line with hypotheses in current speech learning models, and demonstrate that familiar phonetic categories are easier to acquire than are unfamiliar ones. To provide a more complete picture of L2 speech acquisition, future research should investigate L2 tone acquisition using both production and perception data obtained from participants with a greater variety of L1s.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1226,Impaired Prosodic Processing but Not Hearing Function Is Associated with an Age-Related Reduction in AI Speech Recognition,"Background/Objectives: Voice artificial intelligence (AI) technology is becoming increasingly common. Recent work indicates that middle-aged to older adults are less able to identify modern AI speech compared to younger adults, but the underlying causes are unclear. Methods: The current study with younger and middle-aged to older adults investigated factors that could explain the age-related reduction in AI speech identification. Experiment 1 investigated whether high-frequency information in speech-to which middle-aged to older adults often have less access due sensitivity loss at high frequencies-contributes to age-group differences. Experiment 2 investigated whether an age-related reduction in the ability to process prosodic information in speech predicts the reduction in AI speech identification. Results: Results for Experiment 1 show that middle-aged to older adults are less able to identify AI speech for both full-bandwidth speech and speech for which information above 4 kHz is removed, making the contribution of high-frequency hearing loss unlikely. Experiment 2 shows that the ability to identify AI speech is greater in individuals who also show a greater ability to identify emotions from prosodic speech information, after accounting for hearing function and self-rated experience with voice-AI systems. Conclusions: The current results suggest that the ability to identify AI speech is related to the accurate processing of prosodic information.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1227,Extended Expressive Intonation: An Application of the Convergents and Semiconvergents in Pythagorean Tuning,"Cyclic scales are associated with convergents and semiconvergents of the continued fraction expansions of the generator tone. After each convergent, a scale lineage ends and another begins. Along a lineage, a constant number of generic accidentals are successively added to its first scale, becoming regularly interspersed. In this way, it is easier to know where each note is to go. This process, applied to the lineage of the 7-, 12-, and 17-tone scales, is related to expressive intonation. Such a concept is extended to larger scales with added microtones and it is described how they can be chosen in terms of the starting index of the scale. An automorphism in terms of the step and co-step indices associated with the two elementary intervals provides a two-dimensional representation that shares some common features with the musical staff.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1228,Bimodal Benefits for Lexical Tone Recognition: An Investigation on Mandarin-speaking Preschoolers with a Cochlear Implant and a Contralateral Hearing Aid,"Pitch perception is known to be difficult for individuals with cochlear implant (CI), and adding a hearing aid (HA) in the non-implanted ear is potentially beneficial. The current study aimed to investigate the bimodal benefit for lexical tone recognition in Mandarin-speaking preschoolers using a CI and an HA in opposite ears. The child participants were required to complete tone identification in quiet and in noise with CI + HA in comparison with CI alone. While the bimodal listeners showed confusion between Tone 2 and Tone 3 in recognition, the additional acoustic information from the contralateral HA alleviated confusion between these two tones in quiet. Moreover, significant improvement was demonstrated in the CI + HA condition over the CI alone condition in noise. The bimodal benefit for individual subjects could be predicted by the low-frequency hearing threshold of the non-implanted ear and the duration of bimodal use. The findings support the clinical practice to fit a contralateral HA in the non-implanted ear for the potential benefit in Mandarin tone recognition in CI children. The limitations call for further studies on auditory plasticity on an individual basis to gain insights on the contributing factors to the bimodal benefit or its absence.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1229,Just-Noticeable Differences of Fundamental Frequency Change in Mandarin-Speaking Children with Cochlear Implants,"Fundamental frequency (F0) provides the primary acoustic cue for lexical tone perception in tonal languages but remains poorly represented in cochlear implant (CI) systems. Currently, there is still a lack of understanding of sensitivity to F0 change in CI users who speak tonal languages. In the present study, just-noticeable differences (JNDs) of F0 contour and F0 level changes in Mandarin-speaking children with CIs were measured and compared with those in their age-matched normal-hearing (NH) peers. Results showed that children with CIs demonstrated significantly larger JND of F0 contour (JND-C) change and F0 level (JND-L) change compared to NH children. Further within-group comparison revealed that the JND-C change was significantly smaller than the JND-L change among children with CIs, whereas the opposite pattern was observed among NH children. No significant correlations were seen between JND-C change/JND-L change and age at implantation /duration of CI use. The contrast between children with CIs and NH children in sensitivity to F0 contour and F0 level change suggests different mechanisms of F0 processing in these two groups as a result of different hearing experiences.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1230,Learning to Perceive Non-Native Tones via Distributional Training: Effects of Task and Acoustic Cue Weighting,"As many distributional learning (DL) studies have shown, adult listeners can achieve discrimination of a difficult non-native contrast after a short repetitive exposure to tokens falling at the extremes of that contrast. Such studies have shown using behavioural methods that a short distributional training can induce perceptual learning of vowel and consonant contrasts. However, much less is known about the neurological correlates of DL, and few studies have examined non-native lexical tone contrasts. Here, Australian-English speakers underwent DL training on a Mandarin tone contrast using behavioural (discrimination, identification) and neural (oddball-EEG) tasks, with listeners hearing either a bimodal or a unimodal distribution. Behavioural results show that listeners learned to discriminate tones after both unimodal and bimodal training; while EEG responses revealed more learning for listeners exposed to the bimodal distribution. Thus, perceptual learning through exposure to brief sound distributions (a) extends to non-native tonal contrasts, and (b) is sensitive to task, phonetic distance, and acoustic cue-weighting. Our findings have implications for models of how auditory and phonetic constraints influence speech learning.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1231,Explaining L2 Lexical Learning in Multiple Scenarios: Cross-Situational Word Learning in L1 Mandarin L2 English Speakers,"Adults commonly struggle with perceiving and recognizing the sounds and words of a second language (L2), especially when the L2 sounds do not have a counterpart in the learner's first language (L1). We examined how L1 Mandarin L2 English speakers learned pseudo English words within a cross-situational word learning (CSWL) task previously presented to monolingual English and bilingual Mandarin-English speakers. CSWL is ambiguous because participants are not provided with direct mappings of words and object referents. Rather, learners discern word-object correspondences through tracking multiple co-occurrences across learning trials. The monolinguals and bilinguals tested in previous studies showed lower performance for pseudo words that formed vowel minimal pairs (e.g., /dit/-/d?t/) than pseudo word which formed consonant minimal pairs (e.g., /bLATIN SMALL LETTER OPEN On/-/pLATIN SMALL LETTER OPEN On/) or non-minimal pairs which differed in all segments (e.g., /bLATIN SMALL LETTER OPEN On/-/dit/). In contrast, L1 Mandarin L2 English listeners struggled to learn all word pairs. We explain this seemingly contradicting finding by considering the multiplicity of acoustic cues in the stimuli presented to all participant groups. Stimuli were produced in infant-directed-speech (IDS) in order to compare performance by children and adults and because previous research had shown that IDS enhances L1 and L2 acquisition. We propose that the suprasegmental pitch variation in the vowels typical of IDS stimuli might be perceived as lexical tone distinctions for tonal language speakers who cannot fully inhibit their L1 activation, resulting in high lexical competition and diminished learning during an ambiguous word learning task. Our results are in line with the Second Language Linguistic Perception (L2LP) model which proposes that fine-grained acoustic information from multiple sources and the ability to switch between language modes affects non-native phonetic and lexical development.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1232,Spoken Word Recognition across Language Boundary: ERP Evidence of Prosodic Transfer Driven by Pitch,"Extensive research has explored the perception of English lexical stress by Chinese EFL learners and tried to unveil the underlying mechanism of the prosodic transfer from a native tonal language to a non-native stress language. However, the role of the pitch as the shared cue by lexical stress and lexical tone during the transfer remains controversial when the segmental cue (i.e., reduced vowel) is absent. By employing event-related potential (ERP) measurements, the current study aimed to further investigate the role of the pitch during the prosodic transfer from L1 lexical tone to L2 lexical stress and the underlying neural responses. Two groups of adult Chinese EFL learners were compared, as both Mandarin and Cantonese are tonal languages with different levels of complexity. The results showed that Cantonese speakers relied more than Mandarin speakers on pitch cues, not only in their processing of English lexical stress but also in word recognition. Our findings are consistent with the arguments of Cue Weighting and attest to the influence of native tonal language experience on second language acquisition. The results may have implications on pedagogical methods that pitch could be an important clue in second language teaching.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1233,Sex Differences in Processing Emotional Speech Prosody: Preliminary Findings from a Multi-Feature Oddball Study,"Background/Objectives: Emotional prosody, the intonation and rhythm of speech that conveys emotions, is vital for speech communication as it provides essential context and nuance to the words being spoken. This study explored how listeners automatically process emotional prosody in speech, focusing on different neural responses for the prosodic categories and potential sex differences. Methods: The pilot data here involved 11 male and 11 female adult participants (age range: 18-28). A multi-feature oddball paradigm was used, in which participants were exposed to sequences of non-repeating English words with emotional (angry, happy, sad) or neutral prosody while watching a silent movie. Results: Both mismatch negativity (MMN) and P3a components were observed, indicating automatic perceptual grouping and neural sensitivity to emotional variations in speech. Women showed stronger MMN to angry than sad prosody, while men showed stronger MMN to angry than happy prosody. Happy prosody elicited the strongest P3a, but only in men. Conclusions: The findings challenge the notion that all facets of emotion processing are biased toward female superiority. However, these results from 22 young adult native English speakers should be interpreted with caution, as data from a more adequate sample size are needed to test the generalizability of the findings. Combined with results from studies on children and elderly adults, these preliminary data underscore the need to explore the complexities of emotional speech processing mechanisms to account for category and sex differences across the lifespan in a longitudinal perspective.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1234,"The Neural Development of Chinese Lexical Tone Perception: A Mismatch Negativity Study Across Childhood, Adolescence, and Adulthood","Background/Objectives: In a tonal language like Chinese, phonologically contrasting tones signify word meanings at the syllable level. Although the development of lexical tone perception ability has been examined in many behavioral studies, its developmental trajectory from childhood to adulthood at the neural level remains unclear. This cross-sectional study aimed to examine the issue by measuring the mismatch negativity (MMN) response to a Chinese lexical tonal contrast in three groups. Methods: The MMN response to a flat-falling tonal contrast (Tone1 versus Tone4) were recorded from children (25 participants aged 6-8), adolescents (26 participants aged 12-14), and young adults (20 participants aged 18-24). Nonsense speech stimuli were also used by superimposing Tone1 and Tone4 on an English syllable. Results: All three groups demonstrated typical early MMN responses in both the meaningful and nonsense syllable conditions. However, the MMN amplitudes varied significantly across groups, with the child group showing smaller responses compared to the adolescent and adult groups, while the latter two groups had similar MMN amplitudes. Conclusions: Neural sensitivity to tonal contrasts is not fully mature in children and reaches a more adult-like level during adolescence, with no significant difference in sensitivity to meaningful versus nonsense syllables. These results provide new insights into the neural development of lexical tone perception in a tonal language, highlighting its maturation during adolescence in this process.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1235,Changes in Oscillatory Brain Networks after Lexical Tone Training,"Learning foreign speech contrasts involves creating new representations of sound categories in memory. This formation of new memory representations is likely to involve changes in neural networks as reflected by oscillatory brain activity. To explore this, we conducted time-frequency analyses of electro-encephalography (EEG) data recorded in a passive auditory oddball paradigm using Thai language tones. We compared native speakers of English (a non-tone language) and native speakers of Mandarin Chinese (a tone language), before and after a two-day laboratory training. Native English speakers showed a larger gamma-band power and stronger alpha-band synchrony across EEG channels than the native Chinese speakers, especially after training. This is compatible with the view that forming new speech categories on the basis of unfamiliar perceptual dimensions involves stronger gamma activity and more coherent activity in alpha-band networks than forming new categories on the basis of familiar dimensions.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1236,Seeing a Face in a Crowd of Emotional Voices: Changes in Perception and Cortisol in Response to Emotional Information across the Senses,"One source of information we glean from everyday experience, which guides social interaction, is assessing the emotional state of others. Emotional state can be expressed through several modalities: body posture or movements, body odor, touch, facial expression, or the intonation in a voice. Much research has examined emotional processing within one sensory modality or the transfer of emotional processing from one modality to another. Yet, less is known regarding interactions across different modalities when perceiving emotions, despite our common experience of seeing emotion in a face while hearing the corresponding emotion in a voice. Our study examined if visual and auditory emotions of matched valence (congruent) conferred stronger perceptual and physiological effects compared to visual and auditory emotions of unmatched valence (incongruent). We quantified how exposure to emotional faces and/or voices altered perception using psychophysics and how it altered a physiological proxy for stress or arousal using salivary cortisol. While we found no significant advantage of congruent over incongruent emotions, we found that changes in cortisol were associated with perceptual changes. Following exposure to negative emotional content, larger decreases in cortisol, indicative of less stress, correlated with more positive perceptual after-effects, indicative of stronger biases to see neutral faces as happier.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1237,Evaluating the Relative Perceptual Salience of Linguistic and Emotional Prosody in Quiet and Noisy Contexts,"How people recognize linguistic and emotional prosody in different listening conditions is essential for understanding the complex interplay between social context, cognition, and communication. The perception of both lexical tones and emotional prosody depends on prosodic features including pitch, intensity, duration, and voice quality. However, it is unclear which aspect of prosody is perceptually more salient and resistant to noise. This study aimed to investigate the relative perceptual salience of emotional prosody and lexical tone recognition in quiet and in the presence of multi-talker babble noise. Forty young adults randomly sampled from a pool of native Mandarin Chinese with normal hearing listened to monosyllables either with or without background babble noise and completed two identification tasks, one for emotion recognition and the other for lexical tone recognition. Accuracy and speed were recorded and analyzed using generalized linear mixed-effects models. Compared with emotional prosody, lexical tones were more perceptually salient in multi-talker babble noise. Native Mandarin Chinese participants identified lexical tones more accurately and quickly than vocal emotions at the same signal-to-noise ratio. Acoustic and cognitive dissimilarities between linguistic prosody and emotional prosody may have led to the phenomenon, which calls for further explorations into the underlying psychobiological and neurophysiological mechanisms.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1238,"Hearing Tones, Missing Boundaries: Cross-Level Selective Transfer of Prosodic Boundaries Among Chinese-English Learners","Second language (L2) learners often struggle to process prosodic boundaries, which are essential for speech comprehension. This study investigated the nature of these difficulties and how first language (L1) cue-weighting strategies transfer to L2 processing among Chinese (Mandarin)-English learners. The rising pitch that cues English phrase boundaries acoustically overlaps with functionally distinct Chinese lexical tones. Through two experiments comparing Chinese-English learners and native English speakers, we assessed sensitivity across lexical constituent, phrase, and sentence boundaries and manipulated acoustic cues (pause, lengthening, pitch) to estimate their perceptual weights during phrase-boundary identification. L2 learners showed reduced discrimination sensitivity only at the phrase level, performing comparably to native speakers at lexical constituent and sentence boundaries. For phrase boundaries, learners over-relied on pitch and under-relied on pre-boundary lengthening compared to native speakers, though both groups weighted pauses strongly. This selective deficit implicates the transfer of L1 cue-weighting strategies more than a global knowledge deficit. Our findings support a dynamic transfer model where L1 sensitivity to lexical tone transfer of L2 phrase perception, elevating the weight of pitch. While learners show partial adaptation, these results refine the Cue-Weighting Transfer Hypothesis by demonstrating that L2 prosodic acquisition involves both integrated L1 transfer and L2-driven reweighting strategies.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1239,Speech Processing for Language Learning: A Practical Approach to Computer-Assisted Pronunciation Teaching,"This article contributes to the discourse on how contemporary computer and information technology may help in improving foreign language learning not only by supporting better and more flexible workflow and digitizing study materials but also through creating completely new use cases made possible by technological improvements in signal processing algorithms. We discuss an approach and propose a holistic solution to teaching the phonological phenomena which are crucial for correct pronunciation, such as the phonemes; the energy and duration of syllables and pauses, which construct the phrasal rhythm; and the tone movement within an utterance, i.e., the phrasal intonation. The working prototype of StudyIntonation Computer-Assisted Pronunciation Training (CAPT) system is a tool for mobile devices, which offers a set of tasks based on a ""listen and repeat"" approach and gives the audio-visual feedback in real time. The present work summarizes the efforts taken to enrich the current version of this CAPT tool with two new functions: the phonetic transcription and rhythmic patterns of model and learner speech. Both are designed on a base of a third-party automatic speech recognition (ASR) library Kaldi, which was incorporated inside StudyIntonation signal processing software core. We also examine the scope of automatic speech recognition applicability within the CAPT system workflow and evaluate the Levenstein distance between the transcription made by human experts and that obtained automatically in our code. We developed an algorithm of rhythm reconstruction using acoustic and language ASR models. It is also shown that even having sufficiently correct production of phonemes, the learners do not produce a correct phrasal rhythm and intonation, and therefore, the joint training of sounds, rhythm and intonation within a single learning environment is beneficial. To mitigate the recording imperfections voice activity detection (VAD) is applied to all the speech records processed. The try-outs showed that StudyIntonation can create transcriptions and process rhythmic patterns, but some specific problems with connected speech transcription were detected. The learners feedback in the sense of pronunciation assessment was also updated and a conventional mechanism based on dynamic time warping (DTW) was combined with cross-recurrence quantification analysis (CRQA) approach, which resulted in a better discriminating ability. The CRQA metrics combined with those of DTW were shown to add to the accuracy of learner performance estimation. The major implications for computer-assisted English pronunciation teaching are discussed.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1240,Multi-Channel Spectro-Temporal Representations for Speech-Based Parkinson's Disease Detection,"Early, non-invasive detection of Parkinson's Disease (PD) using speech analysis offers promise for scalable screening. In this work, we propose a multi-channel spectro-temporal deep-learning approach for PD detection from sentence-level speech, a clinically relevant yet underexplored modality. We extract and fuse three complementary time-frequency representations-mel spectrogram, constant-Q transform (CQT), and gammatone spectrogram-into a three-channel input analogous to an RGB image. This fused representation is evaluated across CNNs (ResNet, DenseNet, and EfficientNet) and Vision Transformer using the PC-GITA dataset, under 10-fold subject-independent cross-validation for robust assessment. Results showed that fusion consistently improves performance over single representations across architectures. EfficientNet-B2 achieves the highest accuracy (84.39% +/- 5.19%) and F1-score (84.35% +/- 5.52%), outperforming recent methods using handcrafted features or pretrained models (e.g., Wav2Vec2.0, HuBERT) on the same task and dataset. Performance varies with sentence type, with emotionally salient and prosodically emphasized utterances yielding higher AUC, suggesting that richer prosody enhances discriminability. Our findings indicate that multi-channel fusion enhances sensitivity to subtle speech impairments in PD by integrating complementary spectral information. Our approach implies that multi-channel fusion could enhance the detection of discriminative acoustic biomarkers, potentially offering a more robust and effective framework for speech-based PD screening, though further validation is needed before clinical application.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1241,Prosodic Transfer in English Literacy Skills among Chinese Elementary-Age Students: Controlling for Non-Verbal Intelligence,"Building upon the prosodic transfer hypothesis, the current study aims to examine the intermediary effect of English stress on the relation between Chinese lexical tone awareness and English word-level literacy (reading and spelling) as well as the moderating effect of English oral vocabulary proficiency on the cross-linguistic association. Grade 4 Chinese learners of English (N = 224) participated in this study and were assessed for their tone and stress sensitivity, English oral vocabulary, English word reading, and English word spelling. Mediated multivariate analyses with moderation were used to explore: (1) whether the influence of lexical tone perception on L2 word reading and spelling was mediated by English stress as posited in the prosodic transfer hypothesis; (2) whether the effects of tone on English word reading and spelling performance varied as a function of oral vocabulary levels. The findings revealed a direct positive relationship between Chinese tone and English word reading and spelling, and the relationship was mediated by English stress awareness. Furthermore, the direct pathway from tone to English word-level literacy skills were moderated by oral vocabulary and the relationship between tone and English word-level skills became stronger as oral vocabulary levels increased; however, such strength reached a plateau among children without adequate oral vocabulary skills. These findings suggest the necessity to incorporate word spelling as an outcome in the cross-suprasegmental phonological transfer models of early literacy development. Additionally, the current study endorses the complexity of cross-language prosodic transfer. It points to a precise threshold for sufficient L2 oral vocabulary skills to enable tone transfer in English word-level literacy attainment.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1242,English-Learning Infants' Developing Sensitivity to Intonation Contours,"In four experiments, we investigated when and how English-learning infants perceive intonation contours that signal prosodic units. Using visual habituation, we probed infants' ability to discriminate disyllabic sequences with a fall versus a rise in pitch on the final syllable, a salient cue used to distinguish statements from questions. First, we showed that at 8 months, English-learning infants can distinguish statement falls from question rises, as has been reported previously for their European Portuguese-learning peers who have extensive experience with minimal pairs that differ just in pitch rises and falls. Next, we conducted three experiments involving 4-month-olds to determine the developmental roots of how English-learning infants begin to tune into these intonation contours. In Experiment 2, we showed that unlike 8-month-olds, monolingual English-learning 4-month-olds are unable to distinguish statement and question intonation when they are presented with segmentally varied disyllabic sequences. Monolingual English-learning 4-month-olds only partially succeeded even when tested without segmental variability and a sensitive testing procedure (Experiment 3). When tested with stimuli that had been resynthesized to remove correlated duration cues as well, 4-month-olds demonstrated only partial success (Experiment 4). We discuss our results in the context of extant developmental research on how infants tune into linguistically relevant pitch cues in their first year of life.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1243,Using Eye-Movements to Track Bilingual Activation,"Recent research found that the languages of bilingual listeners are active and interact, such that both lexical representations are activated by the spoken input with which they are compatible. However, the time course of bilingual activation and whether suprasegmental information further modulates this cross-language competition are still not well understood. This study investigates the effect of stress placement on the processing of English-Spanish cognates by beginner-to-intermediate Spanish-speaking second-language (L2) learners of English and intermediate-to-advanced English-speaking L2 learners of Spanish using the visual-world eye-tracking paradigm. In each trial, participants saw a target (asado, 'roast'), one of two competitors (stress match: asados, 'roast (pl)'; stress mismatch: asador, 'rotisserie'), and two unrelated distracters, while hearing the target word. The experiment included a non-cognate condition (asado-asados-asador) and a cognate condition, where the stress pattern of the English word corresponding to the Spanish competitor in the stress-mismatch condition (inventor) instead matched that of the Spanish target (invento, 'invent'). Growth-curve analyses revealed cognate-status and stress-mismatch effects for Spanish-speaking L2 learners of English, and cognate-status and stress-mismatch effects, and an interaction for English-speaking L2 learners of Spanish. This suggests that both groups use stress for word recognition, but the English stress pattern only affects the processing of Spanish words in the English-speaking L2 learners of Spanish.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1244,Recursion and the Definition of Universal Prosodic Categories,"It is widely agreed that prosodic constituents should mirror syntactic constituents (unless high-ranking prosodic constraints interfere). Because recursion is a feature of syntactic representations, one expects recursion in prosodic representations as well. However, it is of current controversy what kinds of syntactic representation motivate prosodic recursion. In this paper, the use of Phonological Phrase recursion is reviewed in several case studies, chosen because prosodic recursion mostly does not reflect syntactic recursion as defined in current syntactic theory. We provide reanalyses that do not appeal to prosodic recursion (unless syntactically motivated), showing that Phonological Phrase recursion is not necessary to capture the relevant generalizations. The more restrictive use of prosodic recursion we argue for has the following conceptual advantages. It allows for more consistent cross-linguistic generalizations about the syntax-prosody mapping so that prosodic representations more closely reflect syntactic ones. It allows the fundamental syntactic distinctions between clause (and other phases) and phrase to be reflected in the prosodic representation, and it allows cross-linguistic generalizations to be made about the prosodic domain of intonational processes, such as downstep and continuation rise.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1245,Unsupervised Representation Learning with Task-Agnostic Feature Masking for Robust End-to-End Speech Recognition,"Unsupervised learning-based approaches for training speech vector representations (SVR) have recently been widely applied. While pretrained SVR models excel in relatively clean automatic speech recognition (ASR) tasks, such as those recorded in laboratory environments, they are still insufficient for practical applications with various types of noise, intonation, and dialects. To cope with this problem, we present a novel unsupervised SVR learning method for practical end-to-end ASR models. Our approach involves designing a speech feature masking method to stabilize SVR model learning and improve the performance of the ASR model in a downstream task. By introducing a noise masking strategy into diverse combinations of the time and frequency regions of the spectrogram, the SVR model becomes a robust representation extractor for the ASR model in practical scenarios. In pretraining experiments, we train the SVR model using approximately 18,000 h of Korean speech datasets that included diverse speakers and were recorded in environments with various amounts of noise. The weights of the pretrained SVR extractor are then frozen, and the extracted speech representations are used for ASR model training in a downstream task. The experimental results show that the ASR model using our proposed SVR extractor significantly outperforms conventional methods.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1246,Multimodal Prompt Learning in Emotion Recognition Using Context and Audio Information,"Prompt learning has improved the performance of language models by reducing the gap in language model training methods of pre-training and downstream tasks. However, extending prompt learning in language models pre-trained with unimodal data to multimodal sources is difficult as it requires additional deep-learning layers that cannot be attached. In the natural-language emotion-recognition task, improved emotional classification can be expected when using audio and text to train a model rather than only natural-language text. Audio information, such as voice pitch, tone, and intonation, can give more information that is unavailable in text to predict emotions more effectively. Thus, using both audio and text can enable better emotion prediction in speech emotion-recognition models compared to semantic information alone. In this paper, in contrast to existing studies that use multimodal data with an additional layer, we propose a method for improving the performance of speech emotion recognition using multimodal prompt learning with text-based pre-trained models. The proposed method is using text and audio information in prompt learning by employing a language model pre-trained on natural-language text. In addition, we propose a method to improve the emotion-recognition performance of the current utterance using the emotion and contextual information of the previous utterances for prompt learning in speech emotion-recognition tasks. The performance of the proposed method was evaluated using the English multimodal dataset MELD and the Korean multimodal dataset KEMDy20. Experiments using both the proposed methods obtained an accuracy of 87.49%, F1 score of 44.16, and weighted F1 score of 86.28.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1247,Effective Presentation Speech Support System for Representing Emphasis-Intention,"A research presentation integrates slides and speech. If these two aspects do not represent the same intention, the presentation will probably fail to effectively explain the presenter's intention. This paper focuses on the representation of the critical contents in a presentation. In an effective speech, the speaker adds more intonation and stress to emphasize the importance of the slide contents. Audiences recognize that important contents are those that are explained in a stronger voice or that are said after a short pause. However, in ineffective speeches, such voice effects do not always correspond to the important contents that are indicated by slides. On slides, the important contents are represented by levels of text indentation and size, color, and animation. This research develops a presentation speech support system that estimates important contents from slides and voices that might be recognized by audiences and extracts numerical differences. In addition, the system provides comments and feedback to improve speeches.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1248,Processing of Chinese homophonic two-part allegoric sayings: Effects of familiarity and homophone,"Two-part allegorical sayings are a typical language form in Chinese. Understanding two-part allegorical saying involves the ability to understand figurative meanings. Chinese two-part allegorical sayings convey figurative meanings by activating either homophonic or conceptual associations. Homophonic associations are realized based on a conceptual connection between the two homophonic expressions: the second part of the sayings and the expression of the idiomatic meaning. Within the example of Lao tai tai shang ji wo ((sic))-ben dan ((sic)), a situation is described as an old lady (lao tai tai or (sic)) is about to walk towards a henhouse (shang ji wo or (sic)), which is reflected in the second part that the purpose of doing this is ""heading for eggs"" (ben dan or (sic)). The intended interpretation of the saying ""an idiot"" (ben dan or (sic)) could not be worked out without the help of a very crucial apparatus-sound association; that is, ""heading for eggs"" is pronounced the same with ""an idiot"" in Chinese with respect to the same segmental combinations and tone patterns. Within the paradigm of sound association, the meaning identified in the source domain (the first part; in our example, the old lady's behavior) is also observed in the target domain (the second part; in our example, the figurative meaning of the old lady's behavior) in a metaphoric way through mapping between the two domains, resulting in a shifting from a concrete concept to an abstract one. Mapping, which was described by Lakoff and Johnson in their Conceptual Metaphor Theory, has been considered a powerful theory in interpreting metaphors. Fauconnier proposed Conceptual Blending Theory, emphasizing that mapping happens across spaces via connecting counterparts in the input mental spaces. In our example, it connects one mental space contained the image of an old lady walking towards a henhouse and another mental space describing the purpose of carrying out this behavior. Then the mapping happens when the mental apparatus identifies the sound similarity and generates the intended meaning. Meanwhile, the knowledge of recognizing implicature (Xu, 2005) in pragmatic inference also plays a crucial role in processing two-part allegorical sayings. From this perspective, Chinese two-part allegorical sayings are one of the ideal languages. The successful understanding of them couldn't be accomplished without considering how people interpret in their real usage. There are three theories relevant to interpreting of Chines two-part allegorical sayings, but what we wonder is which theory is more powerful in explaining the processing of homophonic two-part allegorical sayings in terms of various degrees? Does sound association play a crucial role in the processing? In order to answer these questions, two experiments were designed by using eye-movement instrument: experiment 1 investigated the effect of various degrees of familiarity on the processing of two different types of back parts (homophonic association/phonography), for example, (sic) is phonography because there is no metaphoric inference between front and back parts, but (sic) is with homophonic association because the implied meaning (sic) is inferred from the words (sic) through sound similarity. We asked the participants to judge the semantic relatedness between front and back parts and we found that the judgment was determined by the type of back parts, that is, the homophone facilitated the participants' judgment because of the sound association; while phonography forced participants to infer the implied meaning of the sayings. Meanwhile, participants took longer time to process the sayings with high familiarity and made more errors in the judgment task, the reason of which might be caused by the negative effect of long-term memory. The result supported the Conceptual Metaphor theory and Conceptual Blending theory. However, participants adopted a quite different processing strategy called the on-line processing strategy when the sayings were with low familiarity. The result supported the Pragmatic Inference theory. Experiment 2 investigated how various intonations affected the judgment of semantic relatedness between front and back parts. The results showed that the characters with the same sound pattern but not with the same intonation (e. g. (sic)) exerted different influences on the judgment. Specifically, the character ""(sic)"", which does not fit into the meaning of any of the two parts, did not play a role in the processing. The result does not support the Conceptual Blending theory.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1249,Effects of aging on the Mandarin lexical tone perception: Evidence from ERPs,"The accurate perception of lexical tones in Mandarin Chinese is an important foundation for successfully understanding spoken Chinese. Previous behavioral studies have shown that the ability to perceive lexical tones in Mandarin declines in elderly individuals. In addition to other research areas related to language and aging, the central issue in phonetic perception during aging concerns whether perceptual changes related to aging are area-specific or area-general. The area-general language hypothesis of aging assumes that changes in language perception related to aging are caused by a decline in both general sensory perception function and high-order cognitive function. In contrast, the area-specific language hypothesis of aging assumes that changes in aging-related language perception are caused by specific deficits in language processing. Previous studies mostly detected the state of attention and focused on how area-general factors affect the processing of segmental phonemes in elderly individuals. The present study examined neurophysiological responses, particularly that of MMN, to explore whether the aging of lexical tone perception is language-specific for Mandarin. The current study recruited 22 healthy elderly participants (age range: 55.6 similar to 79.6 years) and 18 young participants (age range: 22.7 similar to 29.0 years). In a passive oddball task, we used event-related potentials (ERPs) to examine Mandarin lexical tone perception. Three syllables from a lexical tone continuum were chosen as stimuli to form an across-category stimulus pair and a within-category stimulus pair for the ERP oddball task. A non-speech stimulus pair was generated on the basis of the within-category stimulus pair. During the experiment, participants were instructed to ignore the presented sounds while watching a self-selected movie. ERP data showed that in the across-category condition, compared with the young group, the elderly group had a smaller MMN, and there was no between-group difference in the within-category condition. In the young group, a non-speech tone elicited a larger MMN amplitude than a speech tone that shared the same pitch contour, while the elderly group did not show a speech enhancement effect. In addition, compared with that of the young group, the amplitude of the MMN elicited by the non-speech contrast in the elderly group was significantly smaller. The results indicated that the general decline in central auditory processing function was not related to the pre-attention processing of lexical tone. In addition, when the level at which the auditory input stimulus could be sensed was controlled according to peripheral hearing abilities, the decline in peripheral auditory function was not related to the preservation of or decline in lexical tone perception in the current study. In the current study, there is no evidence that the age-related decline in area-general factors affects tone perception in the pre-attention condition. On this basis, this study further speculated that the ability of elderly Mandarin-speaking individuals to perceive lexical tone in pre-attention conditions was preserved and only declined for specific languages, and the above-mentioned decline in the processing of knowledge of Mandarin tone category and the wider preservation of the processing of speech tones are language-specific. The present study provides evidence for the area-specific language hypothesis of aging.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1250,The effects of music training on categorical perception of Mandarin tones in 4- to 5-year-old children,"Music and speech share many acoustic commonalities and cognitive mechanisms. Previous studies have found that music training can improve categorical perception (CP) of Mandarin tones in adult musicians. However, it remains to be established whether music training can enhance the categorical perception of Mandarin tones in young children and whether the training effects can be influenced by the training duration. The present study used a 2 (group: music training vs no-training) x 3 (test time: pre vs 6-month post vs 12-month post) between-and-within-subjects design to investigate the effects of music training on 4- to 5- year-old children's CP of a Mandarin lexical tone continuum (from Tone 1 to Tone 2). The music training consisted of 110 sessions, 30 minutes per session, and three sessions per week for 12 months involving 20 preschoolers. The children were assigned to two groups, music training group (n = 20, age range from 49.69 months to 51.42 months, SD = 2.91 months) and control group (n = 20, age range from 51.69 to 52.56 months, SD = 3.0 months). In the music training group, the instructor guided children in activities leading to playing the small carillon, while children in the no-training group were given routine class activities. Each session of music training consisted four parts: Part 1 was ""listen and sing songs"" in which children learned to master notes and focus attention on subtle pitch changes; Part 2 was ""listen and discriminate musical notes"", children learned to play a single note accurately according to the background music; Part 3 was ""listen and play the carillon"", children listened to pitch changes in the background music, sang the notes and played the whole song melody; Part 4 was "" play the carillon along with actions"", children listened to the background music and learned to play the carillon along with simple dancing actions. Children's CP of tone continuum was measured before the learning began, after 6- month and after 12-month training using two tasks (identification test and discrimination test). This study investigated if music training can enhance children's boundary position, boundary width, within-category and between-category discrimination accuracy in CP of Mandarin Tone 1 and Tone 2 through 2 (group: music training vs no-training) x3 (test time: pre vs 6-month post vs 12-month post) repeated measures ANOVA. The results revealed that although the perceptual boundary positions and ability to discriminate between-category tone pairs were unaffected by training, the boundary width values and within-category discrimination accuracies differed significantly between the experimental and control groups. The analysis of boundary width values and within-category discrimination accuracy revealed a significant interaction between group and test time. An analysis of simple effects further indicated that in the pretest and 6-month posttest, there was no significant effect between music training group and no-training group. In the 12-month posttest, the boundary width decreased significantly and the within-category discrimination accuracies increased significantly in the music training group, while no significant differences were found on boundary width and within-category discrimination accuracy in the control group. These results suggest that long-term music training can enhance children's CP of Mandarin tonal contrasts. In conclusion, our results supported the OPERA theory that music training can raise the steepness of boundary widths and enhance children's sensitivity to subtle pitch differences between within-category sounds in the presence of robust mental representation in the service of CP of lexical tonal contrasts. Key words music training; cross-domain transfer; categorical perception of tonal contrast; 4- to 5-year-old children",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1251,Lexical tone perception mechanism in 2- to 4-year-old Mandarin-speaking children in the pre-attention stage,"Lexical tones are a key component of tonal language. The accurate perception of different Mandarin lexical tones is essential for Mandarin-speaking children to process spoken Chinese. Previous studies based on the speech perception model of nontonal language proposed perceptual narrowing theory, while later studies indicated that the perception of lexical tones might be more complicated. Event-related potentials (ERPs) are an effective tool that can investigate Mandarin-speaking children's implicit perception of lexical tones. Some studies targeting children's speech perception have demonstrated that mismatch responses (MMRs, including MMN and p-MMR) indicate the development of phonetic representation. Moreover, these studies have provided empirical evidence in this field. However, ages between 2 and 4 years old needs to be explored empirically. The current study used MMN and p-MMR as the neural correlates of lexical tone perception of Mandarin in the pre-attention stage and investigated the development of lexical tone processing mechanisms in 2- to 4-year-old children. In addition, we paid attention to the influence of category information and the size of the deviance at the acoustic level. Sixteen 2- to 4-year-old Mandarin-speaking children (7 boys and 9 girls; mean age: 3.4 years old; range: 2.6 - 3.10 years old) were recruited in this study. The study used the oddball paradigm and designed two experiments: Experiment 1 investigated the mismatch responses in across-category lexical tone perception in the participants. The stimuli consisted of three syllables: /yi1/ (T1), /yi2/ (T2), and /yi3/(T3). T3 was assigned as the standard; T1 was assigned as the large deviant; T2 was the small deviant. This experiment investigated how the deviance size affects children's perception of different lexical tones in the across-category condition. Experiment 2 investigated the mismatch responses of the participants to the perception of tones in the same category and were designated yi3a (T3a) and yi3b (T3b) within the category. These tones have the same phonological information but differ from the standard T3 stimulus in the acoustic information (based on the frequency and contour). The distances between T3a & T3 and T3b & T3 were modulated in Praat to control the distances of T1 & T3 and T2 & T3, respectively. This experiment detected the influence of the degree of similarity on the acoustic information on lexical tone perception without changing the category information. The results of the EEG data showed that (1) only the across-category large-deviance pair (T1/T3) elicited an obvious MMN response, indicating that 2- to 4-year-old Mandarin-speaking children can distinguish tones with obvious category boundaries in the pre- attention stage; (2) the across-category small-deviance pair (T2/T3) and the two kinds of within-category deviations (T3a/T3; T3b/T3) achieved the same data performance: no significant MMN or p-MMR was elicited. However, the internal mechanisms of these two conditions are different. The former may indicate that 2- to 4-year-old Mandarin-speaking children are in the period of transition from p-MMR to MMN. The latter may reflect that children have established a lexical tone category, and due to their immature perception ability, it is impossible for them to distinguish the differences among within-category lexical tones. In summary, the current study has filled the age gap in the relevant lexical tone perception neural mechanism research and has revealed what is crucial to children's lexical tone neural mechanism of perception development. Moreover, this research has identified the size difference between isometric across-category stimulation with within-category stimulation, which innovatively provides a reference for future research. The conclusion indicates that 2- to 4-year-old Mandarin-speaking children are in the period of lexical tone category formation. In the pre-attention stage, the obvious category boundary of lexical tones can be distinguished accurately. For the inconspicuous category boundaries of lexical tones, the neural mechanism is transforming from p-MMR to MMN. The magnitude of the deviance does not affect lexical tone perception within the same category.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1252,Asymmetric processing of lexical tonal contrast in Swedish,"Languages such as Swedish use suprasegmental information such as tone, over and above segments, to mark lexical contrast. Theories differ with respect to the abstractness and specification of tone in the mental lexicon. In a forced choice task, we tested Swedish listeners' responses to words with segmentally identical first syllables differing in tonal contours (characterized as Accents 1 and 2). We assumed Accent I to be lexically specified for a subset of words and hypothesized that this specification would speed up word accent identification. As was predicted, listeners were fastest in choosing the tonally correct word when the accent was lexically specified. We conclude that the processing of surface tonal contours is governed by their underlying lexical structure with tonal specification.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1253,On the (non)categorical perception of lexical tones,"Identification and discrimination of lexical tones in Cantonese were compared in the context of a traditional categorical perception paradigm. Three lexical tone continua were used: one ranging from low level to high level, one from high rising to high level, and one from low falling to high rising. Identification data showed steep slopes at category boundaries, suggesting that lexical tones are perceived categorically. In contrast, discrimination curves generally showed much weaker evidence for categorical perception. Subsequent investigation showed that the presence of a tonal context played a strong role in the identification of target tones and less of a role in discrimination. The results are consistent with the hypothesis that tonal category boundaries are determined by a combination of regions of natural auditory sensitivity and the influence of linguistic experience.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1254,The perception of primary and secondary stress in English,"Most models of word recognition concerned with prosody are based on a distinction between strong syllables (containing a full vowel) and weak syllables (containing a schwa). In these models,the posslibility that listeners take advantage of finer grained prosodic distinctions, such as primary versus secondary stress, is usually rejected on the grounds that these two categories are not discriminable from each other without lexical information or normalization of the speaker's voice. In the present experiment, subjects were presented with word fragments that differed only by their degree of stress-namely, primary or secondary stress (e.g., /'prasi/ vs. /""prasi/). The task was to guess the origin of the fragment (e.g., ""prosecutor"" vs. ""prosecution""). The results showed that guessing performance: significantly exceeds the chance level, which indicates that making fine stress distinctions is possible without lexical information and with minimal speech normalization This finding is discussed in the framework of prosody-based word recognition theories.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1255,Lexical tone in Cantonese spoken-word processing,"In three experiments, the processing of lexical tone in Cantonese was examined. Cantonese listeners more often accepted a nonword as a word when the only difference between the nonword and the word was in tone, especially when the F0 onset difference between correct and erroneous tone was small. Same-different judgments by these listeners were also slower and less accurate when the only difference between two syllables was in tone, and this was true whether the F0 onset difference between the two tones was large or small. Listeners with no knowledge of Cantonese produced essentially the same same-different judgment pattern as that produced by the native listeners, suggesting that the results display the effects of simple perceptual processing rather than of Linguistic knowledge. It is argued that the processing of lexical tone distinctions may be slowed, relative to the processing of segmental distinctions, and that, in speeded-response tasks, tone is thus more likely to be misprocessed than is segmental structure.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1256,Individuals with congenital amusia imitate pitches more accurately in singing than in speaking: Implications for music and language processing,"In this study, we investigated the impact of congenital amusia, a disorder of musical processing, on speech and song imitation in speakers of a tone language, Mandarin. A group of 13 Mandarin-speaking individuals with congenital amusia and 13 matched controls were recorded while imitating a set of speech and two sets of song stimuli with varying pitch and rhythm patterns. The results indicated that individuals with congenital amusia were worse than controls in both speech and song imitation, in terms of both pitch matching (absolute and relative) and rhythm matching (relative time and number of time errors). Like the controls, individuals with congenital amusia achieved better absolute and relative pitch matching and made fewer pitch interval and contour errors in song than in speech imitation. These findings point toward domain-general pitch (and time) production deficits in congenital amusia, suggesting the presence of shared pitch production mechanisms but distinct requirements for pitch-matching accuracy in language and music processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1257,Perceptual assimilation of lexical tone: The roles of language experience and visual information,"Using Best's (1995) perceptual assimilation model (PAM), we investigated auditory-visual (AV), auditory-only (AO), and visual-only (VO) perception of Thai tones. Mandarin and Cantonese (tone-language) speakers were asked to categorize Thai tones according to their own native tone categories, and Australian English (non-tone-language) speakers to categorize Thai tones into their native intonation categories-for instance, question or statement. As comparisons, Thai participants completed a straightforward identification task, and another Australian English group identified the Thai tones using simple symbols. All of the groups also completed an AX discrimination task. Both the Mandarin and Cantonese groups categorized AO and AV Thai falling tones as their native level tones, and Thai rising tones as their native rising tones, although the Mandarin participants found it easier to categorize Thai level tones than did the Cantonese participants. VO information led to very poor categorization for all groups, and AO and AV information also led to very poor categorizations for the English intonation categorization group. PAM's predictions regarding tone discriminability based on these category assimilation patterns were borne out for the Mandarin group's AO and AV discriminations, providing support for the applicability of the PAM to lexical tones. For the Cantonese group, however, PAM was unable to account for one specific discrimination pattern-namely, their relatively good performance on the Thai high-rising contrast in the auditory conditions-and no predictions could be derived for the English groups. A full account of tone assimilation will likely need to incorporate considerations of phonetic, and even acoustic, similarity and overlap between nonnative and native tone categories.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1258,Reassessing the electrophysiological evidence for categorical perception of Mandarin lexical tone: ERP evidence from native and naive non-native Mandarin listeners,"Some studies have argued that native speakers of tonal languages have been shown to perceive lexical tone continua in a more categorical manner than speakers of non-tonal languages. Among these, Zhang and colleagues (NeuroReport 23 (1): 35-9) conducted an event-related potential (ERP) study using an oddball paradigm showing that native Mandarin speakers exhibit different sensitivity to deviant tones that cross category boundaries compared to deviants that belong to the same category as the standard. Other recent ERP findings examining consonant voicing categories question whether perception is truly categorical. The current study investigated these discrepant findings by replicating and extending the Zhang et al. study. Native Mandarin speakers and naive English speakers performed an auditory oddball detection test while ERPs were recorded. Naive English speakers were included to test for language experience effects. We found that Mandarin speakers and English speakers demonstrated qualitatively similar responses, in that both groups showed a larger N2 to the across-category deviant and a larger P3 to the within-category deviant. The N2/P3 pattern also did not differ in scalp topography for the within- versus across-category deviants, as was reported by Zhang et al. Cross-language differences surfaced in behavioral results, where Mandarin speakers showed better discrimination for the across-category deviant, but English speakers showed better discrimination for within-category deviants, though all results were near-ceiling. Our results therefore support models suggesting that listeners remain sensitive to gradient acoustic differences in speech even when they have learned phonological categories along an acoustic dimension.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1259,Interactive effects of linguistic abstraction and stimulus statistics in the online modulation of neural speech encoding,"Speech processing is highly modulated by context. Prior studies examining frequency-following responses (FFRs), an electrophysiological neurophonic' potential that faithfully reflects phase-locked activity from neural ensembles within the auditory network, have demonstrated that stimulus context modulates the integrity of speech encoding. The extent to which context-dependent encoding reflects general auditory properties or interactivities between statistical and higher-level linguistic processes remains unexplored. Our study examined whether speech encoding, as reflected by FFRs, is modulated by abstract phonological relationships between a stimulus and surrounding contexts. FFRs were elicited to a Mandarin rising-tone syllable (/ji-TR/, second') randomly presented with other syllables in three contexts from 17 native listeners. In a contrastive context, /ji-TR/ occurred with meaning-contrastive high-level-tone syllables (/ji-H/, one'). In an allotone context, TR occurred with dipping-tone syllables /ji-D/, a non-meaning-contrastive variant of /ji-TR/. In a repetitive context, the same /ji-TR/ occurred with other speech tokens of /ji-TR/. Consistent with prior work, neural tracking of /ji-TR/ pitch contour was more faithful in the repetitive condition wherein /ji-TR/ occurred more predictably (p =1) than in the contrastive condition (p =0.34). Crucially, in the allotone context, neural tracking of /ji-TR/ was more accurate relative to the contrastive context, despite both having an identical transitional probability (p =0.34). Mechanistically, the non-meaning-contrastive relationship may have augmented the probability to /ji-TR/ occurrence in the allotone context. Results indicate online interactions between bottom-up and top-down mechanisms, which facilitate speech perception. Such interactivities may predictively fine-tune incoming speech encoding using linguistic and statistical information from prior context.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1260,"Spectral contrast effects are modulated by selective attention in ""cocktail party"" settings","Speech sounds are perceived relative to spectral properties of surrounding speech. For instance, target words that are ambiguous between /b?t/ (with low F1) and /bet/ (with high F1) are more likely to be perceived as ""bet"" after a ""low F1"" sentence, but as ""bit"" after a ""high F1"" sentence. However, it is unclear how these spectral contrast effects (SCEs) operate in multi-talker listening conditions. Recently, Feng and Oxenham (J.Exp.Psychol.-Hum.Percept.Perform. 44(9), 1447-1457,2018b) reported that selective attention affected SCEs to a small degree, using two simultaneously presented sentences produced by a single talker. The present study assessed the role of selective attention in more naturalistic ""cocktail party"" settings, with 200 lexically unique sentences, 20 target words, and different talkers. Results indicate that selective attention to one talker in one ear (while ignoring another talker in the other ear) modulates SCEs in such a way that only the spectral properties of the attended talker influences target perception. However, SCEs were much smaller in multi-talker settings (Experiment2) than those in single-talker settings (Experiment1). Therefore, the influence of SCEs on speech comprehension in more naturalistic settings (i.e., with competing talkers) may be smaller than estimated based on studies without competing talkers.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1261,Syllabic tone articulation influences the identification and use of words during Chinese sentence reading: Evidence from ERP and eye movement recordings,"In two experiments, we examined the contribution of articulation-specific features to visual word recognition during the reading of Chinese. In spoken Standard Chinese, a syllable with a full tone can be tone-neutralized through sound weakening and pitch contour change, and there are two types of two-character compound words with respect to their articulation variation. One type requires articulation of a full tone for each constituent character, and the other requires a full- and a neutral-tone articulation for the first and second characters, respectively. Words of these two types with identical first characters were selected and embedded in sentences. Native speakers of Standard Chinese were recruited to read the sentences. In Experiment 1, the individual words of a sentence were presented serially at a fixed pace while event-related potentials were recorded. This resulted in less-negative N100 and anterior N250 amplitudes and in more-negative N400 amplitudes when targets contained a neutral tone. Complete sentences were visible in Experiment 2, and eye movements were recorded while participants read. Analyses of oculomotor activity revealed shorter viewing durations and fewer refixations on-and fewer regressive saccades to-target words when their second syllable was articulated with a neutral rather than a full tone. Together, the results indicate that readers represent articulation-specific word properties, that these representations are routinely activated early during the silent reading of Chinese sentences, and that the representations are also used during later stages of word processing.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1262,Immediate online use of prosody reveals the ironic intentions of a speaker: neurophysiological evidence,"In social interactions, speakers often use their tone of voice (""prosody"") to communicate their interpersonal stance to pragmatically mark an ironic intention (e.g., sarcasm). The neurocognitive effects of prosody as listeners process ironic statements in real time are still poorly understood. In this study, 30 participants judged the friendliness of literal and ironic criticisms and compliments in the absence of context while their electrical brain activity was recorded. Event-related potentials reflecting the uptake of prosodic information were tracked at two time points in the utterance. Prosody robustly modulated P200 and late positivity amplitudes from utterance onset. These early neural responses registered both the speaker's stance (positive/negative) and their intention (literal/ironic). At a later timepoint (You are such a great/horrible cook), P200, N400, and P600 amplitudes were all greater when the critical word valence was congruent with the speaker's vocal stance, suggesting that irony was contextually facilitated by early effects from prosody. Our results exemplify that rapid uptake of salient prosodic features allows listeners to make online predictions about the speaker's ironic intent. This process can constrain their representation of an utterance to uncover nonliteral meanings without violating contextual expectations held about the speaker, as described by parallel-constraint satisfaction models.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1263,Multiple prosodic meanings are conveyed through separate pitch ranges: Evidence from perception of focus and surprise in Mandarin Chinese,"F0 variation is a crucial feature in speech prosody, which can convey linguistic information such as focus and paralinguistic meanings such as surprise. How can multiple layers of information be represented with F0 in speech: are they divided into discrete layers of pitch or overlapped without clear divisions? We investigated this question by assessing pitch perception of focus and surprise in Mandarin Chinese. Seventeen native Mandarin listeners rated the strength of focus and surprise conveyed by the same set of synthetically manipulated sentences. An fMRI experiment was conducted to assess neural correlates of the listeners' perceptual response to the stimuli. The results showed that behaviourally, the perceptual threshold for focus was 3 semitones and that for surprise was 5 semitones above the baseline. Moreover, the pitch range of 5-12 semitones above the baseline signalled both focus and surprise, suggesting a considerable overlap between the two types of prosodic information within this range. The neuroimaging data positively correlated with the variations in behavioural data. Also, a ceiling effect was found as no significant behavioural differences or neural activities were shown after reaching a certain pitch level for the perception of focus and surprise respectively. Together, the results suggest that different layers of prosodic information are represented in F0 through different pitch ranges: paralinguistic information is represented at a pitch range beyond that used by linguistic information. Meanwhile, the representation of paralinguistic information is achieved without obscuring linguistic prosody, thus allowing F0 to represent the two layers of information in parallel.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1264,Impaired categorical perception of lexical tones in Mandarin-speaking congenital amusics,"The degree to which cognitive resources are shared in the processing of musical pitch and lexical tones remains uncertain. Testing Mandarin amusics on their categorical perception of Mandarin lexical tones may provide insight into this issue. In the present study, a group of 15 amusic Mandarin speakers identified and discriminated Mandarin tones presented as continua in separate blocks. The tonal continua employed were from a high-level tone to a mid-rising tone and from a high-level tone to a high-falling tone. The two tonal continua were made in the contexts of natural speech and of nonlinguistic analogues. In contrast to the controls, the participants with amusia showed no improvement for discrimination pairs that crossed the classification boundary for either speech or nonlinguistic analogues, indicating a lack of categorical perception. The lack of categorical perception of Mandarin tones in the amusic group shows that the pitch deficits in amusics may be domain-general, and this suggests that the processing of musical pitch and lexical tones may share certain cognitive resources and/or processes (Patel 2003, 2008, 2012).",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1265,Tone matters for Cantonese-English bilingual children's English word reading development: A unified model of phonological transfer,"Languages differ considerably in how they use prosodic features, or variations in pitch, duration, and intensity, to distinguish one word from another. Prosodic features include lexical tone in Chinese and lexical stress in English. Recent cross-sectional studies show a surprising result that Mandarin Chinese tone sensitivity is related to Mandarin-English bilingual children's English word reading. This study explores the mechanism underlying this relation by testing two explanations of these effects: the prosodic hypothesis and segmental phonological awareness transfer. We administered multiple measures of Cantonese tone sensitivity, English stress sensitivity, segmental phonological awareness in Cantonese and English, nonverbal ability, and English word reading to 123 Cantonese-English bilingual children ages 7 and 8 years. Structural equation modeling revealed a longitudinal prediction of Cantonese tone sensitivity to English word reading between 8 and 9 years of age. This relation was realized through two parallel routes. In one, Cantonese tone sensitivity predicted English stress sensitivity, and English stress sensitivity, in turn, significantly predicted English word reading, as postulated by the prosodic hypothesis. In the second, Cantonese tone sensitivity predicted English word reading through the transfer of segmental phonological awareness between Cantonese and English, as predicted by segmental phonological transfer. These results support a unified model of phonological transfer, emphasizing the role of tone in English word reading for Cantonese-English bilingual children.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1266,The role of tonal information during spoken-word recognition in Chinese: Evidence from a printed-word eye-tracking study,"Two experiments were conducted to investigate the extent to which the lexical tone can affect spoken-word recognition in Chinese using a printed-word paradigm. Participants were presented with a visual display of four words-namely, a target word (e.g., (sic),xiang4xian4, ""quadrant""), a tone-consistent phonological competitor (e.g., (sic), xiang4ce4, ""photo album""), or a tone-inconsistent phonological competitor (e.g., ,xiang1cai4, ""coriander""), and two unrelated distractors. Simultaneously, they were asked to listen to a spoken target word presented in isolation (Experiment1) or embedded in neutral/predictive sentence contexts (Experiment2), and then click on the target word on the screen. Results showed significant phonological competitor effects (i.e., the fixation proportion on the phonological competitor was higher than that on the distractors) under both tone conditions. Specifically, a larger phonological competitor effect was observed in the tone-consistent condition than in the tone-inconsistent condition when the spoken word was presented in isolation and the neutral sentence contexts. This finding suggests a partial role of lexical tone in constraining spoken-word recognition. However, when embedded in a predictive sentence context, the phonological competitor effect was only observed in the tone-consistent condition and absent in the tone-inconsistent condition. This result indicates that the predictive sentence context can strengthen the role of lexical tone.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1267,Expectations from preceding prosody influence segmentation in online sentence processing,"Previous work examining prosodic cues in online spoken-word recognition has focused primarily on local cues to word identity. However, recent studies have suggested that utterance-level prosodic patterns can also influence the interpretation of subsequent sequences of lexically ambiguous syllables (Dilley, Mattys, & Vinke, Journal of Memory and Language, 63:274-294, 2010; Dilley & McAuley, Journal of Memory and Language, 59:294-311, 2008). To test the hypothesis that these distal prosody effects are based on expectations about the organization of upcoming material, we conducted a visual-world experiment. We examined fixations to competing alternatives such as pan and panda upon hearing the target word panda in utterances in which the acoustic properties of the preceding sentence material had been manipulated. The proportions of fixations to the monosyllabic competitor were higher beginning 200 ms after target word onset when the preceding prosody supported a prosodic constituent boundary following pan-, rather than following panda. These findings support the hypothesis that expectations based on perceived prosodic patterns in the distal context influence lexical segmentation and word recognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1268,(Mis) understanding your native language: Regional accent impedes processing of information status,"Native-speaker listeners constantly predict upcoming units of speech as part of language processing, using various cues. However, this process is impeded in second-language listeners, as well as when the speaker has an unfamiliar accent. Whereas previous research has largely concentrated on the pronunciation of individual segments in foreign-accented speech, we show that regional accent impedes higher levels of language processing, making native listeners' processing resemble that of second-language listeners. In Experiment 1, 42 native speakers of Canadian English followed instructions spoken in British English to move objects on a screen while their eye movements were tracked. Native listeners use prosodic cues to information status to disambiguate between two possible referents, a new and a previously mentioned one, before they have heard the complete word. By contrast, the Canadian participants, similarly to second-language speakers, were not able to make full use of prosodic cues in the way native British listeners do. In Experiment 2, 19 native speakers of Canadian English rated the British English instructions used in Experiment 1, as well as the same instructions spoken by a Canadian imitating the British English prosody. While information status had no effect for the Canadian imitations, the original stimuli received higher ratings when prosodic realization and information status of the referent matched than for mismatches, suggesting a native-like competence in these offline ratings. These findings underline the importance of expanding psycholinguistic models of second language/dialect processing and representation to include both prosody and regional variation.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1269,Cross-situational word learning of Cantonese Chinese,"In this study, we recruited 60 native Cantonese speakers to participate in a standard cross-situational word-learning task to explore the cross-situational learning effects of minimal word pairs in Cantonese Chinese. In the cross-situational word-learning task, four different types of word pairs were used: (1) a non-minimal word pair [N]; (2) a consonant minimal word pair [C]; (3) a rime minimal word pair [R]; and (4) a tone minimal word pair [T]. The results showed that the participants could learn the word-referent mapping for all word-pair types, but they performed better on the N and T types than on the other two (i.e., C and R). Together with other previous evidence, these findings suggest that Cantonese language learners can learn and encode those phonetic details while they learn the word-referent co-occurrence probabilities. The results also suggested that the tonal information seemed to be more important than the other phonological components in Cantonese Chinese word learning.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1270,Tracking the time-course of spoken word recognition of Cantonese Chinese in sentence context: Evidence from eye movements,"In this study, we conducted an eye-tracking experiment to investigate the effects of sentence context and tonal information on spoken word recognition processes in Cantonese Chinese. We recruited 60 native Cantonese listeners to participate in the eye-tracking experiment. The target words (phonologically similar words) were manipulated to either (1) a congruent context or (2) an incongruent context in the experiment. The resulting eye-movement patterns in the incongruent context condition clearly revealed that (1) sentence context produced a garden-path effect in the initial stage of the spoken word recognition processes and then (2) the lexical tone of the word (bottom-up information) overrode the contextual effects to help listeners to discriminate between different similar-sounding words during lexical access. In conclusion, the patterns of eye-tracking data show the interactive processes between the lexical tone (an acoustic cue within a Cantonese word) and sentence context played in different phases to the spoken word recognition of Cantonese Chinese.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1271,"From ""I dance"" to ""she danced"" with a flick of the hands: Audiovisual stress perception in Spanish","When talking, speakers naturally produce hand movements (co-speech gestures) that contribute to communication. Evidence in Dutch suggests that the timing of simple up-and-down, non-referential ""beat"" gestures influences spoken word recognition: the same auditory stimulus was perceived as CONtent (noun, capitalized letters indicate stressed syllables) when a beat gesture occurred on the first syllable, but as conTENT (adjective) when the gesture occurred on the second syllable. However, these findings were based on a small number of minimal pairs in Dutch, limiting the generalizability of the findings. We therefore tested this effect in Spanish, where lexical stress is highly relevant in the verb conjugation system, distinguishing bailo, ""I dance"" with word-initial stress from bail & oacute;, ""she danced"" with word-final stress. Testing a larger sample (N = 100), we also assessed whether individual differences in working memory capacity modulated how much individuals relied on the gestures in spoken word recognition. The results showed that, similar to Dutch, Spanish participants were biased to perceive lexical stress on the syllable that visually co-occurred with a beat gesture, with the effect being strongest when the acoustic stress cues were most ambiguous. No evidence was found for by-participant effect sizes to be influenced by individual differences in phonological or visuospatial working memory. These findings reveal gestural-speech coordination impacts lexical stress perception in a language where listeners are regularly confronted with such lexical stress contrasts, highlighting the impact of gestures' timing on prominence perception and spoken word recognition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1272,Encoding lexical tones in jTRACE: a simulation of monosyllabic spoken word recognition in Mandarin Chinese,"Despite its prevalence as one of the most highly influential models of spoken word recognition, the TRACE model has yet to be extended to consider tonal languages such as Mandarin Chinese. A key reason for this is that the model in its current state does not encode lexical tone. In this report, we present a modified version of the jTRACE model in which we borrowed on its existing architecture to code for Mandarin phonemes and tones. Units are coded in a way that is meant to capture the similarity in timing of access to vowel and tone information that has been observed in previous studies of Mandarin spoken word recognition. We validated the model by first simulating a recent experiment that had used the visual world paradigm to investigate how native Mandarin speakers process monosyllabic Mandarin words (Malins & Joanisse, 2010). We then subsequently simulated two psycholinguistic phenomena: (1) differences in the timing of resolution of tonal contrast pairs, and (2) the interaction between syllable frequency and tonal probability. In all cases, the model gave rise to results comparable to those of published data with human subjects, suggesting that it is a viable working model of spoken word recognition in Mandarin. It is our hope that this tool will be of use to practitioners studying the psycholinguistics of Mandarin Chinese and will help inspire similar models for other tonal languages, such as Cantonese and Thai.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1273,Database of word-level statistics for Mandarin Chinese (DoWLS-MAN),"In this article we present the Database of Word-Level Statistics for Mandarin Chinese (DoWLS-MAN). The database addresses the lack of agreement in phonological syllable segmentation specific to Mandarin by offering phonological features for each lexical item according to 16 schematic representations of the syllable (8 with tone and 8 without tone). Those lexical statistics that differ per phonological word and nonword due to changes in syllable segmentation are of the variant category and include subtitle lexical frequency, phonological neighborhood density measures, homophone density, and network science measures. The invariant characteristics consist of each items' lexical tone, phonological transcription, and syllable structure among others. The goal of DoWLS-MAN is to provide researchers both the ability to choose stimuli that are derived from a segmentation schema that supports an existing model of Mandarin speech processing, and the ability to choose stimuli that allow for the testing of hypotheses on phonological segmentation according to multiple schemas. In an exploratory analysis we illustrate how multiple schematic representations of the phonological mental lexicon can aid in hypothesis generation, specifically in terms of phonological processing when reading Chinese orthography. Users of the database can search among over 92,000 words, over 1600 out-of-vocabulary Chinese characters, and 4300 phonological nonwords according to either Chinese orthography, pinyin, or ASCII phonetic script. Users can also generate a list of phonological words and nonwords according to user-defined ranges and categories of lexical characteristics. DoWLS-MAN is available to the public for search or download at .",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1274,A web-based mouse-tracking task for early perceptual language processing,"The study of language processing requires data from a wide range of languages but also data that are free from demand characteristics and meta-linguistic strategies. While eye-tracking has been successfully used to address the later issue, pragmatically, eye-tracking is often difficult to achieve with less well-studied languages. Therefore, the current paper presents a web-based mouse-tracking task that generates data that seem to reflect early perceptual processes similar to eye-tracking but which can be performed remotely. The task uses a set-up similar to early video games to entice participants to use language input as early as possible. The data presented here replicate an earlier eye-tracking study focusing on how reduced words are recognized. Fillers from the same study are also used, which show that the paradigm also reflects predictive semantic processing. It is concluded that the paradigm can be used to investigate lexical access, prosodic processing, and predictive semantic processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1275,Supplemental update,"Supplements have often been characterized as inert with respect to other content. But under closer scrutiny, the data shows that supplements can take scope and participate in anaphoric links, undermining multidimensional accounts of them. I argue that the core empirical facts pertaining to supplements, including projection, can in many cases be accounted for by more general, independently motivated factors such as anaphora resolution in discourse and quantifier scope preferences. Importantly, supplement projection is decoupled from at-issueness, with projection arising instead as an epiphenomenon of various external influences. The account is formalized in a dynamic, compositional, and unidimensional semantics that allows anaphoric links to and from supplement content. Since supplements are modeled as a kind of quantifier phrase modifier, scope interactions with semantic operators are captured without further stipulation. When a supplement takes widest scope, it constitutes a separate at-issue proposal, enabling both supplement projection and (non)deniability. The formal machinery requires no additional rules or representation layers except for the dynamic meaning of the comma intonation, which demarcates a supplement from its surrounding content.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1276,Systematic Hearing Performance Evaluation Process for Adolescents with Cochlear Implantation at Early Ages,"Cochlear implant (CI) provision is the most effective clinical treatment to restore hearing performance in individuals with profound sensorineural hearing loss (SNHL). It has been successful in providing improved speech perception outcomes, especially in quiet environments. However, speech perception performance within complex environments, lexical tone recognition, and music perception have been shown to only improve with newer fine structure coding strategies or related techniques. Therefore, the methods used to assess hearing performance in noisy environments, lexical tone recognition, and music perception are of vital importance. These assessments must reflect the postoperative outcomes and also provide guidance for the programming, rehabilitation, and application of new coding strategies. In this study, hearing performance in simple and complex situations was evaluated before and after upgrading to a fine structure strategy. The participants were a cohort of Mandarin-speaking adolescents, who were experienced CI users. The comprehensive clinical workflow involved assessments of speech in quiet conditions, speech in noisy conditions, lexical tone recognition, and music perception. This battery of tests is explained in detail, from the coding strategy to the test methods, including the test process, environment, device, material, and order. The details that require special attention are discussed, such as the position of the participants, the angle of the loudspeaker, the intensity of the sound, the noise type, the practice test, and the way of answering questions. Each test step, method, and material for speech, lexical tone, and music perception is presented in detail. Finally, the clinical results are discussed.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1277,Acoustic Cues Utilized by Normal-hearing and Hearing-impaired Listeners Are Different for Mandarin Concurrent-vowels Identification,"Lexical Tones provide semantic meaning for Chinese words, and they are inherently bonded with vowels for Chinese speech. The present work examined the effect of lexical Tone contrast on concurrent-vowels identification for both normal-hearing (NH) and sensorineural-hearing-loss (SNHL) listeners. To ensure listeners utilize the primary cues of Tone perception, e.g. F0 contour, acoustic cues of mean F0 and durations were controlled for the speech stimuli. The result showed that benefit from Tone contrast existed for NH listeners, but was weak for SNHL listeners. Further analysis based on the measurement of Tone contrast suggested that F0 contour contrast and temporal envelope contrast were both highly correlated with identification performance for each of the listener groups. However, the importance of F0 contour was bigger than that of temporal envelope, especially for NH listeners. These findings would have implications for developing speech processing strategies for Mandarin-native SNHL listeners. (c) 2018 The Author(s). Published by S. Hirzel Verlag . EAA. This is an open access article under the terms of the Creative Commons Attribution (CC BY 4.0) license",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1278,The contribution of prosody to foreign accent: A study of Spanish as a foreign language,"The aim of this study is to analyze the contribution of prosody on the perception of foreign accent by Brazilian learners of Spanish. The data were collected from 15 participants and a control group of 5 native Spanish speakers. A perceptual test was performed with two different speech styles (reading and storytelling) and with delexicalized and natural speech. The speech production was judged by 24 native Spanish subjects. First, they had to determine the nationality of the speaker by listening to the delexicalized excerpts in Spanish (storytelling). After that, the listeners used a continuous scale to rate the excerpts (reading and storytelling) for the degree of foreign accent in Spanish. The results suggest that it is possible to identify foreign accent only with the prosodic information provided in the delexicalized stimuli, i. e., f(0), duration, and overall intensity. In addition, the perceptual test allowed us to assess the degree of foreign accent of each subject while revealing the great variability of their production. Finally, concerning the external data, the following factors predicted foreign accent among the learners: gender, length of residence in Spain, formal language instruction in Brazil, age of arrival in Spain, and reported use of Brazilian Portuguese in Spain. These results confirm the crucial role of naturalistic learning of a foreign language, as shown by previous studies.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1279,Phrasing and nuclear configurations in authentic English-accented Spanish,"This paper examines the differences in the division of intonation phrases and in the tonal structure of the nuclear configuration (i.e., the last pitch accent and the following boundary tone) in imitated and in authentic English-accented Spanish. The same Spanish text was read by four native speakers of American English, who produced the text with a real English foreign accent in Spanish, and six native speakers of Spanish, who read the text twice: in L1 Spanish and in fake English-accented Spanish. An auditory analysis of the data was carried out along with an inspection of f(0) traces aligned with the spectrographic representation and the segmental string. The results showed that the Spanish speakers produce more intonation breaks when they imitate an English accent in Spanish than when they speak L1 Spanish. Furthermore, they adopt the typical tonal structure of Spanish final accents in their fake English-accented productions. The number of prosodic breaks in real and in imitated English-accented Spanish is similar. The nuclear configurations, on the other hand, present more variability and differ in the frequency of occurrence of some patterns. The high occurrence of the fall-rise pattern (L+H* LH%) and the presence of the high-fall contour (L+H* L%) in the English productions may help discriminate an authentic English-accented Spanish from a fake one.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1280,Pitch metaphors and the body in singing classes,"This paper provides evidence that gesture promotes learning by schematizing a particular sensorimotor feeling associated with bodily action into a metaphorical representation. We analyze gestural depictions that are temporally aligned with musical performance in a video corpus of lyric singing classes conducted in Dutch (Flanders, Belgium) and that are specifically used by teachers to work on difficulties related to high and low pitch. The case of intonation is relevant because the vertical conception of 'ascending' and 'descending' interferes with good practices of singing and teachers often depict 'inverted' pitch contours to counteract this behavior. To motivate this conceptual inversion, teachers draw on the body as a local resource in the interactional context.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1281,Effect of LSVT on Lexical Tone in Speakers with Parkinson's Disease,"Lee Silverman Voice Treatment (LSVT) has well-documented treatment efficacy for individuals with hypokinetic dysarthria associated with Parkinson's disease (PD). Positive changes have been noted after treatment not only for vocal loudness but also for many other speech dimensions, including intonation (monotonicity). There have been few studies investigating the effect of LSVT on lexical tone which, like intonation, is controlled by variations in fundamental frequency. This study involved 12 Cantonese speakers with idiopathic PD who were enrolled in a standard LVST treatment protocol. Speech data were collected 3-4 days before treatment and 1 day after treatment. A wide variety of perceptual and acoustic variables were analyzed. The results showed significant improvements in loudness and intonation after treatment, but no significant changes in lexical tone. These results have theoretical implications for the relationship between tone and intonation and for models of the physiological control of fundamental frequency.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1282,The Effect of English Proficiency on the Production of English Intonation by Chinese EFL Learners,"English intonation is an integral component of English pronunciation teaching. However, as students' proficiency levels in English pronunciation improve, it remains unclear whether their intonation levels also develop. The present study, based on the second language intonation learning theory, aims to investigate the influence of English proficiency on Chinese EFL learners' production of English pitch accents, edge tones, and intonation patterns from the perspective of phonological representation. Two language groups of participants took part in a reading task: native English speakers (12) and Chinese EFL learners (36). The learners were classified into three groups based on their scores in the Chivox National Spoken English Test, ranked from high to low: the advanced, intermediate, and elementary groups. The reading task comprised 90 dialogue pairs. The participants were required to read part B of each dialogue pair aloud, but afterwards, only the Chinese EFL learners attended the semi-structured interview. The results showed that the native English speakers only demonstrated significant differences from each learner group in four of the ten intonation types involving the three aspects of English intonation, which may indicate regional variations in American English and difficulties distinguishing (H*) and (L+H*). In addition, there were no significant differences between the three learner groups in producing the ten intonation types, which maybe attributed to their similar learning experiences.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1283,Music therapy microanalysis of parent-infant interaction in a three-month-old infant later diagnosed with autism,"BACKGROUND Infant research literature has described for a long time the main aspects of parentese (motherese and fatherese) referring to musicality and specifically to musical language. It is believed that there is a deep analogy between the vital affects experienced by the child during interaction with the parent and the type of parentese that is a direct representation of them. Disruption of parentese has been described in early autism. The aim of this paper was to achieve a better understanding of this disruptive process. PARTICIPANTS AND PROCEDURE Sequences of parent-infant interaction extracted from one home movie of a child later diagnosed with autism were analyzed in a micro-musical way in order to create a musical score that allows the description of parent-infant interaction in a new way (considering form, pulse, rhythm, melody, timbre and silence). RESULTS Musical microanalysis is able to highlight features not brought out by other kinds of analysis. The first fragment is dominated by the anxiety of the mother, who attempts to stimulate the unresponsive infant. In the second fragment there is a change in musicality parallel to changes in the relationship: the mother participates in and coordinates the infant's experience through rhythm, prosody and musical dynamics. This change persists in the third fragment. CONCLUSIONS Musical transcription of parent-infant interactions has allowed us to highlight changes occurring in a short time during early interactions and to get a closer view of the disruptive process created by autism. This kind of research represents a potential shift in autism research, by focusing on dynamic parent-infant interactions instead of single behaviors of the child or of the parent. The usefulness of Stern's concept of intersubjective communion is discussed.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1284,China's image in the Spanish press during the COVID-19 pandemic,"This study analyzes the representation of China in the Spanish press during the COVID-19 pandemic, utilizing an analytical framework comprised of appraisal theory, transitivity system, and corpus linguistics. Our research is based on a specialized corpus consisting of 271 articles extracted from the Spanish newspapers El Pais, El Mundo, La Vanguardia, and ABC. Various analytical techniques, including keywords, collocations, and concordances, are applied to identify patterns and trends in the representation of China. The findings reveal that the Spanish newspapers have conveyed a multifaceted portrayal of China, employing various linguistic resources, both explicit and implicit, to express their attitudinal stance. The evaluative prosody towards China is variable, with instances of both positive and negative assessments. On one hand, the effectiveness of the anti-COVID measures implemented by China and the country's capacity in managing the crisis are positively appraised. On the other hand, the Chinese healthcare system is criticized, certain measures are deemed draconian, and an image of ambitious and arrogant China is constructed occasionally. This study provides significant insights into how the Spanish media represents China during the COVID-19 pandemic and contributes to an enhanced understanding of the importance and feasibility of studying images from a discursive approach.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1285,Representing state identity with journalistic attitudes: a corpus-assisted linguistic analysis of CGTN's trade dispute coverage,"This research explores the relationship between journalistic attitudes and the state identity of China Global Television Network (CGTN). It undertakes a corpus-assisted study of the linguistic representation of Affect, Judgment, and Appreciation in the trade dispute coverage of CGTN and the US media outlets at two levels: (1) prosody and social actors; (2) the preference for appraisal relations. It shows that CGTN manifests its state identity in attitudinal officialization and harmonization. Highlighting collectiveness in the choice of news agendas, actors, and prosody, CGTN prioritizes the appraiser's role of the group mind when expressing journalistic attitudes and is more inclined to draw on Judgment to stress the moral basis. A survey of journalistic attitudes in CGTN's trade dispute coverage enables us to elaborate on how the Chinese state media constructs its identity in news discourse.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1286,Zoomorphic Speech Verbs and the Lexical Representation of the Illocutionary Force in Czech Language,"The intention of this paper is to defend that the zoomorphic metaphors in speech verbs of czech language represent the illocutionary force through two different linguistic dimmentions. First, they extend -metaphorically- the animal characteristics to the human lexical domain, and second, these verbs represent lexically different manners of speaking, therefore their lexical meanings reflect the illocutionary force or speech intentions.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1287,Theatre in the FLE Classroom: Annie Ernaux through her Characters. for Dramatisation,"This article presents a series of workshops focused on the study and theatrical analysis of Annie Ernaux's L'& Eacute;v & eacute;nement (2000) and Le Jeune homme (2022), with the aim of their subsequent dramatization and theatrical representation. The activity was carried out with students from the French III course of the English Studies degree program at the University of Almeria, who possess a B2 level in FLE, during the 2022-2023 academic year. Due to the success and excellent results achieved-both in terms of language learning and the knowledge transfer involved-it was decided to include this theatrical adaptation, along with its premiere and staging, in the summer course En torno a la figura y la obra de Annie Ernaux, Premio Nobel de Literatura 2022, held at the University of Almeria.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1288,Chinese Piano Music: the Role of Composer Chu Wanghua in the Evolution of Modern Piano Traditions in China (the Case of the 'Sounds of the Temple' Capriccio Suite),"Chinese piano music is based on folk songs, Chinese poetry, and the ethnic style, which contribute to the expressiveness of the sound and refinement of the intonation and tonal system. The paper addresses the Chinese piano music and the role of composer Chu Wanghua in shaping contemporary piano traditions, using the 'Sounds of the Temple' capriccio suite as an example. Based on the direct method of standardization, the paper determined the significance of the parts of the 'Sounds of the Temple' suite. The second part (0.93) is most significant for displaying the piano music, because it involves the use of polyphonic sounds, imitating other instruments. The first part of the suite (0.87) is based on a linear representation of sounds, imitating the sounds of bells. The third part (0.69) centers around the use of slow tempo as well as major tone deviations. Significant elements of the suite (mirror reprise, rhythmic transformation, monotonous repetition of phrases and sounds, application of guo hua techniques, texture elements) were determined on the basis of the measured complexity and harmony of performance, as well as their influence on national traditions. A comparison of performance using standard deviation suggested that the differences were based on monotonous repetition of phrases and sounds (1.07), which is attributed to a greater emphasis on the difficulty of performance than on other parameters. The paper's practical implications involve the possibility of preserving expressiveness and ethnic elements while playing the piano, based on Chu Wanghua's 'Sounds of the Temple' suite. Further studies might compare the elements that contribute to the expressiveness of performance in the 'Sounds of the Temple' suite by Chu Wanghua and 'Flower Drum' by Qu Wei",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1289,Morphosyntactic encoding of information structure in Akan,"This paper investigates the interpretive and formal properties of the so-called focus construction in Akan. It argues that Akan has only one true morphological focus marker, namely na, whereas the marker de(epsilon) that has been analysed in the linguistic literature on Akan as a focus marker (Boadi 1974; Saah 1988; Boadi 1990; Saah 1994; Marfo and Bodomo 2005) is in fact a marker of contrastive topic. The proposed analysis relies on the idea that the Akan morphological markers na and de(epsilon) carry out exactly the same interpretive function as the falling and rising prosodic markers, respectively, found in intonation languages. It is shown that a number of controversies associated with Akan information-structural marking can be accounted for by assuming a certain parallelism with intonation languages. It is demonstrated that particular types of information-structural partitioning are cross-linguistically encoded via a marked strategy, with the parametric variation resulting from the difference in the choice of the linguistic tool - syntactic, morphological or prosodic - used to create a marked representation.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1290,Phrasal prosody constrains word segmentation in French 16-month-olds,"Infants who are in the process of acquiring their mother tongue have to find a way of segmenting the continuous speech stream into word-sized units. We present an experiment showing that French 16-month-olds are able to exploit phonological phrase boundaries in order to constrain lexical access. Using the conditioned head-turning technique, we showed that infants trained to turn their head for a bisyllabic word responded more often to sentences that contained this word, than to sentences that contained both syllables of this word separated by a phonological phrase boundary. We compare these results with similar results obtained with English-speaking infants, and discuss their implication for lexical and syntactic acquisition.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1291,Towards an International Prosodic Alphabet (IPrA),"In this article we present a set of arguments in favor of having access to two levels of prosodic representation, broad phonetic and phonological, and the motivations for developing a set of cross-linguistically transparent and consistent labels (e. g., an International Prosodic Alphabet, IPrA) based on the Autosegmental-Metrical (AM) framework and the ToBI notation. Regarding segmental phonology, as well as lexical suprasegmentals (lexical tone and stress), both the use of two levels of representation and the existence of an international phonetic alphabet have proved to be very useful. The same benefits of adopting these conventions are likely to accrue in the study of intonation.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1292,Individual empathy levels affect gradual intonation-meaning mapping: The case of biased questions in Salerno Italian,"The paper investigates the interplay between intonational cues and individual variability in the perceptual assessment of speakers' epistemic bias in Salerno Italian yes-no questions. We present a perception experiment in which we manipulated pitch span within the nuclear configuration (both nuclear accent and boundary tone) to predict degree of perceived positive bias (i.e., expected positive answer) to yes-no question stimuli. Our results show that a wider pitch span within the nuclear region predicts a higher degree of perceived positive bias, while negative bias is predicted by narrow pitch span. Crucially, though, two interacting sources of listener variability were uncovered, i.e., prolonged exposure to a non-native dialect as well as degree of empathy (i.e., Empathy Quotient, EQ). Exposure to non-native phonological systems was found to affect the way pitch span is mapped onto perceived epistemic bias, through category interference, though mediated by EQ levels. Specifically, high-empathy listeners were more affected by degree of non-native dialect exposure. EQ scores were hence found to have an effect on gradual span manipulation by interacting with the dialect exposure effect. These results advance our understanding of the intonation-meaning mapping by taking into account both the impact of gradual phonetic cues on meaning processing as well as uncovering sources of cognitive variability at the perceiver's level.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1293,"Foxes, deer, and hedgehogs: The recall of focus alternatives in Vietnamese","In tonal languages, the role of intonation in information-structuring has yet to be fully investigated. Intuitively, one would expect intonation to play only a small role in expressing communicative functions. However, experimental studies with Vietnamese native speakers show that intonation contours vary across different contexts and are used to mark certain types of information, for example, focus (Jannedy, 2007). In non-tonal languages (e.g., English), the marking of focus by intonation can influence the processing of focus alternatives (Fraundorf, Watson, & Benjamin, 2010). If Vietnamese also uses intonation to mark focus, the question arises whether the behavioral consequences of prosodic focus marking in Vietnamese are comparable to languages such as English or German. To test this, we replicate a study on memory for focus alternatives, originally carried out in German (Koch & Spalek, in progress), with Vietnamese language stimuli. In the original study, memory for focus alternatives was improved in a delayed recall task for focused elements produced with contrastive intonation in female speakers. Here, we replicate this finding with Northern Vietnamese native speakers: Contrastive intonation seems to improve later recall for focus alternatives in Northern Vietnamese, but only for female participants, in line with the findings by Koch and Spalek (in progress). These results indicate that prosodic focus marking in Vietnamese makes alternatives to the focused element more salient.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1294,The representation of variable tone sandhi patterns in Shanghai Wu,"Disyllabic verb-noun (V-N) items in Shanghai Wu have variable surface tone patterns: They can undergo either a rightward extension tone sandhi, which extends the lexical tone of the first syllable over the entire word, or tonal reduction on the first syllable. The current study investigates how the phonological properties of these alternation processes as well as variation influence how Shanghai speakers represent and access such words. We conducted an auditoryauditory priming lexical decision experiment on Shanghai V-N items that can undergo either tonal extension or tonal reduction with native Shanghai speakers. Each disyllabic target was preceded by monosyllabic primes with the canonical tone, the tonal-extension tone, the surface tone, or a tone unrelated to the tone of the first syllable of the targets. Results showed both canonical and tonal-extension priming effects, but no surface priming effect. Moreover, although more familiar V-Ns were recognized with shorter reaction time, the priming effect did not interact with speakers' familiarity ratings or sandhi preference ratings of the targets. These data are consistent with the interpretation that both the canonical and tonal-extension forms are represented in Shanghai speakers' mental lexicon due to tone sandhi variation, but the representation does not seem to be modulated by the frequencies of the variants. Also, together with findings from auditory priming studies of other tone sandhi patterns, the current study suggests that certain phonological properties of an alternation, such as its locality and transparency, influence the representation of words undergoing the alternation; but whether the alternation is structure-preserving does not seem to impact the representation.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1295,PITCH VARIATION FOR CHINESE SYLLABLES OF DIFFERENT INFORMATION LOAD (BASED ON COMMERCIAL AND SOCIAL RADIO ADS),"F0 is an important cue present in all vocalic segments, a parameter to differentiate between voiced and voiceless consonants, a relative phonological feature of syllable in tonal languages and a crucial feature of intonation. This paper aims to investigate pitch variation patterns in Mandarin Chinese depending on the syllable information load (Factor 1) in one type of discourse - advertising with the subdivision into social and commercial ads (Factor 2) considering gender differences (Factor 3). 1249 syllable tokens were selected for acoustic measurements, each syllable occurring twice - in the informative and uninformative utterance parts. The information load was determined perceptually: the syllable was considered informative when agreed by the minimum of 60% of listeners, other syllables were considered uninformative. Depending on lexical tone (T1 - T4 and T0), the measurements included average pitch values, declination/inclination starting and ending points (mean values). These parameters were used to judge about relevant pitch features - average height and declination/inclination slope. As a result, a consistent parameter increase in commercial ads vs. social ads and on informative syllables vs. uninformative ones was observed for all tones except T0 that showed the opposite trend in expressing information-based partition. Another finding was tone substitution that marked both informative and uninformative parts but was more frequent in the latter. Substitute tone frequency ranks did not depend on any of the three factors except for T0. High predictability of pitch variation patterns make them applicable in teaching Chinese as L2 in terms of speaking and listening for general and specific purposes.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1296,"Meeting with Infinity. Time, Memory and Return in The Book of Childhood by Elizaveta Mnatsakanova","The article analyzes the poetics of The Book of Childhood by Elizaveta Mnatsakanova. The basis of The Book of Childhood is the phenomenon recollections of childhood. The first section of the article, ""Words and Time"" is devoted to the study of the verbal representation of the theme of time. The section ""Plot and Time"" discusses how the mechanisms of memory determine the plot structure of the book as a whole. Then in the section ""Form and Time"" we focus on some verse features of The Book of Childhood, which in our opinion correlate with the general semantic dynamics. Finally, the last section, ""Idea and Time,"" reviews the author's concept of memory as a literary technique developed in Mnatsakanova's poems and essays.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1297,The representation of collective voices and social actors in the press through ethnonyms: the case of 'Catalans' and 'Spaniards',"This study analyzes the use of the ethnonyms 'Catalan' and 'Spanish' in the Catalan and Spanish press before and after the October 1, 2017 referendum. Based on a journalistic corpus, it examines their role in identity construction, their semantic prosody, and discursive and ideological contexts. The findings show that these terms are not used in a stereotypical manner and that the notion of a 'collective voice' is a manipulable discursive construction, mediated by journalistic narratives that can exert forms of silencing.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1298,Communicating Sustainable Tourism in English and Italian: A Contrastive Analysis,"Sustainable tourism has become a popular field of research over the last decades; yet, while acknowledging that sustainable tourism requires communication strategies different from those of mainstream tourism, scholars have paid little attention to this area of language. Based on two parallel corpora, this study explores the discursive representation of sustainable tourism in web communication in English and in Italian. The methodological framework adopted is that of Corpus Assisted Discourse Studies. The analysis suggests that the Italian representation of sustainable tourism is characterized by a distant stance towards readers and relies on a strong polarization between 'good' tourism and 'bad' tourism. Communication in English instead relies on proximal person deixis and on the creation of value around responsible tourism by means of factual information rather than on mere evaluation.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1299,Works by Maria Shimanovskaya in the Reflection of Slavic Style Line of Biedermeier,"The aim of the work is the analysis of features of Biedermeier in the legacy of M. Shimanovskaya based on the detailed investigation of Mazurkas and Nocturnes which reflect the main characteristics of the ""high Biedermeier"", distinguished by religious and spiritual traits. The methodological base of the given research is the intonation system of the comparative school of B. Asafiev, representation of the results of contribution of authors which were aimed at the development of problems of Slavic Biedermeier by the means of various types of studies. Scientific novelty lies in the predominant accentuation of the Biedermeier style line in the heritage of M. Shimanovskaya. On the whole, M. Shimanovskaya does not cross the limits of abilities of ""easy piano"", giving preference to expression achievement through the eclectic stylistic combinations, while demonstrating an emphasized miniaturization of national or the expansion of culturally dominant quality of the genre. The mentioned eclecticism does not tough upon the demonic opposition of romanticism in any perceptible way, apart from the ""review"" of the latter in the structure of Nocturne. This is the key point of the Biedermeier logic of the ""great in small"", which also emphasizes the romantic principles in the expressiveness of M. Shimanovskaya`s works and performing art.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1300,The effects of high versus low talker variability and individual aptitude on phonetic training of Mandarin lexical tones,"High variability (HV) training has been found to be more effective than low variability (LV) training when learning various non-native phonetic contrasts. However, little research has considered whether this applies to the learning of tone contrasts. The only two relevant studies suggested that the effect of HV training depends on the perceptual aptitude of participants (Perrachione et al., 2011; Sadakata & McQueen, 2014). The present study extends these findings by examining the interaction between individual aptitude and input variability using natural, meaningful second language input (both previous studies used pseudowords). A total of 60 English speakers took part in an eight session phonetic training paradigm. They were assigned to high/low/high-blocked variability training groups and learned real Mandarin tones and words. Individual aptitude was measured following previous work. Learning was measured using one discrimination task, one identification task and two production tasks. All tasks assessed generalization. All groups improved in both the production and perception of tones which transferred to untrained voices and items, demonstrating the effectiveness of training despite the increased complexity compared with previous research. Although the LV group exhibited an advantage with the training stimuli, there was no evidence for a benefit of high-variability in any of the tests of generalisation. Moreover, although aptitude significantly predicted performance in discrimination, identification and training tasks, no interaction between individual aptitude and variability was revealed. Additional Bayes Factor analyses indicated substantial evidence for the null for the hypotheses of a benefit of high-variability in generalisation, however the evidence regarding the interaction was ambiguous. We discuss these results in light of previous findings.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1301,Intonation of absolute interrogatives in the Southern Spanish in spontaneous speech,"The principal objective of this paper is to verify whether the four patterns described by Castilian Spanish interrogatives in perception tests can be found in the speech of native speakers from the South of Spain (Andalusia, Extremadura, Murcia, Castile-La Mancha) and the Canary Islands. In addition, the study aims to discover whether there are specific melodic features in each area, and ultimately to identify the semantic-pragmatic meanings of each pattern. The research, based on 186 absolute questions issued in a context of spontaneous speech, has been carried out following the Melodic Analysis of Speech methodology. The results of the acoustic analysis allow us to a) demonstrate the existence of the four patterns in the five dialectal regions studied; b) determine that, from a linguistic intonation viewpoint, there appear to be no melodic features that distinguish each area, but a different pattern representation, and c) describe the social-pragmatic meanings proposed by Escandell (1998, 2002), allowing us to distinguish the pattern used in each context.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1302,"Perceptual validation of two pitch representation procedures applied to Spanish, Brazilian Portuguese and Spanish as a foreign language","There are many procedures to describe F0 contours in acoustic-phonetic terms. Garrido's (1996, 2001, 2010) and z-score normalization systems have the advantage that they are automatic procedures. Therefore, this paper aims at analyzing both systems in order to determine the possible future application for the analysis of Spanish as a Foreign Language spoken by Brazilians (SFL). To do so, we analyze the melodic curves in declaratives, yes-no questions and wh-questions in Madrid Spanish, in Brazilian Portuguese (BP) spoken in the city of Sao Paulo and finally, in SFL. The validity of both systems was tested through a perception test with 10 Brazilian and 10 Spanish listeners. Listeners rated the degree of similarity between the signals with original intonation and signals with stylized F0 contours. The results showed that Garrido's automatic stylization method is really able to perceptually create representations equivalent to the original ones in Spanish, BP and SFL and also better than those obtained with the procedure based on z-score normalization.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1303,"On argument structure, focusing and modal sentence adverbials in Czech and Russian","Prototypical features of modal sentence adverbial constructions are analyzed and compared with the prototypical features of the so-called rhematizers such as only (focalizers or focus sensitive particles, cf. Hajicova 1995a; Koktova 1986; 1987; McCawley 1996; Boguslavskij 1985) in Czech and Russian sentences. The analysis is based on the assumption that the syntactic position of the surface word order of arguments and adjuncts reflects (also in its relation to sentence prosody) the categorical representation of the cognitive meaning of the sentence. In secondary cases, an adverbial of mood can also occur in the topic position of the sentence (i.e., in the leftmost position) without necessarily ascribing the sentence an existential meaning as proposed for locative sentences by Babby (1980, 101). Following Hajicova (1995ab), it seems to be necessary to distinguish between the ""focus of a rhematizer,"" the ""focus position of the adjuncts (adverbials)"" and the ""focus of the arguments."" In addition, several Czech and Russian modal sentence adverbials and focalizers are discussed which exhibit interesting syntactic relations. The analysis is based on the dichotomy of topic and focus articulation as developed in the tradition of Prague school (Firbas 1992; Hajicova, Partee, Sgall, in prep.), but it entails also some considerations on informational structure of adjuncts and arguments (Grimshaw 1991) as described in the Minimalist program (Chomsky 1995; Kayne 1995; Kosta 1997). Also the notions of pragmatic ordering principle, of contextual boundness (Krifka 1995), of salience and the hierarchy of communicative dynamism offer convenient tools for a sufficiently detailed description of the sentence structure of Czech and Russian modal sentence adverbials.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1304,Statistical modelling of speech segment duration by constrained tree regression,"This paper presents a new method for statistical modelling of prosody control in speech synthesis. The proposed method, which is referred to as Constrained Tree Regression (CTR), can make suitable representation of complex effects of control factors for prosody with a moderate amount of learning data. It is based on recursive splits of predictor variable spares and partial imposition of constraints of linear independence among predictor variables. It incorporates both linear and tree regressions with categorical predictor variables, which have been conventionally used for prosody control, and extends them to more general models. In addition, a hierarchical error function is presented to consider hierarchical structure in prosody control. This nea method is applied to modelling of speech segmental duration. Experimental results show that better duration models are obtained by using the proposed regression method compared with linear and tree regressions using the same number of free parameters. It is also shown that the hierarchical structure of phoneme and syllable durations can be represented efficiently using the hierarchical error function.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1305,Prosody prediction from tree-like structure similarities,"We present ongoing work on prosody prediction for speech synthesis. This approach considers sentences as tree-like structures and decides on the prosody from a corpus of such structures using machine learning techniques. The prediction is achieved from the prosody of the closest sentence of the corpus through tree similarity measurements in a nearest neighbour context. We introduce a syntactic structure and a performance structure representation, the tree similarity metrics considered, and then we discuss the prediction method. Experiments are currently under process to qualify this approach.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1306,Effects of aging on auditory processing of speech,"The focus of this paper is on the effects of age on speech perception, with reference to pertinent psychoacoustic findings. The difficulties of older listeners are related to the well-known effects of high-frequency hearing loss on speech perception in quiet, and to temporal processing declines not predictable from the audiogram that account for reduced ability to listen in complex, noisy conditions. We also discuss issues of research interpretation; e.g. the need for researchers and clinicians to be alert to the frequent confound between degree of hearing loss and age. The implications of age-related changes in auditory speech processing for future practice and research are discussed relative to interactions between older individuals and their acoustic environments.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1307,"Towards symmetric multimodality: Fusion and fission of speech, gesture, and facial expression","We introduce the notion of symmetric multimodality for dialogue systems in which all input modes (eg. speech, gesture, facial expression) are also available for output, and vice versa. A dialogue system with symmetric multimodality must not only understand and represent the user's multimodal input, but also its own multimodal output. We present the SmartKom system, that provides full symmetric multimodality in a mixed-initiative dialogue system with an embodied conversational agent. SmartKom represents a new generation of multimodal dialogue systems, that deal not only with simple modality integration and synchronization, but cover the full spectrum of dialogue phenomena that are associated with symmetric multimodality (including crossmodal references, one-anaphora, and backchannelling). We show that SmartKom's plug-an-play architecture supports multiple recognizers for a single modality, eg. the user's speech signal can be processed by three unimodal recognizers in parallel (speech recognition, emotional prosody, boundary,prosody). Finally, we detail SmartKom's three-tiered representation of multimodal discourse, consisting of a domain layer, a discourse layer, and a modality layer.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1308,"The Harmonic Minor Scale Provides an Optimum Way of Reducing Average Melodic Interval Size, Consistent with Sad Affect Cues","Small pitch movement is known to characterize sadness in speech prosody. Small melodic interval sizes have also been observed in nominally sad music-at least in the case of Western music. Starting with melodies in the major mode, a study is reported which examines the effect of different scale modifications on the average interval size. Compared with all other possible scale modifications, lowering the third and sixth scale tones from the major scale is shown to provide an optimum or near optimum way of reducing the average melodic interval size for a large diverse sample of major-mode melodies. The results are consistent with the view that Western melodic organization and the major-minor polarity are co-adapted, and that the structure of the minor mode contributes to the evoking, expressing or representation of sadness for listeners enculturated to the major scale.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1309,Post-Verbal Monosyllabic Adverbs in Brazilian Portuguese: On the Relation between Syntax and Prosody,"This article discusses the analysis proposed by Costa (1998) for the syntax of monosyllabic adverbs in European Portuguese (EP). Theoretically, the aim is to evaluate the analysis' underlying assumptions about the syntax-prosody interface. Under Costa's proposal, the order [Verb Complement Adverb] involves scrambling of the Complement to the left of the Adverb in EP. His main argument is based on an interaction of the phonological properties of monosyllabic adverbs with Cinque (1993)'s theory of the phrasal stress. Crucially, this theory assumes a very strong relation between syntactic constituents and the prosodic representation relevant for stress assignment. We will argue, however, that the distribution of monosyllabic adverbs, at least in BP, is best explained by autonomous rules of phonological phrase formation, as in Nespor & Vogel (1986)'s framework. Under this view, the relation between syntactic and prosodic constituents is indirect, and leads us to conclude that the distribution of monosyllabic adverbs does not support Complement scrambling in BP.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1310,Are you asking me or requesting me?,"In this paper the intonation contours of yes/no questions and requests in Brazilian Portuguese are compared. Speech synthesis was used to evaluate the relevance of the F0 configuration of nuclear and prenuclear pitch accents in the distinction between these two intonational patterns. Thirty-one resynthesized melodic versions of the sentence ""Na festa destranca?"" (At the party do you unlock it?/ At the party would you please unlock it?) - which is ambiguous in Portuguese in its interpretation as a question or as a request - have been created. These versions, placed in four auditory tests, have been judged by twenty listeners who evaluated the effects of the modifications in the original sentence, in favor of its interpretation either as a question or as a request. The results of the perception tests show that the opposition between the two melodic contours is due to the direction of the F0 curve in the nuclear pitch accent, rising in the yes/no question and falling in the request. A phonological representation of these patterns, based on distinct temporal alignments of the F0 peak in the final stressed syllables, is proposed.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1311,Syntax Analysis and Machine Translation of Bangla Sentences,"This paper addresses a method to analyze syntactically Bangla sentence using context-sensitive grammar rules which accept almost all types of Bangla sentences including simple, complex and compound sentences and then interpret the input Bangla sentence to English using the NLP conversion unit. The grammar rules employed for this system allow parsing five categories of sentences according to Bangla intonation. The system is based on analyzing an input sentence and converting into a structural representation (SR). Once the SR is created for a particular sentence it is then converted to corresponding English sentence by NLP conversion unit. For conversion, the NLP conversion unit takes help of the Corpus. The effectiveness of this method has been justified over the demonstration of different Bangla sentences with 28 decomposition rules and the success rates in all cases are over 90%.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1312,RELATIONSHIP BETWEEN SIGN LANGUAGE AND PORTUGUESE LANGUAGE IN DIDACTIC MATERIALS: THE NOTATION BY THE SEMANTIC NUMBERS,"This article intends to, in a way, present a short analysis of some national publications of books and educational material for the teaching of LIBRAS, focusing on the written representation of the Portuguese language for the teaching of sign language. On the other hand, this paper also shows a different proposal of notation that does not disregard the grammatical structure of the Portuguese language, which we call Semantic Numbers, present in the textbooks used for teaching LIBRAS to university students of Universidade Federal de Mato Grosso (UFMT). Solidifying this proposal, we will expose some concepts about language, themselves proposed by Russian thinker Mikhail Bakhtin and his Circle, among which we highlight the topics of language as interaction, utterance, intonation and theme.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1313,Sounding for Meaning: Using Theories of Knowledge Representation to Analyze Aural Patterns in Texts,"Computational literary analytics that include frequency trends and collocation, topic modeling, and network analysis have relied on rapid and large-scale analysis of the word or strings of words. This essay shows that there are many other features of literary texts by which humanists make meaning other than the word, such as prosody and sound, and how computational methods allow us to do what has historically been a more difficult method of analysis - trying to understand how literary texts make meaning with these features. This paper will discuss a case study that uses theories of knowledge representation and research on phonetic and prosodic symbolism to develop analytics and visualizations that help readers discover aural and prosodic patterns in literary texts. To this end, this paper has two parts: (I) We describe the theories of knowledge representation and research into phonetic and prosodic symbolism that underpin the logics and ontologies of aurality incorporated in our project. This basic theory of aurality is reflected in our use of OpenMary, a text-to-speech application tool for extracting aural features; in the ""flow"" we coordinated to pre-process texts in SEASR's Meandre, a data flow environment; in the instance-based predictive modeling procedure that we developed for the project; and in ProseVis, the reader interface that we created to allow readers to discover aural features across literary texts. And (II), we discuss readings of several works by Gertrude Stein (the portraits ""Matisse"" and ""Picasso"" and the prose poem Tender Buttons) that were facilitated by this work.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1314,HELPING INTERNATIONAL TEACHING ASSISTANTS ACQUIRE DISCOURSE INTONATION: EXPLICIT AND IMPLICITL2 KNOWLEDGE,"This study explored a theoretically-driven permutation of an intervention designed to improve ITAs' spoken Discourse Intonation (DI). The object was to learn if implicit knowledge growth in DI could be found as the result of an experimental group participating in explicit instruction and in audio-assisted repeated reading treatments using twice-weekly easy, popular science texts for 14 weeks. In a read-aloud condition where speech processing burdens were reduced, both an experimental and control group (who received explicit instruction only) improved over time on speech rate, planning pauses versus hesitation pauses, prominence, tone choices, and length of tone choice pause groups. In a free-response task where processing burdens were increased, however, there was little evidence of change in implicit knowledge of DI for the experimental group. One positive thing was learned: Explicit DI instruction did not reduce participants' speech rate and thus participants could focus on form as well as meaning in extended speech. Explicit DI instruction, where form is linked to meaning, is worthwhile in that explicit knowledge may become proceduralized and available for learners' extemporaneous use. Implicit knowledge building in DI, while difficult to demonstrate, may still be worthwhile if it builds learners' knowledge of vocabulary (to improve prominence) and builds their experience hearing DI features linked to meaning within extended texts.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1315,Taiwanese EFL Learners' Perception of English Word Stress,"This paper investigates how Taiwanese EFL learners perceive non-word pairs which differ only in the location of stress (e.g., fercept vs. fercept) when the phonetic cue of pitch is manipulated. Fifty-eight Taiwanese EFL learners participated in two forced choice perceptual experiments, in which they were asked to identify a perceived non-word when its stressed syllable was signified either (i) by higher pitch or (ii) by a low rising tonal contour. The results show that, while these L2 learners had little difficulty in perceiving stress when the stress was signified by higher pitch, they all had great difficulty in doing so when the stress was signified by the low rising tonal contour. In addition, analyses of their errors show that less experienced learners relied mainly on higher pitch or rising pitch contour in guessing the position of stress, which may indicate a persistent effect of their L1 tonal system or L2 learners' universal tendency of perceiving stress, while more experienced learners referred to the information of morpho-syntactic categories as a strategy in guessing the position of stress, suggesting their phonological awareness of the difference between lexical tone and lexical stress at their developmental stage.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1316,Aprosody: A right hemisphere dysarthria?,"Over the years, many descriptions of aprosody have been offered. Even so, to date no clear definition of this speech disorder exists, and the relation between aprosody and dysarthria has remained elusive. This quandary has lingered because of prevailing theoretical perspectives on prosody that have undergirded motor speech classification systems such as the Mayo classification system or models of aprosody like the one developed by Ross (1981). In the former framework, prosody is placed on top of a hierarchically organized speech process, whereas in the latter it is construed to be much like language, that is, representational and highly dependent on cortex. Evidence is reviewed to show that a parallel organization of prosody that pays tribute to asymmetric hemispheric involvement in laryngeal and supralaryngeal gestures should be adopted in theories of neurogenic speech disorders.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1317,VoiceUNL: A semantic representation of emotions within Universal Networking Language formalism based on a dialogue corpus analysis,"The paper aims to propose a semantic representation of emotions for oral dialogues, based on an analysis of real-life conversations, telephone messages and recorded TV programmes, for the purposes of a speech to speech machine translation. Lexicon and phatics are one of important emotion eliciting factors as well as gestures, prosody and voice tone in oral dialogues. So, the semantic representation is made in a way where these factors are taken into account at the same time. Also, it's done within Universal Networking Language (UNL) formalism, where UW (universal word) plays an important role.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1318,Diction based prosody modeling in table-to-speech synthesis,"Transferring a structure from the visual modality to the aural one presents a difficult challenge. In this work we are experimenting with prosody modeling for the synthesized speech representation of tabulated structures. This is achieved by analyzing naturally spoken descriptions of data tables and a following feedback by blind and sighted users. The derived prosodic phrase accent and pause break placement and values are examined in terms of successfully conveying semantically important visual information through prosody control in Table-to-Speech synthesis. Finally, the quality of the information provision of synthesized tables when utilizing the proposed prosody specification is studied against plain synthesis.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1319,Experimental evaluation of tree-based algorithms for intonational breaks representation,"The prosodic specification of an utterance to be spoken by a Text-to-Speech synthesis system can be devised in break indices, pitch accents and boundary tones. In particular, the identification of break indices formulates the intonational phrase breaks that affect all the forthcoming prosody-related procedures. In the present paper we use tree-structured predictors, and specifically the commonly used in similar tasks CART and the introduced C4.5 one, to cope with the task of break placement in the presence of shallow textual features. We have utilized two 500-utterance prosodic corpora offered by two Greek universities in order to compare the machine learning approaches and to argue on the robustness they offer for Greek break modeling. The evaluation of the resulted models revealed that both approaches were positively compared with similar works published for other languages, while the C4.5 method accuracy scaled from 1% to 2,7% better than CART.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1320,Robust recognition of emotion from speech,"This paper presents robust recognition of a subset of emotions by animated agents from salient spoken words. To develop and evaluate the model for each emotion from the chosen subset, both the prosodic and acoustic features were used to extract the intonational patterns and correlates of emotion from speech samples. The computed features were projected using a combination of linear projection techniques for compact and clustered representation of features. The projected features were used to build models of emotions using a set of classifiers organized in hierarchical fashion. The performances of the models were obtained using number of classifiers from the WEKA machine learning toolbox. Empirical analysis indicated that the lexical information computed from both the prosodic and acoustic features at word level yielded robust classification of emotions.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1321,Neuroplasticity in the processing of pitch dimensions: A multidimensional scaling analysis of the mismatch negativity,"Purpose: An auditory electrophysiological study was conducted to explore the influence of language experience on the saliency of dimensions underlying cortical pitch processing. Methods: Mismatch negativity (MMN) responses to Mandarin tones were recorded in Chinese and English participants (n = 10 per group) using a passive oddball paradigm. Stimuli consisted of three tones (T1: high level; T2: high rising; T3: low falling-rising). There were three oddball conditions (standard/deviant): T1/T2, T1/T3, T2/T3. In the T1/T2 and T1/T3 conditions, each tonal pair represented a contrast between a level and a contour tone; the T2/T3 condition, a contrast between two contour tones. Twenty dissimilarity matrices were created using the MMN mean amplitude measured from the Fz location for each condition per participant, and analyzed by an individual differences multidimensional scaling model. Results: Two pitch dimensions were revealed, interpretively labeled as 'height' and 'contour'. The latter was found to be more important for Chinese than English subjects. Using individual weights on the contour dimension, a discriminant function showed that 17 out of 20 participants were correctly classified into their respective language groups. Conclusions: The MMN can serve as an index of pitch features that are differentially weighted depending on a listener's experience with lexical tones and their acoustic correlates within a particular tone space.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1322,PHONETIC WORD - A BASIS FOR UNDERSTANDING AND LEARNING THE SPOKEN FRENCH,"Long ago sociolinguists have indicated that variability is the essential feature of language nature. It is difficult to speak about the ideal spoken form. There are only realised spoken forms, and the criteria for their evaluation depend on the native speakers' Judgement. The standard pronunciation does not always mean the same realisation, the same acoustic utterance form. Those who use a foreign language today must be able to understand the natural speech, but also try hard to speak as naturally as possible so as to be treated as equal partners in verbal communication. In connected speech specific categories of prosodic features are manifested The relevant domains of realisation of these features are syllable, foot, phonetic word, intonation group, utterance. All the mentioned levels change significantly the canonical acoustic word forms as are usually stored in the mental lexicon of foreign language learners. The paper draws particular attention to the role of phonetic word both in understanding (with regard to the fluent speech segmentation problem) and in connecting individual words into accentual speech units. Phonetic word in French pronunciation may be regarded as the natural context for mastering most pronunciation phenomena. Segment phonotactics in syllable constituents can be most adequately presented within phonetic word Resyllabifications, which are traditionally marked as ""enchainement"" and ""liaison"", occur within this unit. Hierarchical and rhythmic, as well as melodic, syllable organisation is also realised in phonetic word. Particular attention should be paid to the final, accented, in articulatory terms the most organised phonetic word syllable, due to its linguistic relevance and perceptual saliency. The elision of the French ""potential"" schwa leads not only to the reduction of the number of syllables of the given phonetic word, but also to assimilation in consonant clusters at word edges within phonetic word, often creating in that way consonant clusters atypical of the French phonotactics, and even segments atypical of the language. Within phonetic word, but at its end too, occur many elisions and simplifications of consonant clusters, typical of standard informal speech in particular.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1323,Measuring emotion in the voice during psychotherapy interventions: A pilot study,"The voice as a representation of the psychic world of patients in psychotherapeutic interventions has not been studied thoroughly, To explore speech prosody in relation to the emotional content of words, voices recorded during a semi-structured interview were analyzed. The subjects had been classified according to their childhood emotional experiences with caregivers and their different attachment representations. In this pilot study, voice quality as spectral parameters extracted from vowels of the key word ""mother"" (German: ""Mutter"") were analyzed. The amplitude of the second harmonic was large relative to the amplitude of the third harmonic for the vowel ""u"" in the secure group as compared to the preoccupied group. Such differences might be related to the subjects' emotional involvement during an interview eliciting reconstructed childhood memories.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1324,The Phonetics of Paiwan Word-Level Prosody,"In this paper the phonetic correlates of word-level prosody of an indigenous language were examined as a case of supporting the argument that phonetic representation plays a role in the documentation of phonology. Piuma Paiwan has a quality-sensitive stress in which peripheral vowels such as /i/, /u/, and /a/ are more optimal than the central schwa, and the primary stress falls on the most sonorant or the most optimal vowel. However, a schwa nucleus can bear stress in the other Paiwan dialects. On the other hand, imperative accent may result in the pitch peak aligned with the final syllable. Stressed syllables always have higher pitch than unstressed syllables, and pitch tends to be a robust cue to word-level stress and accent in Paiwan. The results suggest that word-level prosody in Paiwan is best modeled in terms of f0 realization.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1325,SYNTACTIC TONE AND DISCOURSE PROCESSING IN BEIJING MANDARIN: A CASE STUDY,"Three experiments tested the role of an emerging syntactic tone in spoken word recognition of Beijing Mandarin Chinese. The study examined the role of the new syntactic tone in lexical access and how synchronic change is perceived in the task of word recognition. Experiment 1 used an information-deprived condition where information for the standard interpretation of the word was provided prior to the crucial tone, but the syntactic tone based interpretation was not provided. Experiment 2 used a prosody-reduced condition where the prosodic cues were unclear while information for both standard and syntactic tone based interpretation was provided. Experiment 3 involved a story retell task to further assess tone comprehension. The results show that the emerging syntactic tone had an impact on participant's lexical interpretations, although the impact was more obvious when prosody information was clearly provided. The results also confirmed participants' tone comprehension through story retells. The results support the hypotheses that a) there is a new syntactic tone in spoken Beijing Mandarin whose function goes beyond lexical tone, which has become a precursor for word recognition in discourse processing; and, b) language processing reflects asymmetry in that speakers prefer the traditional grammatical interpretation more than the interpretation derived from the newly developed syntactic tone when prosodic information is unclear. The study demonstrates how usage-based grammatical changes affect speaker's mental model of lexical access.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1326,An F0 Analysis of Discourse Construction and Global Information in Realized Narrative Prosody,"The aim of the present study is to show how the time varying F0 signal in paragraph prosody output is composed of contributions from multiple ranks of local between-unit concatenation and global higher-level layering. Discourse specified positioning, pre-boundary lengthening, and post-boundary pauses are also contributing variables. Both adjacency smoothing and higher level layering collectively make up the output F0, as supported by quantitative results. Results also reveal distinct patterns of phrase-level F0 height modulations within and between paragraphs, confirming the significance of phrase-level global prosody context in addition to tone-intonation interaction. Cross-speaker analysis of speech data varying in prosodic style show systematic patterns of contribution distribution by style, thus proving one base form is sufficient for both the planning and processing of surface variations. The study also shows why both methodology and interpretation must reflect the relative nature of suprasegmental cues, and how corpus analysis sheds new light on acoustic analysis.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1327,Contribution of Spectral Cues to Mandarin Lexical Tone Recognition in Normal-Hearing and Hearing-Impaired Mandarin Chinese Speakers,"Objective: The purpose of this study was to investigate the contribution of spectral fine structure and spectral envelope cues to recognition of Mandarin lexical tones in normal-hearing and sensorineural hearing-impaired Mandarin-speaking listeners. Design: Four groups of subjects participated in the study, including 20 normal-hearing, 20 moderately, 20 moderately to severely, and 8 severely hearing-impaired listeners. The original speech materials consisted of 16 sets of Mandarin monosyllables spoken by a male and a female. Each monosyllable had four tonal patterns, resulting in a total of 64 combinations of consonants, vowels, and tones. A Linear Predictive Coding (LPC) algorithm was used to create two sets of synthesized materials, including 128 tokens with the original spectral fine structure mixed with the spectral envelope from a different tone, as well as 128 tokens with noise fine structure and the original spectral envelope. All subjects participated in tone recognition tests using the two sets of chimeric tone tokens. Oral responses to tones were recorded and scored as percent correct. Results: Hearing-impaired listeners could take advantage of spectral fine structure in the recognition of lexical tones, but with increasing hearing loss, the ability of hearing-impaired listeners to recognize tones became worse, especially for severely hearing-impaired listeners. Hearing-impaired listeners showed significant differences in tone recognition between the male and female voices. Tone 3 was the easiest tone to perceive, followed by tone 2, whereas tones 1 and 4 were hard for all subjects, particularly when only the spectral envelope cue was available. Hearing-impaired listeners showed a significantly lower level of lexical tone recognition than normal-hearing listeners when using spectral envelope cues in comparison with normal-hearing listeners. Conclusions: These results demonstrate that the spectral fine structure cue dominates lexical tone recognition for all subjects. Listeners with sensorineural hearing impairment showed reduced ability in the recognition of lexical tones using both spectral fine structure and spectral envelope cues, which may result from their impaired auditory spectral resolution.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1328,Focus on Form as a Pedagogical Framework for Fostering a Native-like Mandarin Tonal Identification System,"Second-language acquisition (SLA) research has shown that given short-term intensive form-focused training, non-native Mandarin speakers are able to establish a categorical tonal representation, which allows them to identify systematically features of tones borne by isolated words. However, developing a categorical tonal inventory is a necessary but insufficient condition for a native-like tonal identification system. Native Mandarin speakers' tonal inventory is well connected to the mental lexicon, which allows them to efficiently resolve any tonal ambiguities resulting from phonemic sandhi or phonetic transformation in connected speech. Existing L2 form-focused training programs, as seen in Sun (1997), have not shown positive effects on entrenching the link between the tonal inventory and the mental lexicon. Drawing upon the insights from studies conducted within the framework of focus on form (FonF)-a pedagogical intervention used to direct learners' attention to the formal aspect of a linguistic construction in meaning-oriented activities, this paper contends that implicit FonF, when taken into consideration of L2 learners' internal learning agenda and universal processing strategies, can provide an optimal encoding environment for internalizing intricate tonal behaviors. To this end, this paper first elucidates ""what do native and non-native tonal speakers actually do?"" when identifying tone in connected speech. Based upon the reviewed literature, the paper then discusses issues that need to be considered in L2 tonal instruction, and proposes how FonF pedagogical guidelines may be used to remediate problematic tonal tokens in context and hence foster an efficient tonal identification system for non-native tonal speakers-a domain rarely discussed under the FonF framework and in the SLA literature.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1329,Sentence and textual integration,"The article sets up a twofold modelling of verbal communication: i.e. syntactic computing, and textual computing. Syntactic computing consists in producing/interpreting the elementary units which give the message its basic structure. Contrary to commonly received ideas, it is argued that those elementary units follow the 'sentence' pattern, assuming that a sentence may include not only the core (propositional structure) but also some peripheric elements. Both speaker and hearer try (successfully or not) to build (/reconstruct) well-formed sentences, in a step by step ('sequence' by 'sequence') process. Textual computing deals with the immediate integration of processed sequences (which lose their previous autonomy) into a higher level of textual representation. The analysis of the oral text (interview of Anita Musso) shows that the sentence level is easily recognisable (through unequivocal cores, and peripheric elements identifiable as such) and clearly plays a basic structuring role, which no alternative unit is likely to play. Textual units of a higher level than the sentence level are much more difficult to determine and to describe, for at least three reasons: (a) sequences may provide heterogeneous materials; (b) embedding sequences in order to build a higher representation is a continuous process, which leads to complex hierarchical structures; (c) speakers often follow a non-consistent expository path (as is the case in the text under study). Intonation has been deliberately left aside: the aim was to extend a purely segmental analysis as far as possible towards a complete account of the text.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1330,The applications of the comma in contemporary Italian. From the phono-syntactic perspective to the textual perspective,"Angela Ferrari & Letizia Lala, The applications of the comma in contemporary Italian. From the phono-syntactic perspective to the textual perspective Our study focuses on comma use in contemporary Italian, specifically on texts that are by competent writers, with a medium to formal register and, for reasons of method, non-literary. From a descriptive point of view, we will show that comma use increasingly follows uniformities of a ""textual"" nature, in that it relates to creating and prioritizing textual units. From an explanatory point of view, we will first demonstrate that any treatment of comma use in terms of syntax and intonation exclusively is bound to fail since the generalizations that it will lead to are either false or break down in offshoots of exceptions and special cases; we will then argue that a significant set of syntactical uses may be reinterpreted from a textual perspective, enabling a more rational, consistent, and realistic representation of comma use.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1331,Central African French: lexical tone French,"In this article, I present the tonal system of Central African French (CAF), which is the variety of French spoken in Bangui in the Central African Republic. The study is based on a corpus of recordings of spontaneous speech produced by twelve multilingual speakers, using mainly the African language Sango and French in their every-day communication. I show that the tonal system of CAF is to a great extent influenced by the phonological system of Sango.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1332,PITCH PROCESSING IN MUSIC AND SPEECH,"The present paper proposes an overview of research that investigates pitch processing by considering cognitive processes (related to context, learning, memory and/or knowledge) for both music and language materials. Research investigating cross-domain influences of expertise (either in music or tone languages) and deficits (as in congenital amusia), referred to as positive and negative transfer effects, also contributes to our understanding of domain-specificity or - generality of mechanisms involved in pitch processing.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1333,Development of the Adaptive Music Perception Test,"Objectives: Despite vast amounts of research examining the influence of hearing loss on speech perception, comparatively little is known about its influence on music perception. No standardized test exists to quantify music perception of hearing-impaired (HI) persons in a clinically practical manner. This study presents the Adaptive Music Perception (AMP) test as a tool to assess important aspects of music perception with hearing loss. Design: A computer-driven test was developed to determine the discrimination thresholds of 10 low-level physical dimensions (e.g., duration, level) in the context of perceptual judgments about musical dimensions: meter, harmony, melody, and timbre. In the meter test, the listener is asked to judge whether a tone sequence is duple or triple in meter. The harmony test requires that the listener make judgments about the stability of the chord sequences. In the melody test, the listener must judge whether a comparison melody is the same as a standard melody when presented in transposition and in the context of a chordal accompaniment that serves as a mask. The timbre test requires that the listener determines which of two comparison tones is different in timbre from a standard tone (ABX design). Twenty-one HI participants and 19 normal-hearing (NH) participants were recruited to carry out the music tests. Participants were tested twice on separate occasions to evaluate test-retest reliability. Results: The HI group had significantly higher discrimination thresholds than the NH group in 7 of the 10 low-level physical dimensions: frequency discrimination in the meter test, dissonance and intonation perception in the harmony test, melody-to-chord ratio for both melody types in the melody test, and the perception of brightness and spectral irregularity in the timbre test. Small but significant improvement between test and retest was observed in three dimensions: frequency discrimination (meter test), dissonance (harmony test), and attack length (timbre test). All other dimensions did not show a session effect. Test-retest reliability was poor (<0.6) for spectral irregularity (timbre test); acceptable (>0.6) for pitch and duration (meter test), dissonance and intonation (harmony test), and melody-to-chord ratio I and II (melody test); and excellent (>0.8) for level (meter test) and attack (timbre test). Conclusion: The AMP test revealed differences in a wide range of music perceptual abilities between NH and HI listeners. The recognition of meter was more difficult for HI listeners when the listening task was based on frequency discrimination. The HI group was less sensitive to changes in harmony and had more difficulties with distinguishing melodies in a background of music. In addition, the thresholds to discriminate timbre were significantly higher for the HI group in brightness and spectral irregularity dimensions. The AMP test can be used as a research tool to further investigate music perception with hearing aids and compare the benefit of different music processing strategies for the HI listener. Future testing will involve larger samples with the inclusion of hearing aided conditions allowing for the establishment of norms so that the test might be appropriate for use in clinical practice.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1334,MUSICAL TERMINOID REMARKS AS A MEANS OF INTERPRETATIVE READING OF THE TEXT AND DETERMINANTS OF MUSICAL INTONATION CHARACTERISTICS,"Musical remarks, as a specialist's communicative space, the environment of his linguistic existence, have their own terminological system and are a symbolic representation of knowledge, providing mutual understanding between scientists in musicological field The problem of inseparable unity of composer's style and expressive-verbal means of musical intonation has been studied insufficiently in both linguistics and musicology. Pragmatic analysis of musicological texts, in particular, the piano and vocal scores, will give the possibility to identify music as the intonation-artistic activity, which is not just a reflection, but an expression of personality sense; to reveal the peculiarities of musicological discourse functions through describing the key strategies and tactics of composer and performer's verbal behavior, reflected in musical terminoid remarks.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1335,Stress and prosodic constituency in French: issues in phonology and speech processing,"This article discusses the prosodic characteristics of French, in light of a metrical and perceptual analysis of various speaking styles. Our results lead us to propose the level of the prosodic word (pw) as the basic level of representation of French accentuation. This strong proposition, which has never been empirically attested to date, is based on two major results: listeners can perceive the final accent independently from intonation boundaries, at all levels of constituency; the processing of accentuation, particularly the initial accent, takes place at the lowest level of prosodic constituency (pw). The surface bipolar accentual demarcation of the lexical word invites us to reinterpret the status of accentuation in French and to question the notion of stress deafness in French.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1336,Automatic Recognition of Pitch Accent Using Distributed Time-Delay Recursive Neural Network,"This paper presents a method for the automatic recognition of pitch accents over syllables. The method that we propose is based on the time-delay recursive neural network (TDRNN), which is a neural network classifier with two different representation of dynamic context: the delayed input nodes allow the representation of an explicit trajectory FO (t) along time, while the recursive nodes provide long-term context information that reflects the characteristics of pitch accentuation in spoken English. We apply the TDRNN to pitch accent recognition in two forms: in the normal TDRNN, all of the prosodic features (pitch, energy, duration) are used as an entire set in a single TDRNN, while in the distributed TDRNN, the network consists of several TDRNNs each taking a single prosodic feature as the input. The final output of the distributed TDRNN is weighted sum of the output of individual TDRNN. We used the Boston Radio News Corpus (BRNC) for the experiments on the speaker-independent pitch accent recognition. The experimental results show that the distributed TDRNN exhibits an average recognition accuracy of 83.64% over both pitch events and non-events.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1337,"PLURALITY OF LANGUAGES, DATA AND METHODS FOR A GENERAL MODEL OF MELODIC VARIATION IN ROMANCE DIALECTS","Speech prosody depends on multiple factors and presents a significant variation among world languages. Interaction between sentence intonation, stress, information patterning and stylistic effects produce local phenomena contributing to complex outcomes that should be studied in accordance with the specific phonology of each language. However, the description of this interaction relies on traditional approaches and various representation methods: different interpretation models are then applied at each level. If, on the one hand, we begin to manage some tools for a general study which seem to bring to light prosodic universals, on the other hand, a too radical approach to prosodic typology may cause the risk of deleting features which are essential from a dialectological point of view. Based on a series of examples concerning the intonational variation in the Romance area, I will try to show in this paper the need to keep using an ecological approach towards languages prosodic diversity.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1338,THE FUNCTIONS OF PROSODIC ELEMENTS IN TV TALK SHOWS IN ENGLISH,"This paper is a part of the MA thesis ""Prosody in TV Talk Shows (analysed material is in English)"". It explores the role of prosody in achieving the cohesion of a text, as well as the connection of prosody with the lexicogrammatical structure within the same field.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1339,A.S. PUSHKIN IN MODERN ENGLISH TRANSLATIONS: STRATEGIES OF POETRY REPRESENTATION,"Despite all the two-centuries long efforts of numerous translators to render A.S. Pushkin's lyric poetry into English, an English-speaking reader still has not acquired more or less adequate idea about it, as stated by many renowned scholars and translators themselves. The very features of Pushkin's lyric text structure and poetics account for it: he can't be adequately translated because the base of Pushkin's poetic style is a masterful and delightful use of all the shades of meaning which is imparted to the language by the existence of case inflections and a free word order. The absence of them in the target language gives some ''straightforwardness'' to translations' style; the themes and literary images turn out plain and simple. But the interest to Pushkin's poetry on behalf of the English-speaking word-cultures has been genuine and intense, which is obvious from a great number of new continuously emerging translations and translation projects. Without claiming to be a comprehensive survey of all modern translations from Pushkin, this paper focuses on several most recent and interesting issues of Pushkin's poetry in English. One of the strategies to be most productive and relevant to the material is the strategy which has been demonstrated by the translator Julian Henry Lowenfeld. His is the strategy of vocal translation, mostly used to render the texts which display a rhythmical and intonation structure, which is adopted to musical melody and inseparable from it. The existence of this melodic base gives the opportunity to hold the text in memory and enables the translator to find adequate imagery, stylistic, rhythmical and phonetic means of rendering the stylistic effect of the original text. The translator himself names the sense of rhythm among the most important qualities in translating Pushkin's verse. The above-mentioned strategy is opposed to another one, which has been demonstrated in one of the recent editions of Pushkin's poetry in English: ''After Pushkin. Versions of the poems of Alexander Sergeevich Pushkin by contemporary poets'', 1999. On reading this edition which in its contents presents the most traditional set of Pushkin's poetry, readers' expectations turn out to be completely ruined. In some translations Pushkin appears a modernist, in others - an existentialist; some translations in their poetic quality are very much akin to Pushkin's poetics style and adequately represent his style, some of them clearly present word-for-word translations. And in the context of this edition these word-for-word translations acquire the status of an aesthetic phenomenon. The very Pushkin's conception of poetic creativity seems to have fallen apart and is presented through individual conceptions of his interpreters, which is demonstrated by the peculiarities of translations' poetics, new text titles, as well as stated in the editor's introductory word, who comments on the working principles of various translations' authors. Notwithstanding the difference in the approach principles to the material for translation, the strategies described imply one and the same aim: to help an English- speaking reader answer the question: why Pushkin stand side by side with Byron, Shakespeare, Goethe and other genius men of the world word- culture.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1340,"""... SENDS BEST GREETINGS TO THE KING"": REPRESENTATION OF EVIDENTIALITY IN A. S. PUSHKIN'S FAIRY TALES","The article explores a number of evidential meanings, like retelling someone else's speech, witnessing of an event through its outcomes (traces), surprises at the sight of something unexpected. The author seeks to show that meanings of the evidential sphere (including not only those listed above, but also some other ones, very specific) are extremely noteworthy expressed in Russian. Taking into account the main ways of expression of the specified meanings, one may say that evidentiality in Russian is expressed mainly lexically (by the use of special words) and with the help of intonation (by means of the corresponding intonation). Assuming the evidentiality an independent functional and semantic category, the author considers the possibilities of its representation (in other words - means of expression) in Russian. In the author's opinion, the most spectacular and ample use of these means is found in the works A. S. Pushkin. Its illustrations include fragments from four masterpieces of the great Russian poet (""The tale of Tsar Saltan"", ""The tale of the fisherman and the fish"", ""The tale of the dead princess and the seven knights"", ""The tale of the Golden Cockerel"").",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1341,"Code and cultural and historical tradition of ""double coding"" in folklore","Purpose of the research is to define theoretical questions aimed at identifying folklore codes and ""double coding"" traditions in the folk art from the point of view of different concepts and paradigms in the scientific literature. The research methodology is the use of the systematic approach, which reveals features of folklore codes and cultural and historical traditions of ""double coding"" in different genres of folk art and performing. The scientific novelty of this work lies in defining of the folklore (auditory, verbal, visual) codes in different genres of folk art (music, dance, fairy tale and ritual). Through the example of performing folk groups and in the process of ""double coding"", the study defines the following codes: representation, evocative and intermediate. Conclusions. Based on the study of both theoretical works and practice, it is determined that the folk-image codes are associated with the mythology, history and religion. Folklore codes, as well as other types of codes, have a stable system of signs and symbols, in which they can be transcribed. Ritual actions, games, dances, songs and fairy tales have their own system of codes (images, events, etc.). Codes in a song and fairy tale are defined in the text (images, symbols), codes in music (tunes, intonation), codes in the musical and compositional structures (definable melodies in calendar ritual folklore), in choreography - music and movement codes based on texts and rhythm. Each folklore genre has performance specifics, but there are common traditions, contained in improvisation based on traditional musical and dance styles and stereotypes. ""Double coding"" in folklore is a multi-step process of transformation of the information, stored for a long time, constantly transmitted from one generation to the other and varying in details (in music, lyrics and stories, dance accompaniment and moves).",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1342,NON-VERBAL COMMUNICATION MODELS IN SPORTS AND BALLET,"This study analyzes the communication model generated among professional soccer trainers, artistic gymnastics trainers, and folkloric ballet instructors, on the basis of the dynamic body language typical of specialized communication among sportspeople and dancers, which includes a high percentage of non-verbal language. Non-verbal language was observed in both psychomotor and socio-motor practices in order to identify and characterize relations between different concepts and their corresponding gestural representation. This made it possible to generate a communication model that takes into account the non-verbal aspects of specialized communicative contexts. The results indicate that the non-verbal language of trainers and instructors occasionally replaces verbal language when the latter is insufficient or inappropriate to describe an action or movement of great precision, due to circumstances of distance or acoustic interferences. With regard to ballet instructors, it was found that there was a generalized form of guiding rehearsals, through the use of rhythmic counts with hands or feet. In addition, the paralinguistic components of the different speech acts are emphasized, especially concerning intonation, duration and intensity.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1343,"Apres-propos Anthony DeCasper, explorer of a new psychology of intimate intentions in understanding","Forty years ago, Tony DeCasper developed a new psychology of childhood, which would transform scientific understanding of motives we share. By adapting an operant method to study the initiative of the infant seeking experience, he clarified how human actions are generated in measures of time in the mind. His method offered the newborn the opportunity to direct the sensations of their sucking movements to select experiences. He proved the fetus could learn the mother's voice and appreciate the rhythms and melodies of her movements. With colleagues in the University of Carolina and in Paris he explored for the musical dynamics and emotions of narratives in shared life that animate learning of speech and language, and other cultural skills. With French colleagues, he recorded changes in heartbeat to detect the emotions of a fetus, and to show their enjoyment of songs and rhymes recited by the mother. Inspired by the courage and imagination of Tony's life in science, we believe that the aesthetic and moral evaluations of human experience are an essential part of our autopoeitic nature, not only products of acquired reasoning and symbolic representation.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1344,Building Tone Resources for Second Language Learners from Phonetic Documentation: Cherokee Examples,"Lexical tone is a linguistic feature which can present difficulties for second language learners wanting to revitalize their heritage language. This is true not only from the standpoint of understanding and pronunciation, but also because tone is often under-documented and resources are limited or too technical to be useful to community members. Even with these challenges, carefully attending to the intricacies of a language's sound system allows learners to express themselves more ""authentically"" or ""naturally,"" which can be important for confidence and acceptance as language users. Learners can be trained to distinguish tones by attending to acoustic or auditory cues related to tone (e.g., pitch contour). This paper describes multimedia resources designed to focus learner attention on perceiving tone - visual and audio accompaniments helping to increase the perception of tone in Cherokee, a severely endangered Native American language. We created resources for tone in the form of an electronic presentation containing explanations, example recordings, and intuitive images to provide audio and visual support for language learners. Presentation and format choices were collaboratively designed based on community requests, with an explicit attempt to de-jargonize materials and make them less technical and more accessible to community members.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1345,Investigation of Pitch and Duration Range in Speech of Sindhi Adults for Prosody Generation Module,"Prosody refers to structure of sound and rhythm and both are essential parts of speech processing applications. It comprises of tone, stress, intonation and rhythm. Pitch and duration are the core elements of acoustic and that information can make easy to design and development for application module. Through these two peculiarities, the prosody module can be validated. These two factors have been investigated using the sounds of Sindhi adults and presented in this paper. For the experiment and analysis, 245 male and female undergraduate students were selected as speakers belonging from five different districts of upper Sindh and categorized into groups according to their age. Particular sentences were given and recorded individually from the speakers. Afterward, these sentences segmented into words and stored in a database consisting of 1960 sounds. Thus, distance of the frequency in pitch was measured via Standard Deviation (SD). The lowest Mean SD accompanied 0.25Hz and 0.28Hz received from male and female group of district Sukkur. The highest Mean SD has measured with male and female group of district Ghotki along 0.42Hz and 0.49Hz. Generally, the pitch of female's speakers was found high in contrast to male's speaker by 0.072Hz variation.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1346,"Art/Artifact Semiotics of Music, Language, and Sound in Different Trains","Steve Reich's minimalist composition Different Trains (1988) for string quartet and tape uses the speech prosody of recorded testimony as its fundamental melodic material. This article adapts Peircean semiotics to explore how Reich's co-constituting treatments of recorded voices-as testimonial evidence and as musical melody-create two co-constituting operations that make possible Different Trains' meaning-making capacity. First, the taped voices' isolation and reproduction complicate the function of language as a set of stipulated symbols by refiguring the speech fragments as material and indexical traces of testimony, not the testimony itself. Second, the transformation of these prosodic fragments into gestural melodies makes the voice's intonational curve an iconic gesture requiring an attentive, even participatory, listening practice. The two operations work in conjunction toward a representation not of history itself, but of history's recurring and highly mediated experience within the present. The intermedial network of sign systems in Different Trains is made possible through Reich's implementation of sound technology in composition and performance. Careful attention to the work reveals the extent to which word and music studies are complicated when the material conditions and acoustic interventions of aural media are taken into account.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1347,Typography in the Construction of Social Meaning,"The paper describes typography as one of the mechanisms contributing to the construction of social meanings. According to the French discourse analysis, we assume that social meaning results from the nomination activity, which involves multiple and diverse naming of one element of the real world. Obtaining social meaning is possible by recreating the so-called designational paradigm, i.e. establishing a list of regular reformulations of the initial word introduced by metalanguage (definitional sentences), coordination, diaphors and typography. This work mainly focuses on the latter. The subsequent parts of the article discuss the impact of the comma, colon, parentheses, double dash and quotation marks on meaning construction. The description of each of them is complemented by examples derived from the study on the social meaning of the proper name Pologne (Poland), created and disseminated by the French press. It turns out that the typographic symbols are not a simple visual representation of pauses or intonation. They also have a semantic and pragmatic role. Typography signals the reformulations (comma, colon, parenthesis, dash), introduces the secondary predicates (comma, dash) and allows a journalist to distance him/herself from the created meanings (quotation marks).",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1348,"Dramatic Poetry as Rhetorical Form: The Case of Sarah Piatt's ""Mock Diamonds""","The programmatic study of narrative and poetry has stalled without engaging many approaches to narrative inquiry and without comprehending the sui generis achievements of poetical representation. This essay attempts to rejuvenate nar-ratological and specifically rhetorical interest in poetry by carefully examining the dra-matic poem-where poetic form intersects with several of narrative theory's abiding enthusiasms: character, voice, perspective, performance. Marrying theoretical specu-lation to both practical criticism and literary history, the argument extrapolates from Sarah Piatt's ""Mock Diamonds "" a twofold rhetoric of the dramatic-poetic mode. It posits first that, often through segmentation and related phenomena, dramatic poems formally enact a competition between individual communicants and the discursive contexts that threaten to supplant their perspectival authority. Second, it maintains that dramatic poems coordinate a dialogic interplay between (1) character speech, whose semantic content manifests the character-speaker's intentions, and (2) versi-fication, whose formal qualities signify outside the represented scene. The recitative performance conventionally mandated by poetry integrates these two communicative channels such that they mean the embodied expression they specify together. That culminative detail underscores the essay's broadest implication: namely, that poetry's emphases on readerly enactment and sonic, somatic, and typographic patterning gen-erate rhetorical effects not readily available in other representational discourses. Dra-matic poetry, as epitomized by ""Mock Diamonds, "" demonstrates the power of poetic form to renovate, and hence to augment, prevailing theories of even the most familiar narrative constituents.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1349,Kurdish Music?. Music from Dersim?.: Conflicting Identities and the Challenge of Categorization in Central-Eastern Anatolia,"In almost all international literature on music of the Middle East, the main categories are still either states or ethnic and/or religious groups (e.g., ""music in Turkey,"" ""Turkish music,"" ""Kurdish music,"" ""Alevi music,"" ""Armenian music""). The population structure of the central portion of eastern Anatolia, however, is neither homogeneous nor historically stable. Among the numerous ethnic Sunni-Muslim, Alevi, and Christian groups, who variously speak Kurdish, Zaza, Turkish, and Circassian languages, exchange and migration has been constant, leading to a complex and dynamic population structure. A number of tribes have not only migrated over time, but subsets within them have even changed their language or denomination. Faced with all these cases, no consensus exists as to which identity discourses deserve more attention from musicologists. In the interest of musical analysis, further typological distinctions are vital to consider, for example socioeconomic structures and contexts, such as ruralmountainous versus urban environments. Since the twentieth century, mediatization, urbanization, and migration have led to an even growing multiplicity of social identities and musical styles, calling into question static, coherent narratives. In this article, the contradictions and inadequacies of prevailing approaches will be emphasized through comparisons of music drawn from different social categories in central eastern Anatolia. Obviously, no unified musical tradition exists in the region, but rather a complex, uneven, and highly creative field of intensely localized and, recently, increasingly individual musical styles, influenced by numerous spheres of identity discourse. Looking only at musical features, however, the issue does not get easier. Not only a paucity of research but also methodological problems impede the comparison of the numerous traditions of singer-poets, laments, and religious poetic texts of different denominations. For the analysis of vocal styles for example, not only intonation and melodic structure have to be taken into account but also the performative dimensions of timbre, vocal intensity, the use of vibrato, dynamics, and the regulation of rhythm and tempo, all elements for whom satisfactory methods of representation and analysis have yet to be developed.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1350,DESMEMRIA: REPORT ABOUT THE RECRIATION OFA SONG,"This text reports a creative journey that began based on the listening of the work Everywhere at the end of time by composer James Leyland Kirby. Kirby's composition, which he signed with the alias The Caretaker, is a musical representation of memory damage caused by Alzheimer's disease. One of the expressive resources used in the work was the electronic deterioration of a 1930s American love song named Heartaches. An exercise was proposed to me: that I should create new lyrics for this love song, approaching the same subject of Everywhere at the end of time -Alzheimer's effects -so that, after that, it could be arranged and performed as Choro. The exercise was developed based on procedures frequently employed by songwriter Chico Buarque to write song lyrics: these procedures were found in testimonials given by him through his musica l career. Two test versions were created plus a definitive one, through partnership, named Desmemoria (Unmemory): the three versions are about memory loss and the difficulty of registering new ones, treating Alzheimer on its potential analgesic effect by erasing, amongst many memories, heartaches from the past. One of the goals of the creative exercise is to reproduce Buarque's way of putting lyrics into melodies, so the main resources used in the process were the use of a thesaurus and a rhyming dictionary, followed by attention to some literary principles such as poetic forms and prosody. By the end of the report the access link to a recorded version of the choro song Desmemoria (Unmemory) is made avaliable so the listening can be possible.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1351,Singers' Realizations of Linguistic Tone in Chaozhou Song,"This article explores singers' realization of linguistic tones of Chaozhou, a Southern Chinese tone language, in song performance. Analysis of a folk song sung by five singers reveals consistent realization of falling tones with a descending pitch within individual notes. The result suggests that Chaozhou singers may incorporate their spoken language experience into the notes they sing, irrespective of the different routes of learning to sing the song. Additionally, the realization of the phonetic distinctions of the language may also reflect an effort to delineate cultural boundaries and assert a cultural identity of Chaozhou.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1352,ACOUSTIC PATTERNS IN HONG KONG CANTONESE HESITATION MARKERS: VOWEL QUALITY AND OMNISYLLABIC TONE,"Hesitation markers (HMs) lie somewhere on the dividing line between linguistic and sub-linguistic, evincing at the same time crosslinguistic commonalities as well as language-specific features (Candeaet al.2005; Dingemanse andWoensdregt2020). This study seeks to expand on understanding of these lexically peripheral items by analyzing their acoustic properties in Hong Kong Cantonese (HKCT), including vowel quality and F0. Recent work by Dingemanse and Woensdregt (2020) discusses phoneticsimilarities in HMs across languages, which are constrained by adherence to the phonologies of their respective languages. However, it is unclear how HMs are incorporated into HKCT tonal phonology, which can be characterized as 'omnisyllabic' (Matisoff 1995), every syllable being associated with a lexical tone.The present study gathers acoustic data (F0, F1-F2, duration) from the PolyU Corpus of Spoken Chinese (http://wongtaksum.no-ip.info:81/corpus.htm)across 10 speakers and 196 HMs, comparing them against 525 lexical items. Results for F1-F2 clusteredaround mid-front /epsilon/, which is in line both with around the lower end of the pitch range, approaching Tones 3 and 6, which is expected given effort minimization trends. However, establishing a connection with a particular lexical tone was complicated by the similarity of Tones 3 and 6, which could not be statistically distinguished. Cross-speaker analysis showed considerable variation in F0, indicating that context, intonation, or idiolectal variation may play a substantial role. This has implications for our understanding of how peripheral items likehesitation markers are treated in omnisyllabic tone languages like Cantonese.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1353,ON THE SYNTACTIC REPRESENTATION OF CO-SPEECH GESTURES IN EXPRESSIVE LANGUAGE,"Language is assumed to be a multimodal system in which prosody, manual and non-manual gestures, and body positioning interact with syntactic structure in systematic ways. While gestures have often been analyzed from a semantic perspective, recent work has shown that they can be integrated into the syntactic architecture of the clause. We adopt this perspective and propose an analysis of co-speech gestures in expressive contexts, focusing on emotional meanings such as surprise and disapproval. Data from typologically diverse languages are considered. The discussion addresses three main questions: (i) What triggers the use of gesture in expressive utterances? (ii) Are these gestures language-specific or universal? (iii) What is their role within the grammar? We argue that a formal model integrating syntax, prosody, and gestures is required to account for the structural properties of expressive phenomena and to advance a theory of language as a fundamentally multimodal architecture.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1354,MEDIA DISCOURSE AROUND TAXATION IN IRELAND AND THE UK IN THE WAKE OF FINANCIAL CRISIS,"Media representation of tax practices is important because of the impact of public opinion on tax policy. Traditionally viewed as a technical subject and the preserve of professionals, taxation has recently become the focus of widespread and more informed public attention, as a result of, inter alia, tax scandals and the financial crisis. These events, together with wider international tax reform initiatives, provide researchers with an opportunity to explore the impact of tax reform on discourse by the public, as well as by tax experts and professionals. To this end, we analyse the changing treatment of tax-related issues in the mainstream and professional media in Ireland and the United Kingdom (UK) in order to capture expert voices, as well as public discourse. We do so in the immediate and medium-term aftermath of the financial crisis. Our analysis of tax discourse in general, and of the public framing of the term ""tax avoidance"" in particular, in both Ireland and the UK, reveals that a marked change has occurred in the public discourse in Ireland, a country struggling in the aftermath of a severe financial crisis. In contrast, our research finds that there is greater consistency in the UK's mainstream and professional media. Our findings also indicate that expert voices may lag behind public opinion.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1355,Leveraging Intelligent Speech Training to Elevate Phonetic Accuracy and Prosodic Fluency in English Learners,"The successful teaching of pronunciation, as well as prosody, is the significant challenge that still remains to the English as Foreign Learning (EFL) students. Traditional pedagogical theories tend to focus on segmental phoneme accuracy but ignore suprasegmental components (stress or rhythm and intonation) which are natural and intelligible speech components. The currently available systems of computer-assisted pronunciation training (CAPT) are useful, but limited by the fact that they are based on limited acoustic models and incomplete coverage of prosodic characteristics, leading to less than optimal accuracy and limited pedagogical suitability. To overcome these shortcomings, the current paper proposes Attention-Guided Cross-Lingual Self-Supervised Learning (AG-CLSSL), a new model that is both able to combine phoneme-level representations of XLS-R (wav2vec2-large-xlsr-53) and prosodic representations of the pitch, energy, and duration through a Phoneme-Prosody Cross-Attention Fusion (PP-CAF) process. This conglomeration allows the joint and context specific representation of the speech that is further refined by the multi-task Transformer-based scoring model to jointly assess the accuracy of pronunciation, the consistency of the prosody and the general intelligibility. The framework is implemented in Python, with support of PyTorch and Hugging Face Transformers and is trained on an evaluated corpus of EFL learner speech (n=100) with a variety of L1 backgrounds, including Mandarin, Hindi, and Spanish. Experimental assessments indicate significant performance improvement with 55.4% decrease in Phoneme Error rate, 52.0 percent decrease in Word Error rate, 43.3 percent increase in Stress Placement Accuracy and 34.9 percent increase in Pitch Alignment Score. The total acoustic similarity to native speech went up by 36.1, which demonstrates the ability of AG-CLSSL to progress articulatory accuracy as well as the naturalness of prosody and provide interpretable and attention-directed information on scalable AI-based pronunciation and prosody training.",,Yes,The paper investigates lexical tone in relation to intonation or prosodic context.,,,,,
P1356,PARADIGMATIC REPRESENTATION OF COMMON SLAVIC PROSODY,,,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1357,THE EVALUATION AND INTEGRATION OF PITCH HEIGHT AND PITCH CONTOUR IN LEXICAL TONE PERCEPTION IN MANDARIN CHINESE,,,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1358,CONTEXT EFFECTS IN THE PERCEPTION OF LEXICAL TONE,,,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1359,IN PURSUIT OF THE OBJECT,"The author proposes an extension of the transitional phenomenon concept. Begining with Winnicott's well known description, the author reviews the successive transformations of transitional space, first materialized and then increasingly (( intrapsychic )) and which persists in the individual all life long in various forms. Transformations go from the gross blanket to a more figurative little bear, then to play activities, creative drawing of the child, and finally to hypotheses and theories. This progressive hold on the object involves the mediation of transitional space-first found then created-and is instrumented through age-specific different means : concrete handling ; certain characteristics of baby-talk such as voice intonation, use of the past tense before playing and the conditional in playing ; redundancies, parentheses and abstractions. In the course of this paper, the author gives his personal view on the subtle differences between figuration and representation, and on the relationship between abstraction and aesthetical feeling.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1360,SPEECH SYNTHESIS FROM CONCEPT REPRESENTATION IN GENERAL SPEECH OUTPUT INTERFACE,"The goal of this paper is to realize natural and high-quality speech output from various problem-solvers implemented on a computer. The framework for the general speech output interface is discussed, and the basic architecture is designed for SOCS (Speech Output from Case Structure representation). This is the speech synthesis system based on the concept representation converting the input to the interface into speech. The important issue in such a framework is to define a mechanism to transmit information from the problem-solver to the speech output interface. From such a viewpoint, the representation based on the case structure and the phrase patterns is defined as the concept representation to describe the inputs to the interface and the speech synthesis system from the standpoint of problem-solver, speech synthesis, and the dialogue management. The concept inputted to the interface is modified by the dialogue manager and is converted into the speech by SOCS. In SOCS, the prosody as well as the sentence is generated based on the concept representation. The pause markers generated in the sentence generation determine the positions of the pause and the F(phi) resetting. The prosodic patterns also are generated by the prosody modification function described in the custom template.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
P1361,SPEECH-INTELLIGIBILITY IN TONE LANGUAGE (CHINESE) LARYNGECTOMY SPEAKERS,"Tone language speakers use lexical tone or fundamental frequency to signal meaning. Therefore, native tone language alaryngeal speakers encountering difficulty imparting lexical tone variation would suffer loss of speech intelligibility. This study examines the intelligibility of lexical tone produced by four different alaryngeal speech methods, namely: oesophageal speech, electrolarynx, a pneumatic device and tracheo-oesophageal speech. Isolated and embedded monosyllabic Chinese words produced by 53 alaryngeal speakers were presented to three normally hearing young adult listeners with no prior exposure to laryngectomy speech. The listeners transcribed the speech orthographically. Significant differences were found in the intelligibility level between the different speech methods. Listeners' responses were also pooled together and analysed for tone and segmental errors. Errors of tone alone were found to occur more often than segmental errors.",,No,"The paper does not address prosody, lexical tone, or spoken word recognition.",,,,,
P1362,Generating facial expressions for speech,"This article reports results from a program that produces high-quality animation of facial expressions and head movements as automatically as possible in conjunction with meaning-based speech synthesis, including spoken intonation, The goal of the research is as much to test and define our theories of the formal semantics for such gestures, as to produce convincing animation. Towards this end, we have produced a high-level programming language for three-dimensional (3-D) animation of facial expressions, We have been concerned primarily with expressions conveying information correlated with the intonation of the voice: This includes the differences of timing, pitch, and emphasis that are related to such semantic distinctions of discourse as ''focus.'' ''topic,'' and ''comment,'' ''theme'' and ''rheme.'' or ''given'' and ''new'' information, We ore also interested in the relation of affect or emotion to facial expression. Until now, systems have not embodied such rule-governed translation from spoken utterance meaning to facial expressions. Our system embodies rules that describe and coordinate these relations: intonation/information, intonation/effect, and facial expressions/effect. A meaning representation includes discourse information: What is contrastive/background information in the given context, and what is the ''topic'' or ''theme'' of the discourse? The system maps the meaning representation into how accents and their placement ore chosen, how they are conveyed over facial expression, and how speech and facial expressions ore coordinated. This determines a sequence of functional groups: lip shapes, conversational signals, punctuators, regulators, and manipulators. Our algorithms then impose synchrony, create coarticulation effects, end determine affectual signals, eye and head movements. The lowest level representation is the Facial Action Coding System (FACS), which makes the generation system portable to other facial models.",,Maybe,The paper addresses prosody or spoken word recognition in a way potentially transferable to tonal processing.,,,,,
